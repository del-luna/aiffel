{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "going_deeper_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNVdU6LXjDBEg92Lvjzndyr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "75a1d53b3f784fa8902936a6755f5fbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fff7f7f352ba487187c0b9de5d6e344a",
              "IPY_MODEL_ef6dbe71dec74478af1c631b10d678be",
              "IPY_MODEL_04c1943f601741b38ffdda5eb25dbf22"
            ],
            "layout": "IPY_MODEL_82ebceab18664df0ae4e1e7477ee481f"
          }
        },
        "fff7f7f352ba487187c0b9de5d6e344a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c4ffd9d7c32427e8a64833f20a00d89",
            "placeholder": "​",
            "style": "IPY_MODEL_a9f967646de34cf5a1869509cd2a78e1",
            "value": "Dl Completed...: 100%"
          }
        },
        "ef6dbe71dec74478af1c631b10d678be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_568df6a8828d4e858be426559268a650",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_38bc94f06aac4ee6998c789a25fcbc5d",
            "value": 1
          }
        },
        "04c1943f601741b38ffdda5eb25dbf22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02540f718de4492c88583f2a6e5566c0",
            "placeholder": "​",
            "style": "IPY_MODEL_a9fbf7e2af954de48032c7467594de88",
            "value": " 1/1 [00:08&lt;00:00,  8.39s/ url]"
          }
        },
        "82ebceab18664df0ae4e1e7477ee481f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c4ffd9d7c32427e8a64833f20a00d89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9f967646de34cf5a1869509cd2a78e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "568df6a8828d4e858be426559268a650": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "38bc94f06aac4ee6998c789a25fcbc5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "02540f718de4492c88583f2a6e5566c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9fbf7e2af954de48032c7467594de88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85226e5ad62146be9115267e4575127c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_97623d2bb2e14a44aaeda4e3ace7df7a",
              "IPY_MODEL_a0df055fd0344e5a9768eefc452c4249",
              "IPY_MODEL_2dc7d5ae5a8240f9bcb7d8cd5f1c6f02"
            ],
            "layout": "IPY_MODEL_d3a2807101af47818223ae482c0b62d5"
          }
        },
        "97623d2bb2e14a44aaeda4e3ace7df7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5120b4895dde4d17b9b70958cbcccadc",
            "placeholder": "​",
            "style": "IPY_MODEL_a952d2bf6a354545943132012d3a312c",
            "value": "Dl Size...: 100%"
          }
        },
        "a0df055fd0344e5a9768eefc452c4249": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b1e27cf5cf947e9a7d6a8695f82cacd",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9f0ac69adda14d819f82865ff53779e6",
            "value": 1
          }
        },
        "2dc7d5ae5a8240f9bcb7d8cd5f1c6f02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa7481f48dc34324b27998f6c141a20e",
            "placeholder": "​",
            "style": "IPY_MODEL_2e020f1089894cd3ba92c138c8692355",
            "value": " 786/786 [00:08&lt;00:00, 103.39 MiB/s]"
          }
        },
        "d3a2807101af47818223ae482c0b62d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5120b4895dde4d17b9b70958cbcccadc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a952d2bf6a354545943132012d3a312c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b1e27cf5cf947e9a7d6a8695f82cacd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "9f0ac69adda14d819f82865ff53779e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fa7481f48dc34324b27998f6c141a20e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e020f1089894cd3ba92c138c8692355": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22de14b7aec6426ea09c2abc73f026cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_237b1e44db3846299a55432bf94a991c",
              "IPY_MODEL_9732783151ec49c884edf68a437c82d3",
              "IPY_MODEL_756e94762502439bbc792350d4fc05a5"
            ],
            "layout": "IPY_MODEL_939290e5b9d24f7d9bd2eed714e8c09d"
          }
        },
        "237b1e44db3846299a55432bf94a991c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_902050d8ebaf4821b640ae82ce171bff",
            "placeholder": "​",
            "style": "IPY_MODEL_890810b5bfc44ddabaad0c83254863b8",
            "value": ""
          }
        },
        "9732783151ec49c884edf68a437c82d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8db62f7ded0043b7a29e36f46d731df9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b05476e4c69c4eae962fd77f64b75657",
            "value": 1
          }
        },
        "756e94762502439bbc792350d4fc05a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca32eaf6135e40169a7851a79e47091b",
            "placeholder": "​",
            "style": "IPY_MODEL_be3ced166f8c44ef85af0de6157adc66",
            "value": " 23179/0 [00:12&lt;00:00, 1937.37 examples/s]"
          }
        },
        "939290e5b9d24f7d9bd2eed714e8c09d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "902050d8ebaf4821b640ae82ce171bff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "890810b5bfc44ddabaad0c83254863b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8db62f7ded0043b7a29e36f46d731df9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "b05476e4c69c4eae962fd77f64b75657": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ca32eaf6135e40169a7851a79e47091b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be3ced166f8c44ef85af0de6157adc66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9d276c61106458e8cc78b303a858086": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8b64f440c0714096a6d6a6f225bbcb3a",
              "IPY_MODEL_c033935a1c7040d9ba71f6b814b3e3ab",
              "IPY_MODEL_9bd83cf8d7e44e4d85db5a37a1e167bf"
            ],
            "layout": "IPY_MODEL_8a8ce5deac5545b7a8c6c88e1fbe9931"
          }
        },
        "8b64f440c0714096a6d6a6f225bbcb3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_403f4726d5b44512b5fd88f2b7af4813",
            "placeholder": "​",
            "style": "IPY_MODEL_bd8baaee4b664134ad3749f73a5197c5",
            "value": "100%"
          }
        },
        "c033935a1c7040d9ba71f6b814b3e3ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc280a9ae772461ea1469c34a68995ae",
            "max": 23262,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_12d13051296441fe9b7b67ca04451fde",
            "value": 23261
          }
        },
        "9bd83cf8d7e44e4d85db5a37a1e167bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d143d64b494641b18115e5ce69ca68ed",
            "placeholder": "​",
            "style": "IPY_MODEL_3e28ab0963db4b479c04fe4de566e9e3",
            "value": " 23261/23262 [00:01&lt;00:00, 10766.56 examples/s]"
          }
        },
        "8a8ce5deac5545b7a8c6c88e1fbe9931": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "403f4726d5b44512b5fd88f2b7af4813": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd8baaee4b664134ad3749f73a5197c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc280a9ae772461ea1469c34a68995ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12d13051296441fe9b7b67ca04451fde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d143d64b494641b18115e5ce69ca68ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e28ab0963db4b479c04fe4de566e9e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/del-luna/aiffel/blob/main/going_deeper_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fPjcj4bmecqQ"
      },
      "outputs": [],
      "source": [
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(ds_train, ds_test), ds_info = tfds.load(\n",
        "    'cats_vs_dogs',\n",
        "    split = ('train[:80%]', 'train[80%:]'),\n",
        "    with_info = True,\n",
        "    as_supervised = True\n",
        ")"
      ],
      "metadata": {
        "id": "eNsjS6r_ehw5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "75a1d53b3f784fa8902936a6755f5fbc",
            "fff7f7f352ba487187c0b9de5d6e344a",
            "ef6dbe71dec74478af1c631b10d678be",
            "04c1943f601741b38ffdda5eb25dbf22",
            "82ebceab18664df0ae4e1e7477ee481f",
            "9c4ffd9d7c32427e8a64833f20a00d89",
            "a9f967646de34cf5a1869509cd2a78e1",
            "568df6a8828d4e858be426559268a650",
            "38bc94f06aac4ee6998c789a25fcbc5d",
            "02540f718de4492c88583f2a6e5566c0",
            "a9fbf7e2af954de48032c7467594de88",
            "85226e5ad62146be9115267e4575127c",
            "97623d2bb2e14a44aaeda4e3ace7df7a",
            "a0df055fd0344e5a9768eefc452c4249",
            "2dc7d5ae5a8240f9bcb7d8cd5f1c6f02",
            "d3a2807101af47818223ae482c0b62d5",
            "5120b4895dde4d17b9b70958cbcccadc",
            "a952d2bf6a354545943132012d3a312c",
            "3b1e27cf5cf947e9a7d6a8695f82cacd",
            "9f0ac69adda14d819f82865ff53779e6",
            "fa7481f48dc34324b27998f6c141a20e",
            "2e020f1089894cd3ba92c138c8692355",
            "22de14b7aec6426ea09c2abc73f026cd",
            "237b1e44db3846299a55432bf94a991c",
            "9732783151ec49c884edf68a437c82d3",
            "756e94762502439bbc792350d4fc05a5",
            "939290e5b9d24f7d9bd2eed714e8c09d",
            "902050d8ebaf4821b640ae82ce171bff",
            "890810b5bfc44ddabaad0c83254863b8",
            "8db62f7ded0043b7a29e36f46d731df9",
            "b05476e4c69c4eae962fd77f64b75657",
            "ca32eaf6135e40169a7851a79e47091b",
            "be3ced166f8c44ef85af0de6157adc66",
            "b9d276c61106458e8cc78b303a858086",
            "8b64f440c0714096a6d6a6f225bbcb3a",
            "c033935a1c7040d9ba71f6b814b3e3ab",
            "9bd83cf8d7e44e4d85db5a37a1e167bf",
            "8a8ce5deac5545b7a8c6c88e1fbe9931",
            "403f4726d5b44512b5fd88f2b7af4813",
            "bd8baaee4b664134ad3749f73a5197c5",
            "bc280a9ae772461ea1469c34a68995ae",
            "12d13051296441fe9b7b67ca04451fde",
            "d143d64b494641b18115e5ce69ca68ed",
            "3e28ab0963db4b479c04fe4de566e9e3"
          ]
        },
        "outputId": "b25c37f5-8703-4896-96f4-54b217e2921a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDownloading and preparing dataset cats_vs_dogs/4.0.0 (download: 786.68 MiB, generated: Unknown size, total: 786.68 MiB) to /root/tensorflow_datasets/cats_vs_dogs/4.0.0...\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Completed...: 0 url [00:00, ? url/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "75a1d53b3f784fa8902936a6755f5fbc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Size...: 0 MiB [00:00, ? MiB/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85226e5ad62146be9115267e4575127c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22de14b7aec6426ea09c2abc73f026cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:1738 images were corrupted and were skipped\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shuffling and writing examples to /root/tensorflow_datasets/cats_vs_dogs/4.0.0.incompleteQ2FSVB/cats_vs_dogs-train.tfrecord\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/23262 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b9d276c61106458e8cc78b303a858086"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDataset cats_vs_dogs downloaded and prepared to /root/tensorflow_datasets/cats_vs_dogs/4.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_image(image, label):\n",
        "    image = tf.image.resize(image, (SIZE, SIZE))/255.0\n",
        "    return image, label\n",
        "\n",
        "num_examples = ds_info.splits['train'].num_examples\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "SIZE = 224\n",
        "\n",
        "train_batches = ds_train.cache().shuffle(num_examples//4).map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "validation_batches = ds_test.map(format_image).batch(BATCH_SIZE).prefetch(1)"
      ],
      "metadata": {
        "id": "PgPFHvgye9L5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock2(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, filter_num, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = tf.keras.layers.Conv2D(filters=filter_num,\n",
        "                                            kernel_size=(3,3),\n",
        "                                            strides=stride,\n",
        "                                            padding='same')\n",
        "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
        "        self.conv2 = tf.keras.layers.Conv2D(filters=filter_num,\n",
        "                                            kernel_size=(3,3),\n",
        "                                            strides=1,\n",
        "                                            padding='same')\n",
        "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
        "        if stride != 1:\n",
        "            self.shortcut = tf.keras.Sequential()\n",
        "            self.shortcut.add(tf.keras.layers.Conv2D(filters=filter_num,\n",
        "                                                     kernel_size=(1,1),\n",
        "                                                     strides=stride))\n",
        "            self.shortcut.add(tf.keras.layers.BatchNormalization())\n",
        "        else:\n",
        "            self.shortcut = lambda x: x\n",
        "\n",
        "    def call(self, inputs, training=None, **kwargs):\n",
        "        residual = self.shortcut(inputs)\n",
        "\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.bn1(x, training=training)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x, training=training)\n",
        "\n",
        "        output = tf.nn.relu(x)\n",
        "\n",
        "        return output\n",
        "\n",
        "class BottleNeck2(tf.keras.layers.Layer):\n",
        "    def __init__(self, filter_num, stride=1):\n",
        "        super(BottleNeck, self).__init__()\n",
        "        self.conv1 = tf.keras.layers.Conv2D(filters=filter_num,\n",
        "                                            kernel_size=(1, 1),\n",
        "                                            strides=1,\n",
        "                                            padding='same')\n",
        "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
        "        self.conv2 = tf.keras.layers.Conv2D(filters=filter_num,\n",
        "                                            kernel_size=(3, 3),\n",
        "                                            strides=stride,\n",
        "                                            padding='same')\n",
        "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
        "        self.conv3 = tf.keras.layers.Conv2D(filters=filter_num * 4,\n",
        "                                            kernel_size=(1, 1),\n",
        "                                            strides=1,\n",
        "                                            padding='same')\n",
        "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "        self.downsample = tf.keras.Sequential()\n",
        "        self.downsample.add(tf.keras.layers.Conv2D(filters=filter_num * 4,\n",
        "                                                   kernel_size=(1, 1),\n",
        "                                                   strides=stride))\n",
        "        self.downsample.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    def call(self, inputs, training=None, **kwargs):\n",
        "        residual = self.downsample(inputs)\n",
        "\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.bn1(x, training=training)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x, training=training)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x, training=training)\n",
        "\n",
        "        output = tf.nn.relu(x)\n",
        "\n",
        "        return output\n",
        "\n",
        "def make_basic_block_layer2(filter_num, blocks, stride=1):\n",
        "    res_block = tf.keras.Sequential()\n",
        "    res_block.add(BasicBlock(filter_num, stride=stride))\n",
        "\n",
        "    for _ in range(1, blocks):\n",
        "        res_block.add(BasicBlock(filter_num, stride=1))\n",
        "\n",
        "    return res_block\n",
        "\n",
        "\n",
        "def make_bottleneck_layer2(filter_num, blocks, stride=1):\n",
        "    res_block = tf.keras.Sequential()\n",
        "    res_block.add(BottleNeck(filter_num, stride=stride))\n",
        "\n",
        "    for _ in range(1, blocks):\n",
        "        res_block.add(BottleNeck(filter_num, stride=1))\n",
        "\n",
        "    return res_block"
      ],
      "metadata": {
        "id": "fIoy_QSf8-m9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetTypeIII(tf.keras.Model):\n",
        "    def __init__(self, layer_params):\n",
        "        super(ResNetTypeI, self).__init__()\n",
        "\n",
        "        self.conv1 = tf.keras.layers.Conv2D(filters=64,\n",
        "                                            kernel_size=(7, 7),\n",
        "                                            strides=2,\n",
        "                                            padding=\"same\")\n",
        "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
        "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=(3, 3),\n",
        "                                               strides=2,\n",
        "                                               padding=\"same\")\n",
        "\n",
        "        self.layer1 = make_basic_block_layer2(filter_num=64,\n",
        "                                             blocks=layer_params[0])\n",
        "        self.layer2 = make_basic_block_layer2(filter_num=128,\n",
        "                                             blocks=layer_params[1],\n",
        "                                             stride=2)\n",
        "        self.layer3 = make_basic_block_layer2(filter_num=256,\n",
        "                                             blocks=layer_params[2],\n",
        "                                             stride=2)\n",
        "        self.layer4 = make_basic_block_layer2(filter_num=512,\n",
        "                                             blocks=layer_params[3],\n",
        "                                             stride=2)\n",
        "\n",
        "        self.avgpool = tf.keras.layers.GlobalAveragePooling2D()\n",
        "        self.fc = tf.keras.layers.Dense(units=2, activation=tf.keras.activations.softmax)\n",
        "\n",
        "    def call(self, inputs, training=None, mask=None):\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.bn1(x, training=training)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.layer1(x, training=training)\n",
        "        x = self.layer2(x, training=training)\n",
        "        x = self.layer3(x, training=training)\n",
        "        x = self.layer4(x, training=training)\n",
        "        x = self.avgpool(x)\n",
        "        output = self.fc(x)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class ResNetTypeIV(tf.keras.Model):\n",
        "    def __init__(self, layer_params):\n",
        "        super(ResNetTypeII, self).__init__()\n",
        "        self.conv1 = tf.keras.layers.Conv2D(filters=64,\n",
        "                                            kernel_size=(7, 7),\n",
        "                                            strides=2,\n",
        "                                            padding=\"same\")\n",
        "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
        "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=(3, 3),\n",
        "                                               strides=2,\n",
        "                                               padding=\"same\")\n",
        "\n",
        "        self.layer1 = make_bottleneck_layer2(filter_num=64,\n",
        "                                            blocks=layer_params[0])\n",
        "        self.layer2 = make_bottleneck_layer2(filter_num=128,\n",
        "                                            blocks=layer_params[1],\n",
        "                                            stride=2)\n",
        "        self.layer3 = make_bottleneck_layer2(filter_num=256,\n",
        "                                            blocks=layer_params[2],\n",
        "                                            stride=2)\n",
        "        self.layer4 = make_bottleneck_layer2(filter_num=512,\n",
        "                                            blocks=layer_params[3],\n",
        "                                            stride=2)\n",
        "\n",
        "        self.avgpool = tf.keras.layers.GlobalAveragePooling2D()\n",
        "        self.fc = tf.keras.layers.Dense(units=2, activation=tf.keras.activations.softmax)\n",
        "\n",
        "    def call(self, inputs, training=None, mask=None):\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.bn1(x, training=training)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.layer1(x, training=training)\n",
        "        x = self.layer2(x, training=training)\n",
        "        x = self.layer3(x, training=training)\n",
        "        x = self.layer4(x, training=training)\n",
        "        x = self.avgpool(x)\n",
        "        output = self.fc(x)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "def resnet_18_2():\n",
        "    return ResNetTypeIII(layer_params=[2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def resnet_34_2():\n",
        "    return ResNetTypeIII(layer_params=[3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def resnet_50_2():\n",
        "    return ResNetTypeIV(layer_params=[3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def resnet_101_2():\n",
        "    return ResNetTypeIV(layer_params=[3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def resnet_152_2():\n",
        "    return ResNetTypeIV(layer_params=[3, 8, 36, 3])"
      ],
      "metadata": {
        "id": "Jjj_l6CS90zb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, filter_num, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = tf.keras.layers.Conv2D(filters=filter_num,\n",
        "                                            kernel_size=(3,3),\n",
        "                                            strides=stride,\n",
        "                                            padding='same')\n",
        "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
        "        self.conv2 = tf.keras.layers.Conv2D(filters=filter_num,\n",
        "                                            kernel_size=(3,3),\n",
        "                                            strides=1,\n",
        "                                            padding='same')\n",
        "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
        "        if stride != 1:\n",
        "            self.shortcut = tf.keras.Sequential()\n",
        "            self.shortcut.add(tf.keras.layers.Conv2D(filters=filter_num,\n",
        "                                                     kernel_size=(1,1),\n",
        "                                                     strides=stride))\n",
        "            self.shortcut.add(tf.keras.layers.BatchNormalization())\n",
        "        else:\n",
        "            self.shortcut = lambda x: x\n",
        "\n",
        "    def call(self, inputs, training=None, **kwargs):\n",
        "        residual = self.shortcut(inputs)\n",
        "\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.bn1(x, training=training)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x, training=training)\n",
        "\n",
        "        output = tf.nn.relu(tf.keras.layers.add([residual, x]))\n",
        "\n",
        "        return output\n",
        "\n",
        "class BottleNeck(tf.keras.layers.Layer):\n",
        "    def __init__(self, filter_num, stride=1):\n",
        "        super(BottleNeck, self).__init__()\n",
        "        self.conv1 = tf.keras.layers.Conv2D(filters=filter_num,\n",
        "                                            kernel_size=(1, 1),\n",
        "                                            strides=1,\n",
        "                                            padding='same')\n",
        "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
        "        self.conv2 = tf.keras.layers.Conv2D(filters=filter_num,\n",
        "                                            kernel_size=(3, 3),\n",
        "                                            strides=stride,\n",
        "                                            padding='same')\n",
        "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
        "        self.conv3 = tf.keras.layers.Conv2D(filters=filter_num * 4,\n",
        "                                            kernel_size=(1, 1),\n",
        "                                            strides=1,\n",
        "                                            padding='same')\n",
        "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "        self.downsample = tf.keras.Sequential()\n",
        "        self.downsample.add(tf.keras.layers.Conv2D(filters=filter_num * 4,\n",
        "                                                   kernel_size=(1, 1),\n",
        "                                                   strides=stride))\n",
        "        self.downsample.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    def call(self, inputs, training=None, **kwargs):\n",
        "        residual = self.downsample(inputs)\n",
        "\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.bn1(x, training=training)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x, training=training)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x, training=training)\n",
        "\n",
        "        output = tf.nn.relu(tf.keras.layers.add([residual, x]))\n",
        "\n",
        "        return output\n",
        "\n",
        "def make_basic_block_layer(filter_num, blocks, stride=1):\n",
        "    res_block = tf.keras.Sequential()\n",
        "    res_block.add(BasicBlock(filter_num, stride=stride))\n",
        "\n",
        "    for _ in range(1, blocks):\n",
        "        res_block.add(BasicBlock(filter_num, stride=1))\n",
        "\n",
        "    return res_block\n",
        "\n",
        "\n",
        "def make_bottleneck_layer(filter_num, blocks, stride=1):\n",
        "    res_block = tf.keras.Sequential()\n",
        "    res_block.add(BottleNeck(filter_num, stride=stride))\n",
        "\n",
        "    for _ in range(1, blocks):\n",
        "        res_block.add(BottleNeck(filter_num, stride=1))\n",
        "\n",
        "    return res_block"
      ],
      "metadata": {
        "id": "YOiNB_oUgnGf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetTypeI(tf.keras.Model):\n",
        "    def __init__(self, layer_params):\n",
        "        super(ResNetTypeI, self).__init__()\n",
        "\n",
        "        self.conv1 = tf.keras.layers.Conv2D(filters=64,\n",
        "                                            kernel_size=(7, 7),\n",
        "                                            strides=2,\n",
        "                                            padding=\"same\")\n",
        "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
        "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=(3, 3),\n",
        "                                               strides=2,\n",
        "                                               padding=\"same\")\n",
        "\n",
        "        self.layer1 = make_basic_block_layer(filter_num=64,\n",
        "                                             blocks=layer_params[0])\n",
        "        self.layer2 = make_basic_block_layer(filter_num=128,\n",
        "                                             blocks=layer_params[1],\n",
        "                                             stride=2)\n",
        "        self.layer3 = make_basic_block_layer(filter_num=256,\n",
        "                                             blocks=layer_params[2],\n",
        "                                             stride=2)\n",
        "        self.layer4 = make_basic_block_layer(filter_num=512,\n",
        "                                             blocks=layer_params[3],\n",
        "                                             stride=2)\n",
        "\n",
        "        self.avgpool = tf.keras.layers.GlobalAveragePooling2D()\n",
        "        self.fc = tf.keras.layers.Dense(units=2, activation=tf.keras.activations.softmax)\n",
        "\n",
        "    def call(self, inputs, training=None, mask=None):\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.bn1(x, training=training)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.layer1(x, training=training)\n",
        "        x = self.layer2(x, training=training)\n",
        "        x = self.layer3(x, training=training)\n",
        "        x = self.layer4(x, training=training)\n",
        "        x = self.avgpool(x)\n",
        "        output = self.fc(x)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class ResNetTypeII(tf.keras.Model):\n",
        "    def __init__(self, layer_params):\n",
        "        super(ResNetTypeII, self).__init__()\n",
        "        self.conv1 = tf.keras.layers.Conv2D(filters=64,\n",
        "                                            kernel_size=(7, 7),\n",
        "                                            strides=2,\n",
        "                                            padding=\"same\")\n",
        "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
        "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=(3, 3),\n",
        "                                               strides=2,\n",
        "                                               padding=\"same\")\n",
        "\n",
        "        self.layer1 = make_bottleneck_layer(filter_num=64,\n",
        "                                            blocks=layer_params[0])\n",
        "        self.layer2 = make_bottleneck_layer(filter_num=128,\n",
        "                                            blocks=layer_params[1],\n",
        "                                            stride=2)\n",
        "        self.layer3 = make_bottleneck_layer(filter_num=256,\n",
        "                                            blocks=layer_params[2],\n",
        "                                            stride=2)\n",
        "        self.layer4 = make_bottleneck_layer(filter_num=512,\n",
        "                                            blocks=layer_params[3],\n",
        "                                            stride=2)\n",
        "\n",
        "        self.avgpool = tf.keras.layers.GlobalAveragePooling2D()\n",
        "        self.fc = tf.keras.layers.Dense(units=2, activation=tf.keras.activations.softmax)\n",
        "\n",
        "    def call(self, inputs, training=None, mask=None):\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.bn1(x, training=training)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.layer1(x, training=training)\n",
        "        x = self.layer2(x, training=training)\n",
        "        x = self.layer3(x, training=training)\n",
        "        x = self.layer4(x, training=training)\n",
        "        x = self.avgpool(x)\n",
        "        output = self.fc(x)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "def resnet_18():\n",
        "    return ResNetTypeI(layer_params=[2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def resnet_34():\n",
        "    return ResNetTypeI(layer_params=[3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def resnet_50():\n",
        "    return ResNetTypeII(layer_params=[3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def resnet_101():\n",
        "    return ResNetTypeII(layer_params=[3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def resnet_152():\n",
        "    return ResNetTypeII(layer_params=[3, 8, 36, 3])"
      ],
      "metadata": {
        "id": "NmMWnU2Gl6RJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(type='resnet34'):\n",
        "    if type == 'resenet50':\n",
        "        model = resnet_50()\n",
        "    else:\n",
        "        model = resnet_34()\n",
        "    model.build(input_shape=(None, 224, 224, 3))\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "model = get_model()"
      ],
      "metadata": {
        "id": "EaSXiJPamHm8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e210f7d3-1ad6-483d-bd7d-2cd94769d595"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"res_net_type_i\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             multiple                  9472      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  multiple                 256       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  multiple                 0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " sequential (Sequential)     (None, 56, 56, 64)        223104    \n",
            "                                                                 \n",
            " sequential_1 (Sequential)   (None, 28, 28, 128)       1119872   \n",
            "                                                                 \n",
            " sequential_3 (Sequential)   (None, 14, 14, 256)       6832384   \n",
            "                                                                 \n",
            " sequential_5 (Sequential)   (None, 7, 7, 512)         13125120  \n",
            "                                                                 \n",
            " global_average_pooling2d (G  multiple                 0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  1026      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,311,234\n",
            "Trainable params: 21,294,210\n",
            "Non-trainable params: 17,024\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adadelta()\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "valid_loss = tf.keras.metrics.Mean(name='valid_loss')\n",
        "valid_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='valid_accuracy')"
      ],
      "metadata": {
        "id": "egNcqYDZnUXk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(images, training=True)\n",
        "        loss = loss_object(y_true=labels, y_pred=predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(grads_and_vars=zip(gradients, model.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(labels, predictions)\n",
        "\n",
        "@tf.function\n",
        "def valid_step(images, labels):\n",
        "    predictions = model(images, training=False)\n",
        "    v_loss = loss_object(labels, predictions)\n",
        "\n",
        "    valid_loss(v_loss)\n",
        "    valid_accuracy(labels, predictions)\n",
        "\n",
        "# start training\n",
        "for epoch in range(30):\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    valid_loss.reset_states()\n",
        "    valid_accuracy.reset_states()\n",
        "    step = 0\n",
        "    for images, labels in train_batches:\n",
        "        step += 1\n",
        "        train_step(images, labels)\n",
        "        print(\"Epoch: {}/{}, step: {}/{}, loss: {:.5f}, accuracy: {:.5f}\".format(epoch + 1,\n",
        "                                                                                    30,\n",
        "                                                                                    step,\n",
        "                                                                                    math.ceil(num_examples / 64),\n",
        "                                                                                    train_loss.result(),\n",
        "                                                                                    train_accuracy.result()))\n",
        "\n",
        "    for valid_images, valid_labels in validation_batches:\n",
        "        valid_step(valid_images, valid_labels)\n",
        "\n",
        "    print(\"Epoch: {}/{}, train loss: {:.5f}, train accuracy: {:.5f}, \"\n",
        "            \"valid loss: {:.5f}, valid accuracy: {:.5f}\".format(epoch + 1,\n",
        "                                                                30,\n",
        "                                                                train_loss.result(),\n",
        "                                                                train_accuracy.result(),\n",
        "                                                                valid_loss.result(),\n",
        "                                                                valid_accuracy.result()))"
      ],
      "metadata": {
        "id": "GbqKpq6hobZc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a34bea21-5293-4167-e0c7-59a002a4579f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 21/30, step: 99/364, loss: 0.33342, accuracy: 0.86253\n",
            "Epoch: 21/30, step: 100/364, loss: 0.33380, accuracy: 0.86203\n",
            "Epoch: 21/30, step: 101/364, loss: 0.33355, accuracy: 0.86200\n",
            "Epoch: 21/30, step: 102/364, loss: 0.33278, accuracy: 0.86259\n",
            "Epoch: 21/30, step: 103/364, loss: 0.33211, accuracy: 0.86317\n",
            "Epoch: 21/30, step: 104/364, loss: 0.33142, accuracy: 0.86343\n",
            "Epoch: 21/30, step: 105/364, loss: 0.33224, accuracy: 0.86250\n",
            "Epoch: 21/30, step: 106/364, loss: 0.33200, accuracy: 0.86232\n",
            "Epoch: 21/30, step: 107/364, loss: 0.33175, accuracy: 0.86244\n",
            "Epoch: 21/30, step: 108/364, loss: 0.33143, accuracy: 0.86299\n",
            "Epoch: 21/30, step: 109/364, loss: 0.33262, accuracy: 0.86282\n",
            "Epoch: 21/30, step: 110/364, loss: 0.33270, accuracy: 0.86264\n",
            "Epoch: 21/30, step: 111/364, loss: 0.33228, accuracy: 0.86318\n",
            "Epoch: 21/30, step: 112/364, loss: 0.33188, accuracy: 0.86356\n",
            "Epoch: 21/30, step: 113/364, loss: 0.33160, accuracy: 0.86352\n",
            "Epoch: 21/30, step: 114/364, loss: 0.33148, accuracy: 0.86376\n",
            "Epoch: 21/30, step: 115/364, loss: 0.33157, accuracy: 0.86386\n",
            "Epoch: 21/30, step: 116/364, loss: 0.33112, accuracy: 0.86422\n",
            "Epoch: 21/30, step: 117/364, loss: 0.33082, accuracy: 0.86445\n",
            "Epoch: 21/30, step: 118/364, loss: 0.33250, accuracy: 0.86335\n",
            "Epoch: 21/30, step: 119/364, loss: 0.33293, accuracy: 0.86305\n",
            "Epoch: 21/30, step: 120/364, loss: 0.33247, accuracy: 0.86341\n",
            "Epoch: 21/30, step: 121/364, loss: 0.33214, accuracy: 0.86364\n",
            "Epoch: 21/30, step: 122/364, loss: 0.33245, accuracy: 0.86335\n",
            "Epoch: 21/30, step: 123/364, loss: 0.33257, accuracy: 0.86319\n",
            "Epoch: 21/30, step: 124/364, loss: 0.33256, accuracy: 0.86278\n",
            "Epoch: 21/30, step: 125/364, loss: 0.33293, accuracy: 0.86225\n",
            "Epoch: 21/30, step: 126/364, loss: 0.33228, accuracy: 0.86285\n",
            "Epoch: 21/30, step: 127/364, loss: 0.33174, accuracy: 0.86344\n",
            "Epoch: 21/30, step: 128/364, loss: 0.33263, accuracy: 0.86267\n",
            "Epoch: 21/30, step: 129/364, loss: 0.33212, accuracy: 0.86325\n",
            "Epoch: 21/30, step: 130/364, loss: 0.33207, accuracy: 0.86346\n",
            "Epoch: 21/30, step: 131/364, loss: 0.33141, accuracy: 0.86379\n",
            "Epoch: 21/30, step: 132/364, loss: 0.33135, accuracy: 0.86399\n",
            "Epoch: 21/30, step: 133/364, loss: 0.33164, accuracy: 0.86407\n",
            "Epoch: 21/30, step: 134/364, loss: 0.33188, accuracy: 0.86451\n",
            "Epoch: 21/30, step: 135/364, loss: 0.33217, accuracy: 0.86435\n",
            "Epoch: 21/30, step: 136/364, loss: 0.33225, accuracy: 0.86420\n",
            "Epoch: 21/30, step: 137/364, loss: 0.33218, accuracy: 0.86439\n",
            "Epoch: 21/30, step: 138/364, loss: 0.33198, accuracy: 0.86470\n",
            "Epoch: 21/30, step: 139/364, loss: 0.33183, accuracy: 0.86488\n",
            "Epoch: 21/30, step: 140/364, loss: 0.33186, accuracy: 0.86484\n",
            "Epoch: 21/30, step: 141/364, loss: 0.33148, accuracy: 0.86492\n",
            "Epoch: 21/30, step: 142/364, loss: 0.33129, accuracy: 0.86488\n",
            "Epoch: 21/30, step: 143/364, loss: 0.33128, accuracy: 0.86506\n",
            "Epoch: 21/30, step: 144/364, loss: 0.33098, accuracy: 0.86502\n",
            "Epoch: 21/30, step: 145/364, loss: 0.33013, accuracy: 0.86573\n",
            "Epoch: 21/30, step: 146/364, loss: 0.33050, accuracy: 0.86548\n",
            "Epoch: 21/30, step: 147/364, loss: 0.33049, accuracy: 0.86554\n",
            "Epoch: 21/30, step: 148/364, loss: 0.33128, accuracy: 0.86497\n",
            "Epoch: 21/30, step: 149/364, loss: 0.33105, accuracy: 0.86514\n",
            "Epoch: 21/30, step: 150/364, loss: 0.33036, accuracy: 0.86531\n",
            "Epoch: 21/30, step: 151/364, loss: 0.33130, accuracy: 0.86476\n",
            "Epoch: 21/30, step: 152/364, loss: 0.33199, accuracy: 0.86441\n",
            "Epoch: 21/30, step: 153/364, loss: 0.33176, accuracy: 0.86428\n",
            "Epoch: 21/30, step: 154/364, loss: 0.33156, accuracy: 0.86435\n",
            "Epoch: 21/30, step: 155/364, loss: 0.33187, accuracy: 0.86431\n",
            "Epoch: 21/30, step: 156/364, loss: 0.33263, accuracy: 0.86368\n",
            "Epoch: 21/30, step: 157/364, loss: 0.33283, accuracy: 0.86365\n",
            "Epoch: 21/30, step: 158/364, loss: 0.33274, accuracy: 0.86363\n",
            "Epoch: 21/30, step: 159/364, loss: 0.33235, accuracy: 0.86370\n",
            "Epoch: 21/30, step: 160/364, loss: 0.33215, accuracy: 0.86377\n",
            "Epoch: 21/30, step: 161/364, loss: 0.33220, accuracy: 0.86384\n",
            "Epoch: 21/30, step: 162/364, loss: 0.33205, accuracy: 0.86400\n",
            "Epoch: 21/30, step: 163/364, loss: 0.33348, accuracy: 0.86321\n",
            "Epoch: 21/30, step: 164/364, loss: 0.33330, accuracy: 0.86319\n",
            "Epoch: 21/30, step: 165/364, loss: 0.33315, accuracy: 0.86307\n",
            "Epoch: 21/30, step: 166/364, loss: 0.33328, accuracy: 0.86295\n",
            "Epoch: 21/30, step: 167/364, loss: 0.33284, accuracy: 0.86302\n",
            "Epoch: 21/30, step: 168/364, loss: 0.33297, accuracy: 0.86272\n",
            "Epoch: 21/30, step: 169/364, loss: 0.33284, accuracy: 0.86280\n",
            "Epoch: 21/30, step: 170/364, loss: 0.33237, accuracy: 0.86305\n",
            "Epoch: 21/30, step: 171/364, loss: 0.33163, accuracy: 0.86349\n",
            "Epoch: 21/30, step: 172/364, loss: 0.33134, accuracy: 0.86383\n",
            "Epoch: 21/30, step: 173/364, loss: 0.33210, accuracy: 0.86335\n",
            "Epoch: 21/30, step: 174/364, loss: 0.33184, accuracy: 0.86351\n",
            "Epoch: 21/30, step: 175/364, loss: 0.33144, accuracy: 0.86393\n",
            "Epoch: 21/30, step: 176/364, loss: 0.33160, accuracy: 0.86381\n",
            "Epoch: 21/30, step: 177/364, loss: 0.33116, accuracy: 0.86405\n",
            "Epoch: 21/30, step: 178/364, loss: 0.33086, accuracy: 0.86438\n",
            "Epoch: 21/30, step: 179/364, loss: 0.33104, accuracy: 0.86426\n",
            "Epoch: 21/30, step: 180/364, loss: 0.33099, accuracy: 0.86450\n",
            "Epoch: 21/30, step: 181/364, loss: 0.33079, accuracy: 0.86464\n",
            "Epoch: 21/30, step: 182/364, loss: 0.33117, accuracy: 0.86444\n",
            "Epoch: 21/30, step: 183/364, loss: 0.33093, accuracy: 0.86467\n",
            "Epoch: 21/30, step: 184/364, loss: 0.33063, accuracy: 0.86506\n",
            "Epoch: 21/30, step: 185/364, loss: 0.33044, accuracy: 0.86520\n",
            "Epoch: 21/30, step: 186/364, loss: 0.33018, accuracy: 0.86526\n",
            "Epoch: 21/30, step: 187/364, loss: 0.33001, accuracy: 0.86522\n",
            "Epoch: 21/30, step: 188/364, loss: 0.32988, accuracy: 0.86503\n",
            "Epoch: 21/30, step: 189/364, loss: 0.32950, accuracy: 0.86508\n",
            "Epoch: 21/30, step: 190/364, loss: 0.32925, accuracy: 0.86513\n",
            "Epoch: 21/30, step: 191/364, loss: 0.32939, accuracy: 0.86510\n",
            "Epoch: 21/30, step: 192/364, loss: 0.32977, accuracy: 0.86499\n",
            "Epoch: 21/30, step: 193/364, loss: 0.32981, accuracy: 0.86488\n",
            "Epoch: 21/30, step: 194/364, loss: 0.32969, accuracy: 0.86501\n",
            "Epoch: 21/30, step: 195/364, loss: 0.32946, accuracy: 0.86522\n",
            "Epoch: 21/30, step: 196/364, loss: 0.32875, accuracy: 0.86551\n",
            "Epoch: 21/30, step: 197/364, loss: 0.32862, accuracy: 0.86556\n",
            "Epoch: 21/30, step: 198/364, loss: 0.32830, accuracy: 0.86592\n",
            "Epoch: 21/30, step: 199/364, loss: 0.32852, accuracy: 0.86581\n",
            "Epoch: 21/30, step: 200/364, loss: 0.32807, accuracy: 0.86609\n",
            "Epoch: 21/30, step: 201/364, loss: 0.32849, accuracy: 0.86583\n",
            "Epoch: 21/30, step: 202/364, loss: 0.32851, accuracy: 0.86580\n",
            "Epoch: 21/30, step: 203/364, loss: 0.32836, accuracy: 0.86569\n",
            "Epoch: 21/30, step: 204/364, loss: 0.32862, accuracy: 0.86535\n",
            "Epoch: 21/30, step: 205/364, loss: 0.32818, accuracy: 0.86555\n",
            "Epoch: 21/30, step: 206/364, loss: 0.32779, accuracy: 0.86590\n",
            "Epoch: 21/30, step: 207/364, loss: 0.32760, accuracy: 0.86594\n",
            "Epoch: 21/30, step: 208/364, loss: 0.32759, accuracy: 0.86606\n",
            "Epoch: 21/30, step: 209/364, loss: 0.32795, accuracy: 0.86595\n",
            "Epoch: 21/30, step: 210/364, loss: 0.32773, accuracy: 0.86607\n",
            "Epoch: 21/30, step: 211/364, loss: 0.32815, accuracy: 0.86560\n",
            "Epoch: 21/30, step: 212/364, loss: 0.32762, accuracy: 0.86586\n",
            "Epoch: 21/30, step: 213/364, loss: 0.32827, accuracy: 0.86546\n",
            "Epoch: 21/30, step: 214/364, loss: 0.32866, accuracy: 0.86536\n",
            "Epoch: 21/30, step: 215/364, loss: 0.32869, accuracy: 0.86533\n",
            "Epoch: 21/30, step: 216/364, loss: 0.32859, accuracy: 0.86552\n",
            "Epoch: 21/30, step: 217/364, loss: 0.32811, accuracy: 0.86593\n",
            "Epoch: 21/30, step: 218/364, loss: 0.32873, accuracy: 0.86540\n",
            "Epoch: 21/30, step: 219/364, loss: 0.32918, accuracy: 0.86508\n",
            "Epoch: 21/30, step: 220/364, loss: 0.32908, accuracy: 0.86527\n",
            "Epoch: 21/30, step: 221/364, loss: 0.32926, accuracy: 0.86524\n",
            "Epoch: 21/30, step: 222/364, loss: 0.32893, accuracy: 0.86543\n",
            "Epoch: 21/30, step: 223/364, loss: 0.32870, accuracy: 0.86554\n",
            "Epoch: 21/30, step: 224/364, loss: 0.32887, accuracy: 0.86544\n",
            "Epoch: 21/30, step: 225/364, loss: 0.32912, accuracy: 0.86535\n",
            "Epoch: 21/30, step: 226/364, loss: 0.32907, accuracy: 0.86539\n",
            "Epoch: 21/30, step: 227/364, loss: 0.32904, accuracy: 0.86543\n",
            "Epoch: 21/30, step: 228/364, loss: 0.32889, accuracy: 0.86527\n",
            "Epoch: 21/30, step: 229/364, loss: 0.32878, accuracy: 0.86538\n",
            "Epoch: 21/30, step: 230/364, loss: 0.32907, accuracy: 0.86501\n",
            "Epoch: 21/30, step: 231/364, loss: 0.32880, accuracy: 0.86519\n",
            "Epoch: 21/30, step: 232/364, loss: 0.32840, accuracy: 0.86564\n",
            "Epoch: 21/30, step: 233/364, loss: 0.32846, accuracy: 0.86534\n",
            "Epoch: 21/30, step: 234/364, loss: 0.32841, accuracy: 0.86552\n",
            "Epoch: 21/30, step: 235/364, loss: 0.32833, accuracy: 0.86569\n",
            "Epoch: 21/30, step: 236/364, loss: 0.32836, accuracy: 0.86566\n",
            "Epoch: 21/30, step: 237/364, loss: 0.32809, accuracy: 0.86564\n",
            "Epoch: 21/30, step: 238/364, loss: 0.32779, accuracy: 0.86587\n",
            "Epoch: 21/30, step: 239/364, loss: 0.32790, accuracy: 0.86585\n",
            "Epoch: 21/30, step: 240/364, loss: 0.32791, accuracy: 0.86589\n",
            "Epoch: 21/30, step: 241/364, loss: 0.32776, accuracy: 0.86592\n",
            "Epoch: 21/30, step: 242/364, loss: 0.32767, accuracy: 0.86609\n",
            "Epoch: 21/30, step: 243/364, loss: 0.32813, accuracy: 0.86587\n",
            "Epoch: 21/30, step: 244/364, loss: 0.32792, accuracy: 0.86603\n",
            "Epoch: 21/30, step: 245/364, loss: 0.32806, accuracy: 0.86582\n",
            "Epoch: 21/30, step: 246/364, loss: 0.32798, accuracy: 0.86585\n",
            "Epoch: 21/30, step: 247/364, loss: 0.32817, accuracy: 0.86570\n",
            "Epoch: 21/30, step: 248/364, loss: 0.32752, accuracy: 0.86618\n",
            "Epoch: 21/30, step: 249/364, loss: 0.32763, accuracy: 0.86603\n",
            "Epoch: 21/30, step: 250/364, loss: 0.32759, accuracy: 0.86600\n",
            "Epoch: 21/30, step: 251/364, loss: 0.32822, accuracy: 0.86579\n",
            "Epoch: 21/30, step: 252/364, loss: 0.32782, accuracy: 0.86601\n",
            "Epoch: 21/30, step: 253/364, loss: 0.32777, accuracy: 0.86580\n",
            "Epoch: 21/30, step: 254/364, loss: 0.32773, accuracy: 0.86596\n",
            "Epoch: 21/30, step: 255/364, loss: 0.32739, accuracy: 0.86618\n",
            "Epoch: 21/30, step: 256/364, loss: 0.32807, accuracy: 0.86572\n",
            "Epoch: 21/30, step: 257/364, loss: 0.32783, accuracy: 0.86570\n",
            "Epoch: 21/30, step: 258/364, loss: 0.32761, accuracy: 0.86598\n",
            "Epoch: 21/30, step: 259/364, loss: 0.32757, accuracy: 0.86583\n",
            "Epoch: 21/30, step: 260/364, loss: 0.32740, accuracy: 0.86587\n",
            "Epoch: 21/30, step: 261/364, loss: 0.32745, accuracy: 0.86560\n",
            "Epoch: 21/30, step: 262/364, loss: 0.32795, accuracy: 0.86528\n",
            "Epoch: 21/30, step: 263/364, loss: 0.32777, accuracy: 0.86555\n",
            "Epoch: 21/30, step: 264/364, loss: 0.32779, accuracy: 0.86553\n",
            "Epoch: 21/30, step: 265/364, loss: 0.32766, accuracy: 0.86568\n",
            "Epoch: 21/30, step: 266/364, loss: 0.32741, accuracy: 0.86578\n",
            "Epoch: 21/30, step: 267/364, loss: 0.32763, accuracy: 0.86552\n",
            "Epoch: 21/30, step: 268/364, loss: 0.32774, accuracy: 0.86544\n",
            "Epoch: 21/30, step: 269/364, loss: 0.32781, accuracy: 0.86542\n",
            "Epoch: 21/30, step: 270/364, loss: 0.32742, accuracy: 0.86563\n",
            "Epoch: 21/30, step: 271/364, loss: 0.32754, accuracy: 0.86554\n",
            "Epoch: 21/30, step: 272/364, loss: 0.32749, accuracy: 0.86564\n",
            "Epoch: 21/30, step: 273/364, loss: 0.32756, accuracy: 0.86556\n",
            "Epoch: 21/30, step: 274/364, loss: 0.32750, accuracy: 0.86565\n",
            "Epoch: 21/30, step: 275/364, loss: 0.32739, accuracy: 0.86574\n",
            "Epoch: 21/30, step: 276/364, loss: 0.32735, accuracy: 0.86572\n",
            "Epoch: 21/30, step: 277/364, loss: 0.32729, accuracy: 0.86581\n",
            "Epoch: 21/30, step: 278/364, loss: 0.32738, accuracy: 0.86573\n",
            "Epoch: 21/30, step: 279/364, loss: 0.32703, accuracy: 0.86610\n",
            "Epoch: 21/30, step: 280/364, loss: 0.32704, accuracy: 0.86613\n",
            "Epoch: 21/30, step: 281/364, loss: 0.32677, accuracy: 0.86644\n",
            "Epoch: 21/30, step: 282/364, loss: 0.32703, accuracy: 0.86625\n",
            "Epoch: 21/30, step: 283/364, loss: 0.32697, accuracy: 0.86628\n",
            "Epoch: 21/30, step: 284/364, loss: 0.32670, accuracy: 0.86647\n",
            "Epoch: 21/30, step: 285/364, loss: 0.32689, accuracy: 0.86612\n",
            "Epoch: 21/30, step: 286/364, loss: 0.32671, accuracy: 0.86637\n",
            "Epoch: 21/30, step: 287/364, loss: 0.32676, accuracy: 0.86623\n",
            "Epoch: 21/30, step: 288/364, loss: 0.32710, accuracy: 0.86610\n",
            "Epoch: 21/30, step: 289/364, loss: 0.32720, accuracy: 0.86603\n",
            "Epoch: 21/30, step: 290/364, loss: 0.32689, accuracy: 0.86627\n",
            "Epoch: 21/30, step: 291/364, loss: 0.32683, accuracy: 0.86631\n",
            "Epoch: 21/30, train loss: 0.32683, train accuracy: 0.86631, valid loss: 0.63050, valid accuracy: 0.69841\n",
            "Epoch: 22/30, step: 1/364, loss: 0.29811, accuracy: 0.85938\n",
            "Epoch: 22/30, step: 2/364, loss: 0.26751, accuracy: 0.89062\n",
            "Epoch: 22/30, step: 3/364, loss: 0.28687, accuracy: 0.86979\n",
            "Epoch: 22/30, step: 4/364, loss: 0.30975, accuracy: 0.85938\n",
            "Epoch: 22/30, step: 5/364, loss: 0.28255, accuracy: 0.88437\n",
            "Epoch: 22/30, step: 6/364, loss: 0.28237, accuracy: 0.89062\n",
            "Epoch: 22/30, step: 7/364, loss: 0.28151, accuracy: 0.88839\n",
            "Epoch: 22/30, step: 8/364, loss: 0.28849, accuracy: 0.88281\n",
            "Epoch: 22/30, step: 9/364, loss: 0.29978, accuracy: 0.87326\n",
            "Epoch: 22/30, step: 10/364, loss: 0.29769, accuracy: 0.87344\n",
            "Epoch: 22/30, step: 11/364, loss: 0.30026, accuracy: 0.86932\n",
            "Epoch: 22/30, step: 12/364, loss: 0.29333, accuracy: 0.87630\n",
            "Epoch: 22/30, step: 13/364, loss: 0.30673, accuracy: 0.86779\n",
            "Epoch: 22/30, step: 14/364, loss: 0.30450, accuracy: 0.87165\n",
            "Epoch: 22/30, step: 15/364, loss: 0.30451, accuracy: 0.86979\n",
            "Epoch: 22/30, step: 16/364, loss: 0.30135, accuracy: 0.87402\n",
            "Epoch: 22/30, step: 17/364, loss: 0.29761, accuracy: 0.87684\n",
            "Epoch: 22/30, step: 18/364, loss: 0.30475, accuracy: 0.87326\n",
            "Epoch: 22/30, step: 19/364, loss: 0.30560, accuracy: 0.87582\n",
            "Epoch: 22/30, step: 20/364, loss: 0.30711, accuracy: 0.87344\n",
            "Epoch: 22/30, step: 21/364, loss: 0.30669, accuracy: 0.87351\n",
            "Epoch: 22/30, step: 22/364, loss: 0.30895, accuracy: 0.87287\n",
            "Epoch: 22/30, step: 23/364, loss: 0.31338, accuracy: 0.86957\n",
            "Epoch: 22/30, step: 24/364, loss: 0.31554, accuracy: 0.86979\n",
            "Epoch: 22/30, step: 25/364, loss: 0.31265, accuracy: 0.87250\n",
            "Epoch: 22/30, step: 26/364, loss: 0.31250, accuracy: 0.87320\n",
            "Epoch: 22/30, step: 27/364, loss: 0.31731, accuracy: 0.86979\n",
            "Epoch: 22/30, step: 28/364, loss: 0.31818, accuracy: 0.86942\n",
            "Epoch: 22/30, step: 29/364, loss: 0.31891, accuracy: 0.86853\n",
            "Epoch: 22/30, step: 30/364, loss: 0.32055, accuracy: 0.86823\n",
            "Epoch: 22/30, step: 31/364, loss: 0.31845, accuracy: 0.86946\n",
            "Epoch: 22/30, step: 32/364, loss: 0.31606, accuracy: 0.87109\n",
            "Epoch: 22/30, step: 33/364, loss: 0.31547, accuracy: 0.87121\n",
            "Epoch: 22/30, step: 34/364, loss: 0.31485, accuracy: 0.87040\n",
            "Epoch: 22/30, step: 35/364, loss: 0.31421, accuracy: 0.86964\n",
            "Epoch: 22/30, step: 36/364, loss: 0.31574, accuracy: 0.86719\n",
            "Epoch: 22/30, step: 37/364, loss: 0.31784, accuracy: 0.86824\n",
            "Epoch: 22/30, step: 38/364, loss: 0.31625, accuracy: 0.86965\n",
            "Epoch: 22/30, step: 39/364, loss: 0.31737, accuracy: 0.86899\n",
            "Epoch: 22/30, step: 40/364, loss: 0.31569, accuracy: 0.87031\n",
            "Epoch: 22/30, step: 41/364, loss: 0.31795, accuracy: 0.86890\n",
            "Epoch: 22/30, step: 42/364, loss: 0.31903, accuracy: 0.86793\n",
            "Epoch: 22/30, step: 43/364, loss: 0.31750, accuracy: 0.87028\n",
            "Epoch: 22/30, step: 44/364, loss: 0.31640, accuracy: 0.87003\n",
            "Epoch: 22/30, step: 45/364, loss: 0.31555, accuracy: 0.87118\n",
            "Epoch: 22/30, step: 46/364, loss: 0.31455, accuracy: 0.87262\n",
            "Epoch: 22/30, step: 47/364, loss: 0.31401, accuracy: 0.87334\n",
            "Epoch: 22/30, step: 48/364, loss: 0.31382, accuracy: 0.87337\n",
            "Epoch: 22/30, step: 49/364, loss: 0.31489, accuracy: 0.87277\n",
            "Epoch: 22/30, step: 50/364, loss: 0.31439, accuracy: 0.87375\n",
            "Epoch: 22/30, step: 51/364, loss: 0.31284, accuracy: 0.87469\n",
            "Epoch: 22/30, step: 52/364, loss: 0.31243, accuracy: 0.87500\n",
            "Epoch: 22/30, step: 53/364, loss: 0.31149, accuracy: 0.87588\n",
            "Epoch: 22/30, step: 54/364, loss: 0.30895, accuracy: 0.87789\n",
            "Epoch: 22/30, step: 55/364, loss: 0.30915, accuracy: 0.87813\n",
            "Epoch: 22/30, step: 56/364, loss: 0.31102, accuracy: 0.87751\n",
            "Epoch: 22/30, step: 57/364, loss: 0.31119, accuracy: 0.87774\n",
            "Epoch: 22/30, step: 58/364, loss: 0.31134, accuracy: 0.87742\n",
            "Epoch: 22/30, step: 59/364, loss: 0.31085, accuracy: 0.87818\n",
            "Epoch: 22/30, step: 60/364, loss: 0.31279, accuracy: 0.87656\n",
            "Epoch: 22/30, step: 61/364, loss: 0.31184, accuracy: 0.87679\n",
            "Epoch: 22/30, step: 62/364, loss: 0.31216, accuracy: 0.87651\n",
            "Epoch: 22/30, step: 63/364, loss: 0.31184, accuracy: 0.87698\n",
            "Epoch: 22/30, step: 64/364, loss: 0.31001, accuracy: 0.87817\n",
            "Epoch: 22/30, step: 65/364, loss: 0.31067, accuracy: 0.87716\n",
            "Epoch: 22/30, step: 66/364, loss: 0.31109, accuracy: 0.87642\n",
            "Epoch: 22/30, step: 67/364, loss: 0.30968, accuracy: 0.87780\n",
            "Epoch: 22/30, step: 68/364, loss: 0.30999, accuracy: 0.87776\n",
            "Epoch: 22/30, step: 69/364, loss: 0.31092, accuracy: 0.87726\n",
            "Epoch: 22/30, step: 70/364, loss: 0.31143, accuracy: 0.87701\n",
            "Epoch: 22/30, step: 71/364, loss: 0.31243, accuracy: 0.87676\n",
            "Epoch: 22/30, step: 72/364, loss: 0.31215, accuracy: 0.87739\n",
            "Epoch: 22/30, step: 73/364, loss: 0.31133, accuracy: 0.87800\n",
            "Epoch: 22/30, step: 74/364, loss: 0.31141, accuracy: 0.87817\n",
            "Epoch: 22/30, step: 75/364, loss: 0.31148, accuracy: 0.87813\n",
            "Epoch: 22/30, step: 76/364, loss: 0.31155, accuracy: 0.87788\n",
            "Epoch: 22/30, step: 77/364, loss: 0.31075, accuracy: 0.87825\n",
            "Epoch: 22/30, step: 78/364, loss: 0.31135, accuracy: 0.87760\n",
            "Epoch: 22/30, step: 79/364, loss: 0.31134, accuracy: 0.87737\n",
            "Epoch: 22/30, step: 80/364, loss: 0.31085, accuracy: 0.87754\n",
            "Epoch: 22/30, step: 81/364, loss: 0.31058, accuracy: 0.87770\n",
            "Epoch: 22/30, step: 82/364, loss: 0.31009, accuracy: 0.87767\n",
            "Epoch: 22/30, step: 83/364, loss: 0.31012, accuracy: 0.87764\n",
            "Epoch: 22/30, step: 84/364, loss: 0.31027, accuracy: 0.87705\n",
            "Epoch: 22/30, step: 85/364, loss: 0.30995, accuracy: 0.87721\n",
            "Epoch: 22/30, step: 86/364, loss: 0.30965, accuracy: 0.87773\n",
            "Epoch: 22/30, step: 87/364, loss: 0.31022, accuracy: 0.87716\n",
            "Epoch: 22/30, step: 88/364, loss: 0.31039, accuracy: 0.87678\n",
            "Epoch: 22/30, step: 89/364, loss: 0.31203, accuracy: 0.87605\n",
            "Epoch: 22/30, step: 90/364, loss: 0.31279, accuracy: 0.87517\n",
            "Epoch: 22/30, step: 91/364, loss: 0.31319, accuracy: 0.87500\n",
            "Epoch: 22/30, step: 92/364, loss: 0.31376, accuracy: 0.87483\n",
            "Epoch: 22/30, step: 93/364, loss: 0.31393, accuracy: 0.87450\n",
            "Epoch: 22/30, step: 94/364, loss: 0.31374, accuracy: 0.87400\n",
            "Epoch: 22/30, step: 95/364, loss: 0.31390, accuracy: 0.87418\n",
            "Epoch: 22/30, step: 96/364, loss: 0.31378, accuracy: 0.87370\n",
            "Epoch: 22/30, step: 97/364, loss: 0.31271, accuracy: 0.87452\n",
            "Epoch: 22/30, step: 98/364, loss: 0.31285, accuracy: 0.87420\n",
            "Epoch: 22/30, step: 99/364, loss: 0.31235, accuracy: 0.87453\n",
            "Epoch: 22/30, step: 100/364, loss: 0.31353, accuracy: 0.87375\n",
            "Epoch: 22/30, step: 101/364, loss: 0.31374, accuracy: 0.87361\n",
            "Epoch: 22/30, step: 102/364, loss: 0.31350, accuracy: 0.87362\n",
            "Epoch: 22/30, step: 103/364, loss: 0.31253, accuracy: 0.87394\n",
            "Epoch: 22/30, step: 104/364, loss: 0.31237, accuracy: 0.87425\n",
            "Epoch: 22/30, step: 105/364, loss: 0.31218, accuracy: 0.87426\n",
            "Epoch: 22/30, step: 106/364, loss: 0.31208, accuracy: 0.87456\n",
            "Epoch: 22/30, step: 107/364, loss: 0.31129, accuracy: 0.87500\n",
            "Epoch: 22/30, step: 108/364, loss: 0.31131, accuracy: 0.87486\n",
            "Epoch: 22/30, step: 109/364, loss: 0.31193, accuracy: 0.87414\n",
            "Epoch: 22/30, step: 110/364, loss: 0.31134, accuracy: 0.87472\n",
            "Epoch: 22/30, step: 111/364, loss: 0.31090, accuracy: 0.87500\n",
            "Epoch: 22/30, step: 112/364, loss: 0.31023, accuracy: 0.87556\n",
            "Epoch: 22/30, step: 113/364, loss: 0.31130, accuracy: 0.87528\n",
            "Epoch: 22/30, step: 114/364, loss: 0.31049, accuracy: 0.87569\n",
            "Epoch: 22/30, step: 115/364, loss: 0.31001, accuracy: 0.87595\n",
            "Epoch: 22/30, step: 116/364, loss: 0.30953, accuracy: 0.87608\n",
            "Epoch: 22/30, step: 117/364, loss: 0.30987, accuracy: 0.87593\n",
            "Epoch: 22/30, step: 118/364, loss: 0.31011, accuracy: 0.87566\n",
            "Epoch: 22/30, step: 119/364, loss: 0.31013, accuracy: 0.87539\n",
            "Epoch: 22/30, step: 120/364, loss: 0.31003, accuracy: 0.87526\n",
            "Epoch: 22/30, step: 121/364, loss: 0.31046, accuracy: 0.87500\n",
            "Epoch: 22/30, step: 122/364, loss: 0.31089, accuracy: 0.87449\n",
            "Epoch: 22/30, step: 123/364, loss: 0.31131, accuracy: 0.87436\n",
            "Epoch: 22/30, step: 124/364, loss: 0.31128, accuracy: 0.87424\n",
            "Epoch: 22/30, step: 125/364, loss: 0.31234, accuracy: 0.87362\n",
            "Epoch: 22/30, step: 126/364, loss: 0.31262, accuracy: 0.87339\n",
            "Epoch: 22/30, step: 127/364, loss: 0.31308, accuracy: 0.87303\n",
            "Epoch: 22/30, step: 128/364, loss: 0.31279, accuracy: 0.87329\n",
            "Epoch: 22/30, step: 129/364, loss: 0.31248, accuracy: 0.87355\n",
            "Epoch: 22/30, step: 130/364, loss: 0.31211, accuracy: 0.87380\n",
            "Epoch: 22/30, step: 131/364, loss: 0.31163, accuracy: 0.87405\n",
            "Epoch: 22/30, step: 132/364, loss: 0.31223, accuracy: 0.87346\n",
            "Epoch: 22/30, step: 133/364, loss: 0.31172, accuracy: 0.87406\n",
            "Epoch: 22/30, step: 134/364, loss: 0.31246, accuracy: 0.87348\n",
            "Epoch: 22/30, step: 135/364, loss: 0.31432, accuracy: 0.87245\n",
            "Epoch: 22/30, step: 136/364, loss: 0.31423, accuracy: 0.87247\n",
            "Epoch: 22/30, step: 137/364, loss: 0.31417, accuracy: 0.87238\n",
            "Epoch: 22/30, step: 138/364, loss: 0.31455, accuracy: 0.87194\n",
            "Epoch: 22/30, step: 139/364, loss: 0.31407, accuracy: 0.87219\n",
            "Epoch: 22/30, step: 140/364, loss: 0.31502, accuracy: 0.87165\n",
            "Epoch: 22/30, step: 141/364, loss: 0.31534, accuracy: 0.87112\n",
            "Epoch: 22/30, step: 142/364, loss: 0.31552, accuracy: 0.87093\n",
            "Epoch: 22/30, step: 143/364, loss: 0.31478, accuracy: 0.87150\n",
            "Epoch: 22/30, step: 144/364, loss: 0.31463, accuracy: 0.87153\n",
            "Epoch: 22/30, step: 145/364, loss: 0.31416, accuracy: 0.87198\n",
            "Epoch: 22/30, step: 146/364, loss: 0.31474, accuracy: 0.87168\n",
            "Epoch: 22/30, step: 147/364, loss: 0.31477, accuracy: 0.87160\n",
            "Epoch: 22/30, step: 148/364, loss: 0.31482, accuracy: 0.87152\n",
            "Epoch: 22/30, step: 149/364, loss: 0.31419, accuracy: 0.87206\n",
            "Epoch: 22/30, step: 150/364, loss: 0.31424, accuracy: 0.87208\n",
            "Epoch: 22/30, step: 151/364, loss: 0.31395, accuracy: 0.87231\n",
            "Epoch: 22/30, step: 152/364, loss: 0.31334, accuracy: 0.87264\n",
            "Epoch: 22/30, step: 153/364, loss: 0.31285, accuracy: 0.87286\n",
            "Epoch: 22/30, step: 154/364, loss: 0.31314, accuracy: 0.87277\n",
            "Epoch: 22/30, step: 155/364, loss: 0.31305, accuracy: 0.87298\n",
            "Epoch: 22/30, step: 156/364, loss: 0.31316, accuracy: 0.87310\n",
            "Epoch: 22/30, step: 157/364, loss: 0.31312, accuracy: 0.87311\n",
            "Epoch: 22/30, step: 158/364, loss: 0.31275, accuracy: 0.87322\n",
            "Epoch: 22/30, step: 159/364, loss: 0.31251, accuracy: 0.87343\n",
            "Epoch: 22/30, step: 160/364, loss: 0.31283, accuracy: 0.87324\n",
            "Epoch: 22/30, step: 161/364, loss: 0.31261, accuracy: 0.87345\n",
            "Epoch: 22/30, step: 162/364, loss: 0.31275, accuracy: 0.87346\n",
            "Epoch: 22/30, step: 163/364, loss: 0.31249, accuracy: 0.87366\n",
            "Epoch: 22/30, step: 164/364, loss: 0.31231, accuracy: 0.87367\n",
            "Epoch: 22/30, step: 165/364, loss: 0.31223, accuracy: 0.87396\n",
            "Epoch: 22/30, step: 166/364, loss: 0.31176, accuracy: 0.87396\n",
            "Epoch: 22/30, step: 167/364, loss: 0.31152, accuracy: 0.87416\n",
            "Epoch: 22/30, step: 168/364, loss: 0.31147, accuracy: 0.87416\n",
            "Epoch: 22/30, step: 169/364, loss: 0.31154, accuracy: 0.87408\n",
            "Epoch: 22/30, step: 170/364, loss: 0.31162, accuracy: 0.87417\n",
            "Epoch: 22/30, step: 171/364, loss: 0.31198, accuracy: 0.87409\n",
            "Epoch: 22/30, step: 172/364, loss: 0.31227, accuracy: 0.87364\n",
            "Epoch: 22/30, step: 173/364, loss: 0.31215, accuracy: 0.87365\n",
            "Epoch: 22/30, step: 174/364, loss: 0.31219, accuracy: 0.87347\n",
            "Epoch: 22/30, step: 175/364, loss: 0.31222, accuracy: 0.87330\n",
            "Epoch: 22/30, step: 176/364, loss: 0.31239, accuracy: 0.87314\n",
            "Epoch: 22/30, step: 177/364, loss: 0.31239, accuracy: 0.87306\n",
            "Epoch: 22/30, step: 178/364, loss: 0.31221, accuracy: 0.87307\n",
            "Epoch: 22/30, step: 179/364, loss: 0.31216, accuracy: 0.87308\n",
            "Epoch: 22/30, step: 180/364, loss: 0.31214, accuracy: 0.87326\n",
            "Epoch: 22/30, step: 181/364, loss: 0.31221, accuracy: 0.87327\n",
            "Epoch: 22/30, step: 182/364, loss: 0.31197, accuracy: 0.87320\n",
            "Epoch: 22/30, step: 183/364, loss: 0.31201, accuracy: 0.87304\n",
            "Epoch: 22/30, step: 184/364, loss: 0.31220, accuracy: 0.87296\n",
            "Epoch: 22/30, step: 185/364, loss: 0.31201, accuracy: 0.87323\n",
            "Epoch: 22/30, step: 186/364, loss: 0.31213, accuracy: 0.87315\n",
            "Epoch: 22/30, step: 187/364, loss: 0.31188, accuracy: 0.87333\n",
            "Epoch: 22/30, step: 188/364, loss: 0.31240, accuracy: 0.87292\n",
            "Epoch: 22/30, step: 189/364, loss: 0.31244, accuracy: 0.87302\n",
            "Epoch: 22/30, step: 190/364, loss: 0.31288, accuracy: 0.87270\n",
            "Epoch: 22/30, step: 191/364, loss: 0.31266, accuracy: 0.87279\n",
            "Epoch: 22/30, step: 192/364, loss: 0.31309, accuracy: 0.87248\n",
            "Epoch: 22/30, step: 193/364, loss: 0.31319, accuracy: 0.87225\n",
            "Epoch: 22/30, step: 194/364, loss: 0.31341, accuracy: 0.87202\n",
            "Epoch: 22/30, step: 195/364, loss: 0.31319, accuracy: 0.87212\n",
            "Epoch: 22/30, step: 196/364, loss: 0.31375, accuracy: 0.87205\n",
            "Epoch: 22/30, step: 197/364, loss: 0.31305, accuracy: 0.87262\n",
            "Epoch: 22/30, step: 198/364, loss: 0.31308, accuracy: 0.87232\n",
            "Epoch: 22/30, step: 199/364, loss: 0.31297, accuracy: 0.87241\n",
            "Epoch: 22/30, step: 200/364, loss: 0.31310, accuracy: 0.87227\n",
            "Epoch: 22/30, step: 201/364, loss: 0.31323, accuracy: 0.87220\n",
            "Epoch: 22/30, step: 202/364, loss: 0.31296, accuracy: 0.87260\n",
            "Epoch: 22/30, step: 203/364, loss: 0.31285, accuracy: 0.87246\n",
            "Epoch: 22/30, step: 204/364, loss: 0.31253, accuracy: 0.87270\n",
            "Epoch: 22/30, step: 205/364, loss: 0.31272, accuracy: 0.87279\n",
            "Epoch: 22/30, step: 206/364, loss: 0.31246, accuracy: 0.87295\n",
            "Epoch: 22/30, step: 207/364, loss: 0.31259, accuracy: 0.87289\n",
            "Epoch: 22/30, step: 208/364, loss: 0.31221, accuracy: 0.87305\n",
            "Epoch: 22/30, step: 209/364, loss: 0.31232, accuracy: 0.87291\n",
            "Epoch: 22/30, step: 210/364, loss: 0.31228, accuracy: 0.87254\n",
            "Epoch: 22/30, step: 211/364, loss: 0.31215, accuracy: 0.87241\n",
            "Epoch: 22/30, step: 212/364, loss: 0.31176, accuracy: 0.87257\n",
            "Epoch: 22/30, step: 213/364, loss: 0.31176, accuracy: 0.87258\n",
            "Epoch: 22/30, step: 214/364, loss: 0.31189, accuracy: 0.87259\n",
            "Epoch: 22/30, step: 215/364, loss: 0.31216, accuracy: 0.87260\n",
            "Epoch: 22/30, step: 216/364, loss: 0.31200, accuracy: 0.87276\n",
            "Epoch: 22/30, step: 217/364, loss: 0.31157, accuracy: 0.87298\n",
            "Epoch: 22/30, step: 218/364, loss: 0.31135, accuracy: 0.87306\n",
            "Epoch: 22/30, step: 219/364, loss: 0.31126, accuracy: 0.87307\n",
            "Epoch: 22/30, step: 220/364, loss: 0.31108, accuracy: 0.87315\n",
            "Epoch: 22/30, step: 221/364, loss: 0.31099, accuracy: 0.87323\n",
            "Epoch: 22/30, step: 222/364, loss: 0.31065, accuracy: 0.87338\n",
            "Epoch: 22/30, step: 223/364, loss: 0.31035, accuracy: 0.87353\n",
            "Epoch: 22/30, step: 224/364, loss: 0.31028, accuracy: 0.87381\n",
            "Epoch: 22/30, step: 225/364, loss: 0.31022, accuracy: 0.87375\n",
            "Epoch: 22/30, step: 226/364, loss: 0.31030, accuracy: 0.87376\n",
            "Epoch: 22/30, step: 227/364, loss: 0.31029, accuracy: 0.87349\n",
            "Epoch: 22/30, step: 228/364, loss: 0.30986, accuracy: 0.87370\n",
            "Epoch: 22/30, step: 229/364, loss: 0.30978, accuracy: 0.87357\n",
            "Epoch: 22/30, step: 230/364, loss: 0.30983, accuracy: 0.87364\n",
            "Epoch: 22/30, step: 231/364, loss: 0.30955, accuracy: 0.87385\n",
            "Epoch: 22/30, step: 232/364, loss: 0.30968, accuracy: 0.87386\n",
            "Epoch: 22/30, step: 233/364, loss: 0.30925, accuracy: 0.87413\n",
            "Epoch: 22/30, step: 234/364, loss: 0.30890, accuracy: 0.87440\n",
            "Epoch: 22/30, step: 235/364, loss: 0.30881, accuracy: 0.87440\n",
            "Epoch: 22/30, step: 236/364, loss: 0.30924, accuracy: 0.87434\n",
            "Epoch: 22/30, step: 237/364, loss: 0.30928, accuracy: 0.87421\n",
            "Epoch: 22/30, step: 238/364, loss: 0.30906, accuracy: 0.87421\n",
            "Epoch: 22/30, step: 239/364, loss: 0.30889, accuracy: 0.87422\n",
            "Epoch: 22/30, step: 240/364, loss: 0.30873, accuracy: 0.87435\n",
            "Epoch: 22/30, step: 241/364, loss: 0.30889, accuracy: 0.87422\n",
            "Epoch: 22/30, step: 242/364, loss: 0.30872, accuracy: 0.87429\n",
            "Epoch: 22/30, step: 243/364, loss: 0.30869, accuracy: 0.87429\n",
            "Epoch: 22/30, step: 244/364, loss: 0.30874, accuracy: 0.87442\n",
            "Epoch: 22/30, step: 245/364, loss: 0.30882, accuracy: 0.87436\n",
            "Epoch: 22/30, step: 246/364, loss: 0.30879, accuracy: 0.87436\n",
            "Epoch: 22/30, step: 247/364, loss: 0.30915, accuracy: 0.87411\n",
            "Epoch: 22/30, step: 248/364, loss: 0.30975, accuracy: 0.87387\n",
            "Epoch: 22/30, step: 249/364, loss: 0.31013, accuracy: 0.87362\n",
            "Epoch: 22/30, step: 250/364, loss: 0.31064, accuracy: 0.87331\n",
            "Epoch: 22/30, step: 251/364, loss: 0.31066, accuracy: 0.87313\n",
            "Epoch: 22/30, step: 252/364, loss: 0.31061, accuracy: 0.87314\n",
            "Epoch: 22/30, step: 253/364, loss: 0.31049, accuracy: 0.87315\n",
            "Epoch: 22/30, step: 254/364, loss: 0.31088, accuracy: 0.87291\n",
            "Epoch: 22/30, step: 255/364, loss: 0.31054, accuracy: 0.87310\n",
            "Epoch: 22/30, step: 256/364, loss: 0.31066, accuracy: 0.87317\n",
            "Epoch: 22/30, step: 257/364, loss: 0.31099, accuracy: 0.87318\n",
            "Epoch: 22/30, step: 258/364, loss: 0.31148, accuracy: 0.87282\n",
            "Epoch: 22/30, step: 259/364, loss: 0.31164, accuracy: 0.87277\n",
            "Epoch: 22/30, step: 260/364, loss: 0.31160, accuracy: 0.87272\n",
            "Epoch: 22/30, step: 261/364, loss: 0.31166, accuracy: 0.87278\n",
            "Epoch: 22/30, step: 262/364, loss: 0.31186, accuracy: 0.87267\n",
            "Epoch: 22/30, step: 263/364, loss: 0.31187, accuracy: 0.87256\n",
            "Epoch: 22/30, step: 264/364, loss: 0.31209, accuracy: 0.87228\n",
            "Epoch: 22/30, step: 265/364, loss: 0.31198, accuracy: 0.87235\n",
            "Epoch: 22/30, step: 266/364, loss: 0.31170, accuracy: 0.87253\n",
            "Epoch: 22/30, step: 267/364, loss: 0.31169, accuracy: 0.87248\n",
            "Epoch: 22/30, step: 268/364, loss: 0.31214, accuracy: 0.87220\n",
            "Epoch: 22/30, step: 269/364, loss: 0.31240, accuracy: 0.87175\n",
            "Epoch: 22/30, step: 270/364, loss: 0.31212, accuracy: 0.87199\n",
            "Epoch: 22/30, step: 271/364, loss: 0.31175, accuracy: 0.87223\n",
            "Epoch: 22/30, step: 272/364, loss: 0.31227, accuracy: 0.87184\n",
            "Epoch: 22/30, step: 273/364, loss: 0.31238, accuracy: 0.87174\n",
            "Epoch: 22/30, step: 274/364, loss: 0.31218, accuracy: 0.87175\n",
            "Epoch: 22/30, step: 275/364, loss: 0.31238, accuracy: 0.87170\n",
            "Epoch: 22/30, step: 276/364, loss: 0.31231, accuracy: 0.87183\n",
            "Epoch: 22/30, step: 277/364, loss: 0.31259, accuracy: 0.87156\n",
            "Epoch: 22/30, step: 278/364, loss: 0.31267, accuracy: 0.87157\n",
            "Epoch: 22/30, step: 279/364, loss: 0.31256, accuracy: 0.87170\n",
            "Epoch: 22/30, step: 280/364, loss: 0.31250, accuracy: 0.87165\n",
            "Epoch: 22/30, step: 281/364, loss: 0.31275, accuracy: 0.87139\n",
            "Epoch: 22/30, step: 282/364, loss: 0.31284, accuracy: 0.87140\n",
            "Epoch: 22/30, step: 283/364, loss: 0.31251, accuracy: 0.87163\n",
            "Epoch: 22/30, step: 284/364, loss: 0.31235, accuracy: 0.87153\n",
            "Epoch: 22/30, step: 285/364, loss: 0.31250, accuracy: 0.87138\n",
            "Epoch: 22/30, step: 286/364, loss: 0.31274, accuracy: 0.87128\n",
            "Epoch: 22/30, step: 287/364, loss: 0.31256, accuracy: 0.87146\n",
            "Epoch: 22/30, step: 288/364, loss: 0.31236, accuracy: 0.87147\n",
            "Epoch: 22/30, step: 289/364, loss: 0.31207, accuracy: 0.87159\n",
            "Epoch: 22/30, step: 290/364, loss: 0.31229, accuracy: 0.87155\n",
            "Epoch: 22/30, step: 291/364, loss: 0.31213, accuracy: 0.87168\n",
            "Epoch: 22/30, train loss: 0.31213, train accuracy: 0.87168, valid loss: 0.64156, valid accuracy: 0.70099\n",
            "Epoch: 23/30, step: 1/364, loss: 0.21817, accuracy: 0.90625\n",
            "Epoch: 23/30, step: 2/364, loss: 0.19982, accuracy: 0.94531\n",
            "Epoch: 23/30, step: 3/364, loss: 0.24631, accuracy: 0.91667\n",
            "Epoch: 23/30, step: 4/364, loss: 0.27518, accuracy: 0.90234\n",
            "Epoch: 23/30, step: 5/364, loss: 0.28370, accuracy: 0.89688\n",
            "Epoch: 23/30, step: 6/364, loss: 0.27073, accuracy: 0.90365\n",
            "Epoch: 23/30, step: 7/364, loss: 0.27707, accuracy: 0.89732\n",
            "Epoch: 23/30, step: 8/364, loss: 0.28782, accuracy: 0.88477\n",
            "Epoch: 23/30, step: 9/364, loss: 0.29455, accuracy: 0.88194\n",
            "Epoch: 23/30, step: 10/364, loss: 0.28609, accuracy: 0.88750\n",
            "Epoch: 23/30, step: 11/364, loss: 0.28212, accuracy: 0.88920\n",
            "Epoch: 23/30, step: 12/364, loss: 0.28505, accuracy: 0.88932\n",
            "Epoch: 23/30, step: 13/364, loss: 0.28496, accuracy: 0.88942\n",
            "Epoch: 23/30, step: 14/364, loss: 0.28850, accuracy: 0.88728\n",
            "Epoch: 23/30, step: 15/364, loss: 0.29634, accuracy: 0.88125\n",
            "Epoch: 23/30, step: 16/364, loss: 0.29493, accuracy: 0.88184\n",
            "Epoch: 23/30, step: 17/364, loss: 0.29162, accuracy: 0.88235\n",
            "Epoch: 23/30, step: 18/364, loss: 0.29211, accuracy: 0.88281\n",
            "Epoch: 23/30, step: 19/364, loss: 0.29420, accuracy: 0.88158\n",
            "Epoch: 23/30, step: 20/364, loss: 0.30209, accuracy: 0.87500\n",
            "Epoch: 23/30, step: 21/364, loss: 0.29991, accuracy: 0.87649\n",
            "Epoch: 23/30, step: 22/364, loss: 0.30273, accuracy: 0.87713\n",
            "Epoch: 23/30, step: 23/364, loss: 0.30263, accuracy: 0.87568\n",
            "Epoch: 23/30, step: 24/364, loss: 0.30096, accuracy: 0.87826\n",
            "Epoch: 23/30, step: 25/364, loss: 0.30501, accuracy: 0.87687\n",
            "Epoch: 23/30, step: 26/364, loss: 0.30686, accuracy: 0.87440\n",
            "Epoch: 23/30, step: 27/364, loss: 0.31037, accuracy: 0.87153\n",
            "Epoch: 23/30, step: 28/364, loss: 0.31365, accuracy: 0.86942\n",
            "Epoch: 23/30, step: 29/364, loss: 0.31268, accuracy: 0.86853\n",
            "Epoch: 23/30, step: 30/364, loss: 0.31102, accuracy: 0.86927\n",
            "Epoch: 23/30, step: 31/364, loss: 0.31141, accuracy: 0.86845\n",
            "Epoch: 23/30, step: 32/364, loss: 0.30834, accuracy: 0.87012\n",
            "Epoch: 23/30, step: 33/364, loss: 0.30535, accuracy: 0.87169\n",
            "Epoch: 23/30, step: 34/364, loss: 0.30321, accuracy: 0.87316\n",
            "Epoch: 23/30, step: 35/364, loss: 0.30326, accuracy: 0.87321\n",
            "Epoch: 23/30, step: 36/364, loss: 0.30212, accuracy: 0.87457\n",
            "Epoch: 23/30, step: 37/364, loss: 0.30022, accuracy: 0.87584\n",
            "Epoch: 23/30, step: 38/364, loss: 0.30042, accuracy: 0.87541\n",
            "Epoch: 23/30, step: 39/364, loss: 0.29995, accuracy: 0.87500\n",
            "Epoch: 23/30, step: 40/364, loss: 0.29810, accuracy: 0.87578\n",
            "Epoch: 23/30, step: 41/364, loss: 0.29803, accuracy: 0.87538\n",
            "Epoch: 23/30, step: 42/364, loss: 0.29740, accuracy: 0.87574\n",
            "Epoch: 23/30, step: 43/364, loss: 0.29636, accuracy: 0.87645\n",
            "Epoch: 23/30, step: 44/364, loss: 0.29458, accuracy: 0.87784\n",
            "Epoch: 23/30, step: 45/364, loss: 0.29527, accuracy: 0.87743\n",
            "Epoch: 23/30, step: 46/364, loss: 0.29448, accuracy: 0.87806\n",
            "Epoch: 23/30, step: 47/364, loss: 0.29618, accuracy: 0.87766\n",
            "Epoch: 23/30, step: 48/364, loss: 0.29406, accuracy: 0.87923\n",
            "Epoch: 23/30, step: 49/364, loss: 0.29213, accuracy: 0.88106\n",
            "Epoch: 23/30, step: 50/364, loss: 0.29225, accuracy: 0.88125\n",
            "Epoch: 23/30, step: 51/364, loss: 0.29186, accuracy: 0.88113\n",
            "Epoch: 23/30, step: 52/364, loss: 0.29233, accuracy: 0.88011\n",
            "Epoch: 23/30, step: 53/364, loss: 0.29164, accuracy: 0.88060\n",
            "Epoch: 23/30, step: 54/364, loss: 0.29024, accuracy: 0.88079\n",
            "Epoch: 23/30, step: 55/364, loss: 0.29012, accuracy: 0.87983\n",
            "Epoch: 23/30, step: 56/364, loss: 0.29045, accuracy: 0.87946\n",
            "Epoch: 23/30, step: 57/364, loss: 0.29014, accuracy: 0.87993\n",
            "Epoch: 23/30, step: 58/364, loss: 0.28911, accuracy: 0.88093\n",
            "Epoch: 23/30, step: 59/364, loss: 0.29151, accuracy: 0.87950\n",
            "Epoch: 23/30, step: 60/364, loss: 0.29265, accuracy: 0.87813\n",
            "Epoch: 23/30, step: 61/364, loss: 0.29280, accuracy: 0.87859\n",
            "Epoch: 23/30, step: 62/364, loss: 0.29327, accuracy: 0.87903\n",
            "Epoch: 23/30, step: 63/364, loss: 0.29303, accuracy: 0.87897\n",
            "Epoch: 23/30, step: 64/364, loss: 0.29461, accuracy: 0.87817\n",
            "Epoch: 23/30, step: 65/364, loss: 0.29478, accuracy: 0.87788\n",
            "Epoch: 23/30, step: 66/364, loss: 0.29578, accuracy: 0.87713\n",
            "Epoch: 23/30, step: 67/364, loss: 0.29543, accuracy: 0.87780\n",
            "Epoch: 23/30, step: 68/364, loss: 0.29700, accuracy: 0.87638\n",
            "Epoch: 23/30, step: 69/364, loss: 0.29615, accuracy: 0.87681\n",
            "Epoch: 23/30, step: 70/364, loss: 0.29632, accuracy: 0.87656\n",
            "Epoch: 23/30, step: 71/364, loss: 0.29679, accuracy: 0.87544\n",
            "Epoch: 23/30, step: 72/364, loss: 0.29597, accuracy: 0.87674\n",
            "Epoch: 23/30, step: 73/364, loss: 0.29455, accuracy: 0.87821\n",
            "Epoch: 23/30, step: 74/364, loss: 0.29491, accuracy: 0.87796\n",
            "Epoch: 23/30, step: 75/364, loss: 0.29357, accuracy: 0.87875\n",
            "Epoch: 23/30, step: 76/364, loss: 0.29392, accuracy: 0.87767\n",
            "Epoch: 23/30, step: 77/364, loss: 0.29491, accuracy: 0.87703\n",
            "Epoch: 23/30, step: 78/364, loss: 0.29566, accuracy: 0.87660\n",
            "Epoch: 23/30, step: 79/364, loss: 0.29622, accuracy: 0.87619\n",
            "Epoch: 23/30, step: 80/364, loss: 0.29666, accuracy: 0.87598\n",
            "Epoch: 23/30, step: 81/364, loss: 0.29624, accuracy: 0.87635\n",
            "Epoch: 23/30, step: 82/364, loss: 0.29534, accuracy: 0.87710\n",
            "Epoch: 23/30, step: 83/364, loss: 0.29569, accuracy: 0.87632\n",
            "Epoch: 23/30, step: 84/364, loss: 0.29631, accuracy: 0.87593\n",
            "Epoch: 23/30, step: 85/364, loss: 0.29642, accuracy: 0.87555\n",
            "Epoch: 23/30, step: 86/364, loss: 0.29628, accuracy: 0.87591\n",
            "Epoch: 23/30, step: 87/364, loss: 0.29524, accuracy: 0.87680\n",
            "Epoch: 23/30, step: 88/364, loss: 0.29456, accuracy: 0.87713\n",
            "Epoch: 23/30, step: 89/364, loss: 0.29430, accuracy: 0.87746\n",
            "Epoch: 23/30, step: 90/364, loss: 0.29407, accuracy: 0.87760\n",
            "Epoch: 23/30, step: 91/364, loss: 0.29438, accuracy: 0.87706\n",
            "Epoch: 23/30, step: 92/364, loss: 0.29480, accuracy: 0.87670\n",
            "Epoch: 23/30, step: 93/364, loss: 0.29493, accuracy: 0.87668\n",
            "Epoch: 23/30, step: 94/364, loss: 0.29516, accuracy: 0.87683\n",
            "Epoch: 23/30, step: 95/364, loss: 0.29541, accuracy: 0.87681\n",
            "Epoch: 23/30, step: 96/364, loss: 0.29540, accuracy: 0.87679\n",
            "Epoch: 23/30, step: 97/364, loss: 0.29466, accuracy: 0.87726\n",
            "Epoch: 23/30, step: 98/364, loss: 0.29673, accuracy: 0.87596\n",
            "Epoch: 23/30, step: 99/364, loss: 0.29726, accuracy: 0.87563\n",
            "Epoch: 23/30, step: 100/364, loss: 0.29739, accuracy: 0.87578\n",
            "Epoch: 23/30, step: 101/364, loss: 0.29669, accuracy: 0.87670\n",
            "Epoch: 23/30, step: 102/364, loss: 0.29676, accuracy: 0.87669\n",
            "Epoch: 23/30, step: 103/364, loss: 0.29594, accuracy: 0.87712\n",
            "Epoch: 23/30, step: 104/364, loss: 0.29647, accuracy: 0.87650\n",
            "Epoch: 23/30, step: 105/364, loss: 0.29748, accuracy: 0.87560\n",
            "Epoch: 23/30, step: 106/364, loss: 0.29822, accuracy: 0.87529\n",
            "Epoch: 23/30, step: 107/364, loss: 0.29799, accuracy: 0.87558\n",
            "Epoch: 23/30, step: 108/364, loss: 0.29810, accuracy: 0.87558\n",
            "Epoch: 23/30, step: 109/364, loss: 0.29777, accuracy: 0.87572\n",
            "Epoch: 23/30, step: 110/364, loss: 0.29667, accuracy: 0.87656\n",
            "Epoch: 23/30, step: 111/364, loss: 0.29593, accuracy: 0.87711\n",
            "Epoch: 23/30, step: 112/364, loss: 0.29695, accuracy: 0.87667\n",
            "Epoch: 23/30, step: 113/364, loss: 0.29736, accuracy: 0.87666\n",
            "Epoch: 23/30, step: 114/364, loss: 0.29836, accuracy: 0.87623\n",
            "Epoch: 23/30, step: 115/364, loss: 0.29842, accuracy: 0.87595\n",
            "Epoch: 23/30, step: 116/364, loss: 0.29923, accuracy: 0.87527\n",
            "Epoch: 23/30, step: 117/364, loss: 0.29951, accuracy: 0.87540\n",
            "Epoch: 23/30, step: 118/364, loss: 0.30024, accuracy: 0.87513\n",
            "Epoch: 23/30, step: 119/364, loss: 0.30147, accuracy: 0.87434\n",
            "Epoch: 23/30, step: 120/364, loss: 0.30171, accuracy: 0.87396\n",
            "Epoch: 23/30, step: 121/364, loss: 0.30172, accuracy: 0.87410\n",
            "Epoch: 23/30, step: 122/364, loss: 0.30143, accuracy: 0.87436\n",
            "Epoch: 23/30, step: 123/364, loss: 0.30119, accuracy: 0.87462\n",
            "Epoch: 23/30, step: 124/364, loss: 0.30079, accuracy: 0.87525\n",
            "Epoch: 23/30, step: 125/364, loss: 0.30077, accuracy: 0.87537\n",
            "Epoch: 23/30, step: 126/364, loss: 0.30021, accuracy: 0.87550\n",
            "Epoch: 23/30, step: 127/364, loss: 0.29958, accuracy: 0.87598\n",
            "Epoch: 23/30, step: 128/364, loss: 0.30044, accuracy: 0.87561\n",
            "Epoch: 23/30, step: 129/364, loss: 0.29985, accuracy: 0.87597\n",
            "Epoch: 23/30, step: 130/364, loss: 0.29967, accuracy: 0.87584\n",
            "Epoch: 23/30, step: 131/364, loss: 0.30043, accuracy: 0.87524\n",
            "Epoch: 23/30, step: 132/364, loss: 0.30119, accuracy: 0.87441\n",
            "Epoch: 23/30, step: 133/364, loss: 0.30146, accuracy: 0.87453\n",
            "Epoch: 23/30, step: 134/364, loss: 0.30150, accuracy: 0.87418\n",
            "Epoch: 23/30, step: 135/364, loss: 0.30117, accuracy: 0.87454\n",
            "Epoch: 23/30, step: 136/364, loss: 0.30184, accuracy: 0.87397\n",
            "Epoch: 23/30, step: 137/364, loss: 0.30184, accuracy: 0.87397\n",
            "Epoch: 23/30, step: 138/364, loss: 0.30201, accuracy: 0.87375\n",
            "Epoch: 23/30, step: 139/364, loss: 0.30245, accuracy: 0.87343\n",
            "Epoch: 23/30, step: 140/364, loss: 0.30223, accuracy: 0.87355\n",
            "Epoch: 23/30, step: 141/364, loss: 0.30212, accuracy: 0.87378\n",
            "Epoch: 23/30, step: 142/364, loss: 0.30230, accuracy: 0.87346\n",
            "Epoch: 23/30, step: 143/364, loss: 0.30225, accuracy: 0.87358\n",
            "Epoch: 23/30, step: 144/364, loss: 0.30270, accuracy: 0.87316\n",
            "Epoch: 23/30, step: 145/364, loss: 0.30284, accuracy: 0.87295\n",
            "Epoch: 23/30, step: 146/364, loss: 0.30279, accuracy: 0.87307\n",
            "Epoch: 23/30, step: 147/364, loss: 0.30242, accuracy: 0.87351\n",
            "Epoch: 23/30, step: 148/364, loss: 0.30160, accuracy: 0.87405\n",
            "Epoch: 23/30, step: 149/364, loss: 0.30190, accuracy: 0.87374\n",
            "Epoch: 23/30, step: 150/364, loss: 0.30218, accuracy: 0.87323\n",
            "Epoch: 23/30, step: 151/364, loss: 0.30193, accuracy: 0.87345\n",
            "Epoch: 23/30, step: 152/364, loss: 0.30138, accuracy: 0.87356\n",
            "Epoch: 23/30, step: 153/364, loss: 0.30107, accuracy: 0.87398\n",
            "Epoch: 23/30, step: 154/364, loss: 0.30097, accuracy: 0.87399\n",
            "Epoch: 23/30, step: 155/364, loss: 0.30026, accuracy: 0.87450\n",
            "Epoch: 23/30, step: 156/364, loss: 0.29996, accuracy: 0.87460\n",
            "Epoch: 23/30, step: 157/364, loss: 0.30002, accuracy: 0.87460\n",
            "Epoch: 23/30, step: 158/364, loss: 0.29954, accuracy: 0.87510\n",
            "Epoch: 23/30, step: 159/364, loss: 0.29895, accuracy: 0.87539\n",
            "Epoch: 23/30, step: 160/364, loss: 0.29837, accuracy: 0.87578\n",
            "Epoch: 23/30, step: 161/364, loss: 0.29851, accuracy: 0.87568\n",
            "Epoch: 23/30, step: 162/364, loss: 0.29819, accuracy: 0.87587\n",
            "Epoch: 23/30, step: 163/364, loss: 0.29873, accuracy: 0.87548\n",
            "Epoch: 23/30, step: 164/364, loss: 0.29891, accuracy: 0.87519\n",
            "Epoch: 23/30, step: 165/364, loss: 0.29867, accuracy: 0.87528\n",
            "Epoch: 23/30, step: 166/364, loss: 0.29891, accuracy: 0.87509\n",
            "Epoch: 23/30, step: 167/364, loss: 0.29992, accuracy: 0.87463\n",
            "Epoch: 23/30, step: 168/364, loss: 0.29954, accuracy: 0.87509\n",
            "Epoch: 23/30, step: 169/364, loss: 0.29949, accuracy: 0.87518\n",
            "Epoch: 23/30, step: 170/364, loss: 0.29944, accuracy: 0.87546\n",
            "Epoch: 23/30, step: 171/364, loss: 0.29929, accuracy: 0.87564\n",
            "Epoch: 23/30, step: 172/364, loss: 0.29957, accuracy: 0.87536\n",
            "Epoch: 23/30, step: 173/364, loss: 0.29976, accuracy: 0.87527\n",
            "Epoch: 23/30, step: 174/364, loss: 0.29957, accuracy: 0.87554\n",
            "Epoch: 23/30, step: 175/364, loss: 0.29960, accuracy: 0.87554\n",
            "Epoch: 23/30, step: 176/364, loss: 0.29923, accuracy: 0.87571\n",
            "Epoch: 23/30, step: 177/364, loss: 0.29936, accuracy: 0.87553\n",
            "Epoch: 23/30, step: 178/364, loss: 0.29969, accuracy: 0.87518\n",
            "Epoch: 23/30, step: 179/364, loss: 0.29944, accuracy: 0.87526\n",
            "Epoch: 23/30, step: 180/364, loss: 0.29928, accuracy: 0.87535\n",
            "Epoch: 23/30, step: 181/364, loss: 0.29838, accuracy: 0.87586\n",
            "Epoch: 23/30, step: 182/364, loss: 0.29873, accuracy: 0.87552\n",
            "Epoch: 23/30, step: 183/364, loss: 0.29887, accuracy: 0.87543\n",
            "Epoch: 23/30, step: 184/364, loss: 0.29881, accuracy: 0.87525\n",
            "Epoch: 23/30, step: 185/364, loss: 0.29885, accuracy: 0.87534\n",
            "Epoch: 23/30, step: 186/364, loss: 0.29910, accuracy: 0.87525\n",
            "Epoch: 23/30, step: 187/364, loss: 0.29918, accuracy: 0.87517\n",
            "Epoch: 23/30, step: 188/364, loss: 0.29892, accuracy: 0.87542\n",
            "Epoch: 23/30, step: 189/364, loss: 0.29862, accuracy: 0.87574\n",
            "Epoch: 23/30, step: 190/364, loss: 0.29857, accuracy: 0.87574\n",
            "Epoch: 23/30, step: 191/364, loss: 0.29861, accuracy: 0.87574\n",
            "Epoch: 23/30, step: 192/364, loss: 0.29957, accuracy: 0.87508\n",
            "Epoch: 23/30, step: 193/364, loss: 0.29994, accuracy: 0.87484\n",
            "Epoch: 23/30, step: 194/364, loss: 0.29968, accuracy: 0.87500\n",
            "Epoch: 23/30, step: 195/364, loss: 0.29977, accuracy: 0.87508\n",
            "Epoch: 23/30, step: 196/364, loss: 0.29963, accuracy: 0.87516\n",
            "Epoch: 23/30, step: 197/364, loss: 0.30008, accuracy: 0.87508\n",
            "Epoch: 23/30, step: 198/364, loss: 0.30026, accuracy: 0.87500\n",
            "Epoch: 23/30, step: 199/364, loss: 0.30012, accuracy: 0.87500\n",
            "Epoch: 23/30, step: 200/364, loss: 0.30026, accuracy: 0.87500\n",
            "Epoch: 23/30, step: 201/364, loss: 0.30027, accuracy: 0.87484\n",
            "Epoch: 23/30, step: 202/364, loss: 0.29969, accuracy: 0.87515\n",
            "Epoch: 23/30, step: 203/364, loss: 0.29946, accuracy: 0.87538\n",
            "Epoch: 23/30, step: 204/364, loss: 0.29924, accuracy: 0.87554\n",
            "Epoch: 23/30, step: 205/364, loss: 0.29915, accuracy: 0.87576\n",
            "Epoch: 23/30, step: 206/364, loss: 0.29878, accuracy: 0.87614\n",
            "Epoch: 23/30, step: 207/364, loss: 0.29837, accuracy: 0.87651\n",
            "Epoch: 23/30, step: 208/364, loss: 0.29789, accuracy: 0.87658\n",
            "Epoch: 23/30, step: 209/364, loss: 0.29808, accuracy: 0.87635\n",
            "Epoch: 23/30, step: 210/364, loss: 0.29835, accuracy: 0.87612\n",
            "Epoch: 23/30, step: 211/364, loss: 0.29840, accuracy: 0.87604\n",
            "Epoch: 23/30, step: 212/364, loss: 0.29825, accuracy: 0.87618\n",
            "Epoch: 23/30, step: 213/364, loss: 0.29785, accuracy: 0.87639\n",
            "Epoch: 23/30, step: 214/364, loss: 0.29745, accuracy: 0.87653\n",
            "Epoch: 23/30, step: 215/364, loss: 0.29776, accuracy: 0.87631\n",
            "Epoch: 23/30, step: 216/364, loss: 0.29826, accuracy: 0.87601\n",
            "Epoch: 23/30, step: 217/364, loss: 0.29795, accuracy: 0.87615\n",
            "Epoch: 23/30, step: 218/364, loss: 0.29777, accuracy: 0.87636\n",
            "Epoch: 23/30, step: 219/364, loss: 0.29752, accuracy: 0.87664\n",
            "Epoch: 23/30, step: 220/364, loss: 0.29755, accuracy: 0.87663\n",
            "Epoch: 23/30, step: 221/364, loss: 0.29795, accuracy: 0.87620\n",
            "Epoch: 23/30, step: 222/364, loss: 0.29795, accuracy: 0.87599\n",
            "Epoch: 23/30, step: 223/364, loss: 0.29762, accuracy: 0.87626\n",
            "Epoch: 23/30, step: 224/364, loss: 0.29776, accuracy: 0.87626\n",
            "Epoch: 23/30, step: 225/364, loss: 0.29811, accuracy: 0.87611\n",
            "Epoch: 23/30, step: 226/364, loss: 0.29822, accuracy: 0.87590\n",
            "Epoch: 23/30, step: 227/364, loss: 0.29807, accuracy: 0.87583\n",
            "Epoch: 23/30, step: 228/364, loss: 0.29834, accuracy: 0.87589\n",
            "Epoch: 23/30, step: 229/364, loss: 0.29824, accuracy: 0.87582\n",
            "Epoch: 23/30, step: 230/364, loss: 0.29828, accuracy: 0.87582\n",
            "Epoch: 23/30, step: 231/364, loss: 0.29832, accuracy: 0.87581\n",
            "Epoch: 23/30, step: 232/364, loss: 0.29806, accuracy: 0.87601\n",
            "Epoch: 23/30, step: 233/364, loss: 0.29807, accuracy: 0.87594\n",
            "Epoch: 23/30, step: 234/364, loss: 0.29764, accuracy: 0.87614\n",
            "Epoch: 23/30, step: 235/364, loss: 0.29744, accuracy: 0.87606\n",
            "Epoch: 23/30, step: 236/364, loss: 0.29739, accuracy: 0.87606\n",
            "Epoch: 23/30, step: 237/364, loss: 0.29731, accuracy: 0.87619\n",
            "Epoch: 23/30, step: 238/364, loss: 0.29722, accuracy: 0.87631\n",
            "Epoch: 23/30, step: 239/364, loss: 0.29739, accuracy: 0.87611\n",
            "Epoch: 23/30, step: 240/364, loss: 0.29721, accuracy: 0.87637\n",
            "Epoch: 23/30, step: 241/364, loss: 0.29721, accuracy: 0.87617\n",
            "Epoch: 23/30, step: 242/364, loss: 0.29708, accuracy: 0.87610\n",
            "Epoch: 23/30, step: 243/364, loss: 0.29679, accuracy: 0.87629\n",
            "Epoch: 23/30, step: 244/364, loss: 0.29636, accuracy: 0.87666\n",
            "Epoch: 23/30, step: 245/364, loss: 0.29611, accuracy: 0.87672\n",
            "Epoch: 23/30, step: 246/364, loss: 0.29605, accuracy: 0.87665\n",
            "Epoch: 23/30, step: 247/364, loss: 0.29599, accuracy: 0.87652\n",
            "Epoch: 23/30, step: 248/364, loss: 0.29581, accuracy: 0.87670\n",
            "Epoch: 23/30, step: 249/364, loss: 0.29628, accuracy: 0.87644\n",
            "Epoch: 23/30, step: 250/364, loss: 0.29601, accuracy: 0.87663\n",
            "Epoch: 23/30, step: 251/364, loss: 0.29572, accuracy: 0.87674\n",
            "Epoch: 23/30, step: 252/364, loss: 0.29580, accuracy: 0.87680\n",
            "Epoch: 23/30, step: 253/364, loss: 0.29572, accuracy: 0.87691\n",
            "Epoch: 23/30, step: 254/364, loss: 0.29574, accuracy: 0.87703\n",
            "Epoch: 23/30, step: 255/364, loss: 0.29541, accuracy: 0.87721\n",
            "Epoch: 23/30, step: 256/364, loss: 0.29530, accuracy: 0.87726\n",
            "Epoch: 23/30, step: 257/364, loss: 0.29613, accuracy: 0.87676\n",
            "Epoch: 23/30, step: 258/364, loss: 0.29577, accuracy: 0.87688\n",
            "Epoch: 23/30, step: 259/364, loss: 0.29584, accuracy: 0.87669\n",
            "Epoch: 23/30, step: 260/364, loss: 0.29559, accuracy: 0.87680\n",
            "Epoch: 23/30, step: 261/364, loss: 0.29586, accuracy: 0.87674\n",
            "Epoch: 23/30, step: 262/364, loss: 0.29575, accuracy: 0.87691\n",
            "Epoch: 23/30, step: 263/364, loss: 0.29628, accuracy: 0.87666\n",
            "Epoch: 23/30, step: 264/364, loss: 0.29616, accuracy: 0.87672\n",
            "Epoch: 23/30, step: 265/364, loss: 0.29645, accuracy: 0.87677\n",
            "Epoch: 23/30, step: 266/364, loss: 0.29644, accuracy: 0.87676\n",
            "Epoch: 23/30, step: 267/364, loss: 0.29641, accuracy: 0.87676\n",
            "Epoch: 23/30, step: 268/364, loss: 0.29623, accuracy: 0.87681\n",
            "Epoch: 23/30, step: 269/364, loss: 0.29646, accuracy: 0.87668\n",
            "Epoch: 23/30, step: 270/364, loss: 0.29621, accuracy: 0.87697\n",
            "Epoch: 23/30, step: 271/364, loss: 0.29620, accuracy: 0.87713\n",
            "Epoch: 23/30, step: 272/364, loss: 0.29596, accuracy: 0.87724\n",
            "Epoch: 23/30, step: 273/364, loss: 0.29566, accuracy: 0.87740\n",
            "Epoch: 23/30, step: 274/364, loss: 0.29607, accuracy: 0.87728\n",
            "Epoch: 23/30, step: 275/364, loss: 0.29599, accuracy: 0.87727\n",
            "Epoch: 23/30, step: 276/364, loss: 0.29553, accuracy: 0.87749\n",
            "Epoch: 23/30, step: 277/364, loss: 0.29530, accuracy: 0.87765\n",
            "Epoch: 23/30, step: 278/364, loss: 0.29503, accuracy: 0.87781\n",
            "Epoch: 23/30, step: 279/364, loss: 0.29479, accuracy: 0.87808\n",
            "Epoch: 23/30, step: 280/364, loss: 0.29497, accuracy: 0.87801\n",
            "Epoch: 23/30, step: 281/364, loss: 0.29520, accuracy: 0.87767\n",
            "Epoch: 23/30, step: 282/364, loss: 0.29555, accuracy: 0.87722\n",
            "Epoch: 23/30, step: 283/364, loss: 0.29524, accuracy: 0.87743\n",
            "Epoch: 23/30, step: 284/364, loss: 0.29578, accuracy: 0.87726\n",
            "Epoch: 23/30, step: 285/364, loss: 0.29615, accuracy: 0.87703\n",
            "Epoch: 23/30, step: 286/364, loss: 0.29640, accuracy: 0.87708\n",
            "Epoch: 23/30, step: 287/364, loss: 0.29648, accuracy: 0.87707\n",
            "Epoch: 23/30, step: 288/364, loss: 0.29640, accuracy: 0.87701\n",
            "Epoch: 23/30, step: 289/364, loss: 0.29661, accuracy: 0.87689\n",
            "Epoch: 23/30, step: 290/364, loss: 0.29639, accuracy: 0.87694\n",
            "Epoch: 23/30, step: 291/364, loss: 0.29633, accuracy: 0.87695\n",
            "Epoch: 23/30, train loss: 0.29633, train accuracy: 0.87695, valid loss: 0.69220, valid accuracy: 0.68465\n",
            "Epoch: 24/30, step: 1/364, loss: 0.25196, accuracy: 0.89062\n",
            "Epoch: 24/30, step: 2/364, loss: 0.30923, accuracy: 0.85938\n",
            "Epoch: 24/30, step: 3/364, loss: 0.33403, accuracy: 0.85938\n",
            "Epoch: 24/30, step: 4/364, loss: 0.31408, accuracy: 0.87500\n",
            "Epoch: 24/30, step: 5/364, loss: 0.31402, accuracy: 0.87187\n",
            "Epoch: 24/30, step: 6/364, loss: 0.31098, accuracy: 0.86719\n",
            "Epoch: 24/30, step: 7/364, loss: 0.30967, accuracy: 0.87054\n",
            "Epoch: 24/30, step: 8/364, loss: 0.30716, accuracy: 0.87305\n",
            "Epoch: 24/30, step: 9/364, loss: 0.30021, accuracy: 0.87674\n",
            "Epoch: 24/30, step: 10/364, loss: 0.28894, accuracy: 0.88437\n",
            "Epoch: 24/30, step: 11/364, loss: 0.28666, accuracy: 0.88494\n",
            "Epoch: 24/30, step: 12/364, loss: 0.29054, accuracy: 0.88151\n",
            "Epoch: 24/30, step: 13/364, loss: 0.30319, accuracy: 0.86779\n",
            "Epoch: 24/30, step: 14/364, loss: 0.30769, accuracy: 0.86607\n",
            "Epoch: 24/30, step: 15/364, loss: 0.30519, accuracy: 0.86875\n",
            "Epoch: 24/30, step: 16/364, loss: 0.30259, accuracy: 0.86914\n",
            "Epoch: 24/30, step: 17/364, loss: 0.29954, accuracy: 0.87132\n",
            "Epoch: 24/30, step: 18/364, loss: 0.29703, accuracy: 0.87326\n",
            "Epoch: 24/30, step: 19/364, loss: 0.29598, accuracy: 0.87500\n",
            "Epoch: 24/30, step: 20/364, loss: 0.28945, accuracy: 0.87969\n",
            "Epoch: 24/30, step: 21/364, loss: 0.28927, accuracy: 0.87946\n",
            "Epoch: 24/30, step: 22/364, loss: 0.29180, accuracy: 0.87855\n",
            "Epoch: 24/30, step: 23/364, loss: 0.28829, accuracy: 0.88247\n",
            "Epoch: 24/30, step: 24/364, loss: 0.28381, accuracy: 0.88607\n",
            "Epoch: 24/30, step: 25/364, loss: 0.28588, accuracy: 0.88437\n",
            "Epoch: 24/30, step: 26/364, loss: 0.28673, accuracy: 0.88281\n",
            "Epoch: 24/30, step: 27/364, loss: 0.29112, accuracy: 0.87847\n",
            "Epoch: 24/30, step: 28/364, loss: 0.29154, accuracy: 0.87723\n",
            "Epoch: 24/30, step: 29/364, loss: 0.28901, accuracy: 0.87985\n",
            "Epoch: 24/30, step: 30/364, loss: 0.28841, accuracy: 0.88021\n",
            "Epoch: 24/30, step: 31/364, loss: 0.28774, accuracy: 0.88105\n",
            "Epoch: 24/30, step: 32/364, loss: 0.29015, accuracy: 0.88135\n",
            "Epoch: 24/30, step: 33/364, loss: 0.29276, accuracy: 0.87926\n",
            "Epoch: 24/30, step: 34/364, loss: 0.29502, accuracy: 0.87914\n",
            "Epoch: 24/30, step: 35/364, loss: 0.29379, accuracy: 0.87857\n",
            "Epoch: 24/30, step: 36/364, loss: 0.29185, accuracy: 0.88064\n",
            "Epoch: 24/30, step: 37/364, loss: 0.29174, accuracy: 0.88091\n",
            "Epoch: 24/30, step: 38/364, loss: 0.29182, accuracy: 0.88158\n",
            "Epoch: 24/30, step: 39/364, loss: 0.29439, accuracy: 0.87981\n",
            "Epoch: 24/30, step: 40/364, loss: 0.29380, accuracy: 0.88047\n",
            "Epoch: 24/30, step: 41/364, loss: 0.29642, accuracy: 0.87957\n",
            "Epoch: 24/30, step: 42/364, loss: 0.29372, accuracy: 0.88132\n",
            "Epoch: 24/30, step: 43/364, loss: 0.29282, accuracy: 0.88227\n",
            "Epoch: 24/30, step: 44/364, loss: 0.29355, accuracy: 0.88210\n",
            "Epoch: 24/30, step: 45/364, loss: 0.29157, accuracy: 0.88333\n",
            "Epoch: 24/30, step: 46/364, loss: 0.29230, accuracy: 0.88145\n",
            "Epoch: 24/30, step: 47/364, loss: 0.29163, accuracy: 0.88165\n",
            "Epoch: 24/30, step: 48/364, loss: 0.28988, accuracy: 0.88281\n",
            "Epoch: 24/30, step: 49/364, loss: 0.28693, accuracy: 0.88425\n",
            "Epoch: 24/30, step: 50/364, loss: 0.28562, accuracy: 0.88469\n",
            "Epoch: 24/30, step: 51/364, loss: 0.28553, accuracy: 0.88480\n",
            "Epoch: 24/30, step: 52/364, loss: 0.28513, accuracy: 0.88552\n",
            "Epoch: 24/30, step: 53/364, loss: 0.28476, accuracy: 0.88591\n",
            "Epoch: 24/30, step: 54/364, loss: 0.28284, accuracy: 0.88715\n",
            "Epoch: 24/30, step: 55/364, loss: 0.28172, accuracy: 0.88750\n",
            "Epoch: 24/30, step: 56/364, loss: 0.28203, accuracy: 0.88728\n",
            "Epoch: 24/30, step: 57/364, loss: 0.28326, accuracy: 0.88651\n",
            "Epoch: 24/30, step: 58/364, loss: 0.28293, accuracy: 0.88631\n",
            "Epoch: 24/30, step: 59/364, loss: 0.28305, accuracy: 0.88586\n",
            "Epoch: 24/30, step: 60/364, loss: 0.28105, accuracy: 0.88724\n",
            "Epoch: 24/30, step: 61/364, loss: 0.28113, accuracy: 0.88730\n",
            "Epoch: 24/30, step: 62/364, loss: 0.28060, accuracy: 0.88710\n",
            "Epoch: 24/30, step: 63/364, loss: 0.28296, accuracy: 0.88517\n",
            "Epoch: 24/30, step: 64/364, loss: 0.28323, accuracy: 0.88501\n",
            "Epoch: 24/30, step: 65/364, loss: 0.28358, accuracy: 0.88510\n",
            "Epoch: 24/30, step: 66/364, loss: 0.28482, accuracy: 0.88423\n",
            "Epoch: 24/30, step: 67/364, loss: 0.28423, accuracy: 0.88456\n",
            "Epoch: 24/30, step: 68/364, loss: 0.28438, accuracy: 0.88442\n",
            "Epoch: 24/30, step: 69/364, loss: 0.28436, accuracy: 0.88428\n",
            "Epoch: 24/30, step: 70/364, loss: 0.28371, accuracy: 0.88482\n",
            "Epoch: 24/30, step: 71/364, loss: 0.28361, accuracy: 0.88556\n",
            "Epoch: 24/30, step: 72/364, loss: 0.28343, accuracy: 0.88520\n",
            "Epoch: 24/30, step: 73/364, loss: 0.28399, accuracy: 0.88485\n",
            "Epoch: 24/30, step: 74/364, loss: 0.28254, accuracy: 0.88598\n",
            "Epoch: 24/30, step: 75/364, loss: 0.28184, accuracy: 0.88646\n",
            "Epoch: 24/30, step: 76/364, loss: 0.28284, accuracy: 0.88528\n",
            "Epoch: 24/30, step: 77/364, loss: 0.28171, accuracy: 0.88596\n",
            "Epoch: 24/30, step: 78/364, loss: 0.28160, accuracy: 0.88602\n",
            "Epoch: 24/30, step: 79/364, loss: 0.28242, accuracy: 0.88509\n",
            "Epoch: 24/30, step: 80/364, loss: 0.28178, accuracy: 0.88555\n",
            "Epoch: 24/30, step: 81/364, loss: 0.28344, accuracy: 0.88445\n",
            "Epoch: 24/30, step: 82/364, loss: 0.28276, accuracy: 0.88453\n",
            "Epoch: 24/30, step: 83/364, loss: 0.28215, accuracy: 0.88479\n",
            "Epoch: 24/30, step: 84/364, loss: 0.28351, accuracy: 0.88430\n",
            "Epoch: 24/30, step: 85/364, loss: 0.28373, accuracy: 0.88364\n",
            "Epoch: 24/30, step: 86/364, loss: 0.28307, accuracy: 0.88390\n",
            "Epoch: 24/30, step: 87/364, loss: 0.28282, accuracy: 0.88416\n",
            "Epoch: 24/30, step: 88/364, loss: 0.28208, accuracy: 0.88477\n",
            "Epoch: 24/30, step: 89/364, loss: 0.28222, accuracy: 0.88518\n",
            "Epoch: 24/30, step: 90/364, loss: 0.28276, accuracy: 0.88524\n",
            "Epoch: 24/30, step: 91/364, loss: 0.28229, accuracy: 0.88565\n",
            "Epoch: 24/30, step: 92/364, loss: 0.28165, accuracy: 0.88621\n",
            "Epoch: 24/30, step: 93/364, loss: 0.28258, accuracy: 0.88575\n",
            "Epoch: 24/30, step: 94/364, loss: 0.28391, accuracy: 0.88481\n",
            "Epoch: 24/30, step: 95/364, loss: 0.28368, accuracy: 0.88520\n",
            "Epoch: 24/30, step: 96/364, loss: 0.28544, accuracy: 0.88395\n",
            "Epoch: 24/30, step: 97/364, loss: 0.28426, accuracy: 0.88466\n",
            "Epoch: 24/30, step: 98/364, loss: 0.28348, accuracy: 0.88520\n",
            "Epoch: 24/30, step: 99/364, loss: 0.28369, accuracy: 0.88510\n",
            "Epoch: 24/30, step: 100/364, loss: 0.28426, accuracy: 0.88484\n",
            "Epoch: 24/30, step: 101/364, loss: 0.28388, accuracy: 0.88490\n",
            "Epoch: 24/30, step: 102/364, loss: 0.28367, accuracy: 0.88480\n",
            "Epoch: 24/30, step: 103/364, loss: 0.28295, accuracy: 0.88532\n",
            "Epoch: 24/30, step: 104/364, loss: 0.28272, accuracy: 0.88552\n",
            "Epoch: 24/30, step: 105/364, loss: 0.28386, accuracy: 0.88467\n",
            "Epoch: 24/30, step: 106/364, loss: 0.28387, accuracy: 0.88429\n",
            "Epoch: 24/30, step: 107/364, loss: 0.28344, accuracy: 0.88508\n",
            "Epoch: 24/30, step: 108/364, loss: 0.28252, accuracy: 0.88556\n",
            "Epoch: 24/30, step: 109/364, loss: 0.28171, accuracy: 0.88604\n",
            "Epoch: 24/30, step: 110/364, loss: 0.28104, accuracy: 0.88665\n",
            "Epoch: 24/30, step: 111/364, loss: 0.28148, accuracy: 0.88640\n",
            "Epoch: 24/30, step: 112/364, loss: 0.28102, accuracy: 0.88686\n",
            "Epoch: 24/30, step: 113/364, loss: 0.28026, accuracy: 0.88758\n",
            "Epoch: 24/30, step: 114/364, loss: 0.28083, accuracy: 0.88775\n",
            "Epoch: 24/30, step: 115/364, loss: 0.28038, accuracy: 0.88804\n",
            "Epoch: 24/30, step: 116/364, loss: 0.28007, accuracy: 0.88807\n",
            "Epoch: 24/30, step: 117/364, loss: 0.28097, accuracy: 0.88795\n",
            "Epoch: 24/30, step: 118/364, loss: 0.28182, accuracy: 0.88784\n",
            "Epoch: 24/30, step: 119/364, loss: 0.28147, accuracy: 0.88761\n",
            "Epoch: 24/30, step: 120/364, loss: 0.28319, accuracy: 0.88646\n",
            "Epoch: 24/30, step: 121/364, loss: 0.28238, accuracy: 0.88714\n",
            "Epoch: 24/30, step: 122/364, loss: 0.28184, accuracy: 0.88755\n",
            "Epoch: 24/30, step: 123/364, loss: 0.28160, accuracy: 0.88783\n",
            "Epoch: 24/30, step: 124/364, loss: 0.28295, accuracy: 0.88697\n",
            "Epoch: 24/30, step: 125/364, loss: 0.28221, accuracy: 0.88750\n",
            "Epoch: 24/30, step: 126/364, loss: 0.28137, accuracy: 0.88802\n",
            "Epoch: 24/30, step: 127/364, loss: 0.28199, accuracy: 0.88804\n",
            "Epoch: 24/30, step: 128/364, loss: 0.28158, accuracy: 0.88843\n",
            "Epoch: 24/30, step: 129/364, loss: 0.28249, accuracy: 0.88784\n",
            "Epoch: 24/30, step: 130/364, loss: 0.28248, accuracy: 0.88786\n",
            "Epoch: 24/30, step: 131/364, loss: 0.28282, accuracy: 0.88740\n",
            "Epoch: 24/30, step: 132/364, loss: 0.28256, accuracy: 0.88755\n",
            "Epoch: 24/30, step: 133/364, loss: 0.28320, accuracy: 0.88734\n",
            "Epoch: 24/30, step: 134/364, loss: 0.28296, accuracy: 0.88724\n",
            "Epoch: 24/30, step: 135/364, loss: 0.28304, accuracy: 0.88738\n",
            "Epoch: 24/30, step: 136/364, loss: 0.28276, accuracy: 0.88764\n",
            "Epoch: 24/30, step: 137/364, loss: 0.28288, accuracy: 0.88777\n",
            "Epoch: 24/30, step: 138/364, loss: 0.28228, accuracy: 0.88825\n",
            "Epoch: 24/30, step: 139/364, loss: 0.28265, accuracy: 0.88781\n",
            "Epoch: 24/30, step: 140/364, loss: 0.28277, accuracy: 0.88750\n",
            "Epoch: 24/30, step: 141/364, loss: 0.28238, accuracy: 0.88774\n",
            "Epoch: 24/30, step: 142/364, loss: 0.28232, accuracy: 0.88776\n",
            "Epoch: 24/30, step: 143/364, loss: 0.28207, accuracy: 0.88767\n",
            "Epoch: 24/30, step: 144/364, loss: 0.28249, accuracy: 0.88748\n",
            "Epoch: 24/30, step: 145/364, loss: 0.28290, accuracy: 0.88718\n",
            "Epoch: 24/30, step: 146/364, loss: 0.28287, accuracy: 0.88699\n",
            "Epoch: 24/30, step: 147/364, loss: 0.28303, accuracy: 0.88690\n",
            "Epoch: 24/30, step: 148/364, loss: 0.28251, accuracy: 0.88746\n",
            "Epoch: 24/30, step: 149/364, loss: 0.28266, accuracy: 0.88706\n",
            "Epoch: 24/30, step: 150/364, loss: 0.28255, accuracy: 0.88719\n",
            "Epoch: 24/30, step: 151/364, loss: 0.28196, accuracy: 0.88752\n",
            "Epoch: 24/30, step: 152/364, loss: 0.28177, accuracy: 0.88764\n",
            "Epoch: 24/30, step: 153/364, loss: 0.28209, accuracy: 0.88766\n",
            "Epoch: 24/30, step: 154/364, loss: 0.28210, accuracy: 0.88748\n",
            "Epoch: 24/30, step: 155/364, loss: 0.28186, accuracy: 0.88740\n",
            "Epoch: 24/30, step: 156/364, loss: 0.28135, accuracy: 0.88782\n",
            "Epoch: 24/30, step: 157/364, loss: 0.28217, accuracy: 0.88754\n",
            "Epoch: 24/30, step: 158/364, loss: 0.28236, accuracy: 0.88726\n",
            "Epoch: 24/30, step: 159/364, loss: 0.28293, accuracy: 0.88689\n",
            "Epoch: 24/30, step: 160/364, loss: 0.28350, accuracy: 0.88682\n",
            "Epoch: 24/30, step: 161/364, loss: 0.28388, accuracy: 0.88626\n",
            "Epoch: 24/30, step: 162/364, loss: 0.28340, accuracy: 0.88657\n",
            "Epoch: 24/30, step: 163/364, loss: 0.28336, accuracy: 0.88641\n",
            "Epoch: 24/30, step: 164/364, loss: 0.28360, accuracy: 0.88672\n",
            "Epoch: 24/30, step: 165/364, loss: 0.28338, accuracy: 0.88684\n",
            "Epoch: 24/30, step: 166/364, loss: 0.28358, accuracy: 0.88677\n",
            "Epoch: 24/30, step: 167/364, loss: 0.28362, accuracy: 0.88670\n",
            "Epoch: 24/30, step: 168/364, loss: 0.28371, accuracy: 0.88663\n",
            "Epoch: 24/30, step: 169/364, loss: 0.28398, accuracy: 0.88637\n",
            "Epoch: 24/30, step: 170/364, loss: 0.28394, accuracy: 0.88621\n",
            "Epoch: 24/30, step: 171/364, loss: 0.28413, accuracy: 0.88615\n",
            "Epoch: 24/30, step: 172/364, loss: 0.28414, accuracy: 0.88608\n",
            "Epoch: 24/30, step: 173/364, loss: 0.28402, accuracy: 0.88629\n",
            "Epoch: 24/30, step: 174/364, loss: 0.28399, accuracy: 0.88631\n",
            "Epoch: 24/30, step: 175/364, loss: 0.28427, accuracy: 0.88607\n",
            "Epoch: 24/30, step: 176/364, loss: 0.28433, accuracy: 0.88610\n",
            "Epoch: 24/30, step: 177/364, loss: 0.28411, accuracy: 0.88639\n",
            "Epoch: 24/30, step: 178/364, loss: 0.28378, accuracy: 0.88650\n",
            "Epoch: 24/30, step: 179/364, loss: 0.28340, accuracy: 0.88661\n",
            "Epoch: 24/30, step: 180/364, loss: 0.28413, accuracy: 0.88602\n",
            "Epoch: 24/30, step: 181/364, loss: 0.28371, accuracy: 0.88622\n",
            "Epoch: 24/30, step: 182/364, loss: 0.28387, accuracy: 0.88625\n",
            "Epoch: 24/30, step: 183/364, loss: 0.28393, accuracy: 0.88601\n",
            "Epoch: 24/30, step: 184/364, loss: 0.28431, accuracy: 0.88587\n",
            "Epoch: 24/30, step: 185/364, loss: 0.28390, accuracy: 0.88590\n",
            "Epoch: 24/30, step: 186/364, loss: 0.28366, accuracy: 0.88617\n",
            "Epoch: 24/30, step: 187/364, loss: 0.28344, accuracy: 0.88628\n",
            "Epoch: 24/30, step: 188/364, loss: 0.28332, accuracy: 0.88630\n",
            "Epoch: 24/30, step: 189/364, loss: 0.28311, accuracy: 0.88633\n",
            "Epoch: 24/30, step: 190/364, loss: 0.28343, accuracy: 0.88627\n",
            "Epoch: 24/30, step: 191/364, loss: 0.28305, accuracy: 0.88670\n",
            "Epoch: 24/30, step: 192/364, loss: 0.28277, accuracy: 0.88680\n",
            "Epoch: 24/30, step: 193/364, loss: 0.28237, accuracy: 0.88706\n",
            "Epoch: 24/30, step: 194/364, loss: 0.28187, accuracy: 0.88740\n",
            "Epoch: 24/30, step: 195/364, loss: 0.28186, accuracy: 0.88734\n",
            "Epoch: 24/30, step: 196/364, loss: 0.28217, accuracy: 0.88728\n",
            "Epoch: 24/30, step: 197/364, loss: 0.28154, accuracy: 0.88761\n",
            "Epoch: 24/30, step: 198/364, loss: 0.28176, accuracy: 0.88747\n",
            "Epoch: 24/30, step: 199/364, loss: 0.28184, accuracy: 0.88733\n",
            "Epoch: 24/30, step: 200/364, loss: 0.28157, accuracy: 0.88750\n",
            "Epoch: 24/30, step: 201/364, loss: 0.28098, accuracy: 0.88798\n",
            "Epoch: 24/30, step: 202/364, loss: 0.28223, accuracy: 0.88730\n",
            "Epoch: 24/30, step: 203/364, loss: 0.28251, accuracy: 0.88701\n",
            "Epoch: 24/30, step: 204/364, loss: 0.28245, accuracy: 0.88703\n",
            "Epoch: 24/30, step: 205/364, loss: 0.28231, accuracy: 0.88697\n",
            "Epoch: 24/30, step: 206/364, loss: 0.28235, accuracy: 0.88683\n",
            "Epoch: 24/30, step: 207/364, loss: 0.28228, accuracy: 0.88685\n",
            "Epoch: 24/30, step: 208/364, loss: 0.28176, accuracy: 0.88717\n",
            "Epoch: 24/30, step: 209/364, loss: 0.28171, accuracy: 0.88719\n",
            "Epoch: 24/30, step: 210/364, loss: 0.28155, accuracy: 0.88720\n",
            "Epoch: 24/30, step: 211/364, loss: 0.28113, accuracy: 0.88737\n",
            "Epoch: 24/30, step: 212/364, loss: 0.28050, accuracy: 0.88768\n",
            "Epoch: 24/30, step: 213/364, loss: 0.28021, accuracy: 0.88769\n",
            "Epoch: 24/30, step: 214/364, loss: 0.28050, accuracy: 0.88719\n",
            "Epoch: 24/30, step: 215/364, loss: 0.28031, accuracy: 0.88743\n",
            "Epoch: 24/30, step: 216/364, loss: 0.27982, accuracy: 0.88773\n",
            "Epoch: 24/30, step: 217/364, loss: 0.27947, accuracy: 0.88796\n",
            "Epoch: 24/30, step: 218/364, loss: 0.27916, accuracy: 0.88812\n",
            "Epoch: 24/30, step: 219/364, loss: 0.27871, accuracy: 0.88841\n",
            "Epoch: 24/30, step: 220/364, loss: 0.27853, accuracy: 0.88842\n",
            "Epoch: 24/30, step: 221/364, loss: 0.27853, accuracy: 0.88843\n",
            "Epoch: 24/30, step: 222/364, loss: 0.27822, accuracy: 0.88851\n",
            "Epoch: 24/30, step: 223/364, loss: 0.27829, accuracy: 0.88845\n",
            "Epoch: 24/30, step: 224/364, loss: 0.27843, accuracy: 0.88846\n",
            "Epoch: 24/30, step: 225/364, loss: 0.27821, accuracy: 0.88861\n",
            "Epoch: 24/30, step: 226/364, loss: 0.27824, accuracy: 0.88876\n",
            "Epoch: 24/30, step: 227/364, loss: 0.27833, accuracy: 0.88863\n",
            "Epoch: 24/30, step: 228/364, loss: 0.27798, accuracy: 0.88884\n",
            "Epoch: 24/30, step: 229/364, loss: 0.27827, accuracy: 0.88865\n",
            "Epoch: 24/30, step: 230/364, loss: 0.27843, accuracy: 0.88838\n",
            "Epoch: 24/30, step: 231/364, loss: 0.27795, accuracy: 0.88866\n",
            "Epoch: 24/30, step: 232/364, loss: 0.27756, accuracy: 0.88887\n",
            "Epoch: 24/30, step: 233/364, loss: 0.27761, accuracy: 0.88902\n",
            "Epoch: 24/30, step: 234/364, loss: 0.27746, accuracy: 0.88922\n",
            "Epoch: 24/30, step: 235/364, loss: 0.27745, accuracy: 0.88943\n",
            "Epoch: 24/30, step: 236/364, loss: 0.27755, accuracy: 0.88943\n",
            "Epoch: 24/30, step: 237/364, loss: 0.27810, accuracy: 0.88911\n",
            "Epoch: 24/30, step: 238/364, loss: 0.27788, accuracy: 0.88925\n",
            "Epoch: 24/30, step: 239/364, loss: 0.27778, accuracy: 0.88938\n",
            "Epoch: 24/30, step: 240/364, loss: 0.27777, accuracy: 0.88932\n",
            "Epoch: 24/30, step: 241/364, loss: 0.27749, accuracy: 0.88959\n",
            "Epoch: 24/30, step: 242/364, loss: 0.27766, accuracy: 0.88940\n",
            "Epoch: 24/30, step: 243/364, loss: 0.27784, accuracy: 0.88927\n",
            "Epoch: 24/30, step: 244/364, loss: 0.27782, accuracy: 0.88915\n",
            "Epoch: 24/30, step: 245/364, loss: 0.27764, accuracy: 0.88929\n",
            "Epoch: 24/30, step: 246/364, loss: 0.27770, accuracy: 0.88923\n",
            "Epoch: 24/30, step: 247/364, loss: 0.27744, accuracy: 0.88930\n",
            "Epoch: 24/30, step: 248/364, loss: 0.27754, accuracy: 0.88924\n",
            "Epoch: 24/30, step: 249/364, loss: 0.27740, accuracy: 0.88950\n",
            "Epoch: 24/30, step: 250/364, loss: 0.27729, accuracy: 0.88937\n",
            "Epoch: 24/30, step: 251/364, loss: 0.27726, accuracy: 0.88926\n",
            "Epoch: 24/30, step: 252/364, loss: 0.27741, accuracy: 0.88926\n",
            "Epoch: 24/30, step: 253/364, loss: 0.27751, accuracy: 0.88908\n",
            "Epoch: 24/30, step: 254/364, loss: 0.27736, accuracy: 0.88921\n",
            "Epoch: 24/30, step: 255/364, loss: 0.27743, accuracy: 0.88909\n",
            "Epoch: 24/30, step: 256/364, loss: 0.27721, accuracy: 0.88934\n",
            "Epoch: 24/30, step: 257/364, loss: 0.27720, accuracy: 0.88935\n",
            "Epoch: 24/30, step: 258/364, loss: 0.27745, accuracy: 0.88911\n",
            "Epoch: 24/30, step: 259/364, loss: 0.27743, accuracy: 0.88918\n",
            "Epoch: 24/30, step: 260/364, loss: 0.27734, accuracy: 0.88930\n",
            "Epoch: 24/30, step: 261/364, loss: 0.27694, accuracy: 0.88955\n",
            "Epoch: 24/30, step: 262/364, loss: 0.27710, accuracy: 0.88943\n",
            "Epoch: 24/30, step: 263/364, loss: 0.27689, accuracy: 0.88956\n",
            "Epoch: 24/30, step: 264/364, loss: 0.27699, accuracy: 0.88932\n",
            "Epoch: 24/30, step: 265/364, loss: 0.27706, accuracy: 0.88939\n",
            "Epoch: 24/30, step: 266/364, loss: 0.27681, accuracy: 0.88939\n",
            "Epoch: 24/30, step: 267/364, loss: 0.27669, accuracy: 0.88951\n",
            "Epoch: 24/30, step: 268/364, loss: 0.27654, accuracy: 0.88963\n",
            "Epoch: 24/30, step: 269/364, loss: 0.27614, accuracy: 0.88999\n",
            "Epoch: 24/30, step: 270/364, loss: 0.27606, accuracy: 0.88993\n",
            "Epoch: 24/30, step: 271/364, loss: 0.27603, accuracy: 0.88999\n",
            "Epoch: 24/30, step: 272/364, loss: 0.27587, accuracy: 0.89005\n",
            "Epoch: 24/30, step: 273/364, loss: 0.27545, accuracy: 0.89022\n",
            "Epoch: 24/30, step: 274/364, loss: 0.27519, accuracy: 0.89034\n",
            "Epoch: 24/30, step: 275/364, loss: 0.27530, accuracy: 0.89040\n",
            "Epoch: 24/30, step: 276/364, loss: 0.27525, accuracy: 0.89046\n",
            "Epoch: 24/30, step: 277/364, loss: 0.27583, accuracy: 0.89023\n",
            "Epoch: 24/30, step: 278/364, loss: 0.27567, accuracy: 0.89029\n",
            "Epoch: 24/30, step: 279/364, loss: 0.27589, accuracy: 0.89012\n",
            "Epoch: 24/30, step: 280/364, loss: 0.27593, accuracy: 0.88996\n",
            "Epoch: 24/30, step: 281/364, loss: 0.27609, accuracy: 0.88974\n",
            "Epoch: 24/30, step: 282/364, loss: 0.27595, accuracy: 0.88985\n",
            "Epoch: 24/30, step: 283/364, loss: 0.27577, accuracy: 0.88996\n",
            "Epoch: 24/30, step: 284/364, loss: 0.27577, accuracy: 0.88985\n",
            "Epoch: 24/30, step: 285/364, loss: 0.27547, accuracy: 0.89008\n",
            "Epoch: 24/30, step: 286/364, loss: 0.27548, accuracy: 0.89019\n",
            "Epoch: 24/30, step: 287/364, loss: 0.27515, accuracy: 0.89030\n",
            "Epoch: 24/30, step: 288/364, loss: 0.27518, accuracy: 0.89019\n",
            "Epoch: 24/30, step: 289/364, loss: 0.27523, accuracy: 0.89019\n",
            "Epoch: 24/30, step: 290/364, loss: 0.27485, accuracy: 0.89052\n",
            "Epoch: 24/30, step: 291/364, loss: 0.27470, accuracy: 0.89065\n",
            "Epoch: 24/30, train loss: 0.27470, train accuracy: 0.89065, valid loss: 0.66431, valid accuracy: 0.69819\n",
            "Epoch: 25/30, step: 1/364, loss: 0.29108, accuracy: 0.85938\n",
            "Epoch: 25/30, step: 2/364, loss: 0.26885, accuracy: 0.89062\n",
            "Epoch: 25/30, step: 3/364, loss: 0.25282, accuracy: 0.89583\n",
            "Epoch: 25/30, step: 4/364, loss: 0.30485, accuracy: 0.85547\n",
            "Epoch: 25/30, step: 5/364, loss: 0.30632, accuracy: 0.86563\n",
            "Epoch: 25/30, step: 6/364, loss: 0.30062, accuracy: 0.86979\n",
            "Epoch: 25/30, step: 7/364, loss: 0.29309, accuracy: 0.87946\n",
            "Epoch: 25/30, step: 8/364, loss: 0.28091, accuracy: 0.88672\n",
            "Epoch: 25/30, step: 9/364, loss: 0.28261, accuracy: 0.88542\n",
            "Epoch: 25/30, step: 10/364, loss: 0.27137, accuracy: 0.89375\n",
            "Epoch: 25/30, step: 11/364, loss: 0.27455, accuracy: 0.89062\n",
            "Epoch: 25/30, step: 12/364, loss: 0.26842, accuracy: 0.89193\n",
            "Epoch: 25/30, step: 13/364, loss: 0.26517, accuracy: 0.89543\n",
            "Epoch: 25/30, step: 14/364, loss: 0.26426, accuracy: 0.89397\n",
            "Epoch: 25/30, step: 15/364, loss: 0.26194, accuracy: 0.89688\n",
            "Epoch: 25/30, step: 16/364, loss: 0.25685, accuracy: 0.90039\n",
            "Epoch: 25/30, step: 17/364, loss: 0.25302, accuracy: 0.90257\n",
            "Epoch: 25/30, step: 18/364, loss: 0.24993, accuracy: 0.90365\n",
            "Epoch: 25/30, step: 19/364, loss: 0.25346, accuracy: 0.90214\n",
            "Epoch: 25/30, step: 20/364, loss: 0.25295, accuracy: 0.90312\n",
            "Epoch: 25/30, step: 21/364, loss: 0.25324, accuracy: 0.90104\n",
            "Epoch: 25/30, step: 22/364, loss: 0.26698, accuracy: 0.89062\n",
            "Epoch: 25/30, step: 23/364, loss: 0.26545, accuracy: 0.89266\n",
            "Epoch: 25/30, step: 24/364, loss: 0.26656, accuracy: 0.89128\n",
            "Epoch: 25/30, step: 25/364, loss: 0.27291, accuracy: 0.88563\n",
            "Epoch: 25/30, step: 26/364, loss: 0.27404, accuracy: 0.88462\n",
            "Epoch: 25/30, step: 27/364, loss: 0.27225, accuracy: 0.88542\n",
            "Epoch: 25/30, step: 28/364, loss: 0.27083, accuracy: 0.88728\n",
            "Epoch: 25/30, step: 29/364, loss: 0.27178, accuracy: 0.88847\n",
            "Epoch: 25/30, step: 30/364, loss: 0.27409, accuracy: 0.88906\n",
            "Epoch: 25/30, step: 31/364, loss: 0.27613, accuracy: 0.88861\n",
            "Epoch: 25/30, step: 32/364, loss: 0.27310, accuracy: 0.89111\n",
            "Epoch: 25/30, step: 33/364, loss: 0.27052, accuracy: 0.89347\n",
            "Epoch: 25/30, step: 34/364, loss: 0.26958, accuracy: 0.89476\n",
            "Epoch: 25/30, step: 35/364, loss: 0.26905, accuracy: 0.89554\n",
            "Epoch: 25/30, step: 36/364, loss: 0.26849, accuracy: 0.89627\n",
            "Epoch: 25/30, step: 37/364, loss: 0.26645, accuracy: 0.89738\n",
            "Epoch: 25/30, step: 38/364, loss: 0.26628, accuracy: 0.89720\n",
            "Epoch: 25/30, step: 39/364, loss: 0.26531, accuracy: 0.89744\n",
            "Epoch: 25/30, step: 40/364, loss: 0.26706, accuracy: 0.89648\n",
            "Epoch: 25/30, step: 41/364, loss: 0.26599, accuracy: 0.89710\n",
            "Epoch: 25/30, step: 42/364, loss: 0.26459, accuracy: 0.89732\n",
            "Epoch: 25/30, step: 43/364, loss: 0.26514, accuracy: 0.89753\n",
            "Epoch: 25/30, step: 44/364, loss: 0.26299, accuracy: 0.89915\n",
            "Epoch: 25/30, step: 45/364, loss: 0.26222, accuracy: 0.89896\n",
            "Epoch: 25/30, step: 46/364, loss: 0.26068, accuracy: 0.89980\n",
            "Epoch: 25/30, step: 47/364, loss: 0.26093, accuracy: 0.89993\n",
            "Epoch: 25/30, step: 48/364, loss: 0.26599, accuracy: 0.89551\n",
            "Epoch: 25/30, step: 49/364, loss: 0.26522, accuracy: 0.89605\n",
            "Epoch: 25/30, step: 50/364, loss: 0.26496, accuracy: 0.89625\n",
            "Epoch: 25/30, step: 51/364, loss: 0.26588, accuracy: 0.89553\n",
            "Epoch: 25/30, step: 52/364, loss: 0.26659, accuracy: 0.89513\n",
            "Epoch: 25/30, step: 53/364, loss: 0.26427, accuracy: 0.89652\n",
            "Epoch: 25/30, step: 54/364, loss: 0.26456, accuracy: 0.89612\n",
            "Epoch: 25/30, step: 55/364, loss: 0.26243, accuracy: 0.89688\n",
            "Epoch: 25/30, step: 56/364, loss: 0.26093, accuracy: 0.89760\n",
            "Epoch: 25/30, step: 57/364, loss: 0.26161, accuracy: 0.89693\n",
            "Epoch: 25/30, step: 58/364, loss: 0.26171, accuracy: 0.89709\n",
            "Epoch: 25/30, step: 59/364, loss: 0.26217, accuracy: 0.89698\n",
            "Epoch: 25/30, step: 60/364, loss: 0.26276, accuracy: 0.89609\n",
            "Epoch: 25/30, step: 61/364, loss: 0.26187, accuracy: 0.89677\n",
            "Epoch: 25/30, step: 62/364, loss: 0.26029, accuracy: 0.89793\n",
            "Epoch: 25/30, step: 63/364, loss: 0.25910, accuracy: 0.89881\n",
            "Epoch: 25/30, step: 64/364, loss: 0.25936, accuracy: 0.89917\n",
            "Epoch: 25/30, step: 65/364, loss: 0.25977, accuracy: 0.89832\n",
            "Epoch: 25/30, step: 66/364, loss: 0.26008, accuracy: 0.89844\n",
            "Epoch: 25/30, step: 67/364, loss: 0.25907, accuracy: 0.89902\n",
            "Epoch: 25/30, step: 68/364, loss: 0.25820, accuracy: 0.89982\n",
            "Epoch: 25/30, step: 69/364, loss: 0.25902, accuracy: 0.89991\n",
            "Epoch: 25/30, step: 70/364, loss: 0.25874, accuracy: 0.90000\n",
            "Epoch: 25/30, step: 71/364, loss: 0.26080, accuracy: 0.89943\n",
            "Epoch: 25/30, step: 72/364, loss: 0.26037, accuracy: 0.89974\n",
            "Epoch: 25/30, step: 73/364, loss: 0.26184, accuracy: 0.89854\n",
            "Epoch: 25/30, step: 74/364, loss: 0.26239, accuracy: 0.89802\n",
            "Epoch: 25/30, step: 75/364, loss: 0.26229, accuracy: 0.89812\n",
            "Epoch: 25/30, step: 76/364, loss: 0.26190, accuracy: 0.89823\n",
            "Epoch: 25/30, step: 77/364, loss: 0.26182, accuracy: 0.89854\n",
            "Epoch: 25/30, step: 78/364, loss: 0.26154, accuracy: 0.89884\n",
            "Epoch: 25/30, step: 79/364, loss: 0.26272, accuracy: 0.89814\n",
            "Epoch: 25/30, step: 80/364, loss: 0.26239, accuracy: 0.89824\n",
            "Epoch: 25/30, step: 81/364, loss: 0.26143, accuracy: 0.89853\n",
            "Epoch: 25/30, step: 82/364, loss: 0.26131, accuracy: 0.89863\n",
            "Epoch: 25/30, step: 83/364, loss: 0.26167, accuracy: 0.89853\n",
            "Epoch: 25/30, step: 84/364, loss: 0.26377, accuracy: 0.89732\n",
            "Epoch: 25/30, step: 85/364, loss: 0.26364, accuracy: 0.89779\n",
            "Epoch: 25/30, step: 86/364, loss: 0.26225, accuracy: 0.89862\n",
            "Epoch: 25/30, step: 87/364, loss: 0.26399, accuracy: 0.89691\n",
            "Epoch: 25/30, step: 88/364, loss: 0.26402, accuracy: 0.89684\n",
            "Epoch: 25/30, step: 89/364, loss: 0.26474, accuracy: 0.89624\n",
            "Epoch: 25/30, step: 90/364, loss: 0.26520, accuracy: 0.89549\n",
            "Epoch: 25/30, step: 91/364, loss: 0.26625, accuracy: 0.89509\n",
            "Epoch: 25/30, step: 92/364, loss: 0.26504, accuracy: 0.89555\n",
            "Epoch: 25/30, step: 93/364, loss: 0.26423, accuracy: 0.89583\n",
            "Epoch: 25/30, step: 94/364, loss: 0.26294, accuracy: 0.89644\n",
            "Epoch: 25/30, step: 95/364, loss: 0.26293, accuracy: 0.89638\n",
            "Epoch: 25/30, step: 96/364, loss: 0.26390, accuracy: 0.89616\n",
            "Epoch: 25/30, step: 97/364, loss: 0.26518, accuracy: 0.89546\n",
            "Epoch: 25/30, step: 98/364, loss: 0.26538, accuracy: 0.89557\n",
            "Epoch: 25/30, step: 99/364, loss: 0.26586, accuracy: 0.89520\n",
            "Epoch: 25/30, step: 100/364, loss: 0.26775, accuracy: 0.89438\n",
            "Epoch: 25/30, step: 101/364, loss: 0.26781, accuracy: 0.89403\n",
            "Epoch: 25/30, step: 102/364, loss: 0.26760, accuracy: 0.89430\n",
            "Epoch: 25/30, step: 103/364, loss: 0.26723, accuracy: 0.89457\n",
            "Epoch: 25/30, step: 104/364, loss: 0.26771, accuracy: 0.89408\n",
            "Epoch: 25/30, step: 105/364, loss: 0.26794, accuracy: 0.89405\n",
            "Epoch: 25/30, step: 106/364, loss: 0.26736, accuracy: 0.89460\n",
            "Epoch: 25/30, step: 107/364, loss: 0.26817, accuracy: 0.89355\n",
            "Epoch: 25/30, step: 108/364, loss: 0.26829, accuracy: 0.89366\n",
            "Epoch: 25/30, step: 109/364, loss: 0.26837, accuracy: 0.89378\n",
            "Epoch: 25/30, step: 110/364, loss: 0.26789, accuracy: 0.89389\n",
            "Epoch: 25/30, step: 111/364, loss: 0.26840, accuracy: 0.89372\n",
            "Epoch: 25/30, step: 112/364, loss: 0.26874, accuracy: 0.89369\n",
            "Epoch: 25/30, step: 113/364, loss: 0.26906, accuracy: 0.89311\n",
            "Epoch: 25/30, step: 114/364, loss: 0.26856, accuracy: 0.89337\n",
            "Epoch: 25/30, step: 115/364, loss: 0.26857, accuracy: 0.89334\n",
            "Epoch: 25/30, step: 116/364, loss: 0.26802, accuracy: 0.89399\n",
            "Epoch: 25/30, step: 117/364, loss: 0.26790, accuracy: 0.89410\n",
            "Epoch: 25/30, step: 118/364, loss: 0.26834, accuracy: 0.89407\n",
            "Epoch: 25/30, step: 119/364, loss: 0.26812, accuracy: 0.89430\n",
            "Epoch: 25/30, step: 120/364, loss: 0.26834, accuracy: 0.89414\n",
            "Epoch: 25/30, step: 121/364, loss: 0.26836, accuracy: 0.89424\n",
            "Epoch: 25/30, step: 122/364, loss: 0.26826, accuracy: 0.89434\n",
            "Epoch: 25/30, step: 123/364, loss: 0.26820, accuracy: 0.89444\n",
            "Epoch: 25/30, step: 124/364, loss: 0.26811, accuracy: 0.89428\n",
            "Epoch: 25/30, step: 125/364, loss: 0.26868, accuracy: 0.89388\n",
            "Epoch: 25/30, step: 126/364, loss: 0.26900, accuracy: 0.89348\n",
            "Epoch: 25/30, step: 127/364, loss: 0.26876, accuracy: 0.89321\n",
            "Epoch: 25/30, step: 128/364, loss: 0.26789, accuracy: 0.89368\n",
            "Epoch: 25/30, step: 129/364, loss: 0.26719, accuracy: 0.89390\n",
            "Epoch: 25/30, step: 130/364, loss: 0.26680, accuracy: 0.89423\n",
            "Epoch: 25/30, step: 131/364, loss: 0.26637, accuracy: 0.89444\n",
            "Epoch: 25/30, step: 132/364, loss: 0.26586, accuracy: 0.89477\n",
            "Epoch: 25/30, step: 133/364, loss: 0.26561, accuracy: 0.89485\n",
            "Epoch: 25/30, step: 134/364, loss: 0.26520, accuracy: 0.89494\n",
            "Epoch: 25/30, step: 135/364, loss: 0.26577, accuracy: 0.89479\n",
            "Epoch: 25/30, step: 136/364, loss: 0.26555, accuracy: 0.89476\n",
            "Epoch: 25/30, step: 137/364, loss: 0.26527, accuracy: 0.89473\n",
            "Epoch: 25/30, step: 138/364, loss: 0.26514, accuracy: 0.89481\n",
            "Epoch: 25/30, step: 139/364, loss: 0.26503, accuracy: 0.89490\n",
            "Epoch: 25/30, step: 140/364, loss: 0.26476, accuracy: 0.89498\n",
            "Epoch: 25/30, step: 141/364, loss: 0.26485, accuracy: 0.89506\n",
            "Epoch: 25/30, step: 142/364, loss: 0.26491, accuracy: 0.89514\n",
            "Epoch: 25/30, step: 143/364, loss: 0.26577, accuracy: 0.89467\n",
            "Epoch: 25/30, step: 144/364, loss: 0.26607, accuracy: 0.89464\n",
            "Epoch: 25/30, step: 145/364, loss: 0.26601, accuracy: 0.89494\n",
            "Epoch: 25/30, step: 146/364, loss: 0.26620, accuracy: 0.89501\n",
            "Epoch: 25/30, step: 147/364, loss: 0.26628, accuracy: 0.89520\n",
            "Epoch: 25/30, step: 148/364, loss: 0.26618, accuracy: 0.89527\n",
            "Epoch: 25/30, step: 149/364, loss: 0.26539, accuracy: 0.89566\n",
            "Epoch: 25/30, step: 150/364, loss: 0.26511, accuracy: 0.89583\n",
            "Epoch: 25/30, step: 151/364, loss: 0.26468, accuracy: 0.89590\n",
            "Epoch: 25/30, step: 152/364, loss: 0.26480, accuracy: 0.89566\n",
            "Epoch: 25/30, step: 153/364, loss: 0.26456, accuracy: 0.89594\n",
            "Epoch: 25/30, step: 154/364, loss: 0.26461, accuracy: 0.89610\n",
            "Epoch: 25/30, step: 155/364, loss: 0.26474, accuracy: 0.89597\n",
            "Epoch: 25/30, step: 156/364, loss: 0.26444, accuracy: 0.89633\n",
            "Epoch: 25/30, step: 157/364, loss: 0.26543, accuracy: 0.89600\n",
            "Epoch: 25/30, step: 158/364, loss: 0.26523, accuracy: 0.89587\n",
            "Epoch: 25/30, step: 159/364, loss: 0.26504, accuracy: 0.89593\n",
            "Epoch: 25/30, step: 160/364, loss: 0.26465, accuracy: 0.89619\n",
            "Epoch: 25/30, step: 161/364, loss: 0.26451, accuracy: 0.89616\n",
            "Epoch: 25/30, step: 162/364, loss: 0.26424, accuracy: 0.89632\n",
            "Epoch: 25/30, step: 163/364, loss: 0.26469, accuracy: 0.89599\n",
            "Epoch: 25/30, step: 164/364, loss: 0.26471, accuracy: 0.89587\n",
            "Epoch: 25/30, step: 165/364, loss: 0.26414, accuracy: 0.89621\n",
            "Epoch: 25/30, step: 166/364, loss: 0.26404, accuracy: 0.89627\n",
            "Epoch: 25/30, step: 167/364, loss: 0.26383, accuracy: 0.89633\n",
            "Epoch: 25/30, step: 168/364, loss: 0.26389, accuracy: 0.89639\n",
            "Epoch: 25/30, step: 169/364, loss: 0.26323, accuracy: 0.89682\n",
            "Epoch: 25/30, step: 170/364, loss: 0.26363, accuracy: 0.89651\n",
            "Epoch: 25/30, step: 171/364, loss: 0.26391, accuracy: 0.89638\n",
            "Epoch: 25/30, step: 172/364, loss: 0.26391, accuracy: 0.89635\n",
            "Epoch: 25/30, step: 173/364, loss: 0.26374, accuracy: 0.89641\n",
            "Epoch: 25/30, step: 174/364, loss: 0.26346, accuracy: 0.89655\n",
            "Epoch: 25/30, step: 175/364, loss: 0.26331, accuracy: 0.89670\n",
            "Epoch: 25/30, step: 176/364, loss: 0.26292, accuracy: 0.89666\n",
            "Epoch: 25/30, step: 177/364, loss: 0.26285, accuracy: 0.89689\n",
            "Epoch: 25/30, step: 178/364, loss: 0.26248, accuracy: 0.89712\n",
            "Epoch: 25/30, step: 179/364, loss: 0.26208, accuracy: 0.89743\n",
            "Epoch: 25/30, step: 180/364, loss: 0.26201, accuracy: 0.89757\n",
            "Epoch: 25/30, step: 181/364, loss: 0.26168, accuracy: 0.89779\n",
            "Epoch: 25/30, step: 182/364, loss: 0.26186, accuracy: 0.89741\n",
            "Epoch: 25/30, step: 183/364, loss: 0.26163, accuracy: 0.89746\n",
            "Epoch: 25/30, step: 184/364, loss: 0.26156, accuracy: 0.89733\n",
            "Epoch: 25/30, step: 185/364, loss: 0.26220, accuracy: 0.89688\n",
            "Epoch: 25/30, step: 186/364, loss: 0.26236, accuracy: 0.89693\n",
            "Epoch: 25/30, step: 187/364, loss: 0.26245, accuracy: 0.89664\n",
            "Epoch: 25/30, step: 188/364, loss: 0.26227, accuracy: 0.89678\n",
            "Epoch: 25/30, step: 189/364, loss: 0.26197, accuracy: 0.89683\n",
            "Epoch: 25/30, step: 190/364, loss: 0.26179, accuracy: 0.89696\n",
            "Epoch: 25/30, step: 191/364, loss: 0.26151, accuracy: 0.89701\n",
            "Epoch: 25/30, step: 192/364, loss: 0.26157, accuracy: 0.89689\n",
            "Epoch: 25/30, step: 193/364, loss: 0.26120, accuracy: 0.89710\n",
            "Epoch: 25/30, step: 194/364, loss: 0.26153, accuracy: 0.89683\n",
            "Epoch: 25/30, step: 195/364, loss: 0.26111, accuracy: 0.89712\n",
            "Epoch: 25/30, step: 196/364, loss: 0.26102, accuracy: 0.89732\n",
            "Epoch: 25/30, step: 197/364, loss: 0.26080, accuracy: 0.89737\n",
            "Epoch: 25/30, step: 198/364, loss: 0.26040, accuracy: 0.89773\n",
            "Epoch: 25/30, step: 199/364, loss: 0.25970, accuracy: 0.89801\n",
            "Epoch: 25/30, step: 200/364, loss: 0.25915, accuracy: 0.89844\n",
            "Epoch: 25/30, step: 201/364, loss: 0.25950, accuracy: 0.89817\n",
            "Epoch: 25/30, step: 202/364, loss: 0.25921, accuracy: 0.89836\n",
            "Epoch: 25/30, step: 203/364, loss: 0.25981, accuracy: 0.89809\n",
            "Epoch: 25/30, step: 204/364, loss: 0.25976, accuracy: 0.89821\n",
            "Epoch: 25/30, step: 205/364, loss: 0.25954, accuracy: 0.89825\n",
            "Epoch: 25/30, step: 206/364, loss: 0.25991, accuracy: 0.89791\n",
            "Epoch: 25/30, step: 207/364, loss: 0.25966, accuracy: 0.89802\n",
            "Epoch: 25/30, step: 208/364, loss: 0.26046, accuracy: 0.89754\n",
            "Epoch: 25/30, step: 209/364, loss: 0.26052, accuracy: 0.89750\n",
            "Epoch: 25/30, step: 210/364, loss: 0.26113, accuracy: 0.89717\n",
            "Epoch: 25/30, step: 211/364, loss: 0.26141, accuracy: 0.89707\n",
            "Epoch: 25/30, step: 212/364, loss: 0.26098, accuracy: 0.89748\n",
            "Epoch: 25/30, step: 213/364, loss: 0.26126, accuracy: 0.89730\n",
            "Epoch: 25/30, step: 214/364, loss: 0.26112, accuracy: 0.89749\n",
            "Epoch: 25/30, step: 215/364, loss: 0.26125, accuracy: 0.89753\n",
            "Epoch: 25/30, step: 216/364, loss: 0.26081, accuracy: 0.89779\n",
            "Epoch: 25/30, step: 217/364, loss: 0.26095, accuracy: 0.89790\n",
            "Epoch: 25/30, step: 218/364, loss: 0.26097, accuracy: 0.89786\n",
            "Epoch: 25/30, step: 219/364, loss: 0.26066, accuracy: 0.89805\n",
            "Epoch: 25/30, step: 220/364, loss: 0.26063, accuracy: 0.89794\n",
            "Epoch: 25/30, step: 221/364, loss: 0.26076, accuracy: 0.89784\n",
            "Epoch: 25/30, step: 222/364, loss: 0.26066, accuracy: 0.89773\n",
            "Epoch: 25/30, step: 223/364, loss: 0.26075, accuracy: 0.89777\n",
            "Epoch: 25/30, step: 224/364, loss: 0.26068, accuracy: 0.89774\n",
            "Epoch: 25/30, step: 225/364, loss: 0.26038, accuracy: 0.89778\n",
            "Epoch: 25/30, step: 226/364, loss: 0.26075, accuracy: 0.89754\n",
            "Epoch: 25/30, step: 227/364, loss: 0.26034, accuracy: 0.89778\n",
            "Epoch: 25/30, step: 228/364, loss: 0.26022, accuracy: 0.89782\n",
            "Epoch: 25/30, step: 229/364, loss: 0.26004, accuracy: 0.89793\n",
            "Epoch: 25/30, step: 230/364, loss: 0.25992, accuracy: 0.89776\n",
            "Epoch: 25/30, step: 231/364, loss: 0.25980, accuracy: 0.89786\n",
            "Epoch: 25/30, step: 232/364, loss: 0.25955, accuracy: 0.89803\n",
            "Epoch: 25/30, step: 233/364, loss: 0.25934, accuracy: 0.89820\n",
            "Epoch: 25/30, step: 234/364, loss: 0.25928, accuracy: 0.89810\n",
            "Epoch: 25/30, step: 235/364, loss: 0.25933, accuracy: 0.89801\n",
            "Epoch: 25/30, step: 236/364, loss: 0.25913, accuracy: 0.89804\n",
            "Epoch: 25/30, step: 237/364, loss: 0.25920, accuracy: 0.89801\n",
            "Epoch: 25/30, step: 238/364, loss: 0.25907, accuracy: 0.89811\n",
            "Epoch: 25/30, step: 239/364, loss: 0.25914, accuracy: 0.89788\n",
            "Epoch: 25/30, step: 240/364, loss: 0.25863, accuracy: 0.89824\n",
            "Epoch: 25/30, step: 241/364, loss: 0.25831, accuracy: 0.89828\n",
            "Epoch: 25/30, step: 242/364, loss: 0.25877, accuracy: 0.89811\n",
            "Epoch: 25/30, step: 243/364, loss: 0.25929, accuracy: 0.89776\n",
            "Epoch: 25/30, step: 244/364, loss: 0.25907, accuracy: 0.89799\n",
            "Epoch: 25/30, step: 245/364, loss: 0.25884, accuracy: 0.89815\n",
            "Epoch: 25/30, step: 246/364, loss: 0.25858, accuracy: 0.89825\n",
            "Epoch: 25/30, step: 247/364, loss: 0.25845, accuracy: 0.89822\n",
            "Epoch: 25/30, step: 248/364, loss: 0.25817, accuracy: 0.89844\n",
            "Epoch: 25/30, step: 249/364, loss: 0.25811, accuracy: 0.89847\n",
            "Epoch: 25/30, step: 250/364, loss: 0.25774, accuracy: 0.89863\n",
            "Epoch: 25/30, step: 251/364, loss: 0.25760, accuracy: 0.89872\n",
            "Epoch: 25/30, step: 252/364, loss: 0.25748, accuracy: 0.89887\n",
            "Epoch: 25/30, step: 253/364, loss: 0.25732, accuracy: 0.89896\n",
            "Epoch: 25/30, step: 254/364, loss: 0.25777, accuracy: 0.89856\n",
            "Epoch: 25/30, step: 255/364, loss: 0.25791, accuracy: 0.89847\n",
            "Epoch: 25/30, step: 256/364, loss: 0.25751, accuracy: 0.89874\n",
            "Epoch: 25/30, step: 257/364, loss: 0.25727, accuracy: 0.89895\n",
            "Epoch: 25/30, step: 258/364, loss: 0.25703, accuracy: 0.89910\n",
            "Epoch: 25/30, step: 259/364, loss: 0.25700, accuracy: 0.89913\n",
            "Epoch: 25/30, step: 260/364, loss: 0.25688, accuracy: 0.89910\n",
            "Epoch: 25/30, step: 261/364, loss: 0.25702, accuracy: 0.89907\n",
            "Epoch: 25/30, step: 262/364, loss: 0.25657, accuracy: 0.89927\n",
            "Epoch: 25/30, step: 263/364, loss: 0.25709, accuracy: 0.89900\n",
            "Epoch: 25/30, step: 264/364, loss: 0.25707, accuracy: 0.89903\n",
            "Epoch: 25/30, step: 265/364, loss: 0.25725, accuracy: 0.89894\n",
            "Epoch: 25/30, step: 266/364, loss: 0.25760, accuracy: 0.89873\n",
            "Epoch: 25/30, step: 267/364, loss: 0.25766, accuracy: 0.89882\n",
            "Epoch: 25/30, step: 268/364, loss: 0.25758, accuracy: 0.89890\n",
            "Epoch: 25/30, step: 269/364, loss: 0.25757, accuracy: 0.89893\n",
            "Epoch: 25/30, step: 270/364, loss: 0.25751, accuracy: 0.89913\n",
            "Epoch: 25/30, step: 271/364, loss: 0.25755, accuracy: 0.89916\n",
            "Epoch: 25/30, step: 272/364, loss: 0.25721, accuracy: 0.89930\n",
            "Epoch: 25/30, step: 273/364, loss: 0.25780, accuracy: 0.89887\n",
            "Epoch: 25/30, step: 274/364, loss: 0.25785, accuracy: 0.89889\n",
            "Epoch: 25/30, step: 275/364, loss: 0.25816, accuracy: 0.89864\n",
            "Epoch: 25/30, step: 276/364, loss: 0.25794, accuracy: 0.89878\n",
            "Epoch: 25/30, step: 277/364, loss: 0.25828, accuracy: 0.89863\n",
            "Epoch: 25/30, step: 278/364, loss: 0.25814, accuracy: 0.89861\n",
            "Epoch: 25/30, step: 279/364, loss: 0.25847, accuracy: 0.89830\n",
            "Epoch: 25/30, step: 280/364, loss: 0.25850, accuracy: 0.89821\n",
            "Epoch: 25/30, step: 281/364, loss: 0.25815, accuracy: 0.89858\n",
            "Epoch: 25/30, step: 282/364, loss: 0.25816, accuracy: 0.89844\n",
            "Epoch: 25/30, step: 283/364, loss: 0.25803, accuracy: 0.89847\n",
            "Epoch: 25/30, step: 284/364, loss: 0.25814, accuracy: 0.89849\n",
            "Epoch: 25/30, step: 285/364, loss: 0.25809, accuracy: 0.89846\n",
            "Epoch: 25/30, step: 286/364, loss: 0.25797, accuracy: 0.89855\n",
            "Epoch: 25/30, step: 287/364, loss: 0.25774, accuracy: 0.89868\n",
            "Epoch: 25/30, step: 288/364, loss: 0.25762, accuracy: 0.89882\n",
            "Epoch: 25/30, step: 289/364, loss: 0.25763, accuracy: 0.89868\n",
            "Epoch: 25/30, step: 290/364, loss: 0.25737, accuracy: 0.89887\n",
            "Epoch: 25/30, step: 291/364, loss: 0.25774, accuracy: 0.89876\n",
            "Epoch: 25/30, train loss: 0.25774, train accuracy: 0.89876, valid loss: 0.71269, valid accuracy: 0.68551\n",
            "Epoch: 26/30, step: 1/364, loss: 0.26277, accuracy: 0.87500\n",
            "Epoch: 26/30, step: 2/364, loss: 0.26423, accuracy: 0.89062\n",
            "Epoch: 26/30, step: 3/364, loss: 0.24908, accuracy: 0.90104\n",
            "Epoch: 26/30, step: 4/364, loss: 0.24224, accuracy: 0.91016\n",
            "Epoch: 26/30, step: 5/364, loss: 0.23187, accuracy: 0.90938\n",
            "Epoch: 26/30, step: 6/364, loss: 0.21692, accuracy: 0.91927\n",
            "Epoch: 26/30, step: 7/364, loss: 0.22142, accuracy: 0.91964\n",
            "Epoch: 26/30, step: 8/364, loss: 0.23194, accuracy: 0.91211\n",
            "Epoch: 26/30, step: 9/364, loss: 0.23383, accuracy: 0.91319\n",
            "Epoch: 26/30, step: 10/364, loss: 0.23825, accuracy: 0.91406\n",
            "Epoch: 26/30, step: 11/364, loss: 0.24231, accuracy: 0.90909\n",
            "Epoch: 26/30, step: 12/364, loss: 0.25028, accuracy: 0.90104\n",
            "Epoch: 26/30, step: 13/364, loss: 0.25030, accuracy: 0.90385\n",
            "Epoch: 26/30, step: 14/364, loss: 0.24741, accuracy: 0.90513\n",
            "Epoch: 26/30, step: 15/364, loss: 0.24509, accuracy: 0.90729\n",
            "Epoch: 26/30, step: 16/364, loss: 0.24400, accuracy: 0.90723\n",
            "Epoch: 26/30, step: 17/364, loss: 0.24508, accuracy: 0.90901\n",
            "Epoch: 26/30, step: 18/364, loss: 0.24548, accuracy: 0.90885\n",
            "Epoch: 26/30, step: 19/364, loss: 0.24009, accuracy: 0.91201\n",
            "Epoch: 26/30, step: 20/364, loss: 0.24270, accuracy: 0.90938\n",
            "Epoch: 26/30, step: 21/364, loss: 0.23953, accuracy: 0.91071\n",
            "Epoch: 26/30, step: 22/364, loss: 0.23972, accuracy: 0.91193\n",
            "Epoch: 26/30, step: 23/364, loss: 0.23766, accuracy: 0.91372\n",
            "Epoch: 26/30, step: 24/364, loss: 0.24044, accuracy: 0.91081\n",
            "Epoch: 26/30, step: 25/364, loss: 0.24053, accuracy: 0.91000\n",
            "Epoch: 26/30, step: 26/364, loss: 0.24396, accuracy: 0.90745\n",
            "Epoch: 26/30, step: 27/364, loss: 0.24306, accuracy: 0.90741\n",
            "Epoch: 26/30, step: 28/364, loss: 0.24521, accuracy: 0.90625\n",
            "Epoch: 26/30, step: 29/364, loss: 0.24649, accuracy: 0.90517\n",
            "Epoch: 26/30, step: 30/364, loss: 0.24676, accuracy: 0.90521\n",
            "Epoch: 26/30, step: 31/364, loss: 0.25097, accuracy: 0.90121\n",
            "Epoch: 26/30, step: 32/364, loss: 0.25280, accuracy: 0.90137\n",
            "Epoch: 26/30, step: 33/364, loss: 0.25361, accuracy: 0.90057\n",
            "Epoch: 26/30, step: 34/364, loss: 0.25211, accuracy: 0.90211\n",
            "Epoch: 26/30, step: 35/364, loss: 0.25165, accuracy: 0.90179\n",
            "Epoch: 26/30, step: 36/364, loss: 0.25256, accuracy: 0.90191\n",
            "Epoch: 26/30, step: 37/364, loss: 0.25177, accuracy: 0.90118\n",
            "Epoch: 26/30, step: 38/364, loss: 0.25029, accuracy: 0.90132\n",
            "Epoch: 26/30, step: 39/364, loss: 0.24874, accuracy: 0.90264\n",
            "Epoch: 26/30, step: 40/364, loss: 0.24908, accuracy: 0.90195\n",
            "Epoch: 26/30, step: 41/364, loss: 0.24939, accuracy: 0.90091\n",
            "Epoch: 26/30, step: 42/364, loss: 0.24975, accuracy: 0.89993\n",
            "Epoch: 26/30, step: 43/364, loss: 0.25191, accuracy: 0.89862\n",
            "Epoch: 26/30, step: 44/364, loss: 0.25465, accuracy: 0.89737\n",
            "Epoch: 26/30, step: 45/364, loss: 0.25469, accuracy: 0.89792\n",
            "Epoch: 26/30, step: 46/364, loss: 0.25704, accuracy: 0.89674\n",
            "Epoch: 26/30, step: 47/364, loss: 0.25478, accuracy: 0.89827\n",
            "Epoch: 26/30, step: 48/364, loss: 0.25277, accuracy: 0.89974\n",
            "Epoch: 26/30, step: 49/364, loss: 0.25108, accuracy: 0.90147\n",
            "Epoch: 26/30, step: 50/364, loss: 0.25040, accuracy: 0.90156\n",
            "Epoch: 26/30, step: 51/364, loss: 0.25114, accuracy: 0.90196\n",
            "Epoch: 26/30, step: 52/364, loss: 0.25338, accuracy: 0.89994\n",
            "Epoch: 26/30, step: 53/364, loss: 0.25311, accuracy: 0.90035\n",
            "Epoch: 26/30, step: 54/364, loss: 0.25097, accuracy: 0.90191\n",
            "Epoch: 26/30, step: 55/364, loss: 0.25369, accuracy: 0.90114\n",
            "Epoch: 26/30, step: 56/364, loss: 0.25254, accuracy: 0.90206\n",
            "Epoch: 26/30, step: 57/364, loss: 0.25110, accuracy: 0.90269\n",
            "Epoch: 26/30, step: 58/364, loss: 0.25033, accuracy: 0.90329\n",
            "Epoch: 26/30, step: 59/364, loss: 0.25132, accuracy: 0.90228\n",
            "Epoch: 26/30, step: 60/364, loss: 0.25089, accuracy: 0.90260\n",
            "Epoch: 26/30, step: 61/364, loss: 0.25084, accuracy: 0.90215\n",
            "Epoch: 26/30, step: 62/364, loss: 0.25082, accuracy: 0.90197\n",
            "Epoch: 26/30, step: 63/364, loss: 0.25081, accuracy: 0.90253\n",
            "Epoch: 26/30, step: 64/364, loss: 0.24881, accuracy: 0.90381\n",
            "Epoch: 26/30, step: 65/364, loss: 0.24884, accuracy: 0.90337\n",
            "Epoch: 26/30, step: 66/364, loss: 0.24834, accuracy: 0.90388\n",
            "Epoch: 26/30, step: 67/364, loss: 0.24814, accuracy: 0.90415\n",
            "Epoch: 26/30, step: 68/364, loss: 0.24917, accuracy: 0.90303\n",
            "Epoch: 26/30, step: 69/364, loss: 0.25096, accuracy: 0.90263\n",
            "Epoch: 26/30, step: 70/364, loss: 0.25103, accuracy: 0.90246\n",
            "Epoch: 26/30, step: 71/364, loss: 0.25036, accuracy: 0.90273\n",
            "Epoch: 26/30, step: 72/364, loss: 0.25001, accuracy: 0.90278\n",
            "Epoch: 26/30, step: 73/364, loss: 0.24896, accuracy: 0.90368\n",
            "Epoch: 26/30, step: 74/364, loss: 0.24786, accuracy: 0.90435\n",
            "Epoch: 26/30, step: 75/364, loss: 0.24713, accuracy: 0.90479\n",
            "Epoch: 26/30, step: 76/364, loss: 0.24744, accuracy: 0.90440\n",
            "Epoch: 26/30, step: 77/364, loss: 0.24727, accuracy: 0.90402\n",
            "Epoch: 26/30, step: 78/364, loss: 0.24637, accuracy: 0.90485\n",
            "Epoch: 26/30, step: 79/364, loss: 0.24690, accuracy: 0.90427\n",
            "Epoch: 26/30, step: 80/364, loss: 0.24746, accuracy: 0.90332\n",
            "Epoch: 26/30, step: 81/364, loss: 0.24691, accuracy: 0.90374\n",
            "Epoch: 26/30, step: 82/364, loss: 0.24940, accuracy: 0.90206\n",
            "Epoch: 26/30, step: 83/364, loss: 0.25160, accuracy: 0.90079\n",
            "Epoch: 26/30, step: 84/364, loss: 0.25144, accuracy: 0.90067\n",
            "Epoch: 26/30, step: 85/364, loss: 0.25150, accuracy: 0.90018\n",
            "Epoch: 26/30, step: 86/364, loss: 0.25155, accuracy: 0.89989\n",
            "Epoch: 26/30, step: 87/364, loss: 0.25255, accuracy: 0.89889\n",
            "Epoch: 26/30, step: 88/364, loss: 0.25323, accuracy: 0.89826\n",
            "Epoch: 26/30, step: 89/364, loss: 0.25237, accuracy: 0.89870\n",
            "Epoch: 26/30, step: 90/364, loss: 0.25187, accuracy: 0.89913\n",
            "Epoch: 26/30, step: 91/364, loss: 0.25202, accuracy: 0.89921\n",
            "Epoch: 26/30, step: 92/364, loss: 0.25151, accuracy: 0.89946\n",
            "Epoch: 26/30, step: 93/364, loss: 0.25065, accuracy: 0.89987\n",
            "Epoch: 26/30, step: 94/364, loss: 0.25063, accuracy: 0.89993\n",
            "Epoch: 26/30, step: 95/364, loss: 0.25008, accuracy: 0.89984\n",
            "Epoch: 26/30, step: 96/364, loss: 0.25000, accuracy: 0.89974\n",
            "Epoch: 26/30, step: 97/364, loss: 0.25084, accuracy: 0.89868\n",
            "Epoch: 26/30, step: 98/364, loss: 0.24969, accuracy: 0.89939\n",
            "Epoch: 26/30, step: 99/364, loss: 0.24860, accuracy: 0.90009\n",
            "Epoch: 26/30, step: 100/364, loss: 0.24947, accuracy: 0.89969\n",
            "Epoch: 26/30, step: 101/364, loss: 0.24919, accuracy: 0.89960\n",
            "Epoch: 26/30, step: 102/364, loss: 0.24807, accuracy: 0.90028\n",
            "Epoch: 26/30, step: 103/364, loss: 0.24834, accuracy: 0.89988\n",
            "Epoch: 26/30, step: 104/364, loss: 0.24785, accuracy: 0.90039\n",
            "Epoch: 26/30, step: 105/364, loss: 0.24737, accuracy: 0.90060\n",
            "Epoch: 26/30, step: 106/364, loss: 0.24698, accuracy: 0.90050\n",
            "Epoch: 26/30, step: 107/364, loss: 0.24699, accuracy: 0.90070\n",
            "Epoch: 26/30, step: 108/364, loss: 0.24681, accuracy: 0.90090\n",
            "Epoch: 26/30, step: 109/364, loss: 0.24702, accuracy: 0.90080\n",
            "Epoch: 26/30, step: 110/364, loss: 0.24665, accuracy: 0.90099\n",
            "Epoch: 26/30, step: 111/364, loss: 0.24696, accuracy: 0.90048\n",
            "Epoch: 26/30, step: 112/364, loss: 0.24683, accuracy: 0.90067\n",
            "Epoch: 26/30, step: 113/364, loss: 0.24751, accuracy: 0.90044\n",
            "Epoch: 26/30, step: 114/364, loss: 0.24711, accuracy: 0.90063\n",
            "Epoch: 26/30, step: 115/364, loss: 0.24693, accuracy: 0.90082\n",
            "Epoch: 26/30, step: 116/364, loss: 0.24604, accuracy: 0.90127\n",
            "Epoch: 26/30, step: 117/364, loss: 0.24511, accuracy: 0.90184\n",
            "Epoch: 26/30, step: 118/364, loss: 0.24474, accuracy: 0.90228\n",
            "Epoch: 26/30, step: 119/364, loss: 0.24457, accuracy: 0.90244\n",
            "Epoch: 26/30, step: 120/364, loss: 0.24428, accuracy: 0.90286\n",
            "Epoch: 26/30, step: 121/364, loss: 0.24373, accuracy: 0.90328\n",
            "Epoch: 26/30, step: 122/364, loss: 0.24347, accuracy: 0.90356\n",
            "Epoch: 26/30, step: 123/364, loss: 0.24286, accuracy: 0.90396\n",
            "Epoch: 26/30, step: 124/364, loss: 0.24308, accuracy: 0.90386\n",
            "Epoch: 26/30, step: 125/364, loss: 0.24297, accuracy: 0.90412\n",
            "Epoch: 26/30, step: 126/364, loss: 0.24289, accuracy: 0.90451\n",
            "Epoch: 26/30, step: 127/364, loss: 0.24334, accuracy: 0.90404\n",
            "Epoch: 26/30, step: 128/364, loss: 0.24278, accuracy: 0.90442\n",
            "Epoch: 26/30, step: 129/364, loss: 0.24235, accuracy: 0.90443\n",
            "Epoch: 26/30, step: 130/364, loss: 0.24214, accuracy: 0.90469\n",
            "Epoch: 26/30, step: 131/364, loss: 0.24171, accuracy: 0.90482\n",
            "Epoch: 26/30, step: 132/364, loss: 0.24152, accuracy: 0.90483\n",
            "Epoch: 26/30, step: 133/364, loss: 0.24173, accuracy: 0.90437\n",
            "Epoch: 26/30, step: 134/364, loss: 0.24097, accuracy: 0.90497\n",
            "Epoch: 26/30, step: 135/364, loss: 0.24152, accuracy: 0.90486\n",
            "Epoch: 26/30, step: 136/364, loss: 0.24123, accuracy: 0.90499\n",
            "Epoch: 26/30, step: 137/364, loss: 0.24106, accuracy: 0.90522\n",
            "Epoch: 26/30, step: 138/364, loss: 0.24043, accuracy: 0.90568\n",
            "Epoch: 26/30, step: 139/364, loss: 0.24112, accuracy: 0.90558\n",
            "Epoch: 26/30, step: 140/364, loss: 0.24052, accuracy: 0.90603\n",
            "Epoch: 26/30, step: 141/364, loss: 0.24078, accuracy: 0.90603\n",
            "Epoch: 26/30, step: 142/364, loss: 0.24070, accuracy: 0.90614\n",
            "Epoch: 26/30, step: 143/364, loss: 0.24116, accuracy: 0.90559\n",
            "Epoch: 26/30, step: 144/364, loss: 0.24045, accuracy: 0.90592\n",
            "Epoch: 26/30, step: 145/364, loss: 0.23980, accuracy: 0.90636\n",
            "Epoch: 26/30, step: 146/364, loss: 0.23976, accuracy: 0.90657\n",
            "Epoch: 26/30, step: 147/364, loss: 0.23963, accuracy: 0.90657\n",
            "Epoch: 26/30, step: 148/364, loss: 0.24022, accuracy: 0.90667\n",
            "Epoch: 26/30, step: 149/364, loss: 0.24010, accuracy: 0.90667\n",
            "Epoch: 26/30, step: 150/364, loss: 0.24050, accuracy: 0.90635\n",
            "Epoch: 26/30, step: 151/364, loss: 0.24107, accuracy: 0.90594\n",
            "Epoch: 26/30, step: 152/364, loss: 0.24085, accuracy: 0.90594\n",
            "Epoch: 26/30, step: 153/364, loss: 0.24137, accuracy: 0.90564\n",
            "Epoch: 26/30, step: 154/364, loss: 0.24211, accuracy: 0.90513\n",
            "Epoch: 26/30, step: 155/364, loss: 0.24215, accuracy: 0.90504\n",
            "Epoch: 26/30, step: 156/364, loss: 0.24164, accuracy: 0.90515\n",
            "Epoch: 26/30, step: 157/364, loss: 0.24180, accuracy: 0.90516\n",
            "Epoch: 26/30, step: 158/364, loss: 0.24104, accuracy: 0.90546\n",
            "Epoch: 26/30, step: 159/364, loss: 0.24104, accuracy: 0.90527\n",
            "Epoch: 26/30, step: 160/364, loss: 0.24073, accuracy: 0.90557\n",
            "Epoch: 26/30, step: 161/364, loss: 0.24154, accuracy: 0.90499\n",
            "Epoch: 26/30, step: 162/364, loss: 0.24120, accuracy: 0.90509\n",
            "Epoch: 26/30, step: 163/364, loss: 0.24139, accuracy: 0.90491\n",
            "Epoch: 26/30, step: 164/364, loss: 0.24131, accuracy: 0.90511\n",
            "Epoch: 26/30, step: 165/364, loss: 0.24141, accuracy: 0.90530\n",
            "Epoch: 26/30, step: 166/364, loss: 0.24122, accuracy: 0.90550\n",
            "Epoch: 26/30, step: 167/364, loss: 0.24109, accuracy: 0.90550\n",
            "Epoch: 26/30, step: 168/364, loss: 0.24065, accuracy: 0.90588\n",
            "Epoch: 26/30, step: 169/364, loss: 0.24069, accuracy: 0.90597\n",
            "Epoch: 26/30, step: 170/364, loss: 0.24048, accuracy: 0.90597\n",
            "Epoch: 26/30, step: 171/364, loss: 0.24111, accuracy: 0.90543\n",
            "Epoch: 26/30, step: 172/364, loss: 0.24177, accuracy: 0.90525\n",
            "Epoch: 26/30, step: 173/364, loss: 0.24134, accuracy: 0.90544\n",
            "Epoch: 26/30, step: 174/364, loss: 0.24099, accuracy: 0.90580\n",
            "Epoch: 26/30, step: 175/364, loss: 0.24137, accuracy: 0.90554\n",
            "Epoch: 26/30, step: 176/364, loss: 0.24164, accuracy: 0.90536\n",
            "Epoch: 26/30, step: 177/364, loss: 0.24149, accuracy: 0.90554\n",
            "Epoch: 26/30, step: 178/364, loss: 0.24169, accuracy: 0.90537\n",
            "Epoch: 26/30, step: 179/364, loss: 0.24123, accuracy: 0.90564\n",
            "Epoch: 26/30, step: 180/364, loss: 0.24073, accuracy: 0.90599\n",
            "Epoch: 26/30, step: 181/364, loss: 0.24066, accuracy: 0.90599\n",
            "Epoch: 26/30, step: 182/364, loss: 0.24032, accuracy: 0.90625\n",
            "Epoch: 26/30, step: 183/364, loss: 0.24090, accuracy: 0.90599\n",
            "Epoch: 26/30, step: 184/364, loss: 0.24052, accuracy: 0.90617\n",
            "Epoch: 26/30, step: 185/364, loss: 0.24039, accuracy: 0.90625\n",
            "Epoch: 26/30, step: 186/364, loss: 0.24102, accuracy: 0.90600\n",
            "Epoch: 26/30, step: 187/364, loss: 0.24124, accuracy: 0.90600\n",
            "Epoch: 26/30, step: 188/364, loss: 0.24119, accuracy: 0.90600\n",
            "Epoch: 26/30, step: 189/364, loss: 0.24082, accuracy: 0.90633\n",
            "Epoch: 26/30, step: 190/364, loss: 0.24064, accuracy: 0.90650\n",
            "Epoch: 26/30, step: 191/364, loss: 0.24045, accuracy: 0.90658\n",
            "Epoch: 26/30, step: 192/364, loss: 0.24087, accuracy: 0.90641\n",
            "Epoch: 26/30, step: 193/364, loss: 0.24063, accuracy: 0.90657\n",
            "Epoch: 26/30, step: 194/364, loss: 0.24102, accuracy: 0.90633\n",
            "Epoch: 26/30, step: 195/364, loss: 0.24252, accuracy: 0.90537\n",
            "Epoch: 26/30, step: 196/364, loss: 0.24207, accuracy: 0.90561\n",
            "Epoch: 26/30, step: 197/364, loss: 0.24175, accuracy: 0.90585\n",
            "Epoch: 26/30, step: 198/364, loss: 0.24156, accuracy: 0.90586\n",
            "Epoch: 26/30, step: 199/364, loss: 0.24168, accuracy: 0.90578\n",
            "Epoch: 26/30, step: 200/364, loss: 0.24122, accuracy: 0.90617\n",
            "Epoch: 26/30, step: 201/364, loss: 0.24130, accuracy: 0.90633\n",
            "Epoch: 26/30, step: 202/364, loss: 0.24102, accuracy: 0.90648\n",
            "Epoch: 26/30, step: 203/364, loss: 0.24156, accuracy: 0.90602\n",
            "Epoch: 26/30, step: 204/364, loss: 0.24122, accuracy: 0.90625\n",
            "Epoch: 26/30, step: 205/364, loss: 0.24111, accuracy: 0.90625\n",
            "Epoch: 26/30, step: 206/364, loss: 0.24084, accuracy: 0.90633\n",
            "Epoch: 26/30, step: 207/364, loss: 0.24065, accuracy: 0.90648\n",
            "Epoch: 26/30, step: 208/364, loss: 0.24064, accuracy: 0.90648\n",
            "Epoch: 26/30, step: 209/364, loss: 0.24137, accuracy: 0.90610\n",
            "Epoch: 26/30, step: 210/364, loss: 0.24124, accuracy: 0.90625\n",
            "Epoch: 26/30, step: 211/364, loss: 0.24087, accuracy: 0.90640\n",
            "Epoch: 26/30, step: 212/364, loss: 0.24052, accuracy: 0.90669\n",
            "Epoch: 26/30, step: 213/364, loss: 0.23984, accuracy: 0.90713\n",
            "Epoch: 26/30, step: 214/364, loss: 0.23987, accuracy: 0.90705\n",
            "Epoch: 26/30, step: 215/364, loss: 0.23975, accuracy: 0.90712\n",
            "Epoch: 26/30, step: 216/364, loss: 0.23945, accuracy: 0.90734\n",
            "Epoch: 26/30, step: 217/364, loss: 0.23994, accuracy: 0.90719\n",
            "Epoch: 26/30, step: 218/364, loss: 0.24017, accuracy: 0.90725\n",
            "Epoch: 26/30, step: 219/364, loss: 0.24035, accuracy: 0.90718\n",
            "Epoch: 26/30, step: 220/364, loss: 0.24003, accuracy: 0.90739\n",
            "Epoch: 26/30, step: 221/364, loss: 0.24014, accuracy: 0.90745\n",
            "Epoch: 26/30, step: 222/364, loss: 0.24002, accuracy: 0.90759\n",
            "Epoch: 26/30, step: 223/364, loss: 0.24008, accuracy: 0.90758\n",
            "Epoch: 26/30, step: 224/364, loss: 0.24011, accuracy: 0.90744\n",
            "Epoch: 26/30, step: 225/364, loss: 0.23996, accuracy: 0.90757\n",
            "Epoch: 26/30, step: 226/364, loss: 0.23967, accuracy: 0.90784\n",
            "Epoch: 26/30, step: 227/364, loss: 0.23947, accuracy: 0.90783\n",
            "Epoch: 26/30, step: 228/364, loss: 0.23943, accuracy: 0.90796\n",
            "Epoch: 26/30, step: 229/364, loss: 0.23993, accuracy: 0.90755\n",
            "Epoch: 26/30, step: 230/364, loss: 0.23969, accuracy: 0.90761\n",
            "Epoch: 26/30, step: 231/364, loss: 0.23992, accuracy: 0.90733\n",
            "Epoch: 26/30, step: 232/364, loss: 0.23953, accuracy: 0.90766\n",
            "Epoch: 26/30, step: 233/364, loss: 0.24005, accuracy: 0.90739\n",
            "Epoch: 26/30, step: 234/364, loss: 0.23986, accuracy: 0.90745\n",
            "Epoch: 26/30, step: 235/364, loss: 0.23957, accuracy: 0.90758\n",
            "Epoch: 26/30, step: 236/364, loss: 0.23985, accuracy: 0.90764\n",
            "Epoch: 26/30, step: 237/364, loss: 0.23948, accuracy: 0.90783\n",
            "Epoch: 26/30, step: 238/364, loss: 0.23931, accuracy: 0.90796\n",
            "Epoch: 26/30, step: 239/364, loss: 0.23890, accuracy: 0.90815\n",
            "Epoch: 26/30, step: 240/364, loss: 0.23940, accuracy: 0.90788\n",
            "Epoch: 26/30, step: 241/364, loss: 0.23907, accuracy: 0.90807\n",
            "Epoch: 26/30, step: 242/364, loss: 0.23892, accuracy: 0.90819\n",
            "Epoch: 26/30, step: 243/364, loss: 0.23926, accuracy: 0.90799\n",
            "Epoch: 26/30, step: 244/364, loss: 0.23903, accuracy: 0.90811\n",
            "Epoch: 26/30, step: 245/364, loss: 0.23906, accuracy: 0.90810\n",
            "Epoch: 26/30, step: 246/364, loss: 0.23909, accuracy: 0.90816\n",
            "Epoch: 26/30, step: 247/364, loss: 0.23935, accuracy: 0.90796\n",
            "Epoch: 26/30, step: 248/364, loss: 0.23929, accuracy: 0.90801\n",
            "Epoch: 26/30, step: 249/364, loss: 0.23919, accuracy: 0.90813\n",
            "Epoch: 26/30, step: 250/364, loss: 0.23883, accuracy: 0.90844\n",
            "Epoch: 26/30, step: 251/364, loss: 0.23895, accuracy: 0.90849\n",
            "Epoch: 26/30, step: 252/364, loss: 0.23925, accuracy: 0.90836\n",
            "Epoch: 26/30, step: 253/364, loss: 0.23913, accuracy: 0.90847\n",
            "Epoch: 26/30, step: 254/364, loss: 0.23902, accuracy: 0.90846\n",
            "Epoch: 26/30, step: 255/364, loss: 0.23892, accuracy: 0.90839\n",
            "Epoch: 26/30, step: 256/364, loss: 0.23866, accuracy: 0.90857\n",
            "Epoch: 26/30, step: 257/364, loss: 0.23842, accuracy: 0.90868\n",
            "Epoch: 26/30, step: 258/364, loss: 0.23854, accuracy: 0.90855\n",
            "Epoch: 26/30, step: 259/364, loss: 0.23860, accuracy: 0.90860\n",
            "Epoch: 26/30, step: 260/364, loss: 0.23861, accuracy: 0.90865\n",
            "Epoch: 26/30, step: 261/364, loss: 0.23864, accuracy: 0.90858\n",
            "Epoch: 26/30, step: 262/364, loss: 0.23856, accuracy: 0.90864\n",
            "Epoch: 26/30, step: 263/364, loss: 0.23845, accuracy: 0.90875\n",
            "Epoch: 26/30, step: 264/364, loss: 0.23835, accuracy: 0.90885\n",
            "Epoch: 26/30, step: 265/364, loss: 0.23822, accuracy: 0.90890\n",
            "Epoch: 26/30, step: 266/364, loss: 0.23788, accuracy: 0.90913\n",
            "Epoch: 26/30, step: 267/364, loss: 0.23846, accuracy: 0.90871\n",
            "Epoch: 26/30, step: 268/364, loss: 0.23833, accuracy: 0.90876\n",
            "Epoch: 26/30, step: 269/364, loss: 0.23841, accuracy: 0.90863\n",
            "Epoch: 26/30, step: 270/364, loss: 0.23857, accuracy: 0.90856\n",
            "Epoch: 26/30, step: 271/364, loss: 0.23853, accuracy: 0.90856\n",
            "Epoch: 26/30, step: 272/364, loss: 0.23824, accuracy: 0.90878\n",
            "Epoch: 26/30, step: 273/364, loss: 0.23812, accuracy: 0.90894\n",
            "Epoch: 26/30, step: 274/364, loss: 0.23806, accuracy: 0.90882\n",
            "Epoch: 26/30, step: 275/364, loss: 0.23771, accuracy: 0.90909\n",
            "Epoch: 26/30, step: 276/364, loss: 0.23754, accuracy: 0.90925\n",
            "Epoch: 26/30, step: 277/364, loss: 0.23738, accuracy: 0.90924\n",
            "Epoch: 26/30, step: 278/364, loss: 0.23729, accuracy: 0.90940\n",
            "Epoch: 26/30, step: 279/364, loss: 0.23735, accuracy: 0.90933\n",
            "Epoch: 26/30, step: 280/364, loss: 0.23735, accuracy: 0.90938\n",
            "Epoch: 26/30, step: 281/364, loss: 0.23761, accuracy: 0.90920\n",
            "Epoch: 26/30, step: 282/364, loss: 0.23752, accuracy: 0.90930\n",
            "Epoch: 26/30, step: 283/364, loss: 0.23714, accuracy: 0.90956\n",
            "Epoch: 26/30, step: 284/364, loss: 0.23720, accuracy: 0.90944\n",
            "Epoch: 26/30, step: 285/364, loss: 0.23705, accuracy: 0.90948\n",
            "Epoch: 26/30, step: 286/364, loss: 0.23707, accuracy: 0.90953\n",
            "Epoch: 26/30, step: 287/364, loss: 0.23694, accuracy: 0.90963\n",
            "Epoch: 26/30, step: 288/364, loss: 0.23704, accuracy: 0.90956\n",
            "Epoch: 26/30, step: 289/364, loss: 0.23699, accuracy: 0.90955\n",
            "Epoch: 26/30, step: 290/364, loss: 0.23667, accuracy: 0.90975\n",
            "Epoch: 26/30, step: 291/364, loss: 0.23654, accuracy: 0.90978\n",
            "Epoch: 26/30, train loss: 0.23654, train accuracy: 0.90978, valid loss: 0.82226, valid accuracy: 0.66939\n",
            "Epoch: 27/30, step: 1/364, loss: 0.25111, accuracy: 0.92188\n",
            "Epoch: 27/30, step: 2/364, loss: 0.24245, accuracy: 0.92188\n",
            "Epoch: 27/30, step: 3/364, loss: 0.21244, accuracy: 0.92708\n",
            "Epoch: 27/30, step: 4/364, loss: 0.24047, accuracy: 0.92188\n",
            "Epoch: 27/30, step: 5/364, loss: 0.22152, accuracy: 0.93125\n",
            "Epoch: 27/30, step: 6/364, loss: 0.21884, accuracy: 0.92708\n",
            "Epoch: 27/30, step: 7/364, loss: 0.21373, accuracy: 0.93304\n",
            "Epoch: 27/30, step: 8/364, loss: 0.20945, accuracy: 0.93359\n",
            "Epoch: 27/30, step: 9/364, loss: 0.21224, accuracy: 0.93229\n",
            "Epoch: 27/30, step: 10/364, loss: 0.21699, accuracy: 0.92813\n",
            "Epoch: 27/30, step: 11/364, loss: 0.21665, accuracy: 0.92756\n",
            "Epoch: 27/30, step: 12/364, loss: 0.21997, accuracy: 0.92318\n",
            "Epoch: 27/30, step: 13/364, loss: 0.21994, accuracy: 0.92308\n",
            "Epoch: 27/30, step: 14/364, loss: 0.22329, accuracy: 0.92188\n",
            "Epoch: 27/30, step: 15/364, loss: 0.22457, accuracy: 0.91979\n",
            "Epoch: 27/30, step: 16/364, loss: 0.22900, accuracy: 0.91602\n",
            "Epoch: 27/30, step: 17/364, loss: 0.22648, accuracy: 0.91636\n",
            "Epoch: 27/30, step: 18/364, loss: 0.22500, accuracy: 0.91753\n",
            "Epoch: 27/30, step: 19/364, loss: 0.21992, accuracy: 0.92023\n",
            "Epoch: 27/30, step: 20/364, loss: 0.21685, accuracy: 0.92266\n",
            "Epoch: 27/30, step: 21/364, loss: 0.21783, accuracy: 0.92336\n",
            "Epoch: 27/30, step: 22/364, loss: 0.21984, accuracy: 0.92188\n",
            "Epoch: 27/30, step: 23/364, loss: 0.22343, accuracy: 0.91848\n",
            "Epoch: 27/30, step: 24/364, loss: 0.22685, accuracy: 0.91797\n",
            "Epoch: 27/30, step: 25/364, loss: 0.22566, accuracy: 0.91750\n",
            "Epoch: 27/30, step: 26/364, loss: 0.22646, accuracy: 0.91767\n",
            "Epoch: 27/30, step: 27/364, loss: 0.22836, accuracy: 0.91725\n",
            "Epoch: 27/30, step: 28/364, loss: 0.22841, accuracy: 0.91629\n",
            "Epoch: 27/30, step: 29/364, loss: 0.22783, accuracy: 0.91649\n",
            "Epoch: 27/30, step: 30/364, loss: 0.22835, accuracy: 0.91615\n",
            "Epoch: 27/30, step: 31/364, loss: 0.22831, accuracy: 0.91633\n",
            "Epoch: 27/30, step: 32/364, loss: 0.22751, accuracy: 0.91748\n",
            "Epoch: 27/30, step: 33/364, loss: 0.22585, accuracy: 0.91809\n",
            "Epoch: 27/30, step: 34/364, loss: 0.22782, accuracy: 0.91728\n",
            "Epoch: 27/30, step: 35/364, loss: 0.22887, accuracy: 0.91652\n",
            "Epoch: 27/30, step: 36/364, loss: 0.22682, accuracy: 0.91797\n",
            "Epoch: 27/30, step: 37/364, loss: 0.22608, accuracy: 0.91807\n",
            "Epoch: 27/30, step: 38/364, loss: 0.22511, accuracy: 0.91776\n",
            "Epoch: 27/30, step: 39/364, loss: 0.22495, accuracy: 0.91827\n",
            "Epoch: 27/30, step: 40/364, loss: 0.22635, accuracy: 0.91758\n",
            "Epoch: 27/30, step: 41/364, loss: 0.22408, accuracy: 0.91921\n",
            "Epoch: 27/30, step: 42/364, loss: 0.22292, accuracy: 0.92001\n",
            "Epoch: 27/30, step: 43/364, loss: 0.22157, accuracy: 0.92115\n",
            "Epoch: 27/30, step: 44/364, loss: 0.22095, accuracy: 0.92152\n",
            "Epoch: 27/30, step: 45/364, loss: 0.22420, accuracy: 0.91944\n",
            "Epoch: 27/30, step: 46/364, loss: 0.22630, accuracy: 0.91814\n",
            "Epoch: 27/30, step: 47/364, loss: 0.22724, accuracy: 0.91755\n",
            "Epoch: 27/30, step: 48/364, loss: 0.22741, accuracy: 0.91732\n",
            "Epoch: 27/30, step: 49/364, loss: 0.22668, accuracy: 0.91709\n",
            "Epoch: 27/30, step: 50/364, loss: 0.22905, accuracy: 0.91531\n",
            "Epoch: 27/30, step: 51/364, loss: 0.22983, accuracy: 0.91422\n",
            "Epoch: 27/30, step: 52/364, loss: 0.22797, accuracy: 0.91556\n",
            "Epoch: 27/30, step: 53/364, loss: 0.22593, accuracy: 0.91686\n",
            "Epoch: 27/30, step: 54/364, loss: 0.22867, accuracy: 0.91522\n",
            "Epoch: 27/30, step: 55/364, loss: 0.22773, accuracy: 0.91562\n",
            "Epoch: 27/30, step: 56/364, loss: 0.22821, accuracy: 0.91602\n",
            "Epoch: 27/30, step: 57/364, loss: 0.22996, accuracy: 0.91530\n",
            "Epoch: 27/30, step: 58/364, loss: 0.23084, accuracy: 0.91433\n",
            "Epoch: 27/30, step: 59/364, loss: 0.22939, accuracy: 0.91525\n",
            "Epoch: 27/30, step: 60/364, loss: 0.22993, accuracy: 0.91432\n",
            "Epoch: 27/30, step: 61/364, loss: 0.23358, accuracy: 0.91240\n",
            "Epoch: 27/30, step: 62/364, loss: 0.23332, accuracy: 0.91305\n",
            "Epoch: 27/30, step: 63/364, loss: 0.23285, accuracy: 0.91270\n",
            "Epoch: 27/30, step: 64/364, loss: 0.23248, accuracy: 0.91284\n",
            "Epoch: 27/30, step: 65/364, loss: 0.23146, accuracy: 0.91322\n",
            "Epoch: 27/30, step: 66/364, loss: 0.23358, accuracy: 0.91241\n",
            "Epoch: 27/30, step: 67/364, loss: 0.23277, accuracy: 0.91278\n",
            "Epoch: 27/30, step: 68/364, loss: 0.23263, accuracy: 0.91291\n",
            "Epoch: 27/30, step: 69/364, loss: 0.23172, accuracy: 0.91327\n",
            "Epoch: 27/30, step: 70/364, loss: 0.23272, accuracy: 0.91228\n",
            "Epoch: 27/30, step: 71/364, loss: 0.23339, accuracy: 0.91175\n",
            "Epoch: 27/30, step: 72/364, loss: 0.23156, accuracy: 0.91298\n",
            "Epoch: 27/30, step: 73/364, loss: 0.23207, accuracy: 0.91246\n",
            "Epoch: 27/30, step: 74/364, loss: 0.23289, accuracy: 0.91174\n",
            "Epoch: 27/30, step: 75/364, loss: 0.23337, accuracy: 0.91167\n",
            "Epoch: 27/30, step: 76/364, loss: 0.23238, accuracy: 0.91201\n",
            "Epoch: 27/30, step: 77/364, loss: 0.23310, accuracy: 0.91153\n",
            "Epoch: 27/30, step: 78/364, loss: 0.23237, accuracy: 0.91166\n",
            "Epoch: 27/30, step: 79/364, loss: 0.23207, accuracy: 0.91179\n",
            "Epoch: 27/30, step: 80/364, loss: 0.23111, accuracy: 0.91250\n",
            "Epoch: 27/30, step: 81/364, loss: 0.23089, accuracy: 0.91262\n",
            "Epoch: 27/30, step: 82/364, loss: 0.23125, accuracy: 0.91273\n",
            "Epoch: 27/30, step: 83/364, loss: 0.23074, accuracy: 0.91284\n",
            "Epoch: 27/30, step: 84/364, loss: 0.22937, accuracy: 0.91369\n",
            "Epoch: 27/30, step: 85/364, loss: 0.22895, accuracy: 0.91379\n",
            "Epoch: 27/30, step: 86/364, loss: 0.22892, accuracy: 0.91370\n",
            "Epoch: 27/30, step: 87/364, loss: 0.23033, accuracy: 0.91307\n",
            "Epoch: 27/30, step: 88/364, loss: 0.22980, accuracy: 0.91353\n",
            "Epoch: 27/30, step: 89/364, loss: 0.22928, accuracy: 0.91380\n",
            "Epoch: 27/30, step: 90/364, loss: 0.22926, accuracy: 0.91372\n",
            "Epoch: 27/30, step: 91/364, loss: 0.22881, accuracy: 0.91398\n",
            "Epoch: 27/30, step: 92/364, loss: 0.22841, accuracy: 0.91440\n",
            "Epoch: 27/30, step: 93/364, loss: 0.22922, accuracy: 0.91381\n",
            "Epoch: 27/30, step: 94/364, loss: 0.22877, accuracy: 0.91423\n",
            "Epoch: 27/30, step: 95/364, loss: 0.22887, accuracy: 0.91414\n",
            "Epoch: 27/30, step: 96/364, loss: 0.22828, accuracy: 0.91439\n",
            "Epoch: 27/30, step: 97/364, loss: 0.22768, accuracy: 0.91479\n",
            "Epoch: 27/30, step: 98/364, loss: 0.22914, accuracy: 0.91406\n",
            "Epoch: 27/30, step: 99/364, loss: 0.22894, accuracy: 0.91446\n",
            "Epoch: 27/30, step: 100/364, loss: 0.22882, accuracy: 0.91484\n",
            "Epoch: 27/30, step: 101/364, loss: 0.22888, accuracy: 0.91460\n",
            "Epoch: 27/30, step: 102/364, loss: 0.22767, accuracy: 0.91529\n",
            "Epoch: 27/30, step: 103/364, loss: 0.22799, accuracy: 0.91520\n",
            "Epoch: 27/30, step: 104/364, loss: 0.22917, accuracy: 0.91436\n",
            "Epoch: 27/30, step: 105/364, loss: 0.22899, accuracy: 0.91458\n",
            "Epoch: 27/30, step: 106/364, loss: 0.22942, accuracy: 0.91465\n",
            "Epoch: 27/30, step: 107/364, loss: 0.22907, accuracy: 0.91487\n",
            "Epoch: 27/30, step: 108/364, loss: 0.22925, accuracy: 0.91479\n",
            "Epoch: 27/30, step: 109/364, loss: 0.22982, accuracy: 0.91442\n",
            "Epoch: 27/30, step: 110/364, loss: 0.22917, accuracy: 0.91477\n",
            "Epoch: 27/30, step: 111/364, loss: 0.22894, accuracy: 0.91484\n",
            "Epoch: 27/30, step: 112/364, loss: 0.22830, accuracy: 0.91518\n",
            "Epoch: 27/30, step: 113/364, loss: 0.22820, accuracy: 0.91538\n",
            "Epoch: 27/30, step: 114/364, loss: 0.22792, accuracy: 0.91557\n",
            "Epoch: 27/30, step: 115/364, loss: 0.22891, accuracy: 0.91535\n",
            "Epoch: 27/30, step: 116/364, loss: 0.22820, accuracy: 0.91568\n",
            "Epoch: 27/30, step: 117/364, loss: 0.22758, accuracy: 0.91613\n",
            "Epoch: 27/30, step: 118/364, loss: 0.22700, accuracy: 0.91658\n",
            "Epoch: 27/30, step: 119/364, loss: 0.22636, accuracy: 0.91689\n",
            "Epoch: 27/30, step: 120/364, loss: 0.22651, accuracy: 0.91693\n",
            "Epoch: 27/30, step: 121/364, loss: 0.22612, accuracy: 0.91723\n",
            "Epoch: 27/30, step: 122/364, loss: 0.22553, accuracy: 0.91752\n",
            "Epoch: 27/30, step: 123/364, loss: 0.22505, accuracy: 0.91781\n",
            "Epoch: 27/30, step: 124/364, loss: 0.22479, accuracy: 0.91784\n",
            "Epoch: 27/30, step: 125/364, loss: 0.22413, accuracy: 0.91838\n",
            "Epoch: 27/30, step: 126/364, loss: 0.22402, accuracy: 0.91853\n",
            "Epoch: 27/30, step: 127/364, loss: 0.22385, accuracy: 0.91843\n",
            "Epoch: 27/30, step: 128/364, loss: 0.22366, accuracy: 0.91858\n",
            "Epoch: 27/30, step: 129/364, loss: 0.22442, accuracy: 0.91812\n",
            "Epoch: 27/30, step: 130/364, loss: 0.22424, accuracy: 0.91827\n",
            "Epoch: 27/30, step: 131/364, loss: 0.22525, accuracy: 0.91794\n",
            "Epoch: 27/30, step: 132/364, loss: 0.22510, accuracy: 0.91797\n",
            "Epoch: 27/30, step: 133/364, loss: 0.22486, accuracy: 0.91800\n",
            "Epoch: 27/30, step: 134/364, loss: 0.22468, accuracy: 0.91791\n",
            "Epoch: 27/30, step: 135/364, loss: 0.22523, accuracy: 0.91736\n",
            "Epoch: 27/30, step: 136/364, loss: 0.22470, accuracy: 0.91785\n",
            "Epoch: 27/30, step: 137/364, loss: 0.22428, accuracy: 0.91811\n",
            "Epoch: 27/30, step: 138/364, loss: 0.22418, accuracy: 0.91814\n",
            "Epoch: 27/30, step: 139/364, loss: 0.22420, accuracy: 0.91828\n",
            "Epoch: 27/30, step: 140/364, loss: 0.22394, accuracy: 0.91830\n",
            "Epoch: 27/30, step: 141/364, loss: 0.22386, accuracy: 0.91844\n",
            "Epoch: 27/30, step: 142/364, loss: 0.22314, accuracy: 0.91890\n",
            "Epoch: 27/30, step: 143/364, loss: 0.22237, accuracy: 0.91936\n",
            "Epoch: 27/30, step: 144/364, loss: 0.22257, accuracy: 0.91905\n",
            "Epoch: 27/30, step: 145/364, loss: 0.22333, accuracy: 0.91864\n",
            "Epoch: 27/30, step: 146/364, loss: 0.22385, accuracy: 0.91824\n",
            "Epoch: 27/30, step: 147/364, loss: 0.22366, accuracy: 0.91826\n",
            "Epoch: 27/30, step: 148/364, loss: 0.22420, accuracy: 0.91829\n",
            "Epoch: 27/30, step: 149/364, loss: 0.22374, accuracy: 0.91862\n",
            "Epoch: 27/30, step: 150/364, loss: 0.22520, accuracy: 0.91802\n",
            "Epoch: 27/30, step: 151/364, loss: 0.22455, accuracy: 0.91825\n",
            "Epoch: 27/30, step: 152/364, loss: 0.22446, accuracy: 0.91817\n",
            "Epoch: 27/30, step: 153/364, loss: 0.22515, accuracy: 0.91779\n",
            "Epoch: 27/30, step: 154/364, loss: 0.22514, accuracy: 0.91792\n",
            "Epoch: 27/30, step: 155/364, loss: 0.22611, accuracy: 0.91673\n",
            "Epoch: 27/30, step: 156/364, loss: 0.22605, accuracy: 0.91657\n",
            "Epoch: 27/30, step: 157/364, loss: 0.22577, accuracy: 0.91670\n",
            "Epoch: 27/30, step: 158/364, loss: 0.22707, accuracy: 0.91594\n",
            "Epoch: 27/30, step: 159/364, loss: 0.22697, accuracy: 0.91588\n",
            "Epoch: 27/30, step: 160/364, loss: 0.22649, accuracy: 0.91611\n",
            "Epoch: 27/30, step: 161/364, loss: 0.22641, accuracy: 0.91615\n",
            "Epoch: 27/30, step: 162/364, loss: 0.22616, accuracy: 0.91647\n",
            "Epoch: 27/30, step: 163/364, loss: 0.22620, accuracy: 0.91641\n",
            "Epoch: 27/30, step: 164/364, loss: 0.22581, accuracy: 0.91654\n",
            "Epoch: 27/30, step: 165/364, loss: 0.22589, accuracy: 0.91648\n",
            "Epoch: 27/30, step: 166/364, loss: 0.22624, accuracy: 0.91604\n",
            "Epoch: 27/30, step: 167/364, loss: 0.22612, accuracy: 0.91607\n",
            "Epoch: 27/30, step: 168/364, loss: 0.22632, accuracy: 0.91592\n",
            "Epoch: 27/30, step: 169/364, loss: 0.22639, accuracy: 0.91577\n",
            "Epoch: 27/30, step: 170/364, loss: 0.22605, accuracy: 0.91608\n",
            "Epoch: 27/30, step: 171/364, loss: 0.22639, accuracy: 0.91594\n",
            "Epoch: 27/30, step: 172/364, loss: 0.22676, accuracy: 0.91579\n",
            "Epoch: 27/30, step: 173/364, loss: 0.22747, accuracy: 0.91519\n",
            "Epoch: 27/30, step: 174/364, loss: 0.22720, accuracy: 0.91541\n",
            "Epoch: 27/30, step: 175/364, loss: 0.22692, accuracy: 0.91545\n",
            "Epoch: 27/30, step: 176/364, loss: 0.22747, accuracy: 0.91504\n",
            "Epoch: 27/30, step: 177/364, loss: 0.22809, accuracy: 0.91455\n",
            "Epoch: 27/30, step: 178/364, loss: 0.22780, accuracy: 0.91485\n",
            "Epoch: 27/30, step: 179/364, loss: 0.22845, accuracy: 0.91419\n",
            "Epoch: 27/30, step: 180/364, loss: 0.22905, accuracy: 0.91424\n",
            "Epoch: 27/30, step: 181/364, loss: 0.22877, accuracy: 0.91445\n",
            "Epoch: 27/30, step: 182/364, loss: 0.22837, accuracy: 0.91466\n",
            "Epoch: 27/30, step: 183/364, loss: 0.22897, accuracy: 0.91428\n",
            "Epoch: 27/30, step: 184/364, loss: 0.22892, accuracy: 0.91432\n",
            "Epoch: 27/30, step: 185/364, loss: 0.22879, accuracy: 0.91427\n",
            "Epoch: 27/30, step: 186/364, loss: 0.22902, accuracy: 0.91406\n",
            "Epoch: 27/30, step: 187/364, loss: 0.22848, accuracy: 0.91427\n",
            "Epoch: 27/30, step: 188/364, loss: 0.22791, accuracy: 0.91456\n",
            "Epoch: 27/30, step: 189/364, loss: 0.22773, accuracy: 0.91477\n",
            "Epoch: 27/30, step: 190/364, loss: 0.22761, accuracy: 0.91488\n",
            "Epoch: 27/30, step: 191/364, loss: 0.22722, accuracy: 0.91509\n",
            "Epoch: 27/30, step: 192/364, loss: 0.22725, accuracy: 0.91520\n",
            "Epoch: 27/30, step: 193/364, loss: 0.22726, accuracy: 0.91516\n",
            "Epoch: 27/30, step: 194/364, loss: 0.22690, accuracy: 0.91551\n",
            "Epoch: 27/30, step: 195/364, loss: 0.22683, accuracy: 0.91546\n",
            "Epoch: 27/30, step: 196/364, loss: 0.22674, accuracy: 0.91534\n",
            "Epoch: 27/30, step: 197/364, loss: 0.22648, accuracy: 0.91553\n",
            "Epoch: 27/30, step: 198/364, loss: 0.22659, accuracy: 0.91548\n",
            "Epoch: 27/30, step: 199/364, loss: 0.22632, accuracy: 0.91559\n",
            "Epoch: 27/30, step: 200/364, loss: 0.22606, accuracy: 0.91578\n",
            "Epoch: 27/30, step: 201/364, loss: 0.22603, accuracy: 0.91566\n",
            "Epoch: 27/30, step: 202/364, loss: 0.22575, accuracy: 0.91561\n",
            "Epoch: 27/30, step: 203/364, loss: 0.22559, accuracy: 0.91572\n",
            "Epoch: 27/30, step: 204/364, loss: 0.22561, accuracy: 0.91567\n",
            "Epoch: 27/30, step: 205/364, loss: 0.22577, accuracy: 0.91578\n",
            "Epoch: 27/30, step: 206/364, loss: 0.22562, accuracy: 0.91573\n",
            "Epoch: 27/30, step: 207/364, loss: 0.22606, accuracy: 0.91546\n",
            "Epoch: 27/30, step: 208/364, loss: 0.22558, accuracy: 0.91572\n",
            "Epoch: 27/30, step: 209/364, loss: 0.22533, accuracy: 0.91574\n",
            "Epoch: 27/30, step: 210/364, loss: 0.22557, accuracy: 0.91562\n",
            "Epoch: 27/30, step: 211/364, loss: 0.22586, accuracy: 0.91551\n",
            "Epoch: 27/30, step: 212/364, loss: 0.22612, accuracy: 0.91524\n",
            "Epoch: 27/30, step: 213/364, loss: 0.22581, accuracy: 0.91535\n",
            "Epoch: 27/30, step: 214/364, loss: 0.22556, accuracy: 0.91545\n",
            "Epoch: 27/30, step: 215/364, loss: 0.22583, accuracy: 0.91512\n",
            "Epoch: 27/30, step: 216/364, loss: 0.22599, accuracy: 0.91508\n",
            "Epoch: 27/30, step: 217/364, loss: 0.22581, accuracy: 0.91511\n",
            "Epoch: 27/30, step: 218/364, loss: 0.22561, accuracy: 0.91521\n",
            "Epoch: 27/30, step: 219/364, loss: 0.22558, accuracy: 0.91524\n",
            "Epoch: 27/30, step: 220/364, loss: 0.22561, accuracy: 0.91506\n",
            "Epoch: 27/30, step: 221/364, loss: 0.22557, accuracy: 0.91509\n",
            "Epoch: 27/30, step: 222/364, loss: 0.22527, accuracy: 0.91526\n",
            "Epoch: 27/30, step: 223/364, loss: 0.22519, accuracy: 0.91536\n",
            "Epoch: 27/30, step: 224/364, loss: 0.22535, accuracy: 0.91525\n",
            "Epoch: 27/30, step: 225/364, loss: 0.22584, accuracy: 0.91500\n",
            "Epoch: 27/30, step: 226/364, loss: 0.22541, accuracy: 0.91517\n",
            "Epoch: 27/30, step: 227/364, loss: 0.22505, accuracy: 0.91547\n",
            "Epoch: 27/30, step: 228/364, loss: 0.22495, accuracy: 0.91550\n",
            "Epoch: 27/30, step: 229/364, loss: 0.22538, accuracy: 0.91519\n",
            "Epoch: 27/30, step: 230/364, loss: 0.22542, accuracy: 0.91515\n",
            "Epoch: 27/30, step: 231/364, loss: 0.22508, accuracy: 0.91538\n",
            "Epoch: 27/30, step: 232/364, loss: 0.22500, accuracy: 0.91541\n",
            "Epoch: 27/30, step: 233/364, loss: 0.22457, accuracy: 0.91557\n",
            "Epoch: 27/30, step: 234/364, loss: 0.22444, accuracy: 0.91580\n",
            "Epoch: 27/30, step: 235/364, loss: 0.22460, accuracy: 0.91556\n",
            "Epoch: 27/30, step: 236/364, loss: 0.22432, accuracy: 0.91585\n",
            "Epoch: 27/30, step: 237/364, loss: 0.22460, accuracy: 0.91568\n",
            "Epoch: 27/30, step: 238/364, loss: 0.22412, accuracy: 0.91603\n",
            "Epoch: 27/30, step: 239/364, loss: 0.22387, accuracy: 0.91619\n",
            "Epoch: 27/30, step: 240/364, loss: 0.22458, accuracy: 0.91569\n",
            "Epoch: 27/30, step: 241/364, loss: 0.22442, accuracy: 0.91585\n",
            "Epoch: 27/30, step: 242/364, loss: 0.22441, accuracy: 0.91581\n",
            "Epoch: 27/30, step: 243/364, loss: 0.22448, accuracy: 0.91570\n",
            "Epoch: 27/30, step: 244/364, loss: 0.22453, accuracy: 0.91566\n",
            "Epoch: 27/30, step: 245/364, loss: 0.22460, accuracy: 0.91556\n",
            "Epoch: 27/30, step: 246/364, loss: 0.22427, accuracy: 0.91565\n",
            "Epoch: 27/30, step: 247/364, loss: 0.22444, accuracy: 0.91549\n",
            "Epoch: 27/30, step: 248/364, loss: 0.22419, accuracy: 0.91564\n",
            "Epoch: 27/30, step: 249/364, loss: 0.22406, accuracy: 0.91566\n",
            "Epoch: 27/30, step: 250/364, loss: 0.22375, accuracy: 0.91581\n",
            "Epoch: 27/30, step: 251/364, loss: 0.22386, accuracy: 0.91577\n",
            "Epoch: 27/30, step: 252/364, loss: 0.22395, accuracy: 0.91567\n",
            "Epoch: 27/30, step: 253/364, loss: 0.22380, accuracy: 0.91576\n",
            "Epoch: 27/30, step: 254/364, loss: 0.22402, accuracy: 0.91566\n",
            "Epoch: 27/30, step: 255/364, loss: 0.22418, accuracy: 0.91550\n",
            "Epoch: 27/30, step: 256/364, loss: 0.22402, accuracy: 0.91553\n",
            "Epoch: 27/30, step: 257/364, loss: 0.22424, accuracy: 0.91555\n",
            "Epoch: 27/30, step: 258/364, loss: 0.22416, accuracy: 0.91558\n",
            "Epoch: 27/30, step: 259/364, loss: 0.22474, accuracy: 0.91524\n",
            "Epoch: 27/30, step: 260/364, loss: 0.22507, accuracy: 0.91502\n",
            "Epoch: 27/30, step: 261/364, loss: 0.22505, accuracy: 0.91499\n",
            "Epoch: 27/30, step: 262/364, loss: 0.22522, accuracy: 0.91496\n",
            "Epoch: 27/30, step: 263/364, loss: 0.22491, accuracy: 0.91516\n",
            "Epoch: 27/30, step: 264/364, loss: 0.22473, accuracy: 0.91525\n",
            "Epoch: 27/30, step: 265/364, loss: 0.22459, accuracy: 0.91521\n",
            "Epoch: 27/30, step: 266/364, loss: 0.22448, accuracy: 0.91518\n",
            "Epoch: 27/30, step: 267/364, loss: 0.22453, accuracy: 0.91515\n",
            "Epoch: 27/30, step: 268/364, loss: 0.22462, accuracy: 0.91511\n",
            "Epoch: 27/30, step: 269/364, loss: 0.22486, accuracy: 0.91485\n",
            "Epoch: 27/30, step: 270/364, loss: 0.22505, accuracy: 0.91464\n",
            "Epoch: 27/30, step: 271/364, loss: 0.22495, accuracy: 0.91461\n",
            "Epoch: 27/30, step: 272/364, loss: 0.22499, accuracy: 0.91458\n",
            "Epoch: 27/30, step: 273/364, loss: 0.22509, accuracy: 0.91449\n",
            "Epoch: 27/30, step: 274/364, loss: 0.22501, accuracy: 0.91452\n",
            "Epoch: 27/30, step: 275/364, loss: 0.22561, accuracy: 0.91409\n",
            "Epoch: 27/30, step: 276/364, loss: 0.22563, accuracy: 0.91412\n",
            "Epoch: 27/30, step: 277/364, loss: 0.22567, accuracy: 0.91415\n",
            "Epoch: 27/30, step: 278/364, loss: 0.22546, accuracy: 0.91429\n",
            "Epoch: 27/30, step: 279/364, loss: 0.22522, accuracy: 0.91443\n",
            "Epoch: 27/30, step: 280/364, loss: 0.22489, accuracy: 0.91462\n",
            "Epoch: 27/30, step: 281/364, loss: 0.22527, accuracy: 0.91454\n",
            "Epoch: 27/30, step: 282/364, loss: 0.22505, accuracy: 0.91467\n",
            "Epoch: 27/30, step: 283/364, loss: 0.22491, accuracy: 0.91459\n",
            "Epoch: 27/30, step: 284/364, loss: 0.22463, accuracy: 0.91478\n",
            "Epoch: 27/30, step: 285/364, loss: 0.22459, accuracy: 0.91480\n",
            "Epoch: 27/30, step: 286/364, loss: 0.22531, accuracy: 0.91450\n",
            "Epoch: 27/30, step: 287/364, loss: 0.22507, accuracy: 0.91458\n",
            "Epoch: 27/30, step: 288/364, loss: 0.22493, accuracy: 0.91461\n",
            "Epoch: 27/30, step: 289/364, loss: 0.22592, accuracy: 0.91414\n",
            "Epoch: 27/30, step: 290/364, loss: 0.22569, accuracy: 0.91428\n",
            "Epoch: 27/30, step: 291/364, loss: 0.22632, accuracy: 0.91392\n",
            "Epoch: 27/30, train loss: 0.22632, train accuracy: 0.91392, valid loss: 0.74749, valid accuracy: 0.68487\n",
            "Epoch: 28/30, step: 1/364, loss: 0.24367, accuracy: 0.87500\n",
            "Epoch: 28/30, step: 2/364, loss: 0.22411, accuracy: 0.89844\n",
            "Epoch: 28/30, step: 3/364, loss: 0.19791, accuracy: 0.92708\n",
            "Epoch: 28/30, step: 4/364, loss: 0.26711, accuracy: 0.89453\n",
            "Epoch: 28/30, step: 5/364, loss: 0.25318, accuracy: 0.90312\n",
            "Epoch: 28/30, step: 6/364, loss: 0.25487, accuracy: 0.89583\n",
            "Epoch: 28/30, step: 7/364, loss: 0.24298, accuracy: 0.89955\n",
            "Epoch: 28/30, step: 8/364, loss: 0.23036, accuracy: 0.90625\n",
            "Epoch: 28/30, step: 9/364, loss: 0.22767, accuracy: 0.90799\n",
            "Epoch: 28/30, step: 10/364, loss: 0.22665, accuracy: 0.90938\n",
            "Epoch: 28/30, step: 11/364, loss: 0.22019, accuracy: 0.91193\n",
            "Epoch: 28/30, step: 12/364, loss: 0.21368, accuracy: 0.91797\n",
            "Epoch: 28/30, step: 13/364, loss: 0.20882, accuracy: 0.91947\n",
            "Epoch: 28/30, step: 14/364, loss: 0.21863, accuracy: 0.91295\n",
            "Epoch: 28/30, step: 15/364, loss: 0.21333, accuracy: 0.91458\n",
            "Epoch: 28/30, step: 16/364, loss: 0.21020, accuracy: 0.91699\n",
            "Epoch: 28/30, step: 17/364, loss: 0.21396, accuracy: 0.91268\n",
            "Epoch: 28/30, step: 18/364, loss: 0.20884, accuracy: 0.91406\n",
            "Epoch: 28/30, step: 19/364, loss: 0.21010, accuracy: 0.91447\n",
            "Epoch: 28/30, step: 20/364, loss: 0.21076, accuracy: 0.91328\n",
            "Epoch: 28/30, step: 21/364, loss: 0.21327, accuracy: 0.91220\n",
            "Epoch: 28/30, step: 22/364, loss: 0.20977, accuracy: 0.91264\n",
            "Epoch: 28/30, step: 23/364, loss: 0.21239, accuracy: 0.91168\n",
            "Epoch: 28/30, step: 24/364, loss: 0.21213, accuracy: 0.91341\n",
            "Epoch: 28/30, step: 25/364, loss: 0.21733, accuracy: 0.91188\n",
            "Epoch: 28/30, step: 26/364, loss: 0.22268, accuracy: 0.90865\n",
            "Epoch: 28/30, step: 27/364, loss: 0.21910, accuracy: 0.91146\n",
            "Epoch: 28/30, step: 28/364, loss: 0.21842, accuracy: 0.91239\n",
            "Epoch: 28/30, step: 29/364, loss: 0.21879, accuracy: 0.91325\n",
            "Epoch: 28/30, step: 30/364, loss: 0.21692, accuracy: 0.91406\n",
            "Epoch: 28/30, step: 31/364, loss: 0.21885, accuracy: 0.91280\n",
            "Epoch: 28/30, step: 32/364, loss: 0.22072, accuracy: 0.91064\n",
            "Epoch: 28/30, step: 33/364, loss: 0.21876, accuracy: 0.91241\n",
            "Epoch: 28/30, step: 34/364, loss: 0.21563, accuracy: 0.91452\n",
            "Epoch: 28/30, step: 35/364, loss: 0.21362, accuracy: 0.91562\n",
            "Epoch: 28/30, step: 36/364, loss: 0.21260, accuracy: 0.91667\n",
            "Epoch: 28/30, step: 37/364, loss: 0.21394, accuracy: 0.91596\n",
            "Epoch: 28/30, step: 38/364, loss: 0.21289, accuracy: 0.91653\n",
            "Epoch: 28/30, step: 39/364, loss: 0.21322, accuracy: 0.91627\n",
            "Epoch: 28/30, step: 40/364, loss: 0.21304, accuracy: 0.91602\n",
            "Epoch: 28/30, step: 41/364, loss: 0.21267, accuracy: 0.91616\n",
            "Epoch: 28/30, step: 42/364, loss: 0.21125, accuracy: 0.91704\n",
            "Epoch: 28/30, step: 43/364, loss: 0.21295, accuracy: 0.91570\n",
            "Epoch: 28/30, step: 44/364, loss: 0.21304, accuracy: 0.91584\n",
            "Epoch: 28/30, step: 45/364, loss: 0.21333, accuracy: 0.91597\n",
            "Epoch: 28/30, step: 46/364, loss: 0.21714, accuracy: 0.91508\n",
            "Epoch: 28/30, step: 47/364, loss: 0.21569, accuracy: 0.91622\n",
            "Epoch: 28/30, step: 48/364, loss: 0.21369, accuracy: 0.91732\n",
            "Epoch: 28/30, step: 49/364, loss: 0.21198, accuracy: 0.91869\n",
            "Epoch: 28/30, step: 50/364, loss: 0.21042, accuracy: 0.91969\n",
            "Epoch: 28/30, step: 51/364, loss: 0.21107, accuracy: 0.91912\n",
            "Epoch: 28/30, step: 52/364, loss: 0.21120, accuracy: 0.91917\n",
            "Epoch: 28/30, step: 53/364, loss: 0.21174, accuracy: 0.91922\n",
            "Epoch: 28/30, step: 54/364, loss: 0.21010, accuracy: 0.92043\n",
            "Epoch: 28/30, step: 55/364, loss: 0.20868, accuracy: 0.92102\n",
            "Epoch: 28/30, step: 56/364, loss: 0.20700, accuracy: 0.92188\n",
            "Epoch: 28/30, step: 57/364, loss: 0.20800, accuracy: 0.92160\n",
            "Epoch: 28/30, step: 58/364, loss: 0.20963, accuracy: 0.92080\n",
            "Epoch: 28/30, step: 59/364, loss: 0.20875, accuracy: 0.92135\n",
            "Epoch: 28/30, step: 60/364, loss: 0.20845, accuracy: 0.92188\n",
            "Epoch: 28/30, step: 61/364, loss: 0.20920, accuracy: 0.92162\n",
            "Epoch: 28/30, step: 62/364, loss: 0.20954, accuracy: 0.92137\n",
            "Epoch: 28/30, step: 63/364, loss: 0.20937, accuracy: 0.92138\n",
            "Epoch: 28/30, step: 64/364, loss: 0.20969, accuracy: 0.92139\n",
            "Epoch: 28/30, step: 65/364, loss: 0.20950, accuracy: 0.92139\n",
            "Epoch: 28/30, step: 66/364, loss: 0.21151, accuracy: 0.91998\n",
            "Epoch: 28/30, step: 67/364, loss: 0.21152, accuracy: 0.92048\n",
            "Epoch: 28/30, step: 68/364, loss: 0.21016, accuracy: 0.92142\n",
            "Epoch: 28/30, step: 69/364, loss: 0.21066, accuracy: 0.92074\n",
            "Epoch: 28/30, step: 70/364, loss: 0.21019, accuracy: 0.92121\n",
            "Epoch: 28/30, step: 71/364, loss: 0.20944, accuracy: 0.92165\n",
            "Epoch: 28/30, step: 72/364, loss: 0.20959, accuracy: 0.92166\n",
            "Epoch: 28/30, step: 73/364, loss: 0.21100, accuracy: 0.92059\n",
            "Epoch: 28/30, step: 74/364, loss: 0.21210, accuracy: 0.91976\n",
            "Epoch: 28/30, step: 75/364, loss: 0.21156, accuracy: 0.92021\n",
            "Epoch: 28/30, step: 76/364, loss: 0.21138, accuracy: 0.92002\n",
            "Epoch: 28/30, step: 77/364, loss: 0.21204, accuracy: 0.91883\n",
            "Epoch: 28/30, step: 78/364, loss: 0.21185, accuracy: 0.91927\n",
            "Epoch: 28/30, step: 79/364, loss: 0.21192, accuracy: 0.91911\n",
            "Epoch: 28/30, step: 80/364, loss: 0.21172, accuracy: 0.91914\n",
            "Epoch: 28/30, step: 81/364, loss: 0.21138, accuracy: 0.91937\n",
            "Epoch: 28/30, step: 82/364, loss: 0.21359, accuracy: 0.91806\n",
            "Epoch: 28/30, step: 83/364, loss: 0.21355, accuracy: 0.91792\n",
            "Epoch: 28/30, step: 84/364, loss: 0.21294, accuracy: 0.91834\n",
            "Epoch: 28/30, step: 85/364, loss: 0.21289, accuracy: 0.91857\n",
            "Epoch: 28/30, step: 86/364, loss: 0.21305, accuracy: 0.91824\n",
            "Epoch: 28/30, step: 87/364, loss: 0.21325, accuracy: 0.91828\n",
            "Epoch: 28/30, step: 88/364, loss: 0.21340, accuracy: 0.91815\n",
            "Epoch: 28/30, step: 89/364, loss: 0.21267, accuracy: 0.91907\n",
            "Epoch: 28/30, step: 90/364, loss: 0.21517, accuracy: 0.91719\n",
            "Epoch: 28/30, step: 91/364, loss: 0.21473, accuracy: 0.91758\n",
            "Epoch: 28/30, step: 92/364, loss: 0.21441, accuracy: 0.91780\n",
            "Epoch: 28/30, step: 93/364, loss: 0.21372, accuracy: 0.91818\n",
            "Epoch: 28/30, step: 94/364, loss: 0.21537, accuracy: 0.91755\n",
            "Epoch: 28/30, step: 95/364, loss: 0.21540, accuracy: 0.91743\n",
            "Epoch: 28/30, step: 96/364, loss: 0.21477, accuracy: 0.91781\n",
            "Epoch: 28/30, step: 97/364, loss: 0.21400, accuracy: 0.91833\n",
            "Epoch: 28/30, step: 98/364, loss: 0.21436, accuracy: 0.91805\n",
            "Epoch: 28/30, step: 99/364, loss: 0.21411, accuracy: 0.91809\n",
            "Epoch: 28/30, step: 100/364, loss: 0.21481, accuracy: 0.91781\n",
            "Epoch: 28/30, step: 101/364, loss: 0.21389, accuracy: 0.91832\n",
            "Epoch: 28/30, step: 102/364, loss: 0.21523, accuracy: 0.91774\n",
            "Epoch: 28/30, step: 103/364, loss: 0.21548, accuracy: 0.91763\n",
            "Epoch: 28/30, step: 104/364, loss: 0.21542, accuracy: 0.91752\n",
            "Epoch: 28/30, step: 105/364, loss: 0.21550, accuracy: 0.91726\n",
            "Epoch: 28/30, step: 106/364, loss: 0.21458, accuracy: 0.91790\n",
            "Epoch: 28/30, step: 107/364, loss: 0.21464, accuracy: 0.91808\n",
            "Epoch: 28/30, step: 108/364, loss: 0.21456, accuracy: 0.91840\n",
            "Epoch: 28/30, step: 109/364, loss: 0.21513, accuracy: 0.91757\n",
            "Epoch: 28/30, step: 110/364, loss: 0.21414, accuracy: 0.91818\n",
            "Epoch: 28/30, step: 111/364, loss: 0.21399, accuracy: 0.91807\n",
            "Epoch: 28/30, step: 112/364, loss: 0.21438, accuracy: 0.91755\n",
            "Epoch: 28/30, step: 113/364, loss: 0.21409, accuracy: 0.91787\n",
            "Epoch: 28/30, step: 114/364, loss: 0.21401, accuracy: 0.91790\n",
            "Epoch: 28/30, step: 115/364, loss: 0.21365, accuracy: 0.91780\n",
            "Epoch: 28/30, step: 116/364, loss: 0.21365, accuracy: 0.91783\n",
            "Epoch: 28/30, step: 117/364, loss: 0.21454, accuracy: 0.91733\n",
            "Epoch: 28/30, step: 118/364, loss: 0.21416, accuracy: 0.91751\n",
            "Epoch: 28/30, step: 119/364, loss: 0.21432, accuracy: 0.91741\n",
            "Epoch: 28/30, step: 120/364, loss: 0.21388, accuracy: 0.91771\n",
            "Epoch: 28/30, step: 121/364, loss: 0.21385, accuracy: 0.91761\n",
            "Epoch: 28/30, step: 122/364, loss: 0.21353, accuracy: 0.91803\n",
            "Epoch: 28/30, step: 123/364, loss: 0.21356, accuracy: 0.91781\n",
            "Epoch: 28/30, step: 124/364, loss: 0.21370, accuracy: 0.91746\n",
            "Epoch: 28/30, step: 125/364, loss: 0.21324, accuracy: 0.91763\n",
            "Epoch: 28/30, step: 126/364, loss: 0.21306, accuracy: 0.91766\n",
            "Epoch: 28/30, step: 127/364, loss: 0.21269, accuracy: 0.91769\n",
            "Epoch: 28/30, step: 128/364, loss: 0.21280, accuracy: 0.91785\n",
            "Epoch: 28/30, step: 129/364, loss: 0.21273, accuracy: 0.91800\n",
            "Epoch: 28/30, step: 130/364, loss: 0.21230, accuracy: 0.91827\n",
            "Epoch: 28/30, step: 131/364, loss: 0.21259, accuracy: 0.91806\n",
            "Epoch: 28/30, step: 132/364, loss: 0.21257, accuracy: 0.91809\n",
            "Epoch: 28/30, step: 133/364, loss: 0.21209, accuracy: 0.91835\n",
            "Epoch: 28/30, step: 134/364, loss: 0.21209, accuracy: 0.91826\n",
            "Epoch: 28/30, step: 135/364, loss: 0.21318, accuracy: 0.91748\n",
            "Epoch: 28/30, step: 136/364, loss: 0.21288, accuracy: 0.91751\n",
            "Epoch: 28/30, step: 137/364, loss: 0.21249, accuracy: 0.91788\n",
            "Epoch: 28/30, step: 138/364, loss: 0.21229, accuracy: 0.91803\n",
            "Epoch: 28/30, step: 139/364, loss: 0.21181, accuracy: 0.91817\n",
            "Epoch: 28/30, step: 140/364, loss: 0.21139, accuracy: 0.91830\n",
            "Epoch: 28/30, step: 141/364, loss: 0.21086, accuracy: 0.91866\n",
            "Epoch: 28/30, step: 142/364, loss: 0.21167, accuracy: 0.91846\n",
            "Epoch: 28/30, step: 143/364, loss: 0.21157, accuracy: 0.91860\n",
            "Epoch: 28/30, step: 144/364, loss: 0.21098, accuracy: 0.91905\n",
            "Epoch: 28/30, step: 145/364, loss: 0.21275, accuracy: 0.91810\n",
            "Epoch: 28/30, step: 146/364, loss: 0.21245, accuracy: 0.91834\n",
            "Epoch: 28/30, step: 147/364, loss: 0.21241, accuracy: 0.91847\n",
            "Epoch: 28/30, step: 148/364, loss: 0.21204, accuracy: 0.91860\n",
            "Epoch: 28/30, step: 149/364, loss: 0.21160, accuracy: 0.91904\n",
            "Epoch: 28/30, step: 150/364, loss: 0.21225, accuracy: 0.91875\n",
            "Epoch: 28/30, step: 151/364, loss: 0.21279, accuracy: 0.91856\n",
            "Epoch: 28/30, step: 152/364, loss: 0.21292, accuracy: 0.91848\n",
            "Epoch: 28/30, step: 153/364, loss: 0.21250, accuracy: 0.91881\n",
            "Epoch: 28/30, step: 154/364, loss: 0.21206, accuracy: 0.91903\n",
            "Epoch: 28/30, step: 155/364, loss: 0.21195, accuracy: 0.91915\n",
            "Epoch: 28/30, step: 156/364, loss: 0.21255, accuracy: 0.91887\n",
            "Epoch: 28/30, step: 157/364, loss: 0.21226, accuracy: 0.91899\n",
            "Epoch: 28/30, step: 158/364, loss: 0.21264, accuracy: 0.91871\n",
            "Epoch: 28/30, step: 159/364, loss: 0.21265, accuracy: 0.91873\n",
            "Epoch: 28/30, step: 160/364, loss: 0.21244, accuracy: 0.91885\n",
            "Epoch: 28/30, step: 161/364, loss: 0.21288, accuracy: 0.91867\n",
            "Epoch: 28/30, step: 162/364, loss: 0.21313, accuracy: 0.91850\n",
            "Epoch: 28/30, step: 163/364, loss: 0.21247, accuracy: 0.91890\n",
            "Epoch: 28/30, step: 164/364, loss: 0.21173, accuracy: 0.91930\n",
            "Epoch: 28/30, step: 165/364, loss: 0.21144, accuracy: 0.91951\n",
            "Epoch: 28/30, step: 166/364, loss: 0.21097, accuracy: 0.91990\n",
            "Epoch: 28/30, step: 167/364, loss: 0.21052, accuracy: 0.92019\n",
            "Epoch: 28/30, step: 168/364, loss: 0.21086, accuracy: 0.92011\n",
            "Epoch: 28/30, step: 169/364, loss: 0.21072, accuracy: 0.92021\n",
            "Epoch: 28/30, step: 170/364, loss: 0.21018, accuracy: 0.92050\n",
            "Epoch: 28/30, step: 171/364, loss: 0.20991, accuracy: 0.92050\n",
            "Epoch: 28/30, step: 172/364, loss: 0.21013, accuracy: 0.92042\n",
            "Epoch: 28/30, step: 173/364, loss: 0.20964, accuracy: 0.92070\n",
            "Epoch: 28/30, step: 174/364, loss: 0.20924, accuracy: 0.92089\n",
            "Epoch: 28/30, step: 175/364, loss: 0.20992, accuracy: 0.92027\n",
            "Epoch: 28/30, step: 176/364, loss: 0.20949, accuracy: 0.92045\n",
            "Epoch: 28/30, step: 177/364, loss: 0.20952, accuracy: 0.92046\n",
            "Epoch: 28/30, step: 178/364, loss: 0.20909, accuracy: 0.92073\n",
            "Epoch: 28/30, step: 179/364, loss: 0.20899, accuracy: 0.92083\n",
            "Epoch: 28/30, step: 180/364, loss: 0.20899, accuracy: 0.92083\n",
            "Epoch: 28/30, step: 181/364, loss: 0.20930, accuracy: 0.92093\n",
            "Epoch: 28/30, step: 182/364, loss: 0.20942, accuracy: 0.92076\n",
            "Epoch: 28/30, step: 183/364, loss: 0.20938, accuracy: 0.92068\n",
            "Epoch: 28/30, step: 184/364, loss: 0.20953, accuracy: 0.92077\n",
            "Epoch: 28/30, step: 185/364, loss: 0.20911, accuracy: 0.92095\n",
            "Epoch: 28/30, step: 186/364, loss: 0.20895, accuracy: 0.92120\n",
            "Epoch: 28/30, step: 187/364, loss: 0.20883, accuracy: 0.92129\n",
            "Epoch: 28/30, step: 188/364, loss: 0.20899, accuracy: 0.92113\n",
            "Epoch: 28/30, step: 189/364, loss: 0.20916, accuracy: 0.92097\n",
            "Epoch: 28/30, step: 190/364, loss: 0.20874, accuracy: 0.92130\n",
            "Epoch: 28/30, step: 191/364, loss: 0.20846, accuracy: 0.92130\n",
            "Epoch: 28/30, step: 192/364, loss: 0.20920, accuracy: 0.92098\n",
            "Epoch: 28/30, step: 193/364, loss: 0.20925, accuracy: 0.92107\n",
            "Epoch: 28/30, step: 194/364, loss: 0.20934, accuracy: 0.92099\n",
            "Epoch: 28/30, step: 195/364, loss: 0.20963, accuracy: 0.92099\n",
            "Epoch: 28/30, step: 196/364, loss: 0.20905, accuracy: 0.92132\n",
            "Epoch: 28/30, step: 197/364, loss: 0.20904, accuracy: 0.92132\n",
            "Epoch: 28/30, step: 198/364, loss: 0.20859, accuracy: 0.92156\n",
            "Epoch: 28/30, step: 199/364, loss: 0.20829, accuracy: 0.92164\n",
            "Epoch: 28/30, step: 200/364, loss: 0.20793, accuracy: 0.92195\n",
            "Epoch: 28/30, step: 201/364, loss: 0.20819, accuracy: 0.92172\n",
            "Epoch: 28/30, step: 202/364, loss: 0.20815, accuracy: 0.92172\n",
            "Epoch: 28/30, step: 203/364, loss: 0.20792, accuracy: 0.92180\n",
            "Epoch: 28/30, step: 204/364, loss: 0.20817, accuracy: 0.92165\n",
            "Epoch: 28/30, step: 205/364, loss: 0.20831, accuracy: 0.92157\n",
            "Epoch: 28/30, step: 206/364, loss: 0.20798, accuracy: 0.92180\n",
            "Epoch: 28/30, step: 207/364, loss: 0.20772, accuracy: 0.92195\n",
            "Epoch: 28/30, step: 208/364, loss: 0.20758, accuracy: 0.92195\n",
            "Epoch: 28/30, step: 209/364, loss: 0.20746, accuracy: 0.92217\n",
            "Epoch: 28/30, step: 210/364, loss: 0.20718, accuracy: 0.92225\n",
            "Epoch: 28/30, step: 211/364, loss: 0.20704, accuracy: 0.92239\n",
            "Epoch: 28/30, step: 212/364, loss: 0.20692, accuracy: 0.92246\n",
            "Epoch: 28/30, step: 213/364, loss: 0.20693, accuracy: 0.92239\n",
            "Epoch: 28/30, step: 214/364, loss: 0.20673, accuracy: 0.92246\n",
            "Epoch: 28/30, step: 215/364, loss: 0.20692, accuracy: 0.92224\n",
            "Epoch: 28/30, step: 216/364, loss: 0.20699, accuracy: 0.92224\n",
            "Epoch: 28/30, step: 217/364, loss: 0.20678, accuracy: 0.92238\n",
            "Epoch: 28/30, step: 218/364, loss: 0.20725, accuracy: 0.92231\n",
            "Epoch: 28/30, step: 219/364, loss: 0.20700, accuracy: 0.92245\n",
            "Epoch: 28/30, step: 220/364, loss: 0.20737, accuracy: 0.92209\n",
            "Epoch: 28/30, step: 221/364, loss: 0.20704, accuracy: 0.92230\n",
            "Epoch: 28/30, step: 222/364, loss: 0.20657, accuracy: 0.92258\n",
            "Epoch: 28/30, step: 223/364, loss: 0.20663, accuracy: 0.92258\n",
            "Epoch: 28/30, step: 224/364, loss: 0.20701, accuracy: 0.92236\n",
            "Epoch: 28/30, step: 225/364, loss: 0.20802, accuracy: 0.92174\n",
            "Epoch: 28/30, step: 226/364, loss: 0.20856, accuracy: 0.92146\n",
            "Epoch: 28/30, step: 227/364, loss: 0.20849, accuracy: 0.92153\n",
            "Epoch: 28/30, step: 228/364, loss: 0.20857, accuracy: 0.92160\n",
            "Epoch: 28/30, step: 229/364, loss: 0.20911, accuracy: 0.92153\n",
            "Epoch: 28/30, step: 230/364, loss: 0.20885, accuracy: 0.92160\n",
            "Epoch: 28/30, step: 231/364, loss: 0.20885, accuracy: 0.92147\n",
            "Epoch: 28/30, step: 232/364, loss: 0.20887, accuracy: 0.92140\n",
            "Epoch: 28/30, step: 233/364, loss: 0.20882, accuracy: 0.92127\n",
            "Epoch: 28/30, step: 234/364, loss: 0.20850, accuracy: 0.92154\n",
            "Epoch: 28/30, step: 235/364, loss: 0.20822, accuracy: 0.92181\n",
            "Epoch: 28/30, step: 236/364, loss: 0.20794, accuracy: 0.92201\n",
            "Epoch: 28/30, step: 237/364, loss: 0.20822, accuracy: 0.92194\n",
            "Epoch: 28/30, step: 238/364, loss: 0.20894, accuracy: 0.92168\n",
            "Epoch: 28/30, step: 239/364, loss: 0.20956, accuracy: 0.92148\n",
            "Epoch: 28/30, step: 240/364, loss: 0.20942, accuracy: 0.92161\n",
            "Epoch: 28/30, step: 241/364, loss: 0.20971, accuracy: 0.92129\n",
            "Epoch: 28/30, step: 242/364, loss: 0.20981, accuracy: 0.92116\n",
            "Epoch: 28/30, step: 243/364, loss: 0.21026, accuracy: 0.92104\n",
            "Epoch: 28/30, step: 244/364, loss: 0.21023, accuracy: 0.92098\n",
            "Epoch: 28/30, step: 245/364, loss: 0.20986, accuracy: 0.92117\n",
            "Epoch: 28/30, step: 246/364, loss: 0.20978, accuracy: 0.92111\n",
            "Epoch: 28/30, step: 247/364, loss: 0.20972, accuracy: 0.92105\n",
            "Epoch: 28/30, step: 248/364, loss: 0.20944, accuracy: 0.92106\n",
            "Epoch: 28/30, step: 249/364, loss: 0.20973, accuracy: 0.92100\n",
            "Epoch: 28/30, step: 250/364, loss: 0.20950, accuracy: 0.92106\n",
            "Epoch: 28/30, step: 251/364, loss: 0.20936, accuracy: 0.92113\n",
            "Epoch: 28/30, step: 252/364, loss: 0.20904, accuracy: 0.92138\n",
            "Epoch: 28/30, step: 253/364, loss: 0.20897, accuracy: 0.92126\n",
            "Epoch: 28/30, step: 254/364, loss: 0.20916, accuracy: 0.92114\n",
            "Epoch: 28/30, step: 255/364, loss: 0.20939, accuracy: 0.92083\n",
            "Epoch: 28/30, step: 256/364, loss: 0.20945, accuracy: 0.92072\n",
            "Epoch: 28/30, step: 257/364, loss: 0.20948, accuracy: 0.92066\n",
            "Epoch: 28/30, step: 258/364, loss: 0.20971, accuracy: 0.92054\n",
            "Epoch: 28/30, step: 259/364, loss: 0.20995, accuracy: 0.92043\n",
            "Epoch: 28/30, step: 260/364, loss: 0.21035, accuracy: 0.92007\n",
            "Epoch: 28/30, step: 261/364, loss: 0.21014, accuracy: 0.92026\n",
            "Epoch: 28/30, step: 262/364, loss: 0.21001, accuracy: 0.92032\n",
            "Epoch: 28/30, step: 263/364, loss: 0.21040, accuracy: 0.92021\n",
            "Epoch: 28/30, step: 264/364, loss: 0.21080, accuracy: 0.92004\n",
            "Epoch: 28/30, step: 265/364, loss: 0.21044, accuracy: 0.92022\n",
            "Epoch: 28/30, step: 266/364, loss: 0.21011, accuracy: 0.92041\n",
            "Epoch: 28/30, step: 267/364, loss: 0.21030, accuracy: 0.92041\n",
            "Epoch: 28/30, step: 268/364, loss: 0.21019, accuracy: 0.92053\n",
            "Epoch: 28/30, step: 269/364, loss: 0.20983, accuracy: 0.92071\n",
            "Epoch: 28/30, step: 270/364, loss: 0.20961, accuracy: 0.92083\n",
            "Epoch: 28/30, step: 271/364, loss: 0.20968, accuracy: 0.92072\n",
            "Epoch: 28/30, step: 272/364, loss: 0.21000, accuracy: 0.92055\n",
            "Epoch: 28/30, step: 273/364, loss: 0.21003, accuracy: 0.92056\n",
            "Epoch: 28/30, step: 274/364, loss: 0.21004, accuracy: 0.92068\n",
            "Epoch: 28/30, step: 275/364, loss: 0.20987, accuracy: 0.92062\n",
            "Epoch: 28/30, step: 276/364, loss: 0.20970, accuracy: 0.92080\n",
            "Epoch: 28/30, step: 277/364, loss: 0.20948, accuracy: 0.92086\n",
            "Epoch: 28/30, step: 278/364, loss: 0.20929, accuracy: 0.92092\n",
            "Epoch: 28/30, step: 279/364, loss: 0.20909, accuracy: 0.92098\n",
            "Epoch: 28/30, step: 280/364, loss: 0.20898, accuracy: 0.92104\n",
            "Epoch: 28/30, step: 281/364, loss: 0.20973, accuracy: 0.92048\n",
            "Epoch: 28/30, step: 282/364, loss: 0.20978, accuracy: 0.92032\n",
            "Epoch: 28/30, step: 283/364, loss: 0.20996, accuracy: 0.92022\n",
            "Epoch: 28/30, step: 284/364, loss: 0.20982, accuracy: 0.92022\n",
            "Epoch: 28/30, step: 285/364, loss: 0.20980, accuracy: 0.92034\n",
            "Epoch: 28/30, step: 286/364, loss: 0.21022, accuracy: 0.92007\n",
            "Epoch: 28/30, step: 287/364, loss: 0.20998, accuracy: 0.92019\n",
            "Epoch: 28/30, step: 288/364, loss: 0.20991, accuracy: 0.92030\n",
            "Epoch: 28/30, step: 289/364, loss: 0.20986, accuracy: 0.92025\n",
            "Epoch: 28/30, step: 290/364, loss: 0.20980, accuracy: 0.92031\n",
            "Epoch: 28/30, step: 291/364, loss: 0.20965, accuracy: 0.92042\n",
            "Epoch: 28/30, train loss: 0.20965, train accuracy: 0.92042, valid loss: 0.79499, valid accuracy: 0.68444\n",
            "Epoch: 29/30, step: 1/364, loss: 0.20846, accuracy: 0.90625\n",
            "Epoch: 29/30, step: 2/364, loss: 0.22119, accuracy: 0.89062\n",
            "Epoch: 29/30, step: 3/364, loss: 0.21863, accuracy: 0.90625\n",
            "Epoch: 29/30, step: 4/364, loss: 0.20808, accuracy: 0.91016\n",
            "Epoch: 29/30, step: 5/364, loss: 0.20585, accuracy: 0.91562\n",
            "Epoch: 29/30, step: 6/364, loss: 0.19216, accuracy: 0.92448\n",
            "Epoch: 29/30, step: 7/364, loss: 0.18889, accuracy: 0.92634\n",
            "Epoch: 29/30, step: 8/364, loss: 0.18614, accuracy: 0.92969\n",
            "Epoch: 29/30, step: 9/364, loss: 0.18036, accuracy: 0.93056\n",
            "Epoch: 29/30, step: 10/364, loss: 0.17701, accuracy: 0.93125\n",
            "Epoch: 29/30, step: 11/364, loss: 0.17627, accuracy: 0.93608\n",
            "Epoch: 29/30, step: 12/364, loss: 0.18999, accuracy: 0.92708\n",
            "Epoch: 29/30, step: 13/364, loss: 0.18244, accuracy: 0.93149\n",
            "Epoch: 29/30, step: 14/364, loss: 0.18319, accuracy: 0.93192\n",
            "Epoch: 29/30, step: 15/364, loss: 0.18275, accuracy: 0.93333\n",
            "Epoch: 29/30, step: 16/364, loss: 0.19064, accuracy: 0.92871\n",
            "Epoch: 29/30, step: 17/364, loss: 0.18975, accuracy: 0.92831\n",
            "Epoch: 29/30, step: 18/364, loss: 0.19176, accuracy: 0.92622\n",
            "Epoch: 29/30, step: 19/364, loss: 0.19610, accuracy: 0.92188\n",
            "Epoch: 29/30, step: 20/364, loss: 0.19146, accuracy: 0.92500\n",
            "Epoch: 29/30, step: 21/364, loss: 0.18947, accuracy: 0.92708\n",
            "Epoch: 29/30, step: 22/364, loss: 0.19565, accuracy: 0.92401\n",
            "Epoch: 29/30, step: 23/364, loss: 0.19343, accuracy: 0.92527\n",
            "Epoch: 29/30, step: 24/364, loss: 0.19016, accuracy: 0.92708\n",
            "Epoch: 29/30, step: 25/364, loss: 0.19117, accuracy: 0.92563\n",
            "Epoch: 29/30, step: 26/364, loss: 0.19123, accuracy: 0.92488\n",
            "Epoch: 29/30, step: 27/364, loss: 0.19212, accuracy: 0.92535\n",
            "Epoch: 29/30, step: 28/364, loss: 0.19076, accuracy: 0.92522\n",
            "Epoch: 29/30, step: 29/364, loss: 0.18991, accuracy: 0.92565\n",
            "Epoch: 29/30, step: 30/364, loss: 0.18921, accuracy: 0.92708\n",
            "Epoch: 29/30, step: 31/364, loss: 0.18856, accuracy: 0.92742\n",
            "Epoch: 29/30, step: 32/364, loss: 0.18969, accuracy: 0.92676\n",
            "Epoch: 29/30, step: 33/364, loss: 0.19044, accuracy: 0.92566\n",
            "Epoch: 29/30, step: 34/364, loss: 0.18948, accuracy: 0.92647\n",
            "Epoch: 29/30, step: 35/364, loss: 0.19026, accuracy: 0.92679\n",
            "Epoch: 29/30, step: 36/364, loss: 0.19012, accuracy: 0.92665\n",
            "Epoch: 29/30, step: 37/364, loss: 0.19186, accuracy: 0.92568\n",
            "Epoch: 29/30, step: 38/364, loss: 0.19348, accuracy: 0.92516\n",
            "Epoch: 29/30, step: 39/364, loss: 0.19261, accuracy: 0.92588\n",
            "Epoch: 29/30, step: 40/364, loss: 0.19161, accuracy: 0.92656\n",
            "Epoch: 29/30, step: 41/364, loss: 0.19310, accuracy: 0.92569\n",
            "Epoch: 29/30, step: 42/364, loss: 0.19138, accuracy: 0.92671\n",
            "Epoch: 29/30, step: 43/364, loss: 0.18975, accuracy: 0.92805\n",
            "Epoch: 29/30, step: 44/364, loss: 0.18787, accuracy: 0.92933\n",
            "Epoch: 29/30, step: 45/364, loss: 0.18901, accuracy: 0.92847\n",
            "Epoch: 29/30, step: 46/364, loss: 0.19053, accuracy: 0.92731\n",
            "Epoch: 29/30, step: 47/364, loss: 0.19018, accuracy: 0.92753\n",
            "Epoch: 29/30, step: 48/364, loss: 0.18957, accuracy: 0.92839\n",
            "Epoch: 29/30, step: 49/364, loss: 0.18858, accuracy: 0.92857\n",
            "Epoch: 29/30, step: 50/364, loss: 0.19068, accuracy: 0.92781\n",
            "Epoch: 29/30, step: 51/364, loss: 0.18937, accuracy: 0.92862\n",
            "Epoch: 29/30, step: 52/364, loss: 0.19058, accuracy: 0.92788\n",
            "Epoch: 29/30, step: 53/364, loss: 0.19134, accuracy: 0.92748\n",
            "Epoch: 29/30, step: 54/364, loss: 0.19290, accuracy: 0.92708\n",
            "Epoch: 29/30, step: 55/364, loss: 0.19395, accuracy: 0.92670\n",
            "Epoch: 29/30, step: 56/364, loss: 0.19386, accuracy: 0.92690\n",
            "Epoch: 29/30, step: 57/364, loss: 0.19330, accuracy: 0.92708\n",
            "Epoch: 29/30, step: 58/364, loss: 0.19219, accuracy: 0.92807\n",
            "Epoch: 29/30, step: 59/364, loss: 0.19266, accuracy: 0.92691\n",
            "Epoch: 29/30, step: 60/364, loss: 0.19301, accuracy: 0.92708\n",
            "Epoch: 29/30, step: 61/364, loss: 0.19236, accuracy: 0.92751\n",
            "Epoch: 29/30, step: 62/364, loss: 0.19216, accuracy: 0.92767\n",
            "Epoch: 29/30, step: 63/364, loss: 0.19193, accuracy: 0.92808\n",
            "Epoch: 29/30, step: 64/364, loss: 0.19154, accuracy: 0.92847\n",
            "Epoch: 29/30, step: 65/364, loss: 0.19114, accuracy: 0.92837\n",
            "Epoch: 29/30, step: 66/364, loss: 0.18993, accuracy: 0.92874\n",
            "Epoch: 29/30, step: 67/364, loss: 0.19004, accuracy: 0.92910\n",
            "Epoch: 29/30, step: 68/364, loss: 0.18883, accuracy: 0.92992\n",
            "Epoch: 29/30, step: 69/364, loss: 0.18781, accuracy: 0.93048\n",
            "Epoch: 29/30, step: 70/364, loss: 0.18779, accuracy: 0.93080\n",
            "Epoch: 29/30, step: 71/364, loss: 0.18814, accuracy: 0.93112\n",
            "Epoch: 29/30, step: 72/364, loss: 0.18698, accuracy: 0.93186\n",
            "Epoch: 29/30, step: 73/364, loss: 0.18796, accuracy: 0.93086\n",
            "Epoch: 29/30, step: 74/364, loss: 0.18777, accuracy: 0.93074\n",
            "Epoch: 29/30, step: 75/364, loss: 0.18782, accuracy: 0.93042\n",
            "Epoch: 29/30, step: 76/364, loss: 0.18726, accuracy: 0.93072\n",
            "Epoch: 29/30, step: 77/364, loss: 0.18730, accuracy: 0.93101\n",
            "Epoch: 29/30, step: 78/364, loss: 0.18720, accuracy: 0.93129\n",
            "Epoch: 29/30, step: 79/364, loss: 0.18753, accuracy: 0.93117\n",
            "Epoch: 29/30, step: 80/364, loss: 0.18724, accuracy: 0.93105\n",
            "Epoch: 29/30, step: 81/364, loss: 0.18703, accuracy: 0.93094\n",
            "Epoch: 29/30, step: 82/364, loss: 0.18656, accuracy: 0.93121\n",
            "Epoch: 29/30, step: 83/364, loss: 0.18744, accuracy: 0.93072\n",
            "Epoch: 29/30, step: 84/364, loss: 0.18731, accuracy: 0.93043\n",
            "Epoch: 29/30, step: 85/364, loss: 0.18696, accuracy: 0.93070\n",
            "Epoch: 29/30, step: 86/364, loss: 0.18653, accuracy: 0.93114\n",
            "Epoch: 29/30, step: 87/364, loss: 0.18927, accuracy: 0.92960\n",
            "Epoch: 29/30, step: 88/364, loss: 0.18900, accuracy: 0.92987\n",
            "Epoch: 29/30, step: 89/364, loss: 0.18836, accuracy: 0.93048\n",
            "Epoch: 29/30, step: 90/364, loss: 0.18840, accuracy: 0.93038\n",
            "Epoch: 29/30, step: 91/364, loss: 0.18865, accuracy: 0.93046\n",
            "Epoch: 29/30, step: 92/364, loss: 0.18822, accuracy: 0.93071\n",
            "Epoch: 29/30, step: 93/364, loss: 0.18811, accuracy: 0.93095\n",
            "Epoch: 29/30, step: 94/364, loss: 0.18775, accuracy: 0.93135\n",
            "Epoch: 29/30, step: 95/364, loss: 0.18747, accuracy: 0.93158\n",
            "Epoch: 29/30, step: 96/364, loss: 0.18780, accuracy: 0.93148\n",
            "Epoch: 29/30, step: 97/364, loss: 0.18720, accuracy: 0.93202\n",
            "Epoch: 29/30, step: 98/364, loss: 0.18740, accuracy: 0.93208\n",
            "Epoch: 29/30, step: 99/364, loss: 0.18719, accuracy: 0.93213\n",
            "Epoch: 29/30, step: 100/364, loss: 0.18729, accuracy: 0.93219\n",
            "Epoch: 29/30, step: 101/364, loss: 0.18688, accuracy: 0.93239\n",
            "Epoch: 29/30, step: 102/364, loss: 0.18664, accuracy: 0.93244\n",
            "Epoch: 29/30, step: 103/364, loss: 0.18700, accuracy: 0.93234\n",
            "Epoch: 29/30, step: 104/364, loss: 0.18643, accuracy: 0.93254\n",
            "Epoch: 29/30, step: 105/364, loss: 0.18578, accuracy: 0.93289\n",
            "Epoch: 29/30, step: 106/364, loss: 0.18656, accuracy: 0.93234\n",
            "Epoch: 29/30, step: 107/364, loss: 0.18728, accuracy: 0.93210\n",
            "Epoch: 29/30, step: 108/364, loss: 0.18734, accuracy: 0.93200\n",
            "Epoch: 29/30, step: 109/364, loss: 0.18737, accuracy: 0.93191\n",
            "Epoch: 29/30, step: 110/364, loss: 0.18678, accuracy: 0.93224\n",
            "Epoch: 29/30, step: 111/364, loss: 0.18794, accuracy: 0.93173\n",
            "Epoch: 29/30, step: 112/364, loss: 0.18844, accuracy: 0.93136\n",
            "Epoch: 29/30, step: 113/364, loss: 0.18942, accuracy: 0.93059\n",
            "Epoch: 29/30, step: 114/364, loss: 0.19030, accuracy: 0.92996\n",
            "Epoch: 29/30, step: 115/364, loss: 0.19043, accuracy: 0.92989\n",
            "Epoch: 29/30, step: 116/364, loss: 0.19018, accuracy: 0.92969\n",
            "Epoch: 29/30, step: 117/364, loss: 0.19021, accuracy: 0.92962\n",
            "Epoch: 29/30, step: 118/364, loss: 0.19082, accuracy: 0.92956\n",
            "Epoch: 29/30, step: 119/364, loss: 0.19030, accuracy: 0.92975\n",
            "Epoch: 29/30, step: 120/364, loss: 0.19027, accuracy: 0.92982\n",
            "Epoch: 29/30, step: 121/364, loss: 0.19015, accuracy: 0.92975\n",
            "Epoch: 29/30, step: 122/364, loss: 0.19004, accuracy: 0.92969\n",
            "Epoch: 29/30, step: 123/364, loss: 0.18977, accuracy: 0.92975\n",
            "Epoch: 29/30, step: 124/364, loss: 0.18988, accuracy: 0.92956\n",
            "Epoch: 29/30, step: 125/364, loss: 0.18981, accuracy: 0.92975\n",
            "Epoch: 29/30, step: 126/364, loss: 0.18998, accuracy: 0.92956\n",
            "Epoch: 29/30, step: 127/364, loss: 0.19092, accuracy: 0.92901\n",
            "Epoch: 29/30, step: 128/364, loss: 0.19042, accuracy: 0.92932\n",
            "Epoch: 29/30, step: 129/364, loss: 0.19039, accuracy: 0.92938\n",
            "Epoch: 29/30, step: 130/364, loss: 0.18993, accuracy: 0.92969\n",
            "Epoch: 29/30, step: 131/364, loss: 0.19037, accuracy: 0.92963\n",
            "Epoch: 29/30, step: 132/364, loss: 0.19041, accuracy: 0.92945\n",
            "Epoch: 29/30, step: 133/364, loss: 0.19038, accuracy: 0.92951\n",
            "Epoch: 29/30, step: 134/364, loss: 0.19087, accuracy: 0.92922\n",
            "Epoch: 29/30, step: 135/364, loss: 0.19076, accuracy: 0.92917\n",
            "Epoch: 29/30, step: 136/364, loss: 0.19142, accuracy: 0.92854\n",
            "Epoch: 29/30, step: 137/364, loss: 0.19124, accuracy: 0.92872\n",
            "Epoch: 29/30, step: 138/364, loss: 0.19130, accuracy: 0.92856\n",
            "Epoch: 29/30, step: 139/364, loss: 0.19168, accuracy: 0.92839\n",
            "Epoch: 29/30, step: 140/364, loss: 0.19121, accuracy: 0.92857\n",
            "Epoch: 29/30, step: 141/364, loss: 0.19127, accuracy: 0.92863\n",
            "Epoch: 29/30, step: 142/364, loss: 0.19105, accuracy: 0.92892\n",
            "Epoch: 29/30, step: 143/364, loss: 0.19152, accuracy: 0.92865\n",
            "Epoch: 29/30, step: 144/364, loss: 0.19145, accuracy: 0.92860\n",
            "Epoch: 29/30, step: 145/364, loss: 0.19127, accuracy: 0.92866\n",
            "Epoch: 29/30, step: 146/364, loss: 0.19095, accuracy: 0.92872\n",
            "Epoch: 29/30, step: 147/364, loss: 0.19076, accuracy: 0.92868\n",
            "Epoch: 29/30, step: 148/364, loss: 0.19112, accuracy: 0.92832\n",
            "Epoch: 29/30, step: 149/364, loss: 0.19128, accuracy: 0.92817\n",
            "Epoch: 29/30, step: 150/364, loss: 0.19105, accuracy: 0.92823\n",
            "Epoch: 29/30, step: 151/364, loss: 0.19138, accuracy: 0.92808\n",
            "Epoch: 29/30, step: 152/364, loss: 0.19107, accuracy: 0.92825\n",
            "Epoch: 29/30, step: 153/364, loss: 0.19187, accuracy: 0.92770\n",
            "Epoch: 29/30, step: 154/364, loss: 0.19266, accuracy: 0.92756\n",
            "Epoch: 29/30, step: 155/364, loss: 0.19238, accuracy: 0.92762\n",
            "Epoch: 29/30, step: 156/364, loss: 0.19295, accuracy: 0.92718\n",
            "Epoch: 29/30, step: 157/364, loss: 0.19272, accuracy: 0.92735\n",
            "Epoch: 29/30, step: 158/364, loss: 0.19251, accuracy: 0.92741\n",
            "Epoch: 29/30, step: 159/364, loss: 0.19181, accuracy: 0.92787\n",
            "Epoch: 29/30, step: 160/364, loss: 0.19219, accuracy: 0.92773\n",
            "Epoch: 29/30, step: 161/364, loss: 0.19186, accuracy: 0.92780\n",
            "Epoch: 29/30, step: 162/364, loss: 0.19215, accuracy: 0.92737\n",
            "Epoch: 29/30, step: 163/364, loss: 0.19193, accuracy: 0.92743\n",
            "Epoch: 29/30, step: 164/364, loss: 0.19220, accuracy: 0.92712\n",
            "Epoch: 29/30, step: 165/364, loss: 0.19284, accuracy: 0.92670\n",
            "Epoch: 29/30, step: 166/364, loss: 0.19236, accuracy: 0.92705\n",
            "Epoch: 29/30, step: 167/364, loss: 0.19220, accuracy: 0.92721\n",
            "Epoch: 29/30, step: 168/364, loss: 0.19280, accuracy: 0.92690\n",
            "Epoch: 29/30, step: 169/364, loss: 0.19251, accuracy: 0.92696\n",
            "Epoch: 29/30, step: 170/364, loss: 0.19281, accuracy: 0.92702\n",
            "Epoch: 29/30, step: 171/364, loss: 0.19322, accuracy: 0.92690\n",
            "Epoch: 29/30, step: 172/364, loss: 0.19358, accuracy: 0.92669\n",
            "Epoch: 29/30, step: 173/364, loss: 0.19351, accuracy: 0.92675\n",
            "Epoch: 29/30, step: 174/364, loss: 0.19362, accuracy: 0.92663\n",
            "Epoch: 29/30, step: 175/364, loss: 0.19368, accuracy: 0.92661\n",
            "Epoch: 29/30, step: 176/364, loss: 0.19321, accuracy: 0.92685\n",
            "Epoch: 29/30, step: 177/364, loss: 0.19348, accuracy: 0.92664\n",
            "Epoch: 29/30, step: 178/364, loss: 0.19322, accuracy: 0.92670\n",
            "Epoch: 29/30, step: 179/364, loss: 0.19296, accuracy: 0.92685\n",
            "Epoch: 29/30, step: 180/364, loss: 0.19271, accuracy: 0.92717\n",
            "Epoch: 29/30, step: 181/364, loss: 0.19235, accuracy: 0.92731\n",
            "Epoch: 29/30, step: 182/364, loss: 0.19222, accuracy: 0.92737\n",
            "Epoch: 29/30, step: 183/364, loss: 0.19284, accuracy: 0.92691\n",
            "Epoch: 29/30, step: 184/364, loss: 0.19310, accuracy: 0.92663\n",
            "Epoch: 29/30, step: 185/364, loss: 0.19281, accuracy: 0.92686\n",
            "Epoch: 29/30, step: 186/364, loss: 0.19269, accuracy: 0.92692\n",
            "Epoch: 29/30, step: 187/364, loss: 0.19275, accuracy: 0.92706\n",
            "Epoch: 29/30, step: 188/364, loss: 0.19229, accuracy: 0.92719\n",
            "Epoch: 29/30, step: 189/364, loss: 0.19209, accuracy: 0.92717\n",
            "Epoch: 29/30, step: 190/364, loss: 0.19232, accuracy: 0.92697\n",
            "Epoch: 29/30, step: 191/364, loss: 0.19208, accuracy: 0.92703\n",
            "Epoch: 29/30, step: 192/364, loss: 0.19192, accuracy: 0.92725\n",
            "Epoch: 29/30, step: 193/364, loss: 0.19249, accuracy: 0.92714\n",
            "Epoch: 29/30, step: 194/364, loss: 0.19250, accuracy: 0.92719\n",
            "Epoch: 29/30, step: 195/364, loss: 0.19219, accuracy: 0.92740\n",
            "Epoch: 29/30, step: 196/364, loss: 0.19186, accuracy: 0.92754\n",
            "Epoch: 29/30, step: 197/364, loss: 0.19247, accuracy: 0.92711\n",
            "Epoch: 29/30, step: 198/364, loss: 0.19229, accuracy: 0.92724\n",
            "Epoch: 29/30, step: 199/364, loss: 0.19295, accuracy: 0.92706\n",
            "Epoch: 29/30, step: 200/364, loss: 0.19325, accuracy: 0.92672\n",
            "Epoch: 29/30, step: 201/364, loss: 0.19363, accuracy: 0.92654\n",
            "Epoch: 29/30, step: 202/364, loss: 0.19429, accuracy: 0.92636\n",
            "Epoch: 29/30, step: 203/364, loss: 0.19463, accuracy: 0.92595\n",
            "Epoch: 29/30, step: 204/364, loss: 0.19439, accuracy: 0.92593\n",
            "Epoch: 29/30, step: 205/364, loss: 0.19451, accuracy: 0.92599\n",
            "Epoch: 29/30, step: 206/364, loss: 0.19513, accuracy: 0.92552\n",
            "Epoch: 29/30, step: 207/364, loss: 0.19512, accuracy: 0.92550\n",
            "Epoch: 29/30, step: 208/364, loss: 0.19479, accuracy: 0.92571\n",
            "Epoch: 29/30, step: 209/364, loss: 0.19501, accuracy: 0.92554\n",
            "Epoch: 29/30, step: 210/364, loss: 0.19461, accuracy: 0.92582\n",
            "Epoch: 29/30, step: 211/364, loss: 0.19426, accuracy: 0.92610\n",
            "Epoch: 29/30, step: 212/364, loss: 0.19397, accuracy: 0.92630\n",
            "Epoch: 29/30, step: 213/364, loss: 0.19361, accuracy: 0.92642\n",
            "Epoch: 29/30, step: 214/364, loss: 0.19363, accuracy: 0.92640\n",
            "Epoch: 29/30, step: 215/364, loss: 0.19425, accuracy: 0.92594\n",
            "Epoch: 29/30, step: 216/364, loss: 0.19524, accuracy: 0.92535\n",
            "Epoch: 29/30, step: 217/364, loss: 0.19480, accuracy: 0.92562\n",
            "Epoch: 29/30, step: 218/364, loss: 0.19454, accuracy: 0.92582\n",
            "Epoch: 29/30, step: 219/364, loss: 0.19460, accuracy: 0.92580\n",
            "Epoch: 29/30, step: 220/364, loss: 0.19431, accuracy: 0.92592\n",
            "Epoch: 29/30, step: 221/364, loss: 0.19451, accuracy: 0.92583\n",
            "Epoch: 29/30, step: 222/364, loss: 0.19487, accuracy: 0.92561\n",
            "Epoch: 29/30, step: 223/364, loss: 0.19472, accuracy: 0.92566\n",
            "Epoch: 29/30, step: 224/364, loss: 0.19445, accuracy: 0.92578\n",
            "Epoch: 29/30, step: 225/364, loss: 0.19480, accuracy: 0.92556\n",
            "Epoch: 29/30, step: 226/364, loss: 0.19510, accuracy: 0.92526\n",
            "Epoch: 29/30, step: 227/364, loss: 0.19528, accuracy: 0.92518\n",
            "Epoch: 29/30, step: 228/364, loss: 0.19532, accuracy: 0.92516\n",
            "Epoch: 29/30, step: 229/364, loss: 0.19482, accuracy: 0.92542\n",
            "Epoch: 29/30, step: 230/364, loss: 0.19496, accuracy: 0.92514\n",
            "Epoch: 29/30, step: 231/364, loss: 0.19469, accuracy: 0.92532\n",
            "Epoch: 29/30, step: 232/364, loss: 0.19458, accuracy: 0.92544\n",
            "Epoch: 29/30, step: 233/364, loss: 0.19447, accuracy: 0.92563\n",
            "Epoch: 29/30, step: 234/364, loss: 0.19424, accuracy: 0.92575\n",
            "Epoch: 29/30, step: 235/364, loss: 0.19407, accuracy: 0.92586\n",
            "Epoch: 29/30, step: 236/364, loss: 0.19396, accuracy: 0.92591\n",
            "Epoch: 29/30, step: 237/364, loss: 0.19377, accuracy: 0.92609\n",
            "Epoch: 29/30, step: 238/364, loss: 0.19381, accuracy: 0.92627\n",
            "Epoch: 29/30, step: 239/364, loss: 0.19391, accuracy: 0.92626\n",
            "Epoch: 29/30, step: 240/364, loss: 0.19449, accuracy: 0.92578\n",
            "Epoch: 29/30, step: 241/364, loss: 0.19470, accuracy: 0.92583\n",
            "Epoch: 29/30, step: 242/364, loss: 0.19477, accuracy: 0.92568\n",
            "Epoch: 29/30, step: 243/364, loss: 0.19474, accuracy: 0.92573\n",
            "Epoch: 29/30, step: 244/364, loss: 0.19467, accuracy: 0.92578\n",
            "Epoch: 29/30, step: 245/364, loss: 0.19463, accuracy: 0.92589\n",
            "Epoch: 29/30, step: 246/364, loss: 0.19475, accuracy: 0.92594\n",
            "Epoch: 29/30, step: 247/364, loss: 0.19466, accuracy: 0.92586\n",
            "Epoch: 29/30, step: 248/364, loss: 0.19443, accuracy: 0.92610\n",
            "Epoch: 29/30, step: 249/364, loss: 0.19455, accuracy: 0.92620\n",
            "Epoch: 29/30, step: 250/364, loss: 0.19431, accuracy: 0.92644\n",
            "Epoch: 29/30, step: 251/364, loss: 0.19399, accuracy: 0.92661\n",
            "Epoch: 29/30, step: 252/364, loss: 0.19385, accuracy: 0.92665\n",
            "Epoch: 29/30, step: 253/364, loss: 0.19363, accuracy: 0.92682\n",
            "Epoch: 29/30, step: 254/364, loss: 0.19373, accuracy: 0.92667\n",
            "Epoch: 29/30, step: 255/364, loss: 0.19364, accuracy: 0.92678\n",
            "Epoch: 29/30, step: 256/364, loss: 0.19375, accuracy: 0.92676\n",
            "Epoch: 29/30, step: 257/364, loss: 0.19341, accuracy: 0.92698\n",
            "Epoch: 29/30, step: 258/364, loss: 0.19328, accuracy: 0.92714\n",
            "Epoch: 29/30, step: 259/364, loss: 0.19303, accuracy: 0.92718\n",
            "Epoch: 29/30, step: 260/364, loss: 0.19330, accuracy: 0.92722\n",
            "Epoch: 29/30, step: 261/364, loss: 0.19311, accuracy: 0.92720\n",
            "Epoch: 29/30, step: 262/364, loss: 0.19290, accuracy: 0.92736\n",
            "Epoch: 29/30, step: 263/364, loss: 0.19288, accuracy: 0.92740\n",
            "Epoch: 29/30, step: 264/364, loss: 0.19330, accuracy: 0.92708\n",
            "Epoch: 29/30, step: 265/364, loss: 0.19370, accuracy: 0.92695\n",
            "Epoch: 29/30, step: 266/364, loss: 0.19367, accuracy: 0.92704\n",
            "Epoch: 29/30, step: 267/364, loss: 0.19401, accuracy: 0.92667\n",
            "Epoch: 29/30, step: 268/364, loss: 0.19405, accuracy: 0.92666\n",
            "Epoch: 29/30, step: 269/364, loss: 0.19379, accuracy: 0.92681\n",
            "Epoch: 29/30, step: 270/364, loss: 0.19371, accuracy: 0.92679\n",
            "Epoch: 29/30, step: 271/364, loss: 0.19411, accuracy: 0.92660\n",
            "Epoch: 29/30, step: 272/364, loss: 0.19409, accuracy: 0.92636\n",
            "Epoch: 29/30, step: 273/364, loss: 0.19407, accuracy: 0.92640\n",
            "Epoch: 29/30, step: 274/364, loss: 0.19384, accuracy: 0.92655\n",
            "Epoch: 29/30, step: 275/364, loss: 0.19379, accuracy: 0.92648\n",
            "Epoch: 29/30, step: 276/364, loss: 0.19370, accuracy: 0.92646\n",
            "Epoch: 29/30, step: 277/364, loss: 0.19352, accuracy: 0.92650\n",
            "Epoch: 29/30, step: 278/364, loss: 0.19361, accuracy: 0.92643\n",
            "Epoch: 29/30, step: 279/364, loss: 0.19341, accuracy: 0.92658\n",
            "Epoch: 29/30, step: 280/364, loss: 0.19355, accuracy: 0.92656\n",
            "Epoch: 29/30, step: 281/364, loss: 0.19321, accuracy: 0.92677\n",
            "Epoch: 29/30, step: 282/364, loss: 0.19347, accuracy: 0.92675\n",
            "Epoch: 29/30, step: 283/364, loss: 0.19346, accuracy: 0.92684\n",
            "Epoch: 29/30, step: 284/364, loss: 0.19324, accuracy: 0.92699\n",
            "Epoch: 29/30, step: 285/364, loss: 0.19313, accuracy: 0.92708\n",
            "Epoch: 29/30, step: 286/364, loss: 0.19280, accuracy: 0.92723\n",
            "Epoch: 29/30, step: 287/364, loss: 0.19265, accuracy: 0.92732\n",
            "Epoch: 29/30, step: 288/364, loss: 0.19258, accuracy: 0.92730\n",
            "Epoch: 29/30, step: 289/364, loss: 0.19241, accuracy: 0.92739\n",
            "Epoch: 29/30, step: 290/364, loss: 0.19265, accuracy: 0.92710\n",
            "Epoch: 29/30, step: 291/364, loss: 0.19282, accuracy: 0.92708\n",
            "Epoch: 29/30, train loss: 0.19282, train accuracy: 0.92708, valid loss: 0.76114, valid accuracy: 0.69411\n",
            "Epoch: 30/30, step: 1/364, loss: 0.14062, accuracy: 0.96875\n",
            "Epoch: 30/30, step: 2/364, loss: 0.10217, accuracy: 0.98438\n",
            "Epoch: 30/30, step: 3/364, loss: 0.10824, accuracy: 0.98438\n",
            "Epoch: 30/30, step: 4/364, loss: 0.13789, accuracy: 0.96875\n",
            "Epoch: 30/30, step: 5/364, loss: 0.13933, accuracy: 0.96875\n",
            "Epoch: 30/30, step: 6/364, loss: 0.14666, accuracy: 0.96354\n",
            "Epoch: 30/30, step: 7/364, loss: 0.14957, accuracy: 0.95982\n",
            "Epoch: 30/30, step: 8/364, loss: 0.17816, accuracy: 0.94336\n",
            "Epoch: 30/30, step: 9/364, loss: 0.18322, accuracy: 0.93576\n",
            "Epoch: 30/30, step: 10/364, loss: 0.17600, accuracy: 0.94219\n",
            "Epoch: 30/30, step: 11/364, loss: 0.17658, accuracy: 0.94034\n",
            "Epoch: 30/30, step: 12/364, loss: 0.17621, accuracy: 0.94010\n",
            "Epoch: 30/30, step: 13/364, loss: 0.17741, accuracy: 0.94111\n",
            "Epoch: 30/30, step: 14/364, loss: 0.17296, accuracy: 0.94420\n",
            "Epoch: 30/30, step: 15/364, loss: 0.17122, accuracy: 0.94271\n",
            "Epoch: 30/30, step: 16/364, loss: 0.16873, accuracy: 0.94434\n",
            "Epoch: 30/30, step: 17/364, loss: 0.17052, accuracy: 0.94301\n",
            "Epoch: 30/30, step: 18/364, loss: 0.16827, accuracy: 0.94358\n",
            "Epoch: 30/30, step: 19/364, loss: 0.17063, accuracy: 0.94408\n",
            "Epoch: 30/30, step: 20/364, loss: 0.16686, accuracy: 0.94687\n",
            "Epoch: 30/30, step: 21/364, loss: 0.16456, accuracy: 0.94792\n",
            "Epoch: 30/30, step: 22/364, loss: 0.16382, accuracy: 0.94815\n",
            "Epoch: 30/30, step: 23/364, loss: 0.16639, accuracy: 0.94633\n",
            "Epoch: 30/30, step: 24/364, loss: 0.17136, accuracy: 0.94401\n",
            "Epoch: 30/30, step: 25/364, loss: 0.17381, accuracy: 0.94187\n",
            "Epoch: 30/30, step: 26/364, loss: 0.17217, accuracy: 0.94351\n",
            "Epoch: 30/30, step: 27/364, loss: 0.17213, accuracy: 0.94213\n",
            "Epoch: 30/30, step: 28/364, loss: 0.16957, accuracy: 0.94364\n",
            "Epoch: 30/30, step: 29/364, loss: 0.17205, accuracy: 0.94289\n",
            "Epoch: 30/30, step: 30/364, loss: 0.17260, accuracy: 0.94167\n",
            "Epoch: 30/30, step: 31/364, loss: 0.17361, accuracy: 0.94103\n",
            "Epoch: 30/30, step: 32/364, loss: 0.17127, accuracy: 0.94141\n",
            "Epoch: 30/30, step: 33/364, loss: 0.17003, accuracy: 0.94129\n",
            "Epoch: 30/30, step: 34/364, loss: 0.16780, accuracy: 0.94210\n",
            "Epoch: 30/30, step: 35/364, loss: 0.16984, accuracy: 0.94063\n",
            "Epoch: 30/30, step: 36/364, loss: 0.17078, accuracy: 0.93967\n",
            "Epoch: 30/30, step: 37/364, loss: 0.17240, accuracy: 0.93877\n",
            "Epoch: 30/30, step: 38/364, loss: 0.17433, accuracy: 0.93668\n",
            "Epoch: 30/30, step: 39/364, loss: 0.17930, accuracy: 0.93550\n",
            "Epoch: 30/30, step: 40/364, loss: 0.17891, accuracy: 0.93555\n",
            "Epoch: 30/30, step: 41/364, loss: 0.18111, accuracy: 0.93369\n",
            "Epoch: 30/30, step: 42/364, loss: 0.18328, accuracy: 0.93304\n",
            "Epoch: 30/30, step: 43/364, loss: 0.18654, accuracy: 0.93096\n",
            "Epoch: 30/30, step: 44/364, loss: 0.18638, accuracy: 0.93111\n",
            "Epoch: 30/30, step: 45/364, loss: 0.18555, accuracy: 0.93229\n",
            "Epoch: 30/30, step: 46/364, loss: 0.18745, accuracy: 0.93240\n",
            "Epoch: 30/30, step: 47/364, loss: 0.18622, accuracy: 0.93285\n",
            "Epoch: 30/30, step: 48/364, loss: 0.18458, accuracy: 0.93424\n",
            "Epoch: 30/30, step: 49/364, loss: 0.18321, accuracy: 0.93463\n",
            "Epoch: 30/30, step: 50/364, loss: 0.18280, accuracy: 0.93437\n",
            "Epoch: 30/30, step: 51/364, loss: 0.18277, accuracy: 0.93382\n",
            "Epoch: 30/30, step: 52/364, loss: 0.18336, accuracy: 0.93329\n",
            "Epoch: 30/30, step: 53/364, loss: 0.18260, accuracy: 0.93337\n",
            "Epoch: 30/30, step: 54/364, loss: 0.18377, accuracy: 0.93258\n",
            "Epoch: 30/30, step: 55/364, loss: 0.18322, accuracy: 0.93267\n",
            "Epoch: 30/30, step: 56/364, loss: 0.18152, accuracy: 0.93359\n",
            "Epoch: 30/30, step: 57/364, loss: 0.18096, accuracy: 0.93421\n",
            "Epoch: 30/30, step: 58/364, loss: 0.18009, accuracy: 0.93454\n",
            "Epoch: 30/30, step: 59/364, loss: 0.17983, accuracy: 0.93432\n",
            "Epoch: 30/30, step: 60/364, loss: 0.18063, accuracy: 0.93385\n",
            "Epoch: 30/30, step: 61/364, loss: 0.18009, accuracy: 0.93443\n",
            "Epoch: 30/30, step: 62/364, loss: 0.17996, accuracy: 0.93473\n",
            "Epoch: 30/30, step: 63/364, loss: 0.18025, accuracy: 0.93428\n",
            "Epoch: 30/30, step: 64/364, loss: 0.17959, accuracy: 0.93433\n",
            "Epoch: 30/30, step: 65/364, loss: 0.18425, accuracy: 0.93173\n",
            "Epoch: 30/30, step: 66/364, loss: 0.18297, accuracy: 0.93253\n",
            "Epoch: 30/30, step: 67/364, loss: 0.18292, accuracy: 0.93260\n",
            "Epoch: 30/30, step: 68/364, loss: 0.18255, accuracy: 0.93267\n",
            "Epoch: 30/30, step: 69/364, loss: 0.18252, accuracy: 0.93252\n",
            "Epoch: 30/30, step: 70/364, loss: 0.18252, accuracy: 0.93237\n",
            "Epoch: 30/30, step: 71/364, loss: 0.18321, accuracy: 0.93244\n",
            "Epoch: 30/30, step: 72/364, loss: 0.18460, accuracy: 0.93186\n",
            "Epoch: 30/30, step: 73/364, loss: 0.18514, accuracy: 0.93172\n",
            "Epoch: 30/30, step: 74/364, loss: 0.18430, accuracy: 0.93222\n",
            "Epoch: 30/30, step: 75/364, loss: 0.18347, accuracy: 0.93313\n",
            "Epoch: 30/30, step: 76/364, loss: 0.18349, accuracy: 0.93298\n",
            "Epoch: 30/30, step: 77/364, loss: 0.18252, accuracy: 0.93324\n",
            "Epoch: 30/30, step: 78/364, loss: 0.18128, accuracy: 0.93389\n",
            "Epoch: 30/30, step: 79/364, loss: 0.18125, accuracy: 0.93394\n",
            "Epoch: 30/30, step: 80/364, loss: 0.18206, accuracy: 0.93340\n",
            "Epoch: 30/30, step: 81/364, loss: 0.18186, accuracy: 0.93383\n",
            "Epoch: 30/30, step: 82/364, loss: 0.18165, accuracy: 0.93388\n",
            "Epoch: 30/30, step: 83/364, loss: 0.18093, accuracy: 0.93411\n",
            "Epoch: 30/30, step: 84/364, loss: 0.18082, accuracy: 0.93434\n",
            "Epoch: 30/30, step: 85/364, loss: 0.18055, accuracy: 0.93456\n",
            "Epoch: 30/30, step: 86/364, loss: 0.17987, accuracy: 0.93496\n",
            "Epoch: 30/30, step: 87/364, loss: 0.18015, accuracy: 0.93481\n",
            "Epoch: 30/30, step: 88/364, loss: 0.18077, accuracy: 0.93413\n",
            "Epoch: 30/30, step: 89/364, loss: 0.18152, accuracy: 0.93346\n",
            "Epoch: 30/30, step: 90/364, loss: 0.18107, accuracy: 0.93368\n",
            "Epoch: 30/30, step: 91/364, loss: 0.18158, accuracy: 0.93304\n",
            "Epoch: 30/30, step: 92/364, loss: 0.18111, accuracy: 0.93291\n",
            "Epoch: 30/30, step: 93/364, loss: 0.18040, accuracy: 0.93330\n",
            "Epoch: 30/30, step: 94/364, loss: 0.17955, accuracy: 0.93368\n",
            "Epoch: 30/30, step: 95/364, loss: 0.17896, accuracy: 0.93388\n",
            "Epoch: 30/30, step: 96/364, loss: 0.17950, accuracy: 0.93376\n",
            "Epoch: 30/30, step: 97/364, loss: 0.17948, accuracy: 0.93363\n",
            "Epoch: 30/30, step: 98/364, loss: 0.17911, accuracy: 0.93367\n",
            "Epoch: 30/30, step: 99/364, loss: 0.17841, accuracy: 0.93419\n",
            "Epoch: 30/30, step: 100/364, loss: 0.17841, accuracy: 0.93406\n",
            "Epoch: 30/30, step: 101/364, loss: 0.17862, accuracy: 0.93379\n",
            "Epoch: 30/30, step: 102/364, loss: 0.17843, accuracy: 0.93398\n",
            "Epoch: 30/30, step: 103/364, loss: 0.17746, accuracy: 0.93462\n",
            "Epoch: 30/30, step: 104/364, loss: 0.17721, accuracy: 0.93480\n",
            "Epoch: 30/30, step: 105/364, loss: 0.17771, accuracy: 0.93423\n",
            "Epoch: 30/30, step: 106/364, loss: 0.17777, accuracy: 0.93396\n",
            "Epoch: 30/30, step: 107/364, loss: 0.17858, accuracy: 0.93356\n",
            "Epoch: 30/30, step: 108/364, loss: 0.17884, accuracy: 0.93316\n",
            "Epoch: 30/30, step: 109/364, loss: 0.17963, accuracy: 0.93263\n",
            "Epoch: 30/30, step: 110/364, loss: 0.18005, accuracy: 0.93253\n",
            "Epoch: 30/30, step: 111/364, loss: 0.18084, accuracy: 0.93187\n",
            "Epoch: 30/30, step: 112/364, loss: 0.18084, accuracy: 0.93164\n",
            "Epoch: 30/30, step: 113/364, loss: 0.18085, accuracy: 0.93155\n",
            "Epoch: 30/30, step: 114/364, loss: 0.18045, accuracy: 0.93188\n",
            "Epoch: 30/30, step: 115/364, loss: 0.18008, accuracy: 0.93193\n",
            "Epoch: 30/30, step: 116/364, loss: 0.17963, accuracy: 0.93238\n",
            "Epoch: 30/30, step: 117/364, loss: 0.17967, accuracy: 0.93256\n",
            "Epoch: 30/30, step: 118/364, loss: 0.17997, accuracy: 0.93234\n",
            "Epoch: 30/30, step: 119/364, loss: 0.17993, accuracy: 0.93238\n",
            "Epoch: 30/30, step: 120/364, loss: 0.17951, accuracy: 0.93268\n",
            "Epoch: 30/30, step: 121/364, loss: 0.17921, accuracy: 0.93272\n",
            "Epoch: 30/30, step: 122/364, loss: 0.17885, accuracy: 0.93289\n",
            "Epoch: 30/30, step: 123/364, loss: 0.17879, accuracy: 0.93293\n",
            "Epoch: 30/30, step: 124/364, loss: 0.17860, accuracy: 0.93322\n",
            "Epoch: 30/30, step: 125/364, loss: 0.17826, accuracy: 0.93350\n",
            "Epoch: 30/30, step: 126/364, loss: 0.17797, accuracy: 0.93366\n",
            "Epoch: 30/30, step: 127/364, loss: 0.17793, accuracy: 0.93381\n",
            "Epoch: 30/30, step: 128/364, loss: 0.17771, accuracy: 0.93420\n",
            "Epoch: 30/30, step: 129/364, loss: 0.17799, accuracy: 0.93399\n",
            "Epoch: 30/30, step: 130/364, loss: 0.17875, accuracy: 0.93317\n",
            "Epoch: 30/30, step: 131/364, loss: 0.17862, accuracy: 0.93333\n",
            "Epoch: 30/30, step: 132/364, loss: 0.17840, accuracy: 0.93348\n",
            "Epoch: 30/30, step: 133/364, loss: 0.17832, accuracy: 0.93351\n",
            "Epoch: 30/30, step: 134/364, loss: 0.17771, accuracy: 0.93377\n",
            "Epoch: 30/30, step: 135/364, loss: 0.17755, accuracy: 0.93391\n",
            "Epoch: 30/30, step: 136/364, loss: 0.17706, accuracy: 0.93417\n",
            "Epoch: 30/30, step: 137/364, loss: 0.17659, accuracy: 0.93442\n",
            "Epoch: 30/30, step: 138/364, loss: 0.17619, accuracy: 0.93456\n",
            "Epoch: 30/30, step: 139/364, loss: 0.17565, accuracy: 0.93480\n",
            "Epoch: 30/30, step: 140/364, loss: 0.17516, accuracy: 0.93516\n",
            "Epoch: 30/30, step: 141/364, loss: 0.17485, accuracy: 0.93517\n",
            "Epoch: 30/30, step: 142/364, loss: 0.17639, accuracy: 0.93453\n",
            "Epoch: 30/30, step: 143/364, loss: 0.17640, accuracy: 0.93444\n",
            "Epoch: 30/30, step: 144/364, loss: 0.17723, accuracy: 0.93392\n",
            "Epoch: 30/30, step: 145/364, loss: 0.17725, accuracy: 0.93405\n",
            "Epoch: 30/30, step: 146/364, loss: 0.17697, accuracy: 0.93418\n",
            "Epoch: 30/30, step: 147/364, loss: 0.17645, accuracy: 0.93463\n",
            "Epoch: 30/30, step: 148/364, loss: 0.17615, accuracy: 0.93486\n",
            "Epoch: 30/30, step: 149/364, loss: 0.17646, accuracy: 0.93446\n",
            "Epoch: 30/30, step: 150/364, loss: 0.17671, accuracy: 0.93437\n",
            "Epoch: 30/30, step: 151/364, loss: 0.17649, accuracy: 0.93471\n",
            "Epoch: 30/30, step: 152/364, loss: 0.17627, accuracy: 0.93472\n",
            "Epoch: 30/30, step: 153/364, loss: 0.17619, accuracy: 0.93474\n",
            "Epoch: 30/30, step: 154/364, loss: 0.17637, accuracy: 0.93456\n",
            "Epoch: 30/30, step: 155/364, loss: 0.17640, accuracy: 0.93458\n",
            "Epoch: 30/30, step: 156/364, loss: 0.17619, accuracy: 0.93480\n",
            "Epoch: 30/30, step: 157/364, loss: 0.17581, accuracy: 0.93511\n",
            "Epoch: 30/30, step: 158/364, loss: 0.17572, accuracy: 0.93523\n",
            "Epoch: 30/30, step: 159/364, loss: 0.17532, accuracy: 0.93563\n",
            "Epoch: 30/30, step: 160/364, loss: 0.17539, accuracy: 0.93574\n",
            "Epoch: 30/30, step: 161/364, loss: 0.17508, accuracy: 0.93604\n",
            "Epoch: 30/30, step: 162/364, loss: 0.17559, accuracy: 0.93586\n",
            "Epoch: 30/30, step: 163/364, loss: 0.17632, accuracy: 0.93549\n",
            "Epoch: 30/30, step: 164/364, loss: 0.17597, accuracy: 0.93569\n",
            "Epoch: 30/30, step: 165/364, loss: 0.17730, accuracy: 0.93494\n",
            "Epoch: 30/30, step: 166/364, loss: 0.17753, accuracy: 0.93477\n",
            "Epoch: 30/30, step: 167/364, loss: 0.17792, accuracy: 0.93451\n",
            "Epoch: 30/30, step: 168/364, loss: 0.17822, accuracy: 0.93452\n",
            "Epoch: 30/30, step: 169/364, loss: 0.17785, accuracy: 0.93491\n",
            "Epoch: 30/30, step: 170/364, loss: 0.17814, accuracy: 0.93447\n",
            "Epoch: 30/30, step: 171/364, loss: 0.17794, accuracy: 0.93458\n",
            "Epoch: 30/30, step: 172/364, loss: 0.17804, accuracy: 0.93441\n",
            "Epoch: 30/30, step: 173/364, loss: 0.17810, accuracy: 0.93416\n",
            "Epoch: 30/30, step: 174/364, loss: 0.17850, accuracy: 0.93400\n",
            "Epoch: 30/30, step: 175/364, loss: 0.17838, accuracy: 0.93402\n",
            "Epoch: 30/30, step: 176/364, loss: 0.17846, accuracy: 0.93404\n",
            "Epoch: 30/30, step: 177/364, loss: 0.17862, accuracy: 0.93379\n",
            "Epoch: 30/30, step: 178/364, loss: 0.17848, accuracy: 0.93390\n",
            "Epoch: 30/30, step: 179/364, loss: 0.17800, accuracy: 0.93410\n",
            "Epoch: 30/30, step: 180/364, loss: 0.17783, accuracy: 0.93420\n",
            "Epoch: 30/30, step: 181/364, loss: 0.17813, accuracy: 0.93413\n",
            "Epoch: 30/30, step: 182/364, loss: 0.17766, accuracy: 0.93441\n",
            "Epoch: 30/30, step: 183/364, loss: 0.17738, accuracy: 0.93460\n",
            "Epoch: 30/30, step: 184/364, loss: 0.17727, accuracy: 0.93453\n",
            "Epoch: 30/30, step: 185/364, loss: 0.17693, accuracy: 0.93463\n",
            "Epoch: 30/30, step: 186/364, loss: 0.17753, accuracy: 0.93431\n",
            "Epoch: 30/30, step: 187/364, loss: 0.17741, accuracy: 0.93441\n",
            "Epoch: 30/30, step: 188/364, loss: 0.17759, accuracy: 0.93426\n",
            "Epoch: 30/30, step: 189/364, loss: 0.17764, accuracy: 0.93419\n",
            "Epoch: 30/30, step: 190/364, loss: 0.17766, accuracy: 0.93421\n",
            "Epoch: 30/30, step: 191/364, loss: 0.17767, accuracy: 0.93439\n",
            "Epoch: 30/30, step: 192/364, loss: 0.17759, accuracy: 0.93457\n",
            "Epoch: 30/30, step: 193/364, loss: 0.17792, accuracy: 0.93450\n",
            "Epoch: 30/30, step: 194/364, loss: 0.17782, accuracy: 0.93460\n",
            "Epoch: 30/30, step: 195/364, loss: 0.17763, accuracy: 0.93462\n",
            "Epoch: 30/30, step: 196/364, loss: 0.17742, accuracy: 0.93463\n",
            "Epoch: 30/30, step: 197/364, loss: 0.17716, accuracy: 0.93464\n",
            "Epoch: 30/30, step: 198/364, loss: 0.17765, accuracy: 0.93442\n",
            "Epoch: 30/30, step: 199/364, loss: 0.17761, accuracy: 0.93452\n",
            "Epoch: 30/30, step: 200/364, loss: 0.17799, accuracy: 0.93445\n",
            "Epoch: 30/30, step: 201/364, loss: 0.17766, accuracy: 0.93455\n",
            "Epoch: 30/30, step: 202/364, loss: 0.17777, accuracy: 0.93441\n",
            "Epoch: 30/30, step: 203/364, loss: 0.17754, accuracy: 0.93458\n",
            "Epoch: 30/30, step: 204/364, loss: 0.17751, accuracy: 0.93459\n",
            "Epoch: 30/30, step: 205/364, loss: 0.17759, accuracy: 0.93453\n",
            "Epoch: 30/30, step: 206/364, loss: 0.17768, accuracy: 0.93439\n",
            "Epoch: 30/30, step: 207/364, loss: 0.17757, accuracy: 0.93448\n",
            "Epoch: 30/30, step: 208/364, loss: 0.17749, accuracy: 0.93457\n",
            "Epoch: 30/30, step: 209/364, loss: 0.17752, accuracy: 0.93466\n",
            "Epoch: 30/30, step: 210/364, loss: 0.17735, accuracy: 0.93482\n",
            "Epoch: 30/30, step: 211/364, loss: 0.17703, accuracy: 0.93498\n",
            "Epoch: 30/30, step: 212/364, loss: 0.17714, accuracy: 0.93485\n",
            "Epoch: 30/30, step: 213/364, loss: 0.17693, accuracy: 0.93501\n",
            "Epoch: 30/30, step: 214/364, loss: 0.17664, accuracy: 0.93509\n",
            "Epoch: 30/30, step: 215/364, loss: 0.17717, accuracy: 0.93459\n",
            "Epoch: 30/30, step: 216/364, loss: 0.17698, accuracy: 0.93475\n",
            "Epoch: 30/30, step: 217/364, loss: 0.17694, accuracy: 0.93469\n",
            "Epoch: 30/30, step: 218/364, loss: 0.17730, accuracy: 0.93449\n",
            "Epoch: 30/30, step: 219/364, loss: 0.17742, accuracy: 0.93436\n",
            "Epoch: 30/30, step: 220/364, loss: 0.17785, accuracy: 0.93402\n",
            "Epoch: 30/30, step: 221/364, loss: 0.17798, accuracy: 0.93411\n",
            "Epoch: 30/30, step: 222/364, loss: 0.17783, accuracy: 0.93419\n",
            "Epoch: 30/30, step: 223/364, loss: 0.17771, accuracy: 0.93428\n",
            "Epoch: 30/30, step: 224/364, loss: 0.17755, accuracy: 0.93436\n",
            "Epoch: 30/30, step: 225/364, loss: 0.17756, accuracy: 0.93437\n",
            "Epoch: 30/30, step: 226/364, loss: 0.17737, accuracy: 0.93453\n",
            "Epoch: 30/30, step: 227/364, loss: 0.17804, accuracy: 0.93413\n",
            "Epoch: 30/30, step: 228/364, loss: 0.17781, accuracy: 0.93421\n",
            "Epoch: 30/30, step: 229/364, loss: 0.17746, accuracy: 0.93450\n",
            "Epoch: 30/30, step: 230/364, loss: 0.17737, accuracy: 0.93458\n",
            "Epoch: 30/30, step: 231/364, loss: 0.17738, accuracy: 0.93473\n",
            "Epoch: 30/30, step: 232/364, loss: 0.17717, accuracy: 0.93487\n",
            "Epoch: 30/30, step: 233/364, loss: 0.17704, accuracy: 0.93482\n",
            "Epoch: 30/30, step: 234/364, loss: 0.17741, accuracy: 0.93470\n",
            "Epoch: 30/30, step: 235/364, loss: 0.17759, accuracy: 0.93451\n",
            "Epoch: 30/30, step: 236/364, loss: 0.17732, accuracy: 0.93465\n",
            "Epoch: 30/30, step: 237/364, loss: 0.17747, accuracy: 0.93460\n",
            "Epoch: 30/30, step: 238/364, loss: 0.17775, accuracy: 0.93428\n",
            "Epoch: 30/30, step: 239/364, loss: 0.17733, accuracy: 0.93449\n",
            "Epoch: 30/30, step: 240/364, loss: 0.17747, accuracy: 0.93444\n",
            "Epoch: 30/30, step: 241/364, loss: 0.17727, accuracy: 0.93458\n",
            "Epoch: 30/30, step: 242/364, loss: 0.17754, accuracy: 0.93440\n",
            "Epoch: 30/30, step: 243/364, loss: 0.17737, accuracy: 0.93448\n",
            "Epoch: 30/30, step: 244/364, loss: 0.17717, accuracy: 0.93462\n",
            "Epoch: 30/30, step: 245/364, loss: 0.17725, accuracy: 0.93457\n",
            "Epoch: 30/30, step: 246/364, loss: 0.17746, accuracy: 0.93439\n",
            "Epoch: 30/30, step: 247/364, loss: 0.17729, accuracy: 0.93446\n",
            "Epoch: 30/30, step: 248/364, loss: 0.17708, accuracy: 0.93454\n",
            "Epoch: 30/30, step: 249/364, loss: 0.17679, accuracy: 0.93474\n",
            "Epoch: 30/30, step: 250/364, loss: 0.17714, accuracy: 0.93463\n",
            "Epoch: 30/30, step: 251/364, loss: 0.17688, accuracy: 0.93476\n",
            "Epoch: 30/30, step: 252/364, loss: 0.17692, accuracy: 0.93459\n",
            "Epoch: 30/30, step: 253/364, loss: 0.17725, accuracy: 0.93454\n",
            "Epoch: 30/30, step: 254/364, loss: 0.17713, accuracy: 0.93467\n",
            "Epoch: 30/30, step: 255/364, loss: 0.17699, accuracy: 0.93480\n",
            "Epoch: 30/30, step: 256/364, loss: 0.17687, accuracy: 0.93488\n",
            "Epoch: 30/30, step: 257/364, loss: 0.17681, accuracy: 0.93489\n",
            "Epoch: 30/30, step: 258/364, loss: 0.17695, accuracy: 0.93465\n",
            "Epoch: 30/30, step: 259/364, loss: 0.17666, accuracy: 0.93485\n",
            "Epoch: 30/30, step: 260/364, loss: 0.17667, accuracy: 0.93486\n",
            "Epoch: 30/30, step: 261/364, loss: 0.17650, accuracy: 0.93499\n",
            "Epoch: 30/30, step: 262/364, loss: 0.17646, accuracy: 0.93500\n",
            "Epoch: 30/30, step: 263/364, loss: 0.17633, accuracy: 0.93506\n",
            "Epoch: 30/30, step: 264/364, loss: 0.17624, accuracy: 0.93519\n",
            "Epoch: 30/30, step: 265/364, loss: 0.17605, accuracy: 0.93538\n",
            "Epoch: 30/30, step: 266/364, loss: 0.17600, accuracy: 0.93539\n",
            "Epoch: 30/30, step: 267/364, loss: 0.17581, accuracy: 0.93557\n",
            "Epoch: 30/30, step: 268/364, loss: 0.17562, accuracy: 0.93563\n",
            "Epoch: 30/30, step: 269/364, loss: 0.17571, accuracy: 0.93564\n",
            "Epoch: 30/30, step: 270/364, loss: 0.17562, accuracy: 0.93576\n",
            "Epoch: 30/30, step: 271/364, loss: 0.17559, accuracy: 0.93565\n",
            "Epoch: 30/30, step: 272/364, loss: 0.17605, accuracy: 0.93549\n",
            "Epoch: 30/30, step: 273/364, loss: 0.17650, accuracy: 0.93538\n",
            "Epoch: 30/30, step: 274/364, loss: 0.17627, accuracy: 0.93550\n",
            "Epoch: 30/30, step: 275/364, loss: 0.17618, accuracy: 0.93551\n",
            "Epoch: 30/30, step: 276/364, loss: 0.17613, accuracy: 0.93558\n",
            "Epoch: 30/30, step: 277/364, loss: 0.17625, accuracy: 0.93547\n",
            "Epoch: 30/30, step: 278/364, loss: 0.17638, accuracy: 0.93542\n",
            "Epoch: 30/30, step: 279/364, loss: 0.17632, accuracy: 0.93554\n",
            "Epoch: 30/30, step: 280/364, loss: 0.17652, accuracy: 0.93544\n",
            "Epoch: 30/30, step: 281/364, loss: 0.17664, accuracy: 0.93522\n",
            "Epoch: 30/30, step: 282/364, loss: 0.17637, accuracy: 0.93545\n",
            "Epoch: 30/30, step: 283/364, loss: 0.17608, accuracy: 0.93557\n",
            "Epoch: 30/30, step: 284/364, loss: 0.17641, accuracy: 0.93541\n",
            "Epoch: 30/30, step: 285/364, loss: 0.17656, accuracy: 0.93531\n",
            "Epoch: 30/30, step: 286/364, loss: 0.17691, accuracy: 0.93515\n",
            "Epoch: 30/30, step: 287/364, loss: 0.17696, accuracy: 0.93516\n",
            "Epoch: 30/30, step: 288/364, loss: 0.17715, accuracy: 0.93511\n",
            "Epoch: 30/30, step: 289/364, loss: 0.17706, accuracy: 0.93518\n",
            "Epoch: 30/30, step: 290/364, loss: 0.17680, accuracy: 0.93540\n",
            "Epoch: 30/30, step: 291/364, loss: 0.17675, accuracy: 0.93536\n",
            "Epoch: 30/30, train loss: 0.17675, train accuracy: 0.93536, valid loss: 0.77854, valid accuracy: 0.69626\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "model = get_model('resnet50')\n",
        "\n",
        "import math\n",
        "\n",
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(images, training=True)\n",
        "        loss = loss_object(y_true=labels, y_pred=predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(grads_and_vars=zip(gradients, model.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(labels, predictions)\n",
        "\n",
        "@tf.function\n",
        "def valid_step(images, labels):\n",
        "    predictions = model(images, training=False)\n",
        "    v_loss = loss_object(labels, predictions)\n",
        "\n",
        "    valid_loss(v_loss)\n",
        "    valid_accuracy(labels, predictions)\n",
        "\n",
        "# start training\n",
        "for epoch in range(30):\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    valid_loss.reset_states()\n",
        "    valid_accuracy.reset_states()\n",
        "    step = 0\n",
        "    for images, labels in train_batches:\n",
        "        step += 1\n",
        "        train_step(images, labels)\n",
        "        print(\"Epoch: {}/{}, step: {}/{}, loss: {:.5f}, accuracy: {:.5f}\".format(epoch + 1,\n",
        "                                                                                    30,\n",
        "                                                                                    step,\n",
        "                                                                                    math.ceil(num_examples / 64),\n",
        "                                                                                    train_loss.result(),\n",
        "                                                                                    train_accuracy.result()))\n",
        "\n",
        "    for valid_images, valid_labels in validation_batches:\n",
        "        valid_step(valid_images, valid_labels)\n",
        "\n",
        "    print(\"Epoch: {}/{}, train loss: {:.5f}, train accuracy: {:.5f}, \"\n",
        "            \"valid loss: {:.5f}, valid accuracy: {:.5f}\".format(epoch + 1,\n",
        "                                                                30,\n",
        "                                                                train_loss.result(),\n",
        "                                                                train_accuracy.result(),\n",
        "                                                                valid_loss.result(),\n",
        "                                                                valid_accuracy.result()))"
      ],
      "metadata": {
        "id": "GiS5PYQjo3zK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21314e21-08c2-4b0e-b518-b8f6f9dc0982"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch: 13/30, step: 257/364, loss: 0.46409, accuracy: 0.78861\n",
            "Epoch: 13/30, step: 258/364, loss: 0.46421, accuracy: 0.78858\n",
            "Epoch: 13/30, step: 259/364, loss: 0.46444, accuracy: 0.78843\n",
            "Epoch: 13/30, step: 260/364, loss: 0.46428, accuracy: 0.78858\n",
            "Epoch: 13/30, step: 261/364, loss: 0.46414, accuracy: 0.78885\n",
            "Epoch: 13/30, step: 262/364, loss: 0.46428, accuracy: 0.78870\n",
            "Epoch: 13/30, step: 263/364, loss: 0.46424, accuracy: 0.78874\n",
            "Epoch: 13/30, step: 264/364, loss: 0.46431, accuracy: 0.78847\n",
            "Epoch: 13/30, step: 265/364, loss: 0.46441, accuracy: 0.78838\n",
            "Epoch: 13/30, step: 266/364, loss: 0.46468, accuracy: 0.78812\n",
            "Epoch: 13/30, step: 267/364, loss: 0.46479, accuracy: 0.78798\n",
            "Epoch: 13/30, step: 268/364, loss: 0.46461, accuracy: 0.78807\n",
            "Epoch: 13/30, step: 269/364, loss: 0.46476, accuracy: 0.78810\n",
            "Epoch: 13/30, step: 270/364, loss: 0.46480, accuracy: 0.78819\n",
            "Epoch: 13/30, step: 271/364, loss: 0.46461, accuracy: 0.78857\n",
            "Epoch: 13/30, step: 272/364, loss: 0.46448, accuracy: 0.78843\n",
            "Epoch: 13/30, step: 273/364, loss: 0.46457, accuracy: 0.78840\n",
            "Epoch: 13/30, step: 274/364, loss: 0.46452, accuracy: 0.78844\n",
            "Epoch: 13/30, step: 275/364, loss: 0.46431, accuracy: 0.78858\n",
            "Epoch: 13/30, step: 276/364, loss: 0.46447, accuracy: 0.78833\n",
            "Epoch: 13/30, step: 277/364, loss: 0.46449, accuracy: 0.78824\n",
            "Epoch: 13/30, step: 278/364, loss: 0.46448, accuracy: 0.78828\n",
            "Epoch: 13/30, step: 279/364, loss: 0.46479, accuracy: 0.78797\n",
            "Epoch: 13/30, step: 280/364, loss: 0.46454, accuracy: 0.78817\n",
            "Epoch: 13/30, step: 281/364, loss: 0.46446, accuracy: 0.78820\n",
            "Epoch: 13/30, step: 282/364, loss: 0.46456, accuracy: 0.78807\n",
            "Epoch: 13/30, step: 283/364, loss: 0.46473, accuracy: 0.78804\n",
            "Epoch: 13/30, step: 284/364, loss: 0.46477, accuracy: 0.78796\n",
            "Epoch: 13/30, step: 285/364, loss: 0.46463, accuracy: 0.78799\n",
            "Epoch: 13/30, step: 286/364, loss: 0.46464, accuracy: 0.78792\n",
            "Epoch: 13/30, step: 287/364, loss: 0.46475, accuracy: 0.78784\n",
            "Epoch: 13/30, step: 288/364, loss: 0.46433, accuracy: 0.78803\n",
            "Epoch: 13/30, step: 289/364, loss: 0.46451, accuracy: 0.78806\n",
            "Epoch: 13/30, step: 290/364, loss: 0.46423, accuracy: 0.78831\n",
            "Epoch: 13/30, step: 291/364, loss: 0.46445, accuracy: 0.78812\n",
            "Epoch: 13/30, train loss: 0.46445, train accuracy: 0.78812, valid loss: 0.63065, valid accuracy: 0.66574\n",
            "Epoch: 14/30, step: 1/364, loss: 0.42561, accuracy: 0.82812\n",
            "Epoch: 14/30, step: 2/364, loss: 0.41619, accuracy: 0.82812\n",
            "Epoch: 14/30, step: 3/364, loss: 0.41288, accuracy: 0.81250\n",
            "Epoch: 14/30, step: 4/364, loss: 0.42846, accuracy: 0.80078\n",
            "Epoch: 14/30, step: 5/364, loss: 0.44932, accuracy: 0.77500\n",
            "Epoch: 14/30, step: 6/364, loss: 0.44997, accuracy: 0.78385\n",
            "Epoch: 14/30, step: 7/364, loss: 0.44883, accuracy: 0.78795\n",
            "Epoch: 14/30, step: 8/364, loss: 0.44579, accuracy: 0.78906\n",
            "Epoch: 14/30, step: 9/364, loss: 0.44501, accuracy: 0.79167\n",
            "Epoch: 14/30, step: 10/364, loss: 0.44718, accuracy: 0.78750\n",
            "Epoch: 14/30, step: 11/364, loss: 0.44832, accuracy: 0.78267\n",
            "Epoch: 14/30, step: 12/364, loss: 0.44055, accuracy: 0.79167\n",
            "Epoch: 14/30, step: 13/364, loss: 0.43978, accuracy: 0.79447\n",
            "Epoch: 14/30, step: 14/364, loss: 0.43924, accuracy: 0.79241\n",
            "Epoch: 14/30, step: 15/364, loss: 0.43802, accuracy: 0.79375\n",
            "Epoch: 14/30, step: 16/364, loss: 0.43679, accuracy: 0.79395\n",
            "Epoch: 14/30, step: 17/364, loss: 0.43897, accuracy: 0.79504\n",
            "Epoch: 14/30, step: 18/364, loss: 0.43622, accuracy: 0.80035\n",
            "Epoch: 14/30, step: 19/364, loss: 0.43548, accuracy: 0.80181\n",
            "Epoch: 14/30, step: 20/364, loss: 0.43828, accuracy: 0.79922\n",
            "Epoch: 14/30, step: 21/364, loss: 0.43934, accuracy: 0.79911\n",
            "Epoch: 14/30, step: 22/364, loss: 0.43739, accuracy: 0.80256\n",
            "Epoch: 14/30, step: 23/364, loss: 0.43847, accuracy: 0.80299\n",
            "Epoch: 14/30, step: 24/364, loss: 0.44075, accuracy: 0.79948\n",
            "Epoch: 14/30, step: 25/364, loss: 0.44241, accuracy: 0.79750\n",
            "Epoch: 14/30, step: 26/364, loss: 0.44211, accuracy: 0.79868\n",
            "Epoch: 14/30, step: 27/364, loss: 0.44348, accuracy: 0.79688\n",
            "Epoch: 14/30, step: 28/364, loss: 0.44600, accuracy: 0.79576\n",
            "Epoch: 14/30, step: 29/364, loss: 0.44399, accuracy: 0.79903\n",
            "Epoch: 14/30, step: 30/364, loss: 0.44354, accuracy: 0.79948\n",
            "Epoch: 14/30, step: 31/364, loss: 0.44228, accuracy: 0.80141\n",
            "Epoch: 14/30, step: 32/364, loss: 0.44290, accuracy: 0.80029\n",
            "Epoch: 14/30, step: 33/364, loss: 0.44000, accuracy: 0.80350\n",
            "Epoch: 14/30, step: 34/364, loss: 0.44044, accuracy: 0.80469\n",
            "Epoch: 14/30, step: 35/364, loss: 0.43936, accuracy: 0.80491\n",
            "Epoch: 14/30, step: 36/364, loss: 0.43952, accuracy: 0.80469\n",
            "Epoch: 14/30, step: 37/364, loss: 0.43830, accuracy: 0.80490\n",
            "Epoch: 14/30, step: 38/364, loss: 0.43806, accuracy: 0.80510\n",
            "Epoch: 14/30, step: 39/364, loss: 0.44004, accuracy: 0.80369\n",
            "Epoch: 14/30, step: 40/364, loss: 0.43903, accuracy: 0.80273\n",
            "Epoch: 14/30, step: 41/364, loss: 0.43809, accuracy: 0.80412\n",
            "Epoch: 14/30, step: 42/364, loss: 0.44029, accuracy: 0.80283\n",
            "Epoch: 14/30, step: 43/364, loss: 0.44020, accuracy: 0.80342\n",
            "Epoch: 14/30, step: 44/364, loss: 0.44101, accuracy: 0.80291\n",
            "Epoch: 14/30, step: 45/364, loss: 0.44240, accuracy: 0.80174\n",
            "Epoch: 14/30, step: 46/364, loss: 0.44228, accuracy: 0.80163\n",
            "Epoch: 14/30, step: 47/364, loss: 0.44188, accuracy: 0.80253\n",
            "Epoch: 14/30, step: 48/364, loss: 0.44137, accuracy: 0.80306\n",
            "Epoch: 14/30, step: 49/364, loss: 0.44133, accuracy: 0.80230\n",
            "Epoch: 14/30, step: 50/364, loss: 0.44064, accuracy: 0.80250\n",
            "Epoch: 14/30, step: 51/364, loss: 0.43999, accuracy: 0.80270\n",
            "Epoch: 14/30, step: 52/364, loss: 0.43912, accuracy: 0.80379\n",
            "Epoch: 14/30, step: 53/364, loss: 0.43973, accuracy: 0.80307\n",
            "Epoch: 14/30, step: 54/364, loss: 0.44017, accuracy: 0.80295\n",
            "Epoch: 14/30, step: 55/364, loss: 0.44009, accuracy: 0.80369\n",
            "Epoch: 14/30, step: 56/364, loss: 0.44059, accuracy: 0.80357\n",
            "Epoch: 14/30, step: 57/364, loss: 0.44078, accuracy: 0.80318\n",
            "Epoch: 14/30, step: 58/364, loss: 0.44073, accuracy: 0.80334\n",
            "Epoch: 14/30, step: 59/364, loss: 0.43971, accuracy: 0.80350\n",
            "Epoch: 14/30, step: 60/364, loss: 0.44010, accuracy: 0.80286\n",
            "Epoch: 14/30, step: 61/364, loss: 0.44063, accuracy: 0.80251\n",
            "Epoch: 14/30, step: 62/364, loss: 0.43985, accuracy: 0.80292\n",
            "Epoch: 14/30, step: 63/364, loss: 0.44183, accuracy: 0.80159\n",
            "Epoch: 14/30, step: 64/364, loss: 0.44054, accuracy: 0.80322\n",
            "Epoch: 14/30, step: 65/364, loss: 0.44155, accuracy: 0.80337\n",
            "Epoch: 14/30, step: 66/364, loss: 0.44128, accuracy: 0.80327\n",
            "Epoch: 14/30, step: 67/364, loss: 0.44123, accuracy: 0.80410\n",
            "Epoch: 14/30, step: 68/364, loss: 0.44348, accuracy: 0.80285\n",
            "Epoch: 14/30, step: 69/364, loss: 0.44574, accuracy: 0.80072\n",
            "Epoch: 14/30, step: 70/364, loss: 0.44537, accuracy: 0.80156\n",
            "Epoch: 14/30, step: 71/364, loss: 0.44673, accuracy: 0.80040\n",
            "Epoch: 14/30, step: 72/364, loss: 0.44667, accuracy: 0.79991\n",
            "Epoch: 14/30, step: 73/364, loss: 0.44588, accuracy: 0.80051\n",
            "Epoch: 14/30, step: 74/364, loss: 0.44573, accuracy: 0.80068\n",
            "Epoch: 14/30, step: 75/364, loss: 0.44439, accuracy: 0.80146\n",
            "Epoch: 14/30, step: 76/364, loss: 0.44594, accuracy: 0.79975\n",
            "Epoch: 14/30, step: 77/364, loss: 0.44549, accuracy: 0.80032\n",
            "Epoch: 14/30, step: 78/364, loss: 0.44575, accuracy: 0.79948\n",
            "Epoch: 14/30, step: 79/364, loss: 0.44581, accuracy: 0.80004\n",
            "Epoch: 14/30, step: 80/364, loss: 0.44495, accuracy: 0.80039\n",
            "Epoch: 14/30, step: 81/364, loss: 0.44474, accuracy: 0.80015\n",
            "Epoch: 14/30, step: 82/364, loss: 0.44442, accuracy: 0.80050\n",
            "Epoch: 14/30, step: 83/364, loss: 0.44370, accuracy: 0.80083\n",
            "Epoch: 14/30, step: 84/364, loss: 0.44348, accuracy: 0.80097\n",
            "Epoch: 14/30, step: 85/364, loss: 0.44318, accuracy: 0.80129\n",
            "Epoch: 14/30, step: 86/364, loss: 0.44404, accuracy: 0.80015\n",
            "Epoch: 14/30, step: 87/364, loss: 0.44402, accuracy: 0.80011\n",
            "Epoch: 14/30, step: 88/364, loss: 0.44426, accuracy: 0.79989\n",
            "Epoch: 14/30, step: 89/364, loss: 0.44383, accuracy: 0.80039\n",
            "Epoch: 14/30, step: 90/364, loss: 0.44355, accuracy: 0.80052\n",
            "Epoch: 14/30, step: 91/364, loss: 0.44331, accuracy: 0.80082\n",
            "Epoch: 14/30, step: 92/364, loss: 0.44215, accuracy: 0.80163\n",
            "Epoch: 14/30, step: 93/364, loss: 0.44238, accuracy: 0.80124\n",
            "Epoch: 14/30, step: 94/364, loss: 0.44179, accuracy: 0.80170\n",
            "Epoch: 14/30, step: 95/364, loss: 0.44173, accuracy: 0.80181\n",
            "Epoch: 14/30, step: 96/364, loss: 0.44134, accuracy: 0.80241\n",
            "Epoch: 14/30, step: 97/364, loss: 0.44206, accuracy: 0.80122\n",
            "Epoch: 14/30, step: 98/364, loss: 0.44198, accuracy: 0.80166\n",
            "Epoch: 14/30, step: 99/364, loss: 0.44228, accuracy: 0.80129\n",
            "Epoch: 14/30, step: 100/364, loss: 0.44304, accuracy: 0.80047\n",
            "Epoch: 14/30, step: 101/364, loss: 0.44312, accuracy: 0.80074\n",
            "Epoch: 14/30, step: 102/364, loss: 0.44308, accuracy: 0.80040\n",
            "Epoch: 14/30, step: 103/364, loss: 0.44304, accuracy: 0.80097\n",
            "Epoch: 14/30, step: 104/364, loss: 0.44283, accuracy: 0.80093\n",
            "Epoch: 14/30, step: 105/364, loss: 0.44296, accuracy: 0.80119\n",
            "Epoch: 14/30, step: 106/364, loss: 0.44266, accuracy: 0.80144\n",
            "Epoch: 14/30, step: 107/364, loss: 0.44367, accuracy: 0.80111\n",
            "Epoch: 14/30, step: 108/364, loss: 0.44290, accuracy: 0.80179\n",
            "Epoch: 14/30, step: 109/364, loss: 0.44359, accuracy: 0.80146\n",
            "Epoch: 14/30, step: 110/364, loss: 0.44335, accuracy: 0.80156\n",
            "Epoch: 14/30, step: 111/364, loss: 0.44403, accuracy: 0.80124\n",
            "Epoch: 14/30, step: 112/364, loss: 0.44384, accuracy: 0.80106\n",
            "Epoch: 14/30, step: 113/364, loss: 0.44410, accuracy: 0.80102\n",
            "Epoch: 14/30, step: 114/364, loss: 0.44383, accuracy: 0.80112\n",
            "Epoch: 14/30, step: 115/364, loss: 0.44354, accuracy: 0.80122\n",
            "Epoch: 14/30, step: 116/364, loss: 0.44409, accuracy: 0.80105\n",
            "Epoch: 14/30, step: 117/364, loss: 0.44402, accuracy: 0.80115\n",
            "Epoch: 14/30, step: 118/364, loss: 0.44299, accuracy: 0.80230\n",
            "Epoch: 14/30, step: 119/364, loss: 0.44303, accuracy: 0.80226\n",
            "Epoch: 14/30, step: 120/364, loss: 0.44380, accuracy: 0.80143\n",
            "Epoch: 14/30, step: 121/364, loss: 0.44419, accuracy: 0.80101\n",
            "Epoch: 14/30, step: 122/364, loss: 0.44393, accuracy: 0.80110\n",
            "Epoch: 14/30, step: 123/364, loss: 0.44413, accuracy: 0.80056\n",
            "Epoch: 14/30, step: 124/364, loss: 0.44313, accuracy: 0.80141\n",
            "Epoch: 14/30, step: 125/364, loss: 0.44407, accuracy: 0.80050\n",
            "Epoch: 14/30, step: 126/364, loss: 0.44333, accuracy: 0.80146\n",
            "Epoch: 14/30, step: 127/364, loss: 0.44404, accuracy: 0.80081\n",
            "Epoch: 14/30, step: 128/364, loss: 0.44401, accuracy: 0.80054\n",
            "Epoch: 14/30, step: 129/364, loss: 0.44355, accuracy: 0.80087\n",
            "Epoch: 14/30, step: 130/364, loss: 0.44286, accuracy: 0.80168\n",
            "Epoch: 14/30, step: 131/364, loss: 0.44282, accuracy: 0.80165\n",
            "Epoch: 14/30, step: 132/364, loss: 0.44308, accuracy: 0.80125\n",
            "Epoch: 14/30, step: 133/364, loss: 0.44335, accuracy: 0.80099\n",
            "Epoch: 14/30, step: 134/364, loss: 0.44342, accuracy: 0.80049\n",
            "Epoch: 14/30, step: 135/364, loss: 0.44310, accuracy: 0.80081\n",
            "Epoch: 14/30, step: 136/364, loss: 0.44288, accuracy: 0.80101\n",
            "Epoch: 14/30, step: 137/364, loss: 0.44290, accuracy: 0.80064\n",
            "Epoch: 14/30, step: 138/364, loss: 0.44358, accuracy: 0.79982\n",
            "Epoch: 14/30, step: 139/364, loss: 0.44257, accuracy: 0.80070\n",
            "Epoch: 14/30, step: 140/364, loss: 0.44330, accuracy: 0.80022\n",
            "Epoch: 14/30, step: 141/364, loss: 0.44316, accuracy: 0.80020\n",
            "Epoch: 14/30, step: 142/364, loss: 0.44312, accuracy: 0.80029\n",
            "Epoch: 14/30, step: 143/364, loss: 0.44357, accuracy: 0.80015\n",
            "Epoch: 14/30, step: 144/364, loss: 0.44304, accuracy: 0.80078\n",
            "Epoch: 14/30, step: 145/364, loss: 0.44355, accuracy: 0.80032\n",
            "Epoch: 14/30, step: 146/364, loss: 0.44369, accuracy: 0.79998\n",
            "Epoch: 14/30, step: 147/364, loss: 0.44326, accuracy: 0.80017\n",
            "Epoch: 14/30, step: 148/364, loss: 0.44301, accuracy: 0.80046\n",
            "Epoch: 14/30, step: 149/364, loss: 0.44236, accuracy: 0.80128\n",
            "Epoch: 14/30, step: 150/364, loss: 0.44204, accuracy: 0.80125\n",
            "Epoch: 14/30, step: 151/364, loss: 0.44223, accuracy: 0.80122\n",
            "Epoch: 14/30, step: 152/364, loss: 0.44218, accuracy: 0.80150\n",
            "Epoch: 14/30, step: 153/364, loss: 0.44190, accuracy: 0.80147\n",
            "Epoch: 14/30, step: 154/364, loss: 0.44155, accuracy: 0.80175\n",
            "Epoch: 14/30, step: 155/364, loss: 0.44136, accuracy: 0.80192\n",
            "Epoch: 14/30, step: 156/364, loss: 0.44079, accuracy: 0.80238\n",
            "Epoch: 14/30, step: 157/364, loss: 0.44148, accuracy: 0.80175\n",
            "Epoch: 14/30, step: 158/364, loss: 0.44105, accuracy: 0.80231\n",
            "Epoch: 14/30, step: 159/364, loss: 0.44105, accuracy: 0.80248\n",
            "Epoch: 14/30, step: 160/364, loss: 0.44082, accuracy: 0.80254\n",
            "Epoch: 14/30, step: 161/364, loss: 0.44078, accuracy: 0.80270\n",
            "Epoch: 14/30, step: 162/364, loss: 0.44048, accuracy: 0.80305\n",
            "Epoch: 14/30, step: 163/364, loss: 0.44038, accuracy: 0.80291\n",
            "Epoch: 14/30, step: 164/364, loss: 0.44015, accuracy: 0.80307\n",
            "Epoch: 14/30, step: 165/364, loss: 0.44045, accuracy: 0.80265\n",
            "Epoch: 14/30, step: 166/364, loss: 0.44030, accuracy: 0.80271\n",
            "Epoch: 14/30, step: 167/364, loss: 0.43996, accuracy: 0.80305\n",
            "Epoch: 14/30, step: 168/364, loss: 0.43993, accuracy: 0.80311\n",
            "Epoch: 14/30, step: 169/364, loss: 0.43990, accuracy: 0.80288\n",
            "Epoch: 14/30, step: 170/364, loss: 0.43948, accuracy: 0.80340\n",
            "Epoch: 14/30, step: 171/364, loss: 0.43947, accuracy: 0.80345\n",
            "Epoch: 14/30, step: 172/364, loss: 0.43904, accuracy: 0.80378\n",
            "Epoch: 14/30, step: 173/364, loss: 0.43924, accuracy: 0.80356\n",
            "Epoch: 14/30, step: 174/364, loss: 0.43909, accuracy: 0.80361\n",
            "Epoch: 14/30, step: 175/364, loss: 0.43916, accuracy: 0.80348\n",
            "Epoch: 14/30, step: 176/364, loss: 0.43938, accuracy: 0.80327\n",
            "Epoch: 14/30, step: 177/364, loss: 0.43974, accuracy: 0.80279\n",
            "Epoch: 14/30, step: 178/364, loss: 0.43982, accuracy: 0.80258\n",
            "Epoch: 14/30, step: 179/364, loss: 0.43965, accuracy: 0.80290\n",
            "Epoch: 14/30, step: 180/364, loss: 0.43952, accuracy: 0.80286\n",
            "Epoch: 14/30, step: 181/364, loss: 0.43987, accuracy: 0.80257\n",
            "Epoch: 14/30, step: 182/364, loss: 0.43976, accuracy: 0.80288\n",
            "Epoch: 14/30, step: 183/364, loss: 0.43945, accuracy: 0.80328\n",
            "Epoch: 14/30, step: 184/364, loss: 0.43924, accuracy: 0.80350\n",
            "Epoch: 14/30, step: 185/364, loss: 0.43875, accuracy: 0.80389\n",
            "Epoch: 14/30, step: 186/364, loss: 0.43889, accuracy: 0.80368\n",
            "Epoch: 14/30, step: 187/364, loss: 0.43908, accuracy: 0.80356\n",
            "Epoch: 14/30, step: 188/364, loss: 0.43912, accuracy: 0.80336\n",
            "Epoch: 14/30, step: 189/364, loss: 0.43915, accuracy: 0.80332\n",
            "Epoch: 14/30, step: 190/364, loss: 0.43977, accuracy: 0.80288\n",
            "Epoch: 14/30, step: 191/364, loss: 0.43973, accuracy: 0.80285\n",
            "Epoch: 14/30, step: 192/364, loss: 0.43969, accuracy: 0.80273\n",
            "Epoch: 14/30, step: 193/364, loss: 0.43983, accuracy: 0.80254\n",
            "Epoch: 14/30, step: 194/364, loss: 0.43978, accuracy: 0.80251\n",
            "Epoch: 14/30, step: 195/364, loss: 0.44006, accuracy: 0.80232\n",
            "Epoch: 14/30, step: 196/364, loss: 0.44014, accuracy: 0.80230\n",
            "Epoch: 14/30, step: 197/364, loss: 0.43992, accuracy: 0.80251\n",
            "Epoch: 14/30, step: 198/364, loss: 0.43968, accuracy: 0.80279\n",
            "Epoch: 14/30, step: 199/364, loss: 0.43978, accuracy: 0.80276\n",
            "Epoch: 14/30, step: 200/364, loss: 0.43959, accuracy: 0.80297\n",
            "Epoch: 14/30, step: 201/364, loss: 0.43979, accuracy: 0.80302\n",
            "Epoch: 14/30, step: 202/364, loss: 0.43980, accuracy: 0.80291\n",
            "Epoch: 14/30, step: 203/364, loss: 0.43979, accuracy: 0.80296\n",
            "Epoch: 14/30, step: 204/364, loss: 0.43983, accuracy: 0.80277\n",
            "Epoch: 14/30, step: 205/364, loss: 0.44032, accuracy: 0.80259\n",
            "Epoch: 14/30, step: 206/364, loss: 0.44012, accuracy: 0.80272\n",
            "Epoch: 14/30, step: 207/364, loss: 0.44038, accuracy: 0.80291\n",
            "Epoch: 14/30, step: 208/364, loss: 0.44081, accuracy: 0.80273\n",
            "Epoch: 14/30, step: 209/364, loss: 0.44104, accuracy: 0.80248\n",
            "Epoch: 14/30, step: 210/364, loss: 0.44108, accuracy: 0.80260\n",
            "Epoch: 14/30, step: 211/364, loss: 0.44111, accuracy: 0.80273\n",
            "Epoch: 14/30, step: 212/364, loss: 0.44123, accuracy: 0.80270\n",
            "Epoch: 14/30, step: 213/364, loss: 0.44141, accuracy: 0.80274\n",
            "Epoch: 14/30, step: 214/364, loss: 0.44144, accuracy: 0.80250\n",
            "Epoch: 14/30, step: 215/364, loss: 0.44179, accuracy: 0.80218\n",
            "Epoch: 14/30, step: 216/364, loss: 0.44220, accuracy: 0.80172\n",
            "Epoch: 14/30, step: 217/364, loss: 0.44232, accuracy: 0.80134\n",
            "Epoch: 14/30, step: 218/364, loss: 0.44265, accuracy: 0.80110\n",
            "Epoch: 14/30, step: 219/364, loss: 0.44261, accuracy: 0.80130\n",
            "Epoch: 14/30, step: 220/364, loss: 0.44313, accuracy: 0.80071\n",
            "Epoch: 14/30, step: 221/364, loss: 0.44311, accuracy: 0.80098\n",
            "Epoch: 14/30, step: 222/364, loss: 0.44296, accuracy: 0.80117\n",
            "Epoch: 14/30, step: 223/364, loss: 0.44286, accuracy: 0.80115\n",
            "Epoch: 14/30, step: 224/364, loss: 0.44299, accuracy: 0.80092\n",
            "Epoch: 14/30, step: 225/364, loss: 0.44314, accuracy: 0.80063\n",
            "Epoch: 14/30, step: 226/364, loss: 0.44313, accuracy: 0.80075\n",
            "Epoch: 14/30, step: 227/364, loss: 0.44294, accuracy: 0.80094\n",
            "Epoch: 14/30, step: 228/364, loss: 0.44323, accuracy: 0.80085\n",
            "Epoch: 14/30, step: 229/364, loss: 0.44363, accuracy: 0.80042\n",
            "Epoch: 14/30, step: 230/364, loss: 0.44327, accuracy: 0.80082\n",
            "Epoch: 14/30, step: 231/364, loss: 0.44306, accuracy: 0.80087\n",
            "Epoch: 14/30, step: 232/364, loss: 0.44295, accuracy: 0.80112\n",
            "Epoch: 14/30, step: 233/364, loss: 0.44310, accuracy: 0.80103\n",
            "Epoch: 14/30, step: 234/364, loss: 0.44312, accuracy: 0.80095\n",
            "Epoch: 14/30, step: 235/364, loss: 0.44281, accuracy: 0.80126\n",
            "Epoch: 14/30, step: 236/364, loss: 0.44306, accuracy: 0.80105\n",
            "Epoch: 14/30, step: 237/364, loss: 0.44334, accuracy: 0.80083\n",
            "Epoch: 14/30, step: 238/364, loss: 0.44368, accuracy: 0.80068\n",
            "Epoch: 14/30, step: 239/364, loss: 0.44373, accuracy: 0.80073\n",
            "Epoch: 14/30, step: 240/364, loss: 0.44382, accuracy: 0.80085\n",
            "Epoch: 14/30, step: 241/364, loss: 0.44395, accuracy: 0.80096\n",
            "Epoch: 14/30, step: 242/364, loss: 0.44413, accuracy: 0.80081\n",
            "Epoch: 14/30, step: 243/364, loss: 0.44431, accuracy: 0.80086\n",
            "Epoch: 14/30, step: 244/364, loss: 0.44410, accuracy: 0.80110\n",
            "Epoch: 14/30, step: 245/364, loss: 0.44401, accuracy: 0.80108\n",
            "Epoch: 14/30, step: 246/364, loss: 0.44376, accuracy: 0.80126\n",
            "Epoch: 14/30, step: 247/364, loss: 0.44371, accuracy: 0.80143\n",
            "Epoch: 14/30, step: 248/364, loss: 0.44349, accuracy: 0.80160\n",
            "Epoch: 14/30, step: 249/364, loss: 0.44337, accuracy: 0.80164\n",
            "Epoch: 14/30, step: 250/364, loss: 0.44358, accuracy: 0.80150\n",
            "Epoch: 14/30, step: 251/364, loss: 0.44359, accuracy: 0.80154\n",
            "Epoch: 14/30, step: 252/364, loss: 0.44397, accuracy: 0.80122\n",
            "Epoch: 14/30, step: 253/364, loss: 0.44372, accuracy: 0.80145\n",
            "Epoch: 14/30, step: 254/364, loss: 0.44356, accuracy: 0.80155\n",
            "Epoch: 14/30, step: 255/364, loss: 0.44356, accuracy: 0.80141\n",
            "Epoch: 14/30, step: 256/364, loss: 0.44341, accuracy: 0.80145\n",
            "Epoch: 14/30, step: 257/364, loss: 0.44353, accuracy: 0.80137\n",
            "Epoch: 14/30, step: 258/364, loss: 0.44354, accuracy: 0.80148\n",
            "Epoch: 14/30, step: 259/364, loss: 0.44363, accuracy: 0.80134\n",
            "Epoch: 14/30, step: 260/364, loss: 0.44308, accuracy: 0.80174\n",
            "Epoch: 14/30, step: 261/364, loss: 0.44314, accuracy: 0.80184\n",
            "Epoch: 14/30, step: 262/364, loss: 0.44286, accuracy: 0.80206\n",
            "Epoch: 14/30, step: 263/364, loss: 0.44317, accuracy: 0.80187\n",
            "Epoch: 14/30, step: 264/364, loss: 0.44293, accuracy: 0.80202\n",
            "Epoch: 14/30, step: 265/364, loss: 0.44276, accuracy: 0.80230\n",
            "Epoch: 14/30, step: 266/364, loss: 0.44260, accuracy: 0.80240\n",
            "Epoch: 14/30, step: 267/364, loss: 0.44290, accuracy: 0.80191\n",
            "Epoch: 14/30, step: 268/364, loss: 0.44305, accuracy: 0.80183\n",
            "Epoch: 14/30, step: 269/364, loss: 0.44283, accuracy: 0.80199\n",
            "Epoch: 14/30, step: 270/364, loss: 0.44282, accuracy: 0.80197\n",
            "Epoch: 14/30, step: 271/364, loss: 0.44283, accuracy: 0.80195\n",
            "Epoch: 14/30, step: 272/364, loss: 0.44304, accuracy: 0.80170\n",
            "Epoch: 14/30, step: 273/364, loss: 0.44291, accuracy: 0.80191\n",
            "Epoch: 14/30, step: 274/364, loss: 0.44270, accuracy: 0.80201\n",
            "Epoch: 14/30, step: 275/364, loss: 0.44301, accuracy: 0.80193\n",
            "Epoch: 14/30, step: 276/364, loss: 0.44331, accuracy: 0.80169\n",
            "Epoch: 14/30, step: 277/364, loss: 0.44326, accuracy: 0.80144\n",
            "Epoch: 14/30, step: 278/364, loss: 0.44314, accuracy: 0.80143\n",
            "Epoch: 14/30, step: 279/364, loss: 0.44298, accuracy: 0.80152\n",
            "Epoch: 14/30, step: 280/364, loss: 0.44309, accuracy: 0.80156\n",
            "Epoch: 14/30, step: 281/364, loss: 0.44285, accuracy: 0.80160\n",
            "Epoch: 14/30, step: 282/364, loss: 0.44288, accuracy: 0.80175\n",
            "Epoch: 14/30, step: 283/364, loss: 0.44291, accuracy: 0.80195\n",
            "Epoch: 14/30, step: 284/364, loss: 0.44295, accuracy: 0.80183\n",
            "Epoch: 14/30, step: 285/364, loss: 0.44315, accuracy: 0.80164\n",
            "Epoch: 14/30, step: 286/364, loss: 0.44297, accuracy: 0.80185\n",
            "Epoch: 14/30, step: 287/364, loss: 0.44290, accuracy: 0.80188\n",
            "Epoch: 14/30, step: 288/364, loss: 0.44288, accuracy: 0.80187\n",
            "Epoch: 14/30, step: 289/364, loss: 0.44259, accuracy: 0.80212\n",
            "Epoch: 14/30, step: 290/364, loss: 0.44253, accuracy: 0.80210\n",
            "Epoch: 14/30, step: 291/364, loss: 0.44287, accuracy: 0.80183\n",
            "Epoch: 14/30, train loss: 0.44287, train accuracy: 0.80183, valid loss: 0.60941, valid accuracy: 0.68121\n",
            "Epoch: 15/30, step: 1/364, loss: 0.43328, accuracy: 0.79688\n",
            "Epoch: 15/30, step: 2/364, loss: 0.46977, accuracy: 0.76562\n",
            "Epoch: 15/30, step: 3/364, loss: 0.47234, accuracy: 0.75521\n",
            "Epoch: 15/30, step: 4/364, loss: 0.47678, accuracy: 0.76562\n",
            "Epoch: 15/30, step: 5/364, loss: 0.45499, accuracy: 0.78750\n",
            "Epoch: 15/30, step: 6/364, loss: 0.46205, accuracy: 0.78385\n",
            "Epoch: 15/30, step: 7/364, loss: 0.45934, accuracy: 0.79241\n",
            "Epoch: 15/30, step: 8/364, loss: 0.45426, accuracy: 0.79492\n",
            "Epoch: 15/30, step: 9/364, loss: 0.44714, accuracy: 0.80208\n",
            "Epoch: 15/30, step: 10/364, loss: 0.44668, accuracy: 0.80156\n",
            "Epoch: 15/30, step: 11/364, loss: 0.45572, accuracy: 0.79972\n",
            "Epoch: 15/30, step: 12/364, loss: 0.44973, accuracy: 0.80208\n",
            "Epoch: 15/30, step: 13/364, loss: 0.44640, accuracy: 0.80769\n",
            "Epoch: 15/30, step: 14/364, loss: 0.44338, accuracy: 0.80915\n",
            "Epoch: 15/30, step: 15/364, loss: 0.44472, accuracy: 0.80625\n",
            "Epoch: 15/30, step: 16/364, loss: 0.44130, accuracy: 0.80859\n",
            "Epoch: 15/30, step: 17/364, loss: 0.43856, accuracy: 0.80974\n",
            "Epoch: 15/30, step: 18/364, loss: 0.43631, accuracy: 0.80903\n",
            "Epoch: 15/30, step: 19/364, loss: 0.43369, accuracy: 0.81250\n",
            "Epoch: 15/30, step: 20/364, loss: 0.43287, accuracy: 0.81328\n",
            "Epoch: 15/30, step: 21/364, loss: 0.43353, accuracy: 0.81622\n",
            "Epoch: 15/30, step: 22/364, loss: 0.43062, accuracy: 0.82031\n",
            "Epoch: 15/30, step: 23/364, loss: 0.42844, accuracy: 0.82201\n",
            "Epoch: 15/30, step: 24/364, loss: 0.42957, accuracy: 0.81901\n",
            "Epoch: 15/30, step: 25/364, loss: 0.43140, accuracy: 0.81875\n",
            "Epoch: 15/30, step: 26/364, loss: 0.43043, accuracy: 0.81791\n",
            "Epoch: 15/30, step: 27/364, loss: 0.42988, accuracy: 0.81829\n",
            "Epoch: 15/30, step: 28/364, loss: 0.42796, accuracy: 0.81752\n",
            "Epoch: 15/30, step: 29/364, loss: 0.42838, accuracy: 0.81681\n",
            "Epoch: 15/30, step: 30/364, loss: 0.42750, accuracy: 0.81667\n",
            "Epoch: 15/30, step: 31/364, loss: 0.42927, accuracy: 0.81552\n",
            "Epoch: 15/30, step: 32/364, loss: 0.42966, accuracy: 0.81592\n",
            "Epoch: 15/30, step: 33/364, loss: 0.42744, accuracy: 0.81771\n",
            "Epoch: 15/30, step: 34/364, loss: 0.42465, accuracy: 0.82031\n",
            "Epoch: 15/30, step: 35/364, loss: 0.42328, accuracy: 0.82143\n",
            "Epoch: 15/30, step: 36/364, loss: 0.42346, accuracy: 0.82075\n",
            "Epoch: 15/30, step: 37/364, loss: 0.42490, accuracy: 0.82010\n",
            "Epoch: 15/30, step: 38/364, loss: 0.42548, accuracy: 0.81990\n",
            "Epoch: 15/30, step: 39/364, loss: 0.42579, accuracy: 0.81811\n",
            "Epoch: 15/30, step: 40/364, loss: 0.42501, accuracy: 0.81797\n",
            "Epoch: 15/30, step: 41/364, loss: 0.42349, accuracy: 0.81974\n",
            "Epoch: 15/30, step: 42/364, loss: 0.42028, accuracy: 0.82254\n",
            "Epoch: 15/30, step: 43/364, loss: 0.41909, accuracy: 0.82340\n",
            "Epoch: 15/30, step: 44/364, loss: 0.41739, accuracy: 0.82528\n",
            "Epoch: 15/30, step: 45/364, loss: 0.41646, accuracy: 0.82639\n",
            "Epoch: 15/30, step: 46/364, loss: 0.41626, accuracy: 0.82575\n",
            "Epoch: 15/30, step: 47/364, loss: 0.41616, accuracy: 0.82580\n",
            "Epoch: 15/30, step: 48/364, loss: 0.41688, accuracy: 0.82487\n",
            "Epoch: 15/30, step: 49/364, loss: 0.41573, accuracy: 0.82589\n",
            "Epoch: 15/30, step: 50/364, loss: 0.41565, accuracy: 0.82531\n",
            "Epoch: 15/30, step: 51/364, loss: 0.41435, accuracy: 0.82659\n",
            "Epoch: 15/30, step: 52/364, loss: 0.41400, accuracy: 0.82752\n",
            "Epoch: 15/30, step: 53/364, loss: 0.41417, accuracy: 0.82636\n",
            "Epoch: 15/30, step: 54/364, loss: 0.41339, accuracy: 0.82726\n",
            "Epoch: 15/30, step: 55/364, loss: 0.41418, accuracy: 0.82557\n",
            "Epoch: 15/30, step: 56/364, loss: 0.41443, accuracy: 0.82561\n",
            "Epoch: 15/30, step: 57/364, loss: 0.41400, accuracy: 0.82621\n",
            "Epoch: 15/30, step: 58/364, loss: 0.41397, accuracy: 0.82570\n",
            "Epoch: 15/30, step: 59/364, loss: 0.41329, accuracy: 0.82654\n",
            "Epoch: 15/30, step: 60/364, loss: 0.41319, accuracy: 0.82656\n",
            "Epoch: 15/30, step: 61/364, loss: 0.41537, accuracy: 0.82403\n",
            "Epoch: 15/30, step: 62/364, loss: 0.41541, accuracy: 0.82384\n",
            "Epoch: 15/30, step: 63/364, loss: 0.41540, accuracy: 0.82391\n",
            "Epoch: 15/30, step: 64/364, loss: 0.41608, accuracy: 0.82373\n",
            "Epoch: 15/30, step: 65/364, loss: 0.41590, accuracy: 0.82404\n",
            "Epoch: 15/30, step: 66/364, loss: 0.41673, accuracy: 0.82363\n",
            "Epoch: 15/30, step: 67/364, loss: 0.41749, accuracy: 0.82299\n",
            "Epoch: 15/30, step: 68/364, loss: 0.41864, accuracy: 0.82215\n",
            "Epoch: 15/30, step: 69/364, loss: 0.41969, accuracy: 0.82156\n",
            "Epoch: 15/30, step: 70/364, loss: 0.42034, accuracy: 0.82143\n",
            "Epoch: 15/30, step: 71/364, loss: 0.42055, accuracy: 0.82020\n",
            "Epoch: 15/30, step: 72/364, loss: 0.42017, accuracy: 0.81966\n",
            "Epoch: 15/30, step: 73/364, loss: 0.42008, accuracy: 0.81978\n",
            "Epoch: 15/30, step: 74/364, loss: 0.41966, accuracy: 0.82010\n",
            "Epoch: 15/30, step: 75/364, loss: 0.41959, accuracy: 0.82000\n",
            "Epoch: 15/30, step: 76/364, loss: 0.42015, accuracy: 0.82031\n",
            "Epoch: 15/30, step: 77/364, loss: 0.42049, accuracy: 0.81960\n",
            "Epoch: 15/30, step: 78/364, loss: 0.41979, accuracy: 0.82011\n",
            "Epoch: 15/30, step: 79/364, loss: 0.41934, accuracy: 0.82002\n",
            "Epoch: 15/30, step: 80/364, loss: 0.41842, accuracy: 0.82090\n",
            "Epoch: 15/30, step: 81/364, loss: 0.41845, accuracy: 0.82079\n",
            "Epoch: 15/30, step: 82/364, loss: 0.41782, accuracy: 0.82146\n",
            "Epoch: 15/30, step: 83/364, loss: 0.41746, accuracy: 0.82229\n",
            "Epoch: 15/30, step: 84/364, loss: 0.41674, accuracy: 0.82254\n",
            "Epoch: 15/30, step: 85/364, loss: 0.41734, accuracy: 0.82187\n",
            "Epoch: 15/30, step: 86/364, loss: 0.41886, accuracy: 0.82031\n",
            "Epoch: 15/30, step: 87/364, loss: 0.41857, accuracy: 0.82040\n",
            "Epoch: 15/30, step: 88/364, loss: 0.41897, accuracy: 0.82013\n",
            "Epoch: 15/30, step: 89/364, loss: 0.41920, accuracy: 0.81970\n",
            "Epoch: 15/30, step: 90/364, loss: 0.41879, accuracy: 0.82031\n",
            "Epoch: 15/30, step: 91/364, loss: 0.41867, accuracy: 0.82023\n",
            "Epoch: 15/30, step: 92/364, loss: 0.41818, accuracy: 0.82065\n",
            "Epoch: 15/30, step: 93/364, loss: 0.41835, accuracy: 0.82006\n",
            "Epoch: 15/30, step: 94/364, loss: 0.41920, accuracy: 0.81915\n",
            "Epoch: 15/30, step: 95/364, loss: 0.42012, accuracy: 0.81859\n",
            "Epoch: 15/30, step: 96/364, loss: 0.42046, accuracy: 0.81820\n",
            "Epoch: 15/30, step: 97/364, loss: 0.42122, accuracy: 0.81749\n",
            "Epoch: 15/30, step: 98/364, loss: 0.42177, accuracy: 0.81728\n",
            "Epoch: 15/30, step: 99/364, loss: 0.42137, accuracy: 0.81818\n",
            "Epoch: 15/30, step: 100/364, loss: 0.42110, accuracy: 0.81828\n",
            "Epoch: 15/30, step: 101/364, loss: 0.42123, accuracy: 0.81853\n",
            "Epoch: 15/30, step: 102/364, loss: 0.42190, accuracy: 0.81863\n",
            "Epoch: 15/30, step: 103/364, loss: 0.42191, accuracy: 0.81872\n",
            "Epoch: 15/30, step: 104/364, loss: 0.42210, accuracy: 0.81806\n",
            "Epoch: 15/30, step: 105/364, loss: 0.42250, accuracy: 0.81756\n",
            "Epoch: 15/30, step: 106/364, loss: 0.42189, accuracy: 0.81825\n",
            "Epoch: 15/30, step: 107/364, loss: 0.42137, accuracy: 0.81849\n",
            "Epoch: 15/30, step: 108/364, loss: 0.42089, accuracy: 0.81872\n",
            "Epoch: 15/30, step: 109/364, loss: 0.42109, accuracy: 0.81866\n",
            "Epoch: 15/30, step: 110/364, loss: 0.42042, accuracy: 0.81918\n",
            "Epoch: 15/30, step: 111/364, loss: 0.42044, accuracy: 0.81912\n",
            "Epoch: 15/30, step: 112/364, loss: 0.42050, accuracy: 0.81892\n",
            "Epoch: 15/30, step: 113/364, loss: 0.42085, accuracy: 0.81845\n",
            "Epoch: 15/30, step: 114/364, loss: 0.42059, accuracy: 0.81826\n",
            "Epoch: 15/30, step: 115/364, loss: 0.42027, accuracy: 0.81807\n",
            "Epoch: 15/30, step: 116/364, loss: 0.42020, accuracy: 0.81816\n",
            "Epoch: 15/30, step: 117/364, loss: 0.42076, accuracy: 0.81757\n",
            "Epoch: 15/30, step: 118/364, loss: 0.42069, accuracy: 0.81766\n",
            "Epoch: 15/30, step: 119/364, loss: 0.42017, accuracy: 0.81815\n",
            "Epoch: 15/30, step: 120/364, loss: 0.42070, accuracy: 0.81784\n",
            "Epoch: 15/30, step: 121/364, loss: 0.42218, accuracy: 0.81663\n",
            "Epoch: 15/30, step: 122/364, loss: 0.42227, accuracy: 0.81621\n",
            "Epoch: 15/30, step: 123/364, loss: 0.42211, accuracy: 0.81657\n",
            "Epoch: 15/30, step: 124/364, loss: 0.42203, accuracy: 0.81641\n",
            "Epoch: 15/30, step: 125/364, loss: 0.42204, accuracy: 0.81612\n",
            "Epoch: 15/30, step: 126/364, loss: 0.42216, accuracy: 0.81634\n",
            "Epoch: 15/30, step: 127/364, loss: 0.42178, accuracy: 0.81681\n",
            "Epoch: 15/30, step: 128/364, loss: 0.42200, accuracy: 0.81677\n",
            "Epoch: 15/30, step: 129/364, loss: 0.42229, accuracy: 0.81662\n",
            "Epoch: 15/30, step: 130/364, loss: 0.42246, accuracy: 0.81635\n",
            "Epoch: 15/30, step: 131/364, loss: 0.42239, accuracy: 0.81620\n",
            "Epoch: 15/30, step: 132/364, loss: 0.42305, accuracy: 0.81581\n",
            "Epoch: 15/30, step: 133/364, loss: 0.42326, accuracy: 0.81579\n",
            "Epoch: 15/30, step: 134/364, loss: 0.42302, accuracy: 0.81600\n",
            "Epoch: 15/30, step: 135/364, loss: 0.42313, accuracy: 0.81632\n",
            "Epoch: 15/30, step: 136/364, loss: 0.42396, accuracy: 0.81560\n",
            "Epoch: 15/30, step: 137/364, loss: 0.42329, accuracy: 0.81615\n",
            "Epoch: 15/30, step: 138/364, loss: 0.42297, accuracy: 0.81646\n",
            "Epoch: 15/30, step: 139/364, loss: 0.42338, accuracy: 0.81621\n",
            "Epoch: 15/30, step: 140/364, loss: 0.42326, accuracy: 0.81618\n",
            "Epoch: 15/30, step: 141/364, loss: 0.42314, accuracy: 0.81616\n",
            "Epoch: 15/30, step: 142/364, loss: 0.42360, accuracy: 0.81591\n",
            "Epoch: 15/30, step: 143/364, loss: 0.42358, accuracy: 0.81567\n",
            "Epoch: 15/30, step: 144/364, loss: 0.42374, accuracy: 0.81597\n",
            "Epoch: 15/30, step: 145/364, loss: 0.42385, accuracy: 0.81595\n",
            "Epoch: 15/30, step: 146/364, loss: 0.42424, accuracy: 0.81571\n",
            "Epoch: 15/30, step: 147/364, loss: 0.42427, accuracy: 0.81548\n",
            "Epoch: 15/30, step: 148/364, loss: 0.42384, accuracy: 0.81577\n",
            "Epoch: 15/30, step: 149/364, loss: 0.42373, accuracy: 0.81586\n",
            "Epoch: 15/30, step: 150/364, loss: 0.42411, accuracy: 0.81573\n",
            "Epoch: 15/30, step: 151/364, loss: 0.42341, accuracy: 0.81612\n",
            "Epoch: 15/30, step: 152/364, loss: 0.42357, accuracy: 0.81579\n",
            "Epoch: 15/30, step: 153/364, loss: 0.42373, accuracy: 0.81577\n",
            "Epoch: 15/30, step: 154/364, loss: 0.42406, accuracy: 0.81554\n",
            "Epoch: 15/30, step: 155/364, loss: 0.42446, accuracy: 0.81542\n",
            "Epoch: 15/30, step: 156/364, loss: 0.42403, accuracy: 0.81560\n",
            "Epoch: 15/30, step: 157/364, loss: 0.42377, accuracy: 0.81559\n",
            "Epoch: 15/30, step: 158/364, loss: 0.42378, accuracy: 0.81557\n",
            "Epoch: 15/30, step: 159/364, loss: 0.42307, accuracy: 0.81614\n",
            "Epoch: 15/30, step: 160/364, loss: 0.42323, accuracy: 0.81563\n",
            "Epoch: 15/30, step: 161/364, loss: 0.42365, accuracy: 0.81531\n",
            "Epoch: 15/30, step: 162/364, loss: 0.42365, accuracy: 0.81501\n",
            "Epoch: 15/30, step: 163/364, loss: 0.42388, accuracy: 0.81451\n",
            "Epoch: 15/30, step: 164/364, loss: 0.42341, accuracy: 0.81507\n",
            "Epoch: 15/30, step: 165/364, loss: 0.42385, accuracy: 0.81496\n",
            "Epoch: 15/30, step: 166/364, loss: 0.42376, accuracy: 0.81504\n",
            "Epoch: 15/30, step: 167/364, loss: 0.42410, accuracy: 0.81475\n",
            "Epoch: 15/30, step: 168/364, loss: 0.42360, accuracy: 0.81501\n",
            "Epoch: 15/30, step: 169/364, loss: 0.42313, accuracy: 0.81537\n",
            "Epoch: 15/30, step: 170/364, loss: 0.42319, accuracy: 0.81517\n",
            "Epoch: 15/30, step: 171/364, loss: 0.42290, accuracy: 0.81524\n",
            "Epoch: 15/30, step: 172/364, loss: 0.42290, accuracy: 0.81513\n",
            "Epoch: 15/30, step: 173/364, loss: 0.42335, accuracy: 0.81458\n",
            "Epoch: 15/30, step: 174/364, loss: 0.42360, accuracy: 0.81457\n",
            "Epoch: 15/30, step: 175/364, loss: 0.42370, accuracy: 0.81473\n",
            "Epoch: 15/30, step: 176/364, loss: 0.42361, accuracy: 0.81499\n",
            "Epoch: 15/30, step: 177/364, loss: 0.42388, accuracy: 0.81444\n",
            "Epoch: 15/30, step: 178/364, loss: 0.42375, accuracy: 0.81469\n",
            "Epoch: 15/30, step: 179/364, loss: 0.42371, accuracy: 0.81477\n",
            "Epoch: 15/30, step: 180/364, loss: 0.42389, accuracy: 0.81450\n",
            "Epoch: 15/30, step: 181/364, loss: 0.42378, accuracy: 0.81474\n",
            "Epoch: 15/30, step: 182/364, loss: 0.42408, accuracy: 0.81439\n",
            "Epoch: 15/30, step: 183/364, loss: 0.42386, accuracy: 0.81455\n",
            "Epoch: 15/30, step: 184/364, loss: 0.42392, accuracy: 0.81428\n",
            "Epoch: 15/30, step: 185/364, loss: 0.42400, accuracy: 0.81385\n",
            "Epoch: 15/30, step: 186/364, loss: 0.42406, accuracy: 0.81384\n",
            "Epoch: 15/30, step: 187/364, loss: 0.42412, accuracy: 0.81375\n",
            "Epoch: 15/30, step: 188/364, loss: 0.42403, accuracy: 0.81366\n",
            "Epoch: 15/30, step: 189/364, loss: 0.42397, accuracy: 0.81374\n",
            "Epoch: 15/30, step: 190/364, loss: 0.42435, accuracy: 0.81324\n",
            "Epoch: 15/30, step: 191/364, loss: 0.42405, accuracy: 0.81315\n",
            "Epoch: 15/30, step: 192/364, loss: 0.42365, accuracy: 0.81356\n",
            "Epoch: 15/30, step: 193/364, loss: 0.42339, accuracy: 0.81371\n",
            "Epoch: 15/30, step: 194/364, loss: 0.42342, accuracy: 0.81379\n",
            "Epoch: 15/30, step: 195/364, loss: 0.42346, accuracy: 0.81378\n",
            "Epoch: 15/30, step: 196/364, loss: 0.42383, accuracy: 0.81346\n",
            "Epoch: 15/30, step: 197/364, loss: 0.42349, accuracy: 0.81377\n",
            "Epoch: 15/30, step: 198/364, loss: 0.42301, accuracy: 0.81408\n",
            "Epoch: 15/30, step: 199/364, loss: 0.42316, accuracy: 0.81391\n",
            "Epoch: 15/30, step: 200/364, loss: 0.42308, accuracy: 0.81406\n",
            "Epoch: 15/30, step: 201/364, loss: 0.42305, accuracy: 0.81405\n",
            "Epoch: 15/30, step: 202/364, loss: 0.42334, accuracy: 0.81374\n",
            "Epoch: 15/30, step: 203/364, loss: 0.42335, accuracy: 0.81373\n",
            "Epoch: 15/30, step: 204/364, loss: 0.42287, accuracy: 0.81419\n",
            "Epoch: 15/30, step: 205/364, loss: 0.42264, accuracy: 0.81410\n",
            "Epoch: 15/30, step: 206/364, loss: 0.42270, accuracy: 0.81417\n",
            "Epoch: 15/30, step: 207/364, loss: 0.42260, accuracy: 0.81401\n",
            "Epoch: 15/30, step: 208/364, loss: 0.42233, accuracy: 0.81393\n",
            "Epoch: 15/30, step: 209/364, loss: 0.42248, accuracy: 0.81414\n",
            "Epoch: 15/30, step: 210/364, loss: 0.42241, accuracy: 0.81421\n",
            "Epoch: 15/30, step: 211/364, loss: 0.42228, accuracy: 0.81428\n",
            "Epoch: 15/30, step: 212/364, loss: 0.42226, accuracy: 0.81434\n",
            "Epoch: 15/30, step: 213/364, loss: 0.42215, accuracy: 0.81411\n",
            "Epoch: 15/30, step: 214/364, loss: 0.42209, accuracy: 0.81425\n",
            "Epoch: 15/30, step: 215/364, loss: 0.42235, accuracy: 0.81417\n",
            "Epoch: 15/30, step: 216/364, loss: 0.42238, accuracy: 0.81402\n",
            "Epoch: 15/30, step: 217/364, loss: 0.42235, accuracy: 0.81387\n",
            "Epoch: 15/30, step: 218/364, loss: 0.42224, accuracy: 0.81408\n",
            "Epoch: 15/30, step: 219/364, loss: 0.42228, accuracy: 0.81407\n",
            "Epoch: 15/30, step: 220/364, loss: 0.42241, accuracy: 0.81406\n",
            "Epoch: 15/30, step: 221/364, loss: 0.42198, accuracy: 0.81455\n",
            "Epoch: 15/30, step: 222/364, loss: 0.42228, accuracy: 0.81433\n",
            "Epoch: 15/30, step: 223/364, loss: 0.42212, accuracy: 0.81425\n",
            "Epoch: 15/30, step: 224/364, loss: 0.42195, accuracy: 0.81452\n",
            "Epoch: 15/30, step: 225/364, loss: 0.42192, accuracy: 0.81451\n",
            "Epoch: 15/30, step: 226/364, loss: 0.42142, accuracy: 0.81485\n",
            "Epoch: 15/30, step: 227/364, loss: 0.42146, accuracy: 0.81477\n",
            "Epoch: 15/30, step: 228/364, loss: 0.42113, accuracy: 0.81490\n",
            "Epoch: 15/30, step: 229/364, loss: 0.42103, accuracy: 0.81489\n",
            "Epoch: 15/30, step: 230/364, loss: 0.42079, accuracy: 0.81508\n",
            "Epoch: 15/30, step: 231/364, loss: 0.42079, accuracy: 0.81521\n",
            "Epoch: 15/30, step: 232/364, loss: 0.42024, accuracy: 0.81546\n",
            "Epoch: 15/30, step: 233/364, loss: 0.41981, accuracy: 0.81592\n",
            "Epoch: 15/30, step: 234/364, loss: 0.41999, accuracy: 0.81584\n",
            "Epoch: 15/30, step: 235/364, loss: 0.42008, accuracy: 0.81569\n",
            "Epoch: 15/30, step: 236/364, loss: 0.41993, accuracy: 0.81588\n",
            "Epoch: 15/30, step: 237/364, loss: 0.42008, accuracy: 0.81553\n",
            "Epoch: 15/30, step: 238/364, loss: 0.41994, accuracy: 0.81565\n",
            "Epoch: 15/30, step: 239/364, loss: 0.42014, accuracy: 0.81570\n",
            "Epoch: 15/30, step: 240/364, loss: 0.41990, accuracy: 0.81608\n",
            "Epoch: 15/30, step: 241/364, loss: 0.41956, accuracy: 0.81626\n",
            "Epoch: 15/30, step: 242/364, loss: 0.41987, accuracy: 0.81612\n",
            "Epoch: 15/30, step: 243/364, loss: 0.41932, accuracy: 0.81655\n",
            "Epoch: 15/30, step: 244/364, loss: 0.41958, accuracy: 0.81621\n",
            "Epoch: 15/30, step: 245/364, loss: 0.41935, accuracy: 0.81633\n",
            "Epoch: 15/30, step: 246/364, loss: 0.41978, accuracy: 0.81612\n",
            "Epoch: 15/30, step: 247/364, loss: 0.41967, accuracy: 0.81630\n",
            "Epoch: 15/30, step: 248/364, loss: 0.41978, accuracy: 0.81641\n",
            "Epoch: 15/30, step: 249/364, loss: 0.41978, accuracy: 0.81639\n",
            "Epoch: 15/30, step: 250/364, loss: 0.42010, accuracy: 0.81638\n",
            "Epoch: 15/30, step: 251/364, loss: 0.42013, accuracy: 0.81611\n",
            "Epoch: 15/30, step: 252/364, loss: 0.42004, accuracy: 0.81610\n",
            "Epoch: 15/30, step: 253/364, loss: 0.41974, accuracy: 0.81627\n",
            "Epoch: 15/30, step: 254/364, loss: 0.42018, accuracy: 0.81588\n",
            "Epoch: 15/30, step: 255/364, loss: 0.42039, accuracy: 0.81556\n",
            "Epoch: 15/30, step: 256/364, loss: 0.42008, accuracy: 0.81580\n",
            "Epoch: 15/30, step: 257/364, loss: 0.42011, accuracy: 0.81572\n",
            "Epoch: 15/30, step: 258/364, loss: 0.42012, accuracy: 0.81577\n",
            "Epoch: 15/30, step: 259/364, loss: 0.42026, accuracy: 0.81558\n",
            "Epoch: 15/30, step: 260/364, loss: 0.42071, accuracy: 0.81532\n",
            "Epoch: 15/30, step: 261/364, loss: 0.42056, accuracy: 0.81537\n",
            "Epoch: 15/30, step: 262/364, loss: 0.42103, accuracy: 0.81477\n",
            "Epoch: 15/30, step: 263/364, loss: 0.42094, accuracy: 0.81500\n",
            "Epoch: 15/30, step: 264/364, loss: 0.42057, accuracy: 0.81528\n",
            "Epoch: 15/30, step: 265/364, loss: 0.42073, accuracy: 0.81521\n",
            "Epoch: 15/30, step: 266/364, loss: 0.42070, accuracy: 0.81526\n",
            "Epoch: 15/30, step: 267/364, loss: 0.42074, accuracy: 0.81531\n",
            "Epoch: 15/30, step: 268/364, loss: 0.42044, accuracy: 0.81547\n",
            "Epoch: 15/30, step: 269/364, loss: 0.42059, accuracy: 0.81540\n",
            "Epoch: 15/30, step: 270/364, loss: 0.42025, accuracy: 0.81545\n",
            "Epoch: 15/30, step: 271/364, loss: 0.41986, accuracy: 0.81579\n",
            "Epoch: 15/30, step: 272/364, loss: 0.42020, accuracy: 0.81520\n",
            "Epoch: 15/30, step: 273/364, loss: 0.42035, accuracy: 0.81513\n",
            "Epoch: 15/30, step: 274/364, loss: 0.42048, accuracy: 0.81495\n",
            "Epoch: 15/30, step: 275/364, loss: 0.42054, accuracy: 0.81483\n",
            "Epoch: 15/30, step: 276/364, loss: 0.42034, accuracy: 0.81499\n",
            "Epoch: 15/30, step: 277/364, loss: 0.42033, accuracy: 0.81509\n",
            "Epoch: 15/30, step: 278/364, loss: 0.42049, accuracy: 0.81503\n",
            "Epoch: 15/30, step: 279/364, loss: 0.42037, accuracy: 0.81513\n",
            "Epoch: 15/30, step: 280/364, loss: 0.42037, accuracy: 0.81512\n",
            "Epoch: 15/30, step: 281/364, loss: 0.42049, accuracy: 0.81506\n",
            "Epoch: 15/30, step: 282/364, loss: 0.42052, accuracy: 0.81488\n",
            "Epoch: 15/30, step: 283/364, loss: 0.42016, accuracy: 0.81526\n",
            "Epoch: 15/30, step: 284/364, loss: 0.42008, accuracy: 0.81509\n",
            "Epoch: 15/30, step: 285/364, loss: 0.42016, accuracy: 0.81486\n",
            "Epoch: 15/30, step: 286/364, loss: 0.41983, accuracy: 0.81512\n",
            "Epoch: 15/30, step: 287/364, loss: 0.41996, accuracy: 0.81495\n",
            "Epoch: 15/30, step: 288/364, loss: 0.41982, accuracy: 0.81500\n",
            "Epoch: 15/30, step: 289/364, loss: 0.41974, accuracy: 0.81504\n",
            "Epoch: 15/30, step: 290/364, loss: 0.41983, accuracy: 0.81482\n",
            "Epoch: 15/30, step: 291/364, loss: 0.41975, accuracy: 0.81488\n",
            "Epoch: 15/30, train loss: 0.41975, train accuracy: 0.81488, valid loss: 0.60487, valid accuracy: 0.68637\n",
            "Epoch: 16/30, step: 1/364, loss: 0.39383, accuracy: 0.84375\n",
            "Epoch: 16/30, step: 2/364, loss: 0.39873, accuracy: 0.83594\n",
            "Epoch: 16/30, step: 3/364, loss: 0.38606, accuracy: 0.84375\n",
            "Epoch: 16/30, step: 4/364, loss: 0.40283, accuracy: 0.83203\n",
            "Epoch: 16/30, step: 5/364, loss: 0.42203, accuracy: 0.81563\n",
            "Epoch: 16/30, step: 6/364, loss: 0.40710, accuracy: 0.82031\n",
            "Epoch: 16/30, step: 7/364, loss: 0.39701, accuracy: 0.82589\n",
            "Epoch: 16/30, step: 8/364, loss: 0.40091, accuracy: 0.82227\n",
            "Epoch: 16/30, step: 9/364, loss: 0.40997, accuracy: 0.81424\n",
            "Epoch: 16/30, step: 10/364, loss: 0.40167, accuracy: 0.82344\n",
            "Epoch: 16/30, step: 11/364, loss: 0.39954, accuracy: 0.82244\n",
            "Epoch: 16/30, step: 12/364, loss: 0.39929, accuracy: 0.82292\n",
            "Epoch: 16/30, step: 13/364, loss: 0.39801, accuracy: 0.82332\n",
            "Epoch: 16/30, step: 14/364, loss: 0.39843, accuracy: 0.82366\n",
            "Epoch: 16/30, step: 15/364, loss: 0.39930, accuracy: 0.82604\n",
            "Epoch: 16/30, step: 16/364, loss: 0.40160, accuracy: 0.82910\n",
            "Epoch: 16/30, step: 17/364, loss: 0.40165, accuracy: 0.82812\n",
            "Epoch: 16/30, step: 18/364, loss: 0.40109, accuracy: 0.82726\n",
            "Epoch: 16/30, step: 19/364, loss: 0.40279, accuracy: 0.82648\n",
            "Epoch: 16/30, step: 20/364, loss: 0.40324, accuracy: 0.82578\n",
            "Epoch: 16/30, step: 21/364, loss: 0.40413, accuracy: 0.82515\n",
            "Epoch: 16/30, step: 22/364, loss: 0.40173, accuracy: 0.82670\n",
            "Epoch: 16/30, step: 23/364, loss: 0.39933, accuracy: 0.82812\n",
            "Epoch: 16/30, step: 24/364, loss: 0.39710, accuracy: 0.83073\n",
            "Epoch: 16/30, step: 25/364, loss: 0.39680, accuracy: 0.83188\n",
            "Epoch: 16/30, step: 26/364, loss: 0.39465, accuracy: 0.83413\n",
            "Epoch: 16/30, step: 27/364, loss: 0.39606, accuracy: 0.83275\n",
            "Epoch: 16/30, step: 28/364, loss: 0.39425, accuracy: 0.83538\n",
            "Epoch: 16/30, step: 29/364, loss: 0.39316, accuracy: 0.83567\n",
            "Epoch: 16/30, step: 30/364, loss: 0.39208, accuracy: 0.83594\n",
            "Epoch: 16/30, step: 31/364, loss: 0.39051, accuracy: 0.83821\n",
            "Epoch: 16/30, step: 32/364, loss: 0.39009, accuracy: 0.83936\n",
            "Epoch: 16/30, step: 33/364, loss: 0.39198, accuracy: 0.83665\n",
            "Epoch: 16/30, step: 34/364, loss: 0.39091, accuracy: 0.83824\n",
            "Epoch: 16/30, step: 35/364, loss: 0.39010, accuracy: 0.83884\n",
            "Epoch: 16/30, step: 36/364, loss: 0.39194, accuracy: 0.83811\n",
            "Epoch: 16/30, step: 37/364, loss: 0.39250, accuracy: 0.83784\n",
            "Epoch: 16/30, step: 38/364, loss: 0.39270, accuracy: 0.83676\n",
            "Epoch: 16/30, step: 39/364, loss: 0.39575, accuracy: 0.83413\n",
            "Epoch: 16/30, step: 40/364, loss: 0.39583, accuracy: 0.83438\n",
            "Epoch: 16/30, step: 41/364, loss: 0.39648, accuracy: 0.83384\n",
            "Epoch: 16/30, step: 42/364, loss: 0.39513, accuracy: 0.83445\n",
            "Epoch: 16/30, step: 43/364, loss: 0.39442, accuracy: 0.83503\n",
            "Epoch: 16/30, step: 44/364, loss: 0.39503, accuracy: 0.83381\n",
            "Epoch: 16/30, step: 45/364, loss: 0.39435, accuracy: 0.83472\n",
            "Epoch: 16/30, step: 46/364, loss: 0.39295, accuracy: 0.83628\n",
            "Epoch: 16/30, step: 47/364, loss: 0.39508, accuracy: 0.83511\n",
            "Epoch: 16/30, step: 48/364, loss: 0.39432, accuracy: 0.83594\n",
            "Epoch: 16/30, step: 49/364, loss: 0.39601, accuracy: 0.83386\n",
            "Epoch: 16/30, step: 50/364, loss: 0.39689, accuracy: 0.83344\n",
            "Epoch: 16/30, step: 51/364, loss: 0.39620, accuracy: 0.83303\n",
            "Epoch: 16/30, step: 52/364, loss: 0.39694, accuracy: 0.83203\n",
            "Epoch: 16/30, step: 53/364, loss: 0.39692, accuracy: 0.83225\n",
            "Epoch: 16/30, step: 54/364, loss: 0.39675, accuracy: 0.83247\n",
            "Epoch: 16/30, step: 55/364, loss: 0.39532, accuracy: 0.83381\n",
            "Epoch: 16/30, step: 56/364, loss: 0.39531, accuracy: 0.83287\n",
            "Epoch: 16/30, step: 57/364, loss: 0.39557, accuracy: 0.83279\n",
            "Epoch: 16/30, step: 58/364, loss: 0.39515, accuracy: 0.83244\n",
            "Epoch: 16/30, step: 59/364, loss: 0.39525, accuracy: 0.83210\n",
            "Epoch: 16/30, step: 60/364, loss: 0.39741, accuracy: 0.83021\n",
            "Epoch: 16/30, step: 61/364, loss: 0.39729, accuracy: 0.83069\n",
            "Epoch: 16/30, step: 62/364, loss: 0.39632, accuracy: 0.83165\n",
            "Epoch: 16/30, step: 63/364, loss: 0.39766, accuracy: 0.83085\n",
            "Epoch: 16/30, step: 64/364, loss: 0.39765, accuracy: 0.83130\n",
            "Epoch: 16/30, step: 65/364, loss: 0.39811, accuracy: 0.83149\n",
            "Epoch: 16/30, step: 66/364, loss: 0.39863, accuracy: 0.83049\n",
            "Epoch: 16/30, step: 67/364, loss: 0.39796, accuracy: 0.83116\n",
            "Epoch: 16/30, step: 68/364, loss: 0.39961, accuracy: 0.83019\n",
            "Epoch: 16/30, step: 69/364, loss: 0.40052, accuracy: 0.82880\n",
            "Epoch: 16/30, step: 70/364, loss: 0.40178, accuracy: 0.82835\n",
            "Epoch: 16/30, step: 71/364, loss: 0.40312, accuracy: 0.82680\n",
            "Epoch: 16/30, step: 72/364, loss: 0.40324, accuracy: 0.82639\n",
            "Epoch: 16/30, step: 73/364, loss: 0.40253, accuracy: 0.82705\n",
            "Epoch: 16/30, step: 74/364, loss: 0.40249, accuracy: 0.82728\n",
            "Epoch: 16/30, step: 75/364, loss: 0.40138, accuracy: 0.82792\n",
            "Epoch: 16/30, step: 76/364, loss: 0.40151, accuracy: 0.82771\n",
            "Epoch: 16/30, step: 77/364, loss: 0.40152, accuracy: 0.82691\n",
            "Epoch: 16/30, step: 78/364, loss: 0.40130, accuracy: 0.82692\n",
            "Epoch: 16/30, step: 79/364, loss: 0.40170, accuracy: 0.82674\n",
            "Epoch: 16/30, step: 80/364, loss: 0.40074, accuracy: 0.82734\n",
            "Epoch: 16/30, step: 81/364, loss: 0.40041, accuracy: 0.82755\n",
            "Epoch: 16/30, step: 82/364, loss: 0.40149, accuracy: 0.82660\n",
            "Epoch: 16/30, step: 83/364, loss: 0.40147, accuracy: 0.82681\n",
            "Epoch: 16/30, step: 84/364, loss: 0.40209, accuracy: 0.82645\n",
            "Epoch: 16/30, step: 85/364, loss: 0.40223, accuracy: 0.82592\n",
            "Epoch: 16/30, step: 86/364, loss: 0.40254, accuracy: 0.82558\n",
            "Epoch: 16/30, step: 87/364, loss: 0.40215, accuracy: 0.82597\n",
            "Epoch: 16/30, step: 88/364, loss: 0.40261, accuracy: 0.82546\n",
            "Epoch: 16/30, step: 89/364, loss: 0.40276, accuracy: 0.82549\n",
            "Epoch: 16/30, step: 90/364, loss: 0.40215, accuracy: 0.82604\n",
            "Epoch: 16/30, step: 91/364, loss: 0.40278, accuracy: 0.82538\n",
            "Epoch: 16/30, step: 92/364, loss: 0.40381, accuracy: 0.82490\n",
            "Epoch: 16/30, step: 93/364, loss: 0.40469, accuracy: 0.82392\n",
            "Epoch: 16/30, step: 94/364, loss: 0.40387, accuracy: 0.82480\n",
            "Epoch: 16/30, step: 95/364, loss: 0.40379, accuracy: 0.82516\n",
            "Epoch: 16/30, step: 96/364, loss: 0.40312, accuracy: 0.82552\n",
            "Epoch: 16/30, step: 97/364, loss: 0.40331, accuracy: 0.82539\n",
            "Epoch: 16/30, step: 98/364, loss: 0.40349, accuracy: 0.82526\n",
            "Epoch: 16/30, step: 99/364, loss: 0.40418, accuracy: 0.82434\n",
            "Epoch: 16/30, step: 100/364, loss: 0.40470, accuracy: 0.82422\n",
            "Epoch: 16/30, step: 101/364, loss: 0.40445, accuracy: 0.82441\n",
            "Epoch: 16/30, step: 102/364, loss: 0.40420, accuracy: 0.82445\n",
            "Epoch: 16/30, step: 103/364, loss: 0.40382, accuracy: 0.82509\n",
            "Epoch: 16/30, step: 104/364, loss: 0.40414, accuracy: 0.82512\n",
            "Epoch: 16/30, step: 105/364, loss: 0.40356, accuracy: 0.82560\n",
            "Epoch: 16/30, step: 106/364, loss: 0.40325, accuracy: 0.82636\n",
            "Epoch: 16/30, step: 107/364, loss: 0.40291, accuracy: 0.82666\n",
            "Epoch: 16/30, step: 108/364, loss: 0.40354, accuracy: 0.82624\n",
            "Epoch: 16/30, step: 109/364, loss: 0.40342, accuracy: 0.82612\n",
            "Epoch: 16/30, step: 110/364, loss: 0.40328, accuracy: 0.82599\n",
            "Epoch: 16/30, step: 111/364, loss: 0.40401, accuracy: 0.82447\n",
            "Epoch: 16/30, step: 112/364, loss: 0.40364, accuracy: 0.82506\n",
            "Epoch: 16/30, step: 113/364, loss: 0.40411, accuracy: 0.82467\n",
            "Epoch: 16/30, step: 114/364, loss: 0.40512, accuracy: 0.82374\n",
            "Epoch: 16/30, step: 115/364, loss: 0.40585, accuracy: 0.82310\n",
            "Epoch: 16/30, step: 116/364, loss: 0.40586, accuracy: 0.82328\n",
            "Epoch: 16/30, step: 117/364, loss: 0.40582, accuracy: 0.82305\n",
            "Epoch: 16/30, step: 118/364, loss: 0.40550, accuracy: 0.82336\n",
            "Epoch: 16/30, step: 119/364, loss: 0.40541, accuracy: 0.82366\n",
            "Epoch: 16/30, step: 120/364, loss: 0.40559, accuracy: 0.82396\n",
            "Epoch: 16/30, step: 121/364, loss: 0.40569, accuracy: 0.82386\n",
            "Epoch: 16/30, step: 122/364, loss: 0.40568, accuracy: 0.82364\n",
            "Epoch: 16/30, step: 123/364, loss: 0.40538, accuracy: 0.82368\n",
            "Epoch: 16/30, step: 124/364, loss: 0.40439, accuracy: 0.82447\n",
            "Epoch: 16/30, step: 125/364, loss: 0.40388, accuracy: 0.82450\n",
            "Epoch: 16/30, step: 126/364, loss: 0.40418, accuracy: 0.82440\n",
            "Epoch: 16/30, step: 127/364, loss: 0.40364, accuracy: 0.82456\n",
            "Epoch: 16/30, step: 128/364, loss: 0.40367, accuracy: 0.82434\n",
            "Epoch: 16/30, step: 129/364, loss: 0.40324, accuracy: 0.82437\n",
            "Epoch: 16/30, step: 130/364, loss: 0.40309, accuracy: 0.82464\n",
            "Epoch: 16/30, step: 131/364, loss: 0.40367, accuracy: 0.82395\n",
            "Epoch: 16/30, step: 132/364, loss: 0.40363, accuracy: 0.82386\n",
            "Epoch: 16/30, step: 133/364, loss: 0.40318, accuracy: 0.82378\n",
            "Epoch: 16/30, step: 134/364, loss: 0.40283, accuracy: 0.82393\n",
            "Epoch: 16/30, step: 135/364, loss: 0.40267, accuracy: 0.82419\n",
            "Epoch: 16/30, step: 136/364, loss: 0.40223, accuracy: 0.82445\n",
            "Epoch: 16/30, step: 137/364, loss: 0.40223, accuracy: 0.82425\n",
            "Epoch: 16/30, step: 138/364, loss: 0.40165, accuracy: 0.82473\n",
            "Epoch: 16/30, step: 139/364, loss: 0.40136, accuracy: 0.82520\n",
            "Epoch: 16/30, step: 140/364, loss: 0.40211, accuracy: 0.82478\n",
            "Epoch: 16/30, step: 141/364, loss: 0.40265, accuracy: 0.82458\n",
            "Epoch: 16/30, step: 142/364, loss: 0.40281, accuracy: 0.82427\n",
            "Epoch: 16/30, step: 143/364, loss: 0.40259, accuracy: 0.82452\n",
            "Epoch: 16/30, step: 144/364, loss: 0.40237, accuracy: 0.82454\n",
            "Epoch: 16/30, step: 145/364, loss: 0.40326, accuracy: 0.82414\n",
            "Epoch: 16/30, step: 146/364, loss: 0.40357, accuracy: 0.82374\n",
            "Epoch: 16/30, step: 147/364, loss: 0.40319, accuracy: 0.82398\n",
            "Epoch: 16/30, step: 148/364, loss: 0.40268, accuracy: 0.82475\n",
            "Epoch: 16/30, step: 149/364, loss: 0.40240, accuracy: 0.82508\n",
            "Epoch: 16/30, step: 150/364, loss: 0.40218, accuracy: 0.82531\n",
            "Epoch: 16/30, step: 151/364, loss: 0.40202, accuracy: 0.82523\n",
            "Epoch: 16/30, step: 152/364, loss: 0.40201, accuracy: 0.82473\n",
            "Epoch: 16/30, step: 153/364, loss: 0.40214, accuracy: 0.82435\n",
            "Epoch: 16/30, step: 154/364, loss: 0.40210, accuracy: 0.82468\n",
            "Epoch: 16/30, step: 155/364, loss: 0.40210, accuracy: 0.82470\n",
            "Epoch: 16/30, step: 156/364, loss: 0.40167, accuracy: 0.82482\n",
            "Epoch: 16/30, step: 157/364, loss: 0.40239, accuracy: 0.82474\n",
            "Epoch: 16/30, step: 158/364, loss: 0.40190, accuracy: 0.82496\n",
            "Epoch: 16/30, step: 159/364, loss: 0.40136, accuracy: 0.82528\n",
            "Epoch: 16/30, step: 160/364, loss: 0.40099, accuracy: 0.82539\n",
            "Epoch: 16/30, step: 161/364, loss: 0.40106, accuracy: 0.82570\n",
            "Epoch: 16/30, step: 162/364, loss: 0.40161, accuracy: 0.82523\n",
            "Epoch: 16/30, step: 163/364, loss: 0.40149, accuracy: 0.82515\n",
            "Epoch: 16/30, step: 164/364, loss: 0.40111, accuracy: 0.82546\n",
            "Epoch: 16/30, step: 165/364, loss: 0.40136, accuracy: 0.82528\n",
            "Epoch: 16/30, step: 166/364, loss: 0.40135, accuracy: 0.82540\n",
            "Epoch: 16/30, step: 167/364, loss: 0.40149, accuracy: 0.82551\n",
            "Epoch: 16/30, step: 168/364, loss: 0.40128, accuracy: 0.82571\n",
            "Epoch: 16/30, step: 169/364, loss: 0.40076, accuracy: 0.82618\n",
            "Epoch: 16/30, step: 170/364, loss: 0.40108, accuracy: 0.82564\n",
            "Epoch: 16/30, step: 171/364, loss: 0.40055, accuracy: 0.82602\n",
            "Epoch: 16/30, step: 172/364, loss: 0.40080, accuracy: 0.82585\n",
            "Epoch: 16/30, step: 173/364, loss: 0.40093, accuracy: 0.82596\n",
            "Epoch: 16/30, step: 174/364, loss: 0.40046, accuracy: 0.82651\n",
            "Epoch: 16/30, step: 175/364, loss: 0.40042, accuracy: 0.82652\n",
            "Epoch: 16/30, step: 176/364, loss: 0.40104, accuracy: 0.82591\n",
            "Epoch: 16/30, step: 177/364, loss: 0.40072, accuracy: 0.82601\n",
            "Epoch: 16/30, step: 178/364, loss: 0.40053, accuracy: 0.82637\n",
            "Epoch: 16/30, step: 179/364, loss: 0.40064, accuracy: 0.82647\n",
            "Epoch: 16/30, step: 180/364, loss: 0.40070, accuracy: 0.82639\n",
            "Epoch: 16/30, step: 181/364, loss: 0.40032, accuracy: 0.82683\n",
            "Epoch: 16/30, step: 182/364, loss: 0.40024, accuracy: 0.82692\n",
            "Epoch: 16/30, step: 183/364, loss: 0.40041, accuracy: 0.82667\n",
            "Epoch: 16/30, step: 184/364, loss: 0.40055, accuracy: 0.82677\n",
            "Epoch: 16/30, step: 185/364, loss: 0.40042, accuracy: 0.82677\n",
            "Epoch: 16/30, step: 186/364, loss: 0.40049, accuracy: 0.82670\n",
            "Epoch: 16/30, step: 187/364, loss: 0.40079, accuracy: 0.82604\n",
            "Epoch: 16/30, step: 188/364, loss: 0.40131, accuracy: 0.82596\n",
            "Epoch: 16/30, step: 189/364, loss: 0.40087, accuracy: 0.82639\n",
            "Epoch: 16/30, step: 190/364, loss: 0.40043, accuracy: 0.82681\n",
            "Epoch: 16/30, step: 191/364, loss: 0.40096, accuracy: 0.82633\n",
            "Epoch: 16/30, step: 192/364, loss: 0.40068, accuracy: 0.82642\n",
            "Epoch: 16/30, step: 193/364, loss: 0.40065, accuracy: 0.82634\n",
            "Epoch: 16/30, step: 194/364, loss: 0.40068, accuracy: 0.82651\n",
            "Epoch: 16/30, step: 195/364, loss: 0.40039, accuracy: 0.82692\n",
            "Epoch: 16/30, step: 196/364, loss: 0.40037, accuracy: 0.82685\n",
            "Epoch: 16/30, step: 197/364, loss: 0.40048, accuracy: 0.82694\n",
            "Epoch: 16/30, step: 198/364, loss: 0.40038, accuracy: 0.82718\n",
            "Epoch: 16/30, step: 199/364, loss: 0.40048, accuracy: 0.82703\n",
            "Epoch: 16/30, step: 200/364, loss: 0.40059, accuracy: 0.82703\n",
            "Epoch: 16/30, step: 201/364, loss: 0.40047, accuracy: 0.82711\n",
            "Epoch: 16/30, step: 202/364, loss: 0.39989, accuracy: 0.82743\n",
            "Epoch: 16/30, step: 203/364, loss: 0.40001, accuracy: 0.82736\n",
            "Epoch: 16/30, step: 204/364, loss: 0.40059, accuracy: 0.82698\n",
            "Epoch: 16/30, step: 205/364, loss: 0.40027, accuracy: 0.82691\n",
            "Epoch: 16/30, step: 206/364, loss: 0.39956, accuracy: 0.82729\n",
            "Epoch: 16/30, step: 207/364, loss: 0.39987, accuracy: 0.82699\n",
            "Epoch: 16/30, step: 208/364, loss: 0.40018, accuracy: 0.82692\n",
            "Epoch: 16/30, step: 209/364, loss: 0.40022, accuracy: 0.82708\n",
            "Epoch: 16/30, step: 210/364, loss: 0.39994, accuracy: 0.82723\n",
            "Epoch: 16/30, step: 211/364, loss: 0.39948, accuracy: 0.82738\n",
            "Epoch: 16/30, step: 212/364, loss: 0.39938, accuracy: 0.82761\n",
            "Epoch: 16/30, step: 213/364, loss: 0.39927, accuracy: 0.82754\n",
            "Epoch: 16/30, step: 214/364, loss: 0.39912, accuracy: 0.82776\n",
            "Epoch: 16/30, step: 215/364, loss: 0.39936, accuracy: 0.82762\n",
            "Epoch: 16/30, step: 216/364, loss: 0.39949, accuracy: 0.82755\n",
            "Epoch: 16/30, step: 217/364, loss: 0.39926, accuracy: 0.82762\n",
            "Epoch: 16/30, step: 218/364, loss: 0.39922, accuracy: 0.82762\n",
            "Epoch: 16/30, step: 219/364, loss: 0.39936, accuracy: 0.82734\n",
            "Epoch: 16/30, step: 220/364, loss: 0.39902, accuracy: 0.82756\n",
            "Epoch: 16/30, step: 221/364, loss: 0.39896, accuracy: 0.82728\n",
            "Epoch: 16/30, step: 222/364, loss: 0.39926, accuracy: 0.82700\n",
            "Epoch: 16/30, step: 223/364, loss: 0.39932, accuracy: 0.82679\n",
            "Epoch: 16/30, step: 224/364, loss: 0.39949, accuracy: 0.82666\n",
            "Epoch: 16/30, step: 225/364, loss: 0.39959, accuracy: 0.82639\n",
            "Epoch: 16/30, step: 226/364, loss: 0.40002, accuracy: 0.82647\n",
            "Epoch: 16/30, step: 227/364, loss: 0.39969, accuracy: 0.82682\n",
            "Epoch: 16/30, step: 228/364, loss: 0.39926, accuracy: 0.82717\n",
            "Epoch: 16/30, step: 229/364, loss: 0.39938, accuracy: 0.82717\n",
            "Epoch: 16/30, step: 230/364, loss: 0.39920, accuracy: 0.82717\n",
            "Epoch: 16/30, step: 231/364, loss: 0.39903, accuracy: 0.82738\n",
            "Epoch: 16/30, step: 232/364, loss: 0.39977, accuracy: 0.82685\n",
            "Epoch: 16/30, step: 233/364, loss: 0.39962, accuracy: 0.82699\n",
            "Epoch: 16/30, step: 234/364, loss: 0.39996, accuracy: 0.82679\n",
            "Epoch: 16/30, step: 235/364, loss: 0.40003, accuracy: 0.82673\n",
            "Epoch: 16/30, step: 236/364, loss: 0.39985, accuracy: 0.82680\n",
            "Epoch: 16/30, step: 237/364, loss: 0.39973, accuracy: 0.82694\n",
            "Epoch: 16/30, step: 238/364, loss: 0.39951, accuracy: 0.82701\n",
            "Epoch: 16/30, step: 239/364, loss: 0.39929, accuracy: 0.82708\n",
            "Epoch: 16/30, step: 240/364, loss: 0.39927, accuracy: 0.82702\n",
            "Epoch: 16/30, step: 241/364, loss: 0.39926, accuracy: 0.82709\n",
            "Epoch: 16/30, step: 242/364, loss: 0.39928, accuracy: 0.82709\n",
            "Epoch: 16/30, step: 243/364, loss: 0.39916, accuracy: 0.82710\n",
            "Epoch: 16/30, step: 244/364, loss: 0.39897, accuracy: 0.82729\n",
            "Epoch: 16/30, step: 245/364, loss: 0.39893, accuracy: 0.82730\n",
            "Epoch: 16/30, step: 246/364, loss: 0.39880, accuracy: 0.82730\n",
            "Epoch: 16/30, step: 247/364, loss: 0.39928, accuracy: 0.82686\n",
            "Epoch: 16/30, step: 248/364, loss: 0.39924, accuracy: 0.82680\n",
            "Epoch: 16/30, step: 249/364, loss: 0.39931, accuracy: 0.82668\n",
            "Epoch: 16/30, step: 250/364, loss: 0.39906, accuracy: 0.82694\n",
            "Epoch: 16/30, step: 251/364, loss: 0.39903, accuracy: 0.82700\n",
            "Epoch: 16/30, step: 252/364, loss: 0.39982, accuracy: 0.82639\n",
            "Epoch: 16/30, step: 253/364, loss: 0.40004, accuracy: 0.82633\n",
            "Epoch: 16/30, step: 254/364, loss: 0.39999, accuracy: 0.82622\n",
            "Epoch: 16/30, step: 255/364, loss: 0.40016, accuracy: 0.82592\n",
            "Epoch: 16/30, step: 256/364, loss: 0.40015, accuracy: 0.82593\n",
            "Epoch: 16/30, step: 257/364, loss: 0.40001, accuracy: 0.82600\n",
            "Epoch: 16/30, step: 258/364, loss: 0.40049, accuracy: 0.82546\n",
            "Epoch: 16/30, step: 259/364, loss: 0.40031, accuracy: 0.82565\n",
            "Epoch: 16/30, step: 260/364, loss: 0.40039, accuracy: 0.82554\n",
            "Epoch: 16/30, step: 261/364, loss: 0.40015, accuracy: 0.82573\n",
            "Epoch: 16/30, step: 262/364, loss: 0.39995, accuracy: 0.82568\n",
            "Epoch: 16/30, step: 263/364, loss: 0.40008, accuracy: 0.82551\n",
            "Epoch: 16/30, step: 264/364, loss: 0.40003, accuracy: 0.82558\n",
            "Epoch: 16/30, step: 265/364, loss: 0.40050, accuracy: 0.82506\n",
            "Epoch: 16/30, step: 266/364, loss: 0.40045, accuracy: 0.82513\n",
            "Epoch: 16/30, step: 267/364, loss: 0.40027, accuracy: 0.82543\n",
            "Epoch: 16/30, step: 268/364, loss: 0.40029, accuracy: 0.82550\n",
            "Epoch: 16/30, step: 269/364, loss: 0.40026, accuracy: 0.82551\n",
            "Epoch: 16/30, step: 270/364, loss: 0.40011, accuracy: 0.82558\n",
            "Epoch: 16/30, step: 271/364, loss: 0.39983, accuracy: 0.82559\n",
            "Epoch: 16/30, step: 272/364, loss: 0.39989, accuracy: 0.82537\n",
            "Epoch: 16/30, step: 273/364, loss: 0.39983, accuracy: 0.82543\n",
            "Epoch: 16/30, step: 274/364, loss: 0.39990, accuracy: 0.82510\n",
            "Epoch: 16/30, step: 275/364, loss: 0.39962, accuracy: 0.82523\n",
            "Epoch: 16/30, step: 276/364, loss: 0.39980, accuracy: 0.82512\n",
            "Epoch: 16/30, step: 277/364, loss: 0.39986, accuracy: 0.82502\n",
            "Epoch: 16/30, step: 278/364, loss: 0.40017, accuracy: 0.82481\n",
            "Epoch: 16/30, step: 279/364, loss: 0.39990, accuracy: 0.82493\n",
            "Epoch: 16/30, step: 280/364, loss: 0.39980, accuracy: 0.82494\n",
            "Epoch: 16/30, step: 281/364, loss: 0.39975, accuracy: 0.82490\n",
            "Epoch: 16/30, step: 282/364, loss: 0.39971, accuracy: 0.82502\n",
            "Epoch: 16/30, step: 283/364, loss: 0.39954, accuracy: 0.82503\n",
            "Epoch: 16/30, step: 284/364, loss: 0.39986, accuracy: 0.82477\n",
            "Epoch: 16/30, step: 285/364, loss: 0.39982, accuracy: 0.82478\n",
            "Epoch: 16/30, step: 286/364, loss: 0.39983, accuracy: 0.82474\n",
            "Epoch: 16/30, step: 287/364, loss: 0.39965, accuracy: 0.82486\n",
            "Epoch: 16/30, step: 288/364, loss: 0.39978, accuracy: 0.82487\n",
            "Epoch: 16/30, step: 289/364, loss: 0.39957, accuracy: 0.82494\n",
            "Epoch: 16/30, step: 290/364, loss: 0.39934, accuracy: 0.82516\n",
            "Epoch: 16/30, step: 291/364, loss: 0.39928, accuracy: 0.82509\n",
            "Epoch: 16/30, train loss: 0.39928, train accuracy: 0.82509, valid loss: 0.60968, valid accuracy: 0.69347\n",
            "Epoch: 17/30, step: 1/364, loss: 0.28780, accuracy: 0.90625\n",
            "Epoch: 17/30, step: 2/364, loss: 0.33662, accuracy: 0.88281\n",
            "Epoch: 17/30, step: 3/364, loss: 0.32804, accuracy: 0.86979\n",
            "Epoch: 17/30, step: 4/364, loss: 0.34147, accuracy: 0.85938\n",
            "Epoch: 17/30, step: 5/364, loss: 0.33865, accuracy: 0.85938\n",
            "Epoch: 17/30, step: 6/364, loss: 0.34592, accuracy: 0.85156\n",
            "Epoch: 17/30, step: 7/364, loss: 0.35450, accuracy: 0.84598\n",
            "Epoch: 17/30, step: 8/364, loss: 0.36113, accuracy: 0.84375\n",
            "Epoch: 17/30, step: 9/364, loss: 0.36314, accuracy: 0.84201\n",
            "Epoch: 17/30, step: 10/364, loss: 0.36331, accuracy: 0.84062\n",
            "Epoch: 17/30, step: 11/364, loss: 0.36521, accuracy: 0.83807\n",
            "Epoch: 17/30, step: 12/364, loss: 0.37894, accuracy: 0.83203\n",
            "Epoch: 17/30, step: 13/364, loss: 0.38020, accuracy: 0.83173\n",
            "Epoch: 17/30, step: 14/364, loss: 0.37973, accuracy: 0.83371\n",
            "Epoch: 17/30, step: 15/364, loss: 0.38424, accuracy: 0.82708\n",
            "Epoch: 17/30, step: 16/364, loss: 0.38279, accuracy: 0.82812\n",
            "Epoch: 17/30, step: 17/364, loss: 0.38700, accuracy: 0.82537\n",
            "Epoch: 17/30, step: 18/364, loss: 0.38528, accuracy: 0.82552\n",
            "Epoch: 17/30, step: 19/364, loss: 0.38547, accuracy: 0.82319\n",
            "Epoch: 17/30, step: 20/364, loss: 0.38196, accuracy: 0.82656\n",
            "Epoch: 17/30, step: 21/364, loss: 0.38053, accuracy: 0.82812\n",
            "Epoch: 17/30, step: 22/364, loss: 0.37983, accuracy: 0.82955\n",
            "Epoch: 17/30, step: 23/364, loss: 0.38020, accuracy: 0.82948\n",
            "Epoch: 17/30, step: 24/364, loss: 0.38174, accuracy: 0.82747\n",
            "Epoch: 17/30, step: 25/364, loss: 0.38264, accuracy: 0.82687\n",
            "Epoch: 17/30, step: 26/364, loss: 0.38366, accuracy: 0.82752\n",
            "Epoch: 17/30, step: 27/364, loss: 0.38479, accuracy: 0.82812\n",
            "Epoch: 17/30, step: 28/364, loss: 0.38769, accuracy: 0.82589\n",
            "Epoch: 17/30, step: 29/364, loss: 0.38655, accuracy: 0.82705\n",
            "Epoch: 17/30, step: 30/364, loss: 0.38462, accuracy: 0.82812\n",
            "Epoch: 17/30, step: 31/364, loss: 0.38539, accuracy: 0.82611\n",
            "Epoch: 17/30, step: 32/364, loss: 0.38508, accuracy: 0.82617\n",
            "Epoch: 17/30, step: 33/364, loss: 0.38829, accuracy: 0.82386\n",
            "Epoch: 17/30, step: 34/364, loss: 0.38628, accuracy: 0.82583\n",
            "Epoch: 17/30, step: 35/364, loss: 0.38712, accuracy: 0.82545\n",
            "Epoch: 17/30, step: 36/364, loss: 0.38628, accuracy: 0.82639\n",
            "Epoch: 17/30, step: 37/364, loss: 0.38655, accuracy: 0.82644\n",
            "Epoch: 17/30, step: 38/364, loss: 0.38495, accuracy: 0.82771\n",
            "Epoch: 17/30, step: 39/364, loss: 0.38497, accuracy: 0.82692\n",
            "Epoch: 17/30, step: 40/364, loss: 0.38292, accuracy: 0.82852\n",
            "Epoch: 17/30, step: 41/364, loss: 0.38459, accuracy: 0.82660\n",
            "Epoch: 17/30, step: 42/364, loss: 0.38355, accuracy: 0.82812\n",
            "Epoch: 17/30, step: 43/364, loss: 0.38547, accuracy: 0.82631\n",
            "Epoch: 17/30, step: 44/364, loss: 0.38522, accuracy: 0.82635\n",
            "Epoch: 17/30, step: 45/364, loss: 0.38611, accuracy: 0.82569\n",
            "Epoch: 17/30, step: 46/364, loss: 0.38673, accuracy: 0.82643\n",
            "Epoch: 17/30, step: 47/364, loss: 0.38605, accuracy: 0.82746\n",
            "Epoch: 17/30, step: 48/364, loss: 0.38733, accuracy: 0.82747\n",
            "Epoch: 17/30, step: 49/364, loss: 0.38619, accuracy: 0.82876\n",
            "Epoch: 17/30, step: 50/364, loss: 0.38615, accuracy: 0.82875\n",
            "Epoch: 17/30, step: 51/364, loss: 0.38536, accuracy: 0.82935\n",
            "Epoch: 17/30, step: 52/364, loss: 0.38553, accuracy: 0.82933\n",
            "Epoch: 17/30, step: 53/364, loss: 0.38620, accuracy: 0.82901\n",
            "Epoch: 17/30, step: 54/364, loss: 0.38635, accuracy: 0.82841\n",
            "Epoch: 17/30, step: 55/364, loss: 0.38505, accuracy: 0.82983\n",
            "Epoch: 17/30, step: 56/364, loss: 0.38478, accuracy: 0.83064\n",
            "Epoch: 17/30, step: 57/364, loss: 0.38554, accuracy: 0.83004\n",
            "Epoch: 17/30, step: 58/364, loss: 0.38454, accuracy: 0.83028\n",
            "Epoch: 17/30, step: 59/364, loss: 0.38347, accuracy: 0.83157\n",
            "Epoch: 17/30, step: 60/364, loss: 0.38328, accuracy: 0.83203\n",
            "Epoch: 17/30, step: 61/364, loss: 0.38399, accuracy: 0.83171\n",
            "Epoch: 17/30, step: 62/364, loss: 0.38612, accuracy: 0.83014\n",
            "Epoch: 17/30, step: 63/364, loss: 0.38644, accuracy: 0.82961\n",
            "Epoch: 17/30, step: 64/364, loss: 0.38584, accuracy: 0.82983\n",
            "Epoch: 17/30, step: 65/364, loss: 0.38477, accuracy: 0.83101\n",
            "Epoch: 17/30, step: 66/364, loss: 0.38586, accuracy: 0.82955\n",
            "Epoch: 17/30, step: 67/364, loss: 0.38800, accuracy: 0.82906\n",
            "Epoch: 17/30, step: 68/364, loss: 0.38859, accuracy: 0.82812\n",
            "Epoch: 17/30, step: 69/364, loss: 0.38832, accuracy: 0.82835\n",
            "Epoch: 17/30, step: 70/364, loss: 0.38838, accuracy: 0.82723\n",
            "Epoch: 17/30, step: 71/364, loss: 0.38813, accuracy: 0.82768\n",
            "Epoch: 17/30, step: 72/364, loss: 0.38858, accuracy: 0.82726\n",
            "Epoch: 17/30, step: 73/364, loss: 0.38744, accuracy: 0.82834\n",
            "Epoch: 17/30, step: 74/364, loss: 0.38633, accuracy: 0.82897\n",
            "Epoch: 17/30, step: 75/364, loss: 0.38560, accuracy: 0.82958\n",
            "Epoch: 17/30, step: 76/364, loss: 0.38400, accuracy: 0.83039\n",
            "Epoch: 17/30, step: 77/364, loss: 0.38323, accuracy: 0.83097\n",
            "Epoch: 17/30, step: 78/364, loss: 0.38434, accuracy: 0.83033\n",
            "Epoch: 17/30, step: 79/364, loss: 0.38455, accuracy: 0.83050\n",
            "Epoch: 17/30, step: 80/364, loss: 0.38353, accuracy: 0.83105\n",
            "Epoch: 17/30, step: 81/364, loss: 0.38284, accuracy: 0.83198\n",
            "Epoch: 17/30, step: 82/364, loss: 0.38175, accuracy: 0.83308\n",
            "Epoch: 17/30, step: 83/364, loss: 0.38189, accuracy: 0.83302\n",
            "Epoch: 17/30, step: 84/364, loss: 0.38238, accuracy: 0.83259\n",
            "Epoch: 17/30, step: 85/364, loss: 0.38230, accuracy: 0.83272\n",
            "Epoch: 17/30, step: 86/364, loss: 0.38245, accuracy: 0.83285\n",
            "Epoch: 17/30, step: 87/364, loss: 0.38249, accuracy: 0.83315\n",
            "Epoch: 17/30, step: 88/364, loss: 0.38253, accuracy: 0.83310\n",
            "Epoch: 17/30, step: 89/364, loss: 0.38241, accuracy: 0.83357\n",
            "Epoch: 17/30, step: 90/364, loss: 0.38263, accuracy: 0.83368\n",
            "Epoch: 17/30, step: 91/364, loss: 0.38339, accuracy: 0.83345\n",
            "Epoch: 17/30, step: 92/364, loss: 0.38297, accuracy: 0.83407\n",
            "Epoch: 17/30, step: 93/364, loss: 0.38396, accuracy: 0.83350\n",
            "Epoch: 17/30, step: 94/364, loss: 0.38352, accuracy: 0.83394\n",
            "Epoch: 17/30, step: 95/364, loss: 0.38464, accuracy: 0.83355\n",
            "Epoch: 17/30, step: 96/364, loss: 0.38360, accuracy: 0.83447\n",
            "Epoch: 17/30, step: 97/364, loss: 0.38339, accuracy: 0.83473\n",
            "Epoch: 17/30, step: 98/364, loss: 0.38381, accuracy: 0.83418\n",
            "Epoch: 17/30, step: 99/364, loss: 0.38491, accuracy: 0.83365\n",
            "Epoch: 17/30, step: 100/364, loss: 0.38466, accuracy: 0.83359\n",
            "Epoch: 17/30, step: 101/364, loss: 0.38468, accuracy: 0.83400\n",
            "Epoch: 17/30, step: 102/364, loss: 0.38450, accuracy: 0.83410\n",
            "Epoch: 17/30, step: 103/364, loss: 0.38419, accuracy: 0.83465\n",
            "Epoch: 17/30, step: 104/364, loss: 0.38453, accuracy: 0.83413\n",
            "Epoch: 17/30, step: 105/364, loss: 0.38411, accuracy: 0.83423\n",
            "Epoch: 17/30, step: 106/364, loss: 0.38433, accuracy: 0.83432\n",
            "Epoch: 17/30, step: 107/364, loss: 0.38487, accuracy: 0.83397\n",
            "Epoch: 17/30, step: 108/364, loss: 0.38474, accuracy: 0.83391\n",
            "Epoch: 17/30, step: 109/364, loss: 0.38510, accuracy: 0.83372\n",
            "Epoch: 17/30, step: 110/364, loss: 0.38480, accuracy: 0.83395\n",
            "Epoch: 17/30, step: 111/364, loss: 0.38549, accuracy: 0.83376\n",
            "Epoch: 17/30, step: 112/364, loss: 0.38560, accuracy: 0.83371\n",
            "Epoch: 17/30, step: 113/364, loss: 0.38558, accuracy: 0.83338\n",
            "Epoch: 17/30, step: 114/364, loss: 0.38557, accuracy: 0.83347\n",
            "Epoch: 17/30, step: 115/364, loss: 0.38531, accuracy: 0.83356\n",
            "Epoch: 17/30, step: 116/364, loss: 0.38589, accuracy: 0.83351\n",
            "Epoch: 17/30, step: 117/364, loss: 0.38569, accuracy: 0.83360\n",
            "Epoch: 17/30, step: 118/364, loss: 0.38660, accuracy: 0.83263\n",
            "Epoch: 17/30, step: 119/364, loss: 0.38642, accuracy: 0.83259\n",
            "Epoch: 17/30, step: 120/364, loss: 0.38634, accuracy: 0.83255\n",
            "Epoch: 17/30, step: 121/364, loss: 0.38597, accuracy: 0.83264\n",
            "Epoch: 17/30, step: 122/364, loss: 0.38548, accuracy: 0.83286\n",
            "Epoch: 17/30, step: 123/364, loss: 0.38537, accuracy: 0.83283\n",
            "Epoch: 17/30, step: 124/364, loss: 0.38521, accuracy: 0.83291\n",
            "Epoch: 17/30, step: 125/364, loss: 0.38589, accuracy: 0.83250\n",
            "Epoch: 17/30, step: 126/364, loss: 0.38562, accuracy: 0.83271\n",
            "Epoch: 17/30, step: 127/364, loss: 0.38538, accuracy: 0.83317\n",
            "Epoch: 17/30, step: 128/364, loss: 0.38534, accuracy: 0.83325\n",
            "Epoch: 17/30, step: 129/364, loss: 0.38501, accuracy: 0.83345\n",
            "Epoch: 17/30, step: 130/364, loss: 0.38502, accuracy: 0.83353\n",
            "Epoch: 17/30, step: 131/364, loss: 0.38507, accuracy: 0.83325\n",
            "Epoch: 17/30, step: 132/364, loss: 0.38469, accuracy: 0.83345\n",
            "Epoch: 17/30, step: 133/364, loss: 0.38400, accuracy: 0.83412\n",
            "Epoch: 17/30, step: 134/364, loss: 0.38349, accuracy: 0.83454\n",
            "Epoch: 17/30, step: 135/364, loss: 0.38379, accuracy: 0.83426\n",
            "Epoch: 17/30, step: 136/364, loss: 0.38347, accuracy: 0.83467\n",
            "Epoch: 17/30, step: 137/364, loss: 0.38341, accuracy: 0.83474\n",
            "Epoch: 17/30, step: 138/364, loss: 0.38326, accuracy: 0.83458\n",
            "Epoch: 17/30, step: 139/364, loss: 0.38293, accuracy: 0.83453\n",
            "Epoch: 17/30, step: 140/364, loss: 0.38223, accuracy: 0.83516\n",
            "Epoch: 17/30, step: 141/364, loss: 0.38235, accuracy: 0.83522\n",
            "Epoch: 17/30, step: 142/364, loss: 0.38195, accuracy: 0.83539\n",
            "Epoch: 17/30, step: 143/364, loss: 0.38169, accuracy: 0.83545\n",
            "Epoch: 17/30, step: 144/364, loss: 0.38195, accuracy: 0.83539\n",
            "Epoch: 17/30, step: 145/364, loss: 0.38175, accuracy: 0.83545\n",
            "Epoch: 17/30, step: 146/364, loss: 0.38274, accuracy: 0.83465\n",
            "Epoch: 17/30, step: 147/364, loss: 0.38265, accuracy: 0.83493\n",
            "Epoch: 17/30, step: 148/364, loss: 0.38238, accuracy: 0.83499\n",
            "Epoch: 17/30, step: 149/364, loss: 0.38302, accuracy: 0.83463\n",
            "Epoch: 17/30, step: 150/364, loss: 0.38296, accuracy: 0.83469\n",
            "Epoch: 17/30, step: 151/364, loss: 0.38275, accuracy: 0.83485\n",
            "Epoch: 17/30, step: 152/364, loss: 0.38267, accuracy: 0.83481\n",
            "Epoch: 17/30, step: 153/364, loss: 0.38266, accuracy: 0.83466\n",
            "Epoch: 17/30, step: 154/364, loss: 0.38257, accuracy: 0.83462\n",
            "Epoch: 17/30, step: 155/364, loss: 0.38276, accuracy: 0.83478\n",
            "Epoch: 17/30, step: 156/364, loss: 0.38382, accuracy: 0.83393\n",
            "Epoch: 17/30, step: 157/364, loss: 0.38419, accuracy: 0.83360\n",
            "Epoch: 17/30, step: 158/364, loss: 0.38394, accuracy: 0.83386\n",
            "Epoch: 17/30, step: 159/364, loss: 0.38333, accuracy: 0.83441\n",
            "Epoch: 17/30, step: 160/364, loss: 0.38244, accuracy: 0.83496\n",
            "Epoch: 17/30, step: 161/364, loss: 0.38220, accuracy: 0.83472\n",
            "Epoch: 17/30, step: 162/364, loss: 0.38262, accuracy: 0.83439\n",
            "Epoch: 17/30, step: 163/364, loss: 0.38245, accuracy: 0.83426\n",
            "Epoch: 17/30, step: 164/364, loss: 0.38226, accuracy: 0.83441\n",
            "Epoch: 17/30, step: 165/364, loss: 0.38209, accuracy: 0.83466\n",
            "Epoch: 17/30, step: 166/364, loss: 0.38278, accuracy: 0.83387\n",
            "Epoch: 17/30, step: 167/364, loss: 0.38249, accuracy: 0.83402\n",
            "Epoch: 17/30, step: 168/364, loss: 0.38223, accuracy: 0.83417\n",
            "Epoch: 17/30, step: 169/364, loss: 0.38237, accuracy: 0.83404\n",
            "Epoch: 17/30, step: 170/364, loss: 0.38244, accuracy: 0.83364\n",
            "Epoch: 17/30, step: 171/364, loss: 0.38208, accuracy: 0.83361\n",
            "Epoch: 17/30, step: 172/364, loss: 0.38155, accuracy: 0.83412\n",
            "Epoch: 17/30, step: 173/364, loss: 0.38165, accuracy: 0.83400\n",
            "Epoch: 17/30, step: 174/364, loss: 0.38159, accuracy: 0.83387\n",
            "Epoch: 17/30, step: 175/364, loss: 0.38150, accuracy: 0.83384\n",
            "Epoch: 17/30, step: 176/364, loss: 0.38129, accuracy: 0.83398\n",
            "Epoch: 17/30, step: 177/364, loss: 0.38102, accuracy: 0.83422\n",
            "Epoch: 17/30, step: 178/364, loss: 0.38083, accuracy: 0.83409\n",
            "Epoch: 17/30, step: 179/364, loss: 0.38066, accuracy: 0.83450\n",
            "Epoch: 17/30, step: 180/364, loss: 0.38107, accuracy: 0.83438\n",
            "Epoch: 17/30, step: 181/364, loss: 0.38038, accuracy: 0.83486\n",
            "Epoch: 17/30, step: 182/364, loss: 0.38064, accuracy: 0.83474\n",
            "Epoch: 17/30, step: 183/364, loss: 0.38071, accuracy: 0.83436\n",
            "Epoch: 17/30, step: 184/364, loss: 0.38057, accuracy: 0.83424\n",
            "Epoch: 17/30, step: 185/364, loss: 0.38029, accuracy: 0.83446\n",
            "Epoch: 17/30, step: 186/364, loss: 0.38031, accuracy: 0.83451\n",
            "Epoch: 17/30, step: 187/364, loss: 0.38040, accuracy: 0.83448\n",
            "Epoch: 17/30, step: 188/364, loss: 0.38008, accuracy: 0.83477\n",
            "Epoch: 17/30, step: 189/364, loss: 0.38040, accuracy: 0.83457\n",
            "Epoch: 17/30, step: 190/364, loss: 0.38062, accuracy: 0.83462\n",
            "Epoch: 17/30, step: 191/364, loss: 0.38024, accuracy: 0.83508\n",
            "Epoch: 17/30, step: 192/364, loss: 0.37985, accuracy: 0.83545\n",
            "Epoch: 17/30, step: 193/364, loss: 0.37976, accuracy: 0.83541\n",
            "Epoch: 17/30, step: 194/364, loss: 0.38023, accuracy: 0.83489\n",
            "Epoch: 17/30, step: 195/364, loss: 0.38038, accuracy: 0.83478\n",
            "Epoch: 17/30, step: 196/364, loss: 0.38012, accuracy: 0.83498\n",
            "Epoch: 17/30, step: 197/364, loss: 0.37982, accuracy: 0.83510\n",
            "Epoch: 17/30, step: 198/364, loss: 0.37937, accuracy: 0.83523\n",
            "Epoch: 17/30, step: 199/364, loss: 0.37969, accuracy: 0.83480\n",
            "Epoch: 17/30, step: 200/364, loss: 0.37987, accuracy: 0.83461\n",
            "Epoch: 17/30, step: 201/364, loss: 0.37977, accuracy: 0.83458\n",
            "Epoch: 17/30, step: 202/364, loss: 0.37977, accuracy: 0.83439\n",
            "Epoch: 17/30, step: 203/364, loss: 0.38009, accuracy: 0.83397\n",
            "Epoch: 17/30, step: 204/364, loss: 0.38000, accuracy: 0.83410\n",
            "Epoch: 17/30, step: 205/364, loss: 0.38024, accuracy: 0.83422\n",
            "Epoch: 17/30, step: 206/364, loss: 0.38006, accuracy: 0.83450\n",
            "Epoch: 17/30, step: 207/364, loss: 0.37975, accuracy: 0.83454\n",
            "Epoch: 17/30, step: 208/364, loss: 0.38007, accuracy: 0.83421\n",
            "Epoch: 17/30, step: 209/364, loss: 0.37996, accuracy: 0.83403\n",
            "Epoch: 17/30, step: 210/364, loss: 0.38016, accuracy: 0.83378\n",
            "Epoch: 17/30, step: 211/364, loss: 0.37960, accuracy: 0.83442\n",
            "Epoch: 17/30, step: 212/364, loss: 0.37951, accuracy: 0.83446\n",
            "Epoch: 17/30, step: 213/364, loss: 0.37944, accuracy: 0.83443\n",
            "Epoch: 17/30, step: 214/364, loss: 0.37976, accuracy: 0.83455\n",
            "Epoch: 17/30, step: 215/364, loss: 0.38011, accuracy: 0.83430\n",
            "Epoch: 17/30, step: 216/364, loss: 0.37993, accuracy: 0.83427\n",
            "Epoch: 17/30, step: 217/364, loss: 0.37983, accuracy: 0.83446\n",
            "Epoch: 17/30, step: 218/364, loss: 0.37961, accuracy: 0.83486\n",
            "Epoch: 17/30, step: 219/364, loss: 0.37946, accuracy: 0.83497\n",
            "Epoch: 17/30, step: 220/364, loss: 0.37936, accuracy: 0.83501\n",
            "Epoch: 17/30, step: 221/364, loss: 0.37909, accuracy: 0.83505\n",
            "Epoch: 17/30, step: 222/364, loss: 0.37893, accuracy: 0.83509\n",
            "Epoch: 17/30, step: 223/364, loss: 0.37890, accuracy: 0.83499\n",
            "Epoch: 17/30, step: 224/364, loss: 0.37877, accuracy: 0.83517\n",
            "Epoch: 17/30, step: 225/364, loss: 0.37871, accuracy: 0.83542\n",
            "Epoch: 17/30, step: 226/364, loss: 0.37847, accuracy: 0.83566\n",
            "Epoch: 17/30, step: 227/364, loss: 0.37852, accuracy: 0.83577\n",
            "Epoch: 17/30, step: 228/364, loss: 0.37887, accuracy: 0.83539\n",
            "Epoch: 17/30, step: 229/364, loss: 0.37876, accuracy: 0.83556\n",
            "Epoch: 17/30, step: 230/364, loss: 0.37881, accuracy: 0.83539\n",
            "Epoch: 17/30, step: 231/364, loss: 0.37874, accuracy: 0.83543\n",
            "Epoch: 17/30, step: 232/364, loss: 0.37919, accuracy: 0.83526\n",
            "Epoch: 17/30, step: 233/364, loss: 0.37893, accuracy: 0.83537\n",
            "Epoch: 17/30, step: 234/364, loss: 0.37912, accuracy: 0.83514\n",
            "Epoch: 17/30, step: 235/364, loss: 0.37911, accuracy: 0.83504\n",
            "Epoch: 17/30, step: 236/364, loss: 0.37883, accuracy: 0.83534\n",
            "Epoch: 17/30, step: 237/364, loss: 0.37909, accuracy: 0.83525\n",
            "Epoch: 17/30, step: 238/364, loss: 0.37922, accuracy: 0.83508\n",
            "Epoch: 17/30, step: 239/364, loss: 0.37894, accuracy: 0.83532\n",
            "Epoch: 17/30, step: 240/364, loss: 0.37894, accuracy: 0.83516\n",
            "Epoch: 17/30, step: 241/364, loss: 0.37871, accuracy: 0.83532\n",
            "Epoch: 17/30, step: 242/364, loss: 0.37849, accuracy: 0.83549\n",
            "Epoch: 17/30, step: 243/364, loss: 0.37838, accuracy: 0.83539\n",
            "Epoch: 17/30, step: 244/364, loss: 0.37858, accuracy: 0.83530\n",
            "Epoch: 17/30, step: 245/364, loss: 0.37830, accuracy: 0.83546\n",
            "Epoch: 17/30, step: 246/364, loss: 0.37792, accuracy: 0.83575\n",
            "Epoch: 17/30, step: 247/364, loss: 0.37780, accuracy: 0.83597\n",
            "Epoch: 17/30, step: 248/364, loss: 0.37815, accuracy: 0.83581\n",
            "Epoch: 17/30, step: 249/364, loss: 0.37803, accuracy: 0.83584\n",
            "Epoch: 17/30, step: 250/364, loss: 0.37799, accuracy: 0.83569\n",
            "Epoch: 17/30, step: 251/364, loss: 0.37771, accuracy: 0.83584\n",
            "Epoch: 17/30, step: 252/364, loss: 0.37756, accuracy: 0.83606\n",
            "Epoch: 17/30, step: 253/364, loss: 0.37772, accuracy: 0.83591\n",
            "Epoch: 17/30, step: 254/364, loss: 0.37760, accuracy: 0.83600\n",
            "Epoch: 17/30, step: 255/364, loss: 0.37738, accuracy: 0.83621\n",
            "Epoch: 17/30, step: 256/364, loss: 0.37742, accuracy: 0.83594\n",
            "Epoch: 17/30, step: 257/364, loss: 0.37708, accuracy: 0.83615\n",
            "Epoch: 17/30, step: 258/364, loss: 0.37715, accuracy: 0.83588\n",
            "Epoch: 17/30, step: 259/364, loss: 0.37691, accuracy: 0.83609\n",
            "Epoch: 17/30, step: 260/364, loss: 0.37725, accuracy: 0.83588\n",
            "Epoch: 17/30, step: 261/364, loss: 0.37708, accuracy: 0.83591\n",
            "Epoch: 17/30, step: 262/364, loss: 0.37715, accuracy: 0.83564\n",
            "Epoch: 17/30, step: 263/364, loss: 0.37715, accuracy: 0.83549\n",
            "Epoch: 17/30, step: 264/364, loss: 0.37694, accuracy: 0.83570\n",
            "Epoch: 17/30, step: 265/364, loss: 0.37715, accuracy: 0.83526\n",
            "Epoch: 17/30, step: 266/364, loss: 0.37693, accuracy: 0.83547\n",
            "Epoch: 17/30, step: 267/364, loss: 0.37751, accuracy: 0.83515\n",
            "Epoch: 17/30, step: 268/364, loss: 0.37718, accuracy: 0.83541\n",
            "Epoch: 17/30, step: 269/364, loss: 0.37689, accuracy: 0.83556\n",
            "Epoch: 17/30, step: 270/364, loss: 0.37690, accuracy: 0.83553\n",
            "Epoch: 17/30, step: 271/364, loss: 0.37685, accuracy: 0.83568\n",
            "Epoch: 17/30, step: 272/364, loss: 0.37650, accuracy: 0.83594\n",
            "Epoch: 17/30, step: 273/364, loss: 0.37634, accuracy: 0.83591\n",
            "Epoch: 17/30, step: 274/364, loss: 0.37646, accuracy: 0.83594\n",
            "Epoch: 17/30, step: 275/364, loss: 0.37652, accuracy: 0.83585\n",
            "Epoch: 17/30, step: 276/364, loss: 0.37655, accuracy: 0.83582\n",
            "Epoch: 17/30, step: 277/364, loss: 0.37651, accuracy: 0.83574\n",
            "Epoch: 17/30, step: 278/364, loss: 0.37655, accuracy: 0.83583\n",
            "Epoch: 17/30, step: 279/364, loss: 0.37642, accuracy: 0.83602\n",
            "Epoch: 17/30, step: 280/364, loss: 0.37628, accuracy: 0.83599\n",
            "Epoch: 17/30, step: 281/364, loss: 0.37627, accuracy: 0.83613\n",
            "Epoch: 17/30, step: 282/364, loss: 0.37634, accuracy: 0.83627\n",
            "Epoch: 17/30, step: 283/364, loss: 0.37630, accuracy: 0.83613\n",
            "Epoch: 17/30, step: 284/364, loss: 0.37634, accuracy: 0.83621\n",
            "Epoch: 17/30, step: 285/364, loss: 0.37619, accuracy: 0.83640\n",
            "Epoch: 17/30, step: 286/364, loss: 0.37584, accuracy: 0.83670\n",
            "Epoch: 17/30, step: 287/364, loss: 0.37570, accuracy: 0.83684\n",
            "Epoch: 17/30, step: 288/364, loss: 0.37575, accuracy: 0.83691\n",
            "Epoch: 17/30, step: 289/364, loss: 0.37586, accuracy: 0.83688\n",
            "Epoch: 17/30, step: 290/364, loss: 0.37613, accuracy: 0.83675\n",
            "Epoch: 17/30, step: 291/364, loss: 0.37604, accuracy: 0.83675\n",
            "Epoch: 17/30, train loss: 0.37604, train accuracy: 0.83675, valid loss: 0.62448, valid accuracy: 0.68766\n",
            "Epoch: 18/30, step: 1/364, loss: 0.41143, accuracy: 0.75000\n",
            "Epoch: 18/30, step: 2/364, loss: 0.39042, accuracy: 0.80469\n",
            "Epoch: 18/30, step: 3/364, loss: 0.40042, accuracy: 0.81250\n",
            "Epoch: 18/30, step: 4/364, loss: 0.38108, accuracy: 0.83984\n",
            "Epoch: 18/30, step: 5/364, loss: 0.36704, accuracy: 0.85938\n",
            "Epoch: 18/30, step: 6/364, loss: 0.36186, accuracy: 0.85938\n",
            "Epoch: 18/30, step: 7/364, loss: 0.36007, accuracy: 0.85938\n",
            "Epoch: 18/30, step: 8/364, loss: 0.36882, accuracy: 0.84961\n",
            "Epoch: 18/30, step: 9/364, loss: 0.37074, accuracy: 0.84896\n",
            "Epoch: 18/30, step: 10/364, loss: 0.37040, accuracy: 0.84688\n",
            "Epoch: 18/30, step: 11/364, loss: 0.37272, accuracy: 0.84943\n",
            "Epoch: 18/30, step: 12/364, loss: 0.37051, accuracy: 0.85417\n",
            "Epoch: 18/30, step: 13/364, loss: 0.37292, accuracy: 0.85096\n",
            "Epoch: 18/30, step: 14/364, loss: 0.36724, accuracy: 0.85603\n",
            "Epoch: 18/30, step: 15/364, loss: 0.36469, accuracy: 0.85625\n",
            "Epoch: 18/30, step: 16/364, loss: 0.36825, accuracy: 0.85352\n",
            "Epoch: 18/30, step: 17/364, loss: 0.36715, accuracy: 0.85478\n",
            "Epoch: 18/30, step: 18/364, loss: 0.37321, accuracy: 0.84809\n",
            "Epoch: 18/30, step: 19/364, loss: 0.36813, accuracy: 0.85197\n",
            "Epoch: 18/30, step: 20/364, loss: 0.36988, accuracy: 0.85234\n",
            "Epoch: 18/30, step: 21/364, loss: 0.37258, accuracy: 0.84821\n",
            "Epoch: 18/30, step: 22/364, loss: 0.37349, accuracy: 0.84730\n",
            "Epoch: 18/30, step: 23/364, loss: 0.37418, accuracy: 0.84715\n",
            "Epoch: 18/30, step: 24/364, loss: 0.37524, accuracy: 0.84831\n",
            "Epoch: 18/30, step: 25/364, loss: 0.37026, accuracy: 0.85062\n",
            "Epoch: 18/30, step: 26/364, loss: 0.36914, accuracy: 0.85156\n",
            "Epoch: 18/30, step: 27/364, loss: 0.36843, accuracy: 0.85359\n",
            "Epoch: 18/30, step: 28/364, loss: 0.36650, accuracy: 0.85547\n",
            "Epoch: 18/30, step: 29/364, loss: 0.36629, accuracy: 0.85399\n",
            "Epoch: 18/30, step: 30/364, loss: 0.36480, accuracy: 0.85417\n",
            "Epoch: 18/30, step: 31/364, loss: 0.36471, accuracy: 0.85433\n",
            "Epoch: 18/30, step: 32/364, loss: 0.36371, accuracy: 0.85498\n",
            "Epoch: 18/30, step: 33/364, loss: 0.36230, accuracy: 0.85653\n",
            "Epoch: 18/30, step: 34/364, loss: 0.36073, accuracy: 0.85754\n",
            "Epoch: 18/30, step: 35/364, loss: 0.36090, accuracy: 0.85714\n",
            "Epoch: 18/30, step: 36/364, loss: 0.36109, accuracy: 0.85634\n",
            "Epoch: 18/30, step: 37/364, loss: 0.36188, accuracy: 0.85557\n",
            "Epoch: 18/30, step: 38/364, loss: 0.35856, accuracy: 0.85773\n",
            "Epoch: 18/30, step: 39/364, loss: 0.35703, accuracy: 0.85857\n",
            "Epoch: 18/30, step: 40/364, loss: 0.35517, accuracy: 0.85977\n",
            "Epoch: 18/30, step: 41/364, loss: 0.35476, accuracy: 0.85861\n",
            "Epoch: 18/30, step: 42/364, loss: 0.35592, accuracy: 0.85789\n",
            "Epoch: 18/30, step: 43/364, loss: 0.35663, accuracy: 0.85756\n",
            "Epoch: 18/30, step: 44/364, loss: 0.35398, accuracy: 0.85866\n",
            "Epoch: 18/30, step: 45/364, loss: 0.35413, accuracy: 0.85694\n",
            "Epoch: 18/30, step: 46/364, loss: 0.35430, accuracy: 0.85666\n",
            "Epoch: 18/30, step: 47/364, loss: 0.35507, accuracy: 0.85638\n",
            "Epoch: 18/30, step: 48/364, loss: 0.35377, accuracy: 0.85742\n",
            "Epoch: 18/30, step: 49/364, loss: 0.35544, accuracy: 0.85619\n",
            "Epoch: 18/30, step: 50/364, loss: 0.35529, accuracy: 0.85656\n",
            "Epoch: 18/30, step: 51/364, loss: 0.35307, accuracy: 0.85815\n",
            "Epoch: 18/30, step: 52/364, loss: 0.35333, accuracy: 0.85787\n",
            "Epoch: 18/30, step: 53/364, loss: 0.35583, accuracy: 0.85554\n",
            "Epoch: 18/30, step: 54/364, loss: 0.35543, accuracy: 0.85619\n",
            "Epoch: 18/30, step: 55/364, loss: 0.35361, accuracy: 0.85710\n",
            "Epoch: 18/30, step: 56/364, loss: 0.35321, accuracy: 0.85742\n",
            "Epoch: 18/30, step: 57/364, loss: 0.35196, accuracy: 0.85883\n",
            "Epoch: 18/30, step: 58/364, loss: 0.35220, accuracy: 0.85884\n",
            "Epoch: 18/30, step: 59/364, loss: 0.35189, accuracy: 0.85885\n",
            "Epoch: 18/30, step: 60/364, loss: 0.35468, accuracy: 0.85677\n",
            "Epoch: 18/30, step: 61/364, loss: 0.35533, accuracy: 0.85656\n",
            "Epoch: 18/30, step: 62/364, loss: 0.35532, accuracy: 0.85711\n",
            "Epoch: 18/30, step: 63/364, loss: 0.35543, accuracy: 0.85689\n",
            "Epoch: 18/30, step: 64/364, loss: 0.35580, accuracy: 0.85669\n",
            "Epoch: 18/30, step: 65/364, loss: 0.35472, accuracy: 0.85769\n",
            "Epoch: 18/30, step: 66/364, loss: 0.35471, accuracy: 0.85772\n",
            "Epoch: 18/30, step: 67/364, loss: 0.35464, accuracy: 0.85821\n",
            "Epoch: 18/30, step: 68/364, loss: 0.35456, accuracy: 0.85823\n",
            "Epoch: 18/30, step: 69/364, loss: 0.35400, accuracy: 0.85847\n",
            "Epoch: 18/30, step: 70/364, loss: 0.35382, accuracy: 0.85915\n",
            "Epoch: 18/30, step: 71/364, loss: 0.35424, accuracy: 0.85893\n",
            "Epoch: 18/30, step: 72/364, loss: 0.35453, accuracy: 0.85829\n",
            "Epoch: 18/30, step: 73/364, loss: 0.35376, accuracy: 0.85830\n",
            "Epoch: 18/30, step: 74/364, loss: 0.35451, accuracy: 0.85747\n",
            "Epoch: 18/30, step: 75/364, loss: 0.35450, accuracy: 0.85792\n",
            "Epoch: 18/30, step: 76/364, loss: 0.35483, accuracy: 0.85773\n",
            "Epoch: 18/30, step: 77/364, loss: 0.35455, accuracy: 0.85775\n",
            "Epoch: 18/30, step: 78/364, loss: 0.35486, accuracy: 0.85757\n",
            "Epoch: 18/30, step: 79/364, loss: 0.35451, accuracy: 0.85819\n",
            "Epoch: 18/30, step: 80/364, loss: 0.35390, accuracy: 0.85840\n",
            "Epoch: 18/30, step: 81/364, loss: 0.35401, accuracy: 0.85822\n",
            "Epoch: 18/30, step: 82/364, loss: 0.35472, accuracy: 0.85747\n",
            "Epoch: 18/30, step: 83/364, loss: 0.35610, accuracy: 0.85693\n",
            "Epoch: 18/30, step: 84/364, loss: 0.35712, accuracy: 0.85640\n",
            "Epoch: 18/30, step: 85/364, loss: 0.35698, accuracy: 0.85680\n",
            "Epoch: 18/30, step: 86/364, loss: 0.35768, accuracy: 0.85629\n",
            "Epoch: 18/30, step: 87/364, loss: 0.35726, accuracy: 0.85650\n",
            "Epoch: 18/30, step: 88/364, loss: 0.35792, accuracy: 0.85565\n",
            "Epoch: 18/30, step: 89/364, loss: 0.35822, accuracy: 0.85499\n",
            "Epoch: 18/30, step: 90/364, loss: 0.35767, accuracy: 0.85521\n",
            "Epoch: 18/30, step: 91/364, loss: 0.35710, accuracy: 0.85525\n",
            "Epoch: 18/30, step: 92/364, loss: 0.35776, accuracy: 0.85411\n",
            "Epoch: 18/30, step: 93/364, loss: 0.35751, accuracy: 0.85400\n",
            "Epoch: 18/30, step: 94/364, loss: 0.35702, accuracy: 0.85406\n",
            "Epoch: 18/30, step: 95/364, loss: 0.35689, accuracy: 0.85411\n",
            "Epoch: 18/30, step: 96/364, loss: 0.35559, accuracy: 0.85449\n",
            "Epoch: 18/30, step: 97/364, loss: 0.35499, accuracy: 0.85438\n",
            "Epoch: 18/30, step: 98/364, loss: 0.35478, accuracy: 0.85459\n",
            "Epoch: 18/30, step: 99/364, loss: 0.35490, accuracy: 0.85432\n",
            "Epoch: 18/30, step: 100/364, loss: 0.35485, accuracy: 0.85438\n",
            "Epoch: 18/30, step: 101/364, loss: 0.35449, accuracy: 0.85504\n",
            "Epoch: 18/30, step: 102/364, loss: 0.35376, accuracy: 0.85509\n",
            "Epoch: 18/30, step: 103/364, loss: 0.35319, accuracy: 0.85558\n",
            "Epoch: 18/30, step: 104/364, loss: 0.35413, accuracy: 0.85517\n",
            "Epoch: 18/30, step: 105/364, loss: 0.35442, accuracy: 0.85446\n",
            "Epoch: 18/30, step: 106/364, loss: 0.35440, accuracy: 0.85436\n",
            "Epoch: 18/30, step: 107/364, loss: 0.35586, accuracy: 0.85324\n",
            "Epoch: 18/30, step: 108/364, loss: 0.35589, accuracy: 0.85330\n",
            "Epoch: 18/30, step: 109/364, loss: 0.35548, accuracy: 0.85321\n",
            "Epoch: 18/30, step: 110/364, loss: 0.35516, accuracy: 0.85312\n",
            "Epoch: 18/30, step: 111/364, loss: 0.35542, accuracy: 0.85276\n",
            "Epoch: 18/30, step: 112/364, loss: 0.35659, accuracy: 0.85240\n",
            "Epoch: 18/30, step: 113/364, loss: 0.35639, accuracy: 0.85260\n",
            "Epoch: 18/30, step: 114/364, loss: 0.35579, accuracy: 0.85266\n",
            "Epoch: 18/30, step: 115/364, loss: 0.35626, accuracy: 0.85190\n",
            "Epoch: 18/30, step: 116/364, loss: 0.35548, accuracy: 0.85264\n",
            "Epoch: 18/30, step: 117/364, loss: 0.35496, accuracy: 0.85323\n",
            "Epoch: 18/30, step: 118/364, loss: 0.35540, accuracy: 0.85275\n",
            "Epoch: 18/30, step: 119/364, loss: 0.35531, accuracy: 0.85307\n",
            "Epoch: 18/30, step: 120/364, loss: 0.35545, accuracy: 0.85260\n",
            "Epoch: 18/30, step: 121/364, loss: 0.35479, accuracy: 0.85292\n",
            "Epoch: 18/30, step: 122/364, loss: 0.35520, accuracy: 0.85246\n",
            "Epoch: 18/30, step: 123/364, loss: 0.35567, accuracy: 0.85239\n",
            "Epoch: 18/30, step: 124/364, loss: 0.35505, accuracy: 0.85295\n",
            "Epoch: 18/30, step: 125/364, loss: 0.35484, accuracy: 0.85312\n",
            "Epoch: 18/30, step: 126/364, loss: 0.35498, accuracy: 0.85293\n",
            "Epoch: 18/30, step: 127/364, loss: 0.35486, accuracy: 0.85335\n",
            "Epoch: 18/30, step: 128/364, loss: 0.35488, accuracy: 0.85364\n",
            "Epoch: 18/30, step: 129/364, loss: 0.35585, accuracy: 0.85296\n",
            "Epoch: 18/30, step: 130/364, loss: 0.35626, accuracy: 0.85264\n",
            "Epoch: 18/30, step: 131/364, loss: 0.35693, accuracy: 0.85222\n",
            "Epoch: 18/30, step: 132/364, loss: 0.35695, accuracy: 0.85180\n",
            "Epoch: 18/30, step: 133/364, loss: 0.35699, accuracy: 0.85150\n",
            "Epoch: 18/30, step: 134/364, loss: 0.35756, accuracy: 0.85098\n",
            "Epoch: 18/30, step: 135/364, loss: 0.35730, accuracy: 0.85127\n",
            "Epoch: 18/30, step: 136/364, loss: 0.35767, accuracy: 0.85099\n",
            "Epoch: 18/30, step: 137/364, loss: 0.35806, accuracy: 0.85048\n",
            "Epoch: 18/30, step: 138/364, loss: 0.35851, accuracy: 0.85020\n",
            "Epoch: 18/30, step: 139/364, loss: 0.35826, accuracy: 0.85061\n",
            "Epoch: 18/30, step: 140/364, loss: 0.35840, accuracy: 0.85078\n",
            "Epoch: 18/30, step: 141/364, loss: 0.35863, accuracy: 0.85029\n",
            "Epoch: 18/30, step: 142/364, loss: 0.35875, accuracy: 0.85046\n",
            "Epoch: 18/30, step: 143/364, loss: 0.35877, accuracy: 0.85031\n",
            "Epoch: 18/30, step: 144/364, loss: 0.35820, accuracy: 0.85080\n",
            "Epoch: 18/30, step: 145/364, loss: 0.35795, accuracy: 0.85097\n",
            "Epoch: 18/30, step: 146/364, loss: 0.35832, accuracy: 0.85071\n",
            "Epoch: 18/30, step: 147/364, loss: 0.35777, accuracy: 0.85140\n",
            "Epoch: 18/30, step: 148/364, loss: 0.35780, accuracy: 0.85114\n",
            "Epoch: 18/30, step: 149/364, loss: 0.35768, accuracy: 0.85109\n",
            "Epoch: 18/30, step: 150/364, loss: 0.35769, accuracy: 0.85115\n",
            "Epoch: 18/30, step: 151/364, loss: 0.35812, accuracy: 0.85079\n",
            "Epoch: 18/30, step: 152/364, loss: 0.35810, accuracy: 0.85084\n",
            "Epoch: 18/30, step: 153/364, loss: 0.35794, accuracy: 0.85100\n",
            "Epoch: 18/30, step: 154/364, loss: 0.35818, accuracy: 0.85055\n",
            "Epoch: 18/30, step: 155/364, loss: 0.35810, accuracy: 0.85071\n",
            "Epoch: 18/30, step: 156/364, loss: 0.35805, accuracy: 0.85076\n",
            "Epoch: 18/30, step: 157/364, loss: 0.35729, accuracy: 0.85151\n",
            "Epoch: 18/30, step: 158/364, loss: 0.35757, accuracy: 0.85097\n",
            "Epoch: 18/30, step: 159/364, loss: 0.35764, accuracy: 0.85112\n",
            "Epoch: 18/30, step: 160/364, loss: 0.35744, accuracy: 0.85156\n",
            "Epoch: 18/30, step: 161/364, loss: 0.35749, accuracy: 0.85151\n",
            "Epoch: 18/30, step: 162/364, loss: 0.35774, accuracy: 0.85127\n",
            "Epoch: 18/30, step: 163/364, loss: 0.35774, accuracy: 0.85132\n",
            "Epoch: 18/30, step: 164/364, loss: 0.35770, accuracy: 0.85128\n",
            "Epoch: 18/30, step: 165/364, loss: 0.35764, accuracy: 0.85123\n",
            "Epoch: 18/30, step: 166/364, loss: 0.35776, accuracy: 0.85109\n",
            "Epoch: 18/30, step: 167/364, loss: 0.35814, accuracy: 0.85067\n",
            "Epoch: 18/30, step: 168/364, loss: 0.35818, accuracy: 0.85045\n",
            "Epoch: 18/30, step: 169/364, loss: 0.35794, accuracy: 0.85068\n",
            "Epoch: 18/30, step: 170/364, loss: 0.35869, accuracy: 0.85000\n",
            "Epoch: 18/30, step: 171/364, loss: 0.35873, accuracy: 0.84996\n",
            "Epoch: 18/30, step: 172/364, loss: 0.35869, accuracy: 0.85011\n",
            "Epoch: 18/30, step: 173/364, loss: 0.35869, accuracy: 0.84980\n",
            "Epoch: 18/30, step: 174/364, loss: 0.35843, accuracy: 0.84986\n",
            "Epoch: 18/30, step: 175/364, loss: 0.35831, accuracy: 0.84973\n",
            "Epoch: 18/30, step: 176/364, loss: 0.35807, accuracy: 0.85005\n",
            "Epoch: 18/30, step: 177/364, loss: 0.35816, accuracy: 0.84993\n",
            "Epoch: 18/30, step: 178/364, loss: 0.35841, accuracy: 0.84981\n",
            "Epoch: 18/30, step: 179/364, loss: 0.35798, accuracy: 0.85021\n",
            "Epoch: 18/30, step: 180/364, loss: 0.35784, accuracy: 0.85052\n",
            "Epoch: 18/30, step: 181/364, loss: 0.35770, accuracy: 0.85066\n",
            "Epoch: 18/30, step: 182/364, loss: 0.35738, accuracy: 0.85096\n",
            "Epoch: 18/30, step: 183/364, loss: 0.35677, accuracy: 0.85135\n",
            "Epoch: 18/30, step: 184/364, loss: 0.35664, accuracy: 0.85165\n",
            "Epoch: 18/30, step: 185/364, loss: 0.35652, accuracy: 0.85177\n",
            "Epoch: 18/30, step: 186/364, loss: 0.35723, accuracy: 0.85123\n",
            "Epoch: 18/30, step: 187/364, loss: 0.35728, accuracy: 0.85127\n",
            "Epoch: 18/30, step: 188/364, loss: 0.35726, accuracy: 0.85156\n",
            "Epoch: 18/30, step: 189/364, loss: 0.35693, accuracy: 0.85177\n",
            "Epoch: 18/30, step: 190/364, loss: 0.35698, accuracy: 0.85164\n",
            "Epoch: 18/30, step: 191/364, loss: 0.35689, accuracy: 0.85160\n",
            "Epoch: 18/30, step: 192/364, loss: 0.35694, accuracy: 0.85164\n",
            "Epoch: 18/30, step: 193/364, loss: 0.35666, accuracy: 0.85193\n",
            "Epoch: 18/30, step: 194/364, loss: 0.35704, accuracy: 0.85156\n",
            "Epoch: 18/30, step: 195/364, loss: 0.35721, accuracy: 0.85136\n",
            "Epoch: 18/30, step: 196/364, loss: 0.35702, accuracy: 0.85140\n",
            "Epoch: 18/30, step: 197/364, loss: 0.35714, accuracy: 0.85113\n",
            "Epoch: 18/30, step: 198/364, loss: 0.35714, accuracy: 0.85125\n",
            "Epoch: 18/30, step: 199/364, loss: 0.35714, accuracy: 0.85121\n",
            "Epoch: 18/30, step: 200/364, loss: 0.35737, accuracy: 0.85117\n",
            "Epoch: 18/30, step: 201/364, loss: 0.35752, accuracy: 0.85098\n",
            "Epoch: 18/30, step: 202/364, loss: 0.35741, accuracy: 0.85118\n",
            "Epoch: 18/30, step: 203/364, loss: 0.35761, accuracy: 0.85083\n",
            "Epoch: 18/30, step: 204/364, loss: 0.35735, accuracy: 0.85110\n",
            "Epoch: 18/30, step: 205/364, loss: 0.35704, accuracy: 0.85130\n",
            "Epoch: 18/30, step: 206/364, loss: 0.35679, accuracy: 0.85164\n",
            "Epoch: 18/30, step: 207/364, loss: 0.35630, accuracy: 0.85205\n",
            "Epoch: 18/30, step: 208/364, loss: 0.35662, accuracy: 0.85179\n",
            "Epoch: 18/30, step: 209/364, loss: 0.35640, accuracy: 0.85182\n",
            "Epoch: 18/30, step: 210/364, loss: 0.35630, accuracy: 0.85171\n",
            "Epoch: 18/30, step: 211/364, loss: 0.35683, accuracy: 0.85116\n",
            "Epoch: 18/30, step: 212/364, loss: 0.35686, accuracy: 0.85127\n",
            "Epoch: 18/30, step: 213/364, loss: 0.35671, accuracy: 0.85145\n",
            "Epoch: 18/30, step: 214/364, loss: 0.35667, accuracy: 0.85156\n",
            "Epoch: 18/30, step: 215/364, loss: 0.35639, accuracy: 0.85167\n",
            "Epoch: 18/30, step: 216/364, loss: 0.35654, accuracy: 0.85149\n",
            "Epoch: 18/30, step: 217/364, loss: 0.35653, accuracy: 0.85153\n",
            "Epoch: 18/30, step: 218/364, loss: 0.35640, accuracy: 0.85156\n",
            "Epoch: 18/30, step: 219/364, loss: 0.35747, accuracy: 0.85081\n",
            "Epoch: 18/30, step: 220/364, loss: 0.35733, accuracy: 0.85107\n",
            "Epoch: 18/30, step: 221/364, loss: 0.35696, accuracy: 0.85153\n",
            "Epoch: 18/30, step: 222/364, loss: 0.35641, accuracy: 0.85198\n",
            "Epoch: 18/30, step: 223/364, loss: 0.35605, accuracy: 0.85230\n",
            "Epoch: 18/30, step: 224/364, loss: 0.35576, accuracy: 0.85247\n",
            "Epoch: 18/30, step: 225/364, loss: 0.35570, accuracy: 0.85264\n",
            "Epoch: 18/30, step: 226/364, loss: 0.35568, accuracy: 0.85260\n",
            "Epoch: 18/30, step: 227/364, loss: 0.35560, accuracy: 0.85249\n",
            "Epoch: 18/30, step: 228/364, loss: 0.35564, accuracy: 0.85245\n",
            "Epoch: 18/30, step: 229/364, loss: 0.35515, accuracy: 0.85289\n",
            "Epoch: 18/30, step: 230/364, loss: 0.35491, accuracy: 0.85312\n",
            "Epoch: 18/30, step: 231/364, loss: 0.35464, accuracy: 0.85335\n",
            "Epoch: 18/30, step: 232/364, loss: 0.35486, accuracy: 0.85318\n",
            "Epoch: 18/30, step: 233/364, loss: 0.35478, accuracy: 0.85307\n",
            "Epoch: 18/30, step: 234/364, loss: 0.35457, accuracy: 0.85330\n",
            "Epoch: 18/30, step: 235/364, loss: 0.35461, accuracy: 0.85332\n",
            "Epoch: 18/30, step: 236/364, loss: 0.35485, accuracy: 0.85315\n",
            "Epoch: 18/30, step: 237/364, loss: 0.35496, accuracy: 0.85311\n",
            "Epoch: 18/30, step: 238/364, loss: 0.35481, accuracy: 0.85307\n",
            "Epoch: 18/30, step: 239/364, loss: 0.35509, accuracy: 0.85310\n",
            "Epoch: 18/30, step: 240/364, loss: 0.35502, accuracy: 0.85299\n",
            "Epoch: 18/30, step: 241/364, loss: 0.35509, accuracy: 0.85289\n",
            "Epoch: 18/30, step: 242/364, loss: 0.35557, accuracy: 0.85253\n",
            "Epoch: 18/30, step: 243/364, loss: 0.35578, accuracy: 0.85230\n",
            "Epoch: 18/30, step: 244/364, loss: 0.35622, accuracy: 0.85220\n",
            "Epoch: 18/30, step: 245/364, loss: 0.35637, accuracy: 0.85191\n",
            "Epoch: 18/30, step: 246/364, loss: 0.35627, accuracy: 0.85207\n",
            "Epoch: 18/30, step: 247/364, loss: 0.35617, accuracy: 0.85210\n",
            "Epoch: 18/30, step: 248/364, loss: 0.35590, accuracy: 0.85238\n",
            "Epoch: 18/30, step: 249/364, loss: 0.35559, accuracy: 0.85266\n",
            "Epoch: 18/30, step: 250/364, loss: 0.35545, accuracy: 0.85263\n",
            "Epoch: 18/30, step: 251/364, loss: 0.35553, accuracy: 0.85265\n",
            "Epoch: 18/30, step: 252/364, loss: 0.35556, accuracy: 0.85286\n",
            "Epoch: 18/30, step: 253/364, loss: 0.35549, accuracy: 0.85283\n",
            "Epoch: 18/30, step: 254/364, loss: 0.35549, accuracy: 0.85304\n",
            "Epoch: 18/30, step: 255/364, loss: 0.35540, accuracy: 0.85312\n",
            "Epoch: 18/30, step: 256/364, loss: 0.35523, accuracy: 0.85315\n",
            "Epoch: 18/30, step: 257/364, loss: 0.35515, accuracy: 0.85293\n",
            "Epoch: 18/30, step: 258/364, loss: 0.35503, accuracy: 0.85302\n",
            "Epoch: 18/30, step: 259/364, loss: 0.35470, accuracy: 0.85334\n",
            "Epoch: 18/30, step: 260/364, loss: 0.35471, accuracy: 0.85337\n",
            "Epoch: 18/30, step: 261/364, loss: 0.35486, accuracy: 0.85327\n",
            "Epoch: 18/30, step: 262/364, loss: 0.35465, accuracy: 0.85335\n",
            "Epoch: 18/30, step: 263/364, loss: 0.35512, accuracy: 0.85302\n",
            "Epoch: 18/30, step: 264/364, loss: 0.35499, accuracy: 0.85310\n",
            "Epoch: 18/30, step: 265/364, loss: 0.35506, accuracy: 0.85301\n",
            "Epoch: 18/30, step: 266/364, loss: 0.35482, accuracy: 0.85309\n",
            "Epoch: 18/30, step: 267/364, loss: 0.35501, accuracy: 0.85294\n",
            "Epoch: 18/30, step: 268/364, loss: 0.35530, accuracy: 0.85285\n",
            "Epoch: 18/30, step: 269/364, loss: 0.35508, accuracy: 0.85299\n",
            "Epoch: 18/30, step: 270/364, loss: 0.35482, accuracy: 0.85318\n",
            "Epoch: 18/30, step: 271/364, loss: 0.35513, accuracy: 0.85303\n",
            "Epoch: 18/30, step: 272/364, loss: 0.35515, accuracy: 0.85277\n",
            "Epoch: 18/30, step: 273/364, loss: 0.35576, accuracy: 0.85234\n",
            "Epoch: 18/30, step: 274/364, loss: 0.35572, accuracy: 0.85230\n",
            "Epoch: 18/30, step: 275/364, loss: 0.35554, accuracy: 0.85267\n",
            "Epoch: 18/30, step: 276/364, loss: 0.35569, accuracy: 0.85258\n",
            "Epoch: 18/30, step: 277/364, loss: 0.35547, accuracy: 0.85266\n",
            "Epoch: 18/30, step: 278/364, loss: 0.35562, accuracy: 0.85257\n",
            "Epoch: 18/30, step: 279/364, loss: 0.35569, accuracy: 0.85237\n",
            "Epoch: 18/30, step: 280/364, loss: 0.35602, accuracy: 0.85206\n",
            "Epoch: 18/30, step: 281/364, loss: 0.35603, accuracy: 0.85215\n",
            "Epoch: 18/30, step: 282/364, loss: 0.35605, accuracy: 0.85195\n",
            "Epoch: 18/30, step: 283/364, loss: 0.35582, accuracy: 0.85220\n",
            "Epoch: 18/30, step: 284/364, loss: 0.35560, accuracy: 0.85250\n",
            "Epoch: 18/30, step: 285/364, loss: 0.35593, accuracy: 0.85219\n",
            "Epoch: 18/30, step: 286/364, loss: 0.35599, accuracy: 0.85200\n",
            "Epoch: 18/30, step: 287/364, loss: 0.35598, accuracy: 0.85208\n",
            "Epoch: 18/30, step: 288/364, loss: 0.35564, accuracy: 0.85238\n",
            "Epoch: 18/30, step: 289/364, loss: 0.35550, accuracy: 0.85245\n",
            "Epoch: 18/30, step: 290/364, loss: 0.35565, accuracy: 0.85242\n",
            "Epoch: 18/30, step: 291/364, loss: 0.35583, accuracy: 0.85244\n",
            "Epoch: 18/30, train loss: 0.35583, train accuracy: 0.85244, valid loss: 0.65070, valid accuracy: 0.67992\n",
            "Epoch: 19/30, step: 1/364, loss: 0.41790, accuracy: 0.81250\n",
            "Epoch: 19/30, step: 2/364, loss: 0.39683, accuracy: 0.81250\n",
            "Epoch: 19/30, step: 3/364, loss: 0.36862, accuracy: 0.81771\n",
            "Epoch: 19/30, step: 4/364, loss: 0.34520, accuracy: 0.83594\n",
            "Epoch: 19/30, step: 5/364, loss: 0.33721, accuracy: 0.84062\n",
            "Epoch: 19/30, step: 6/364, loss: 0.33147, accuracy: 0.85156\n",
            "Epoch: 19/30, step: 7/364, loss: 0.34047, accuracy: 0.84375\n",
            "Epoch: 19/30, step: 8/364, loss: 0.34658, accuracy: 0.84375\n",
            "Epoch: 19/30, step: 9/364, loss: 0.33926, accuracy: 0.84722\n",
            "Epoch: 19/30, step: 10/364, loss: 0.34237, accuracy: 0.84688\n",
            "Epoch: 19/30, step: 11/364, loss: 0.34247, accuracy: 0.85227\n",
            "Epoch: 19/30, step: 12/364, loss: 0.34970, accuracy: 0.84896\n",
            "Epoch: 19/30, step: 13/364, loss: 0.35332, accuracy: 0.84976\n",
            "Epoch: 19/30, step: 14/364, loss: 0.35442, accuracy: 0.84821\n",
            "Epoch: 19/30, step: 15/364, loss: 0.35607, accuracy: 0.84688\n",
            "Epoch: 19/30, step: 16/364, loss: 0.36165, accuracy: 0.84180\n",
            "Epoch: 19/30, step: 17/364, loss: 0.35634, accuracy: 0.84651\n",
            "Epoch: 19/30, step: 18/364, loss: 0.35315, accuracy: 0.84983\n",
            "Epoch: 19/30, step: 19/364, loss: 0.35016, accuracy: 0.85033\n",
            "Epoch: 19/30, step: 20/364, loss: 0.34841, accuracy: 0.84844\n",
            "Epoch: 19/30, step: 21/364, loss: 0.34493, accuracy: 0.85193\n",
            "Epoch: 19/30, step: 22/364, loss: 0.34492, accuracy: 0.85369\n",
            "Epoch: 19/30, step: 23/364, loss: 0.34102, accuracy: 0.85666\n",
            "Epoch: 19/30, step: 24/364, loss: 0.33837, accuracy: 0.85677\n",
            "Epoch: 19/30, step: 25/364, loss: 0.34310, accuracy: 0.85438\n",
            "Epoch: 19/30, step: 26/364, loss: 0.34348, accuracy: 0.85577\n",
            "Epoch: 19/30, step: 27/364, loss: 0.34209, accuracy: 0.85648\n",
            "Epoch: 19/30, step: 28/364, loss: 0.33932, accuracy: 0.85938\n",
            "Epoch: 19/30, step: 29/364, loss: 0.33968, accuracy: 0.85991\n",
            "Epoch: 19/30, step: 30/364, loss: 0.34213, accuracy: 0.85885\n",
            "Epoch: 19/30, step: 31/364, loss: 0.34233, accuracy: 0.85887\n",
            "Epoch: 19/30, step: 32/364, loss: 0.34259, accuracy: 0.85840\n",
            "Epoch: 19/30, step: 33/364, loss: 0.34419, accuracy: 0.85701\n",
            "Epoch: 19/30, step: 34/364, loss: 0.34265, accuracy: 0.85754\n",
            "Epoch: 19/30, step: 35/364, loss: 0.34346, accuracy: 0.85625\n",
            "Epoch: 19/30, step: 36/364, loss: 0.34313, accuracy: 0.85720\n",
            "Epoch: 19/30, step: 37/364, loss: 0.34060, accuracy: 0.86022\n",
            "Epoch: 19/30, step: 38/364, loss: 0.33830, accuracy: 0.86143\n",
            "Epoch: 19/30, step: 39/364, loss: 0.33922, accuracy: 0.86018\n",
            "Epoch: 19/30, step: 40/364, loss: 0.34061, accuracy: 0.86016\n",
            "Epoch: 19/30, step: 41/364, loss: 0.34133, accuracy: 0.85938\n",
            "Epoch: 19/30, step: 42/364, loss: 0.33935, accuracy: 0.86086\n",
            "Epoch: 19/30, step: 43/364, loss: 0.34008, accuracy: 0.86083\n",
            "Epoch: 19/30, step: 44/364, loss: 0.34081, accuracy: 0.85938\n",
            "Epoch: 19/30, step: 45/364, loss: 0.34102, accuracy: 0.85972\n",
            "Epoch: 19/30, step: 46/364, loss: 0.33981, accuracy: 0.85971\n",
            "Epoch: 19/30, step: 47/364, loss: 0.33782, accuracy: 0.86104\n",
            "Epoch: 19/30, step: 48/364, loss: 0.33913, accuracy: 0.86035\n",
            "Epoch: 19/30, step: 49/364, loss: 0.33886, accuracy: 0.86097\n",
            "Epoch: 19/30, step: 50/364, loss: 0.33949, accuracy: 0.86000\n",
            "Epoch: 19/30, step: 51/364, loss: 0.33803, accuracy: 0.86121\n",
            "Epoch: 19/30, step: 52/364, loss: 0.33751, accuracy: 0.86148\n",
            "Epoch: 19/30, step: 53/364, loss: 0.33685, accuracy: 0.86203\n",
            "Epoch: 19/30, step: 54/364, loss: 0.33645, accuracy: 0.86256\n",
            "Epoch: 19/30, step: 55/364, loss: 0.33588, accuracy: 0.86222\n",
            "Epoch: 19/30, step: 56/364, loss: 0.33583, accuracy: 0.86189\n",
            "Epoch: 19/30, step: 57/364, loss: 0.33641, accuracy: 0.86157\n",
            "Epoch: 19/30, step: 58/364, loss: 0.33445, accuracy: 0.86315\n",
            "Epoch: 19/30, step: 59/364, loss: 0.33299, accuracy: 0.86467\n",
            "Epoch: 19/30, step: 60/364, loss: 0.33418, accuracy: 0.86302\n",
            "Epoch: 19/30, step: 61/364, loss: 0.33398, accuracy: 0.86322\n",
            "Epoch: 19/30, step: 62/364, loss: 0.33347, accuracy: 0.86366\n",
            "Epoch: 19/30, step: 63/364, loss: 0.33419, accuracy: 0.86384\n",
            "Epoch: 19/30, step: 64/364, loss: 0.33346, accuracy: 0.86475\n",
            "Epoch: 19/30, step: 65/364, loss: 0.33456, accuracy: 0.86346\n",
            "Epoch: 19/30, step: 66/364, loss: 0.33457, accuracy: 0.86316\n",
            "Epoch: 19/30, step: 67/364, loss: 0.33509, accuracy: 0.86381\n",
            "Epoch: 19/30, step: 68/364, loss: 0.33417, accuracy: 0.86420\n",
            "Epoch: 19/30, step: 69/364, loss: 0.33292, accuracy: 0.86458\n",
            "Epoch: 19/30, step: 70/364, loss: 0.33252, accuracy: 0.86518\n",
            "Epoch: 19/30, step: 71/364, loss: 0.33268, accuracy: 0.86532\n",
            "Epoch: 19/30, step: 72/364, loss: 0.33204, accuracy: 0.86523\n",
            "Epoch: 19/30, step: 73/364, loss: 0.33291, accuracy: 0.86430\n",
            "Epoch: 19/30, step: 74/364, loss: 0.33307, accuracy: 0.86423\n",
            "Epoch: 19/30, step: 75/364, loss: 0.33282, accuracy: 0.86396\n",
            "Epoch: 19/30, step: 76/364, loss: 0.33178, accuracy: 0.86493\n",
            "Epoch: 19/30, step: 77/364, loss: 0.33120, accuracy: 0.86587\n",
            "Epoch: 19/30, step: 78/364, loss: 0.33108, accuracy: 0.86619\n",
            "Epoch: 19/30, step: 79/364, loss: 0.33237, accuracy: 0.86491\n",
            "Epoch: 19/30, step: 80/364, loss: 0.33140, accuracy: 0.86543\n",
            "Epoch: 19/30, step: 81/364, loss: 0.33047, accuracy: 0.86671\n",
            "Epoch: 19/30, step: 82/364, loss: 0.32970, accuracy: 0.86719\n",
            "Epoch: 19/30, step: 83/364, loss: 0.33005, accuracy: 0.86672\n",
            "Epoch: 19/30, step: 84/364, loss: 0.33039, accuracy: 0.86644\n",
            "Epoch: 19/30, step: 85/364, loss: 0.32996, accuracy: 0.86691\n",
            "Epoch: 19/30, step: 86/364, loss: 0.32951, accuracy: 0.86755\n",
            "Epoch: 19/30, step: 87/364, loss: 0.33015, accuracy: 0.86692\n",
            "Epoch: 19/30, step: 88/364, loss: 0.32968, accuracy: 0.86737\n",
            "Epoch: 19/30, step: 89/364, loss: 0.33107, accuracy: 0.86657\n",
            "Epoch: 19/30, step: 90/364, loss: 0.33040, accuracy: 0.86736\n",
            "Epoch: 19/30, step: 91/364, loss: 0.33164, accuracy: 0.86641\n",
            "Epoch: 19/30, step: 92/364, loss: 0.33177, accuracy: 0.86651\n",
            "Epoch: 19/30, step: 93/364, loss: 0.33308, accuracy: 0.86559\n",
            "Epoch: 19/30, step: 94/364, loss: 0.33243, accuracy: 0.86569\n",
            "Epoch: 19/30, step: 95/364, loss: 0.33276, accuracy: 0.86546\n",
            "Epoch: 19/30, step: 96/364, loss: 0.33304, accuracy: 0.86556\n",
            "Epoch: 19/30, step: 97/364, loss: 0.33238, accuracy: 0.86582\n",
            "Epoch: 19/30, step: 98/364, loss: 0.33202, accuracy: 0.86591\n",
            "Epoch: 19/30, step: 99/364, loss: 0.33169, accuracy: 0.86585\n",
            "Epoch: 19/30, step: 100/364, loss: 0.33087, accuracy: 0.86656\n",
            "Epoch: 19/30, step: 101/364, loss: 0.33058, accuracy: 0.86649\n",
            "Epoch: 19/30, step: 102/364, loss: 0.33065, accuracy: 0.86596\n",
            "Epoch: 19/30, step: 103/364, loss: 0.33063, accuracy: 0.86575\n",
            "Epoch: 19/30, step: 104/364, loss: 0.33022, accuracy: 0.86599\n",
            "Epoch: 19/30, step: 105/364, loss: 0.33063, accuracy: 0.86563\n",
            "Epoch: 19/30, step: 106/364, loss: 0.33064, accuracy: 0.86571\n",
            "Epoch: 19/30, step: 107/364, loss: 0.33033, accuracy: 0.86580\n",
            "Epoch: 19/30, step: 108/364, loss: 0.33051, accuracy: 0.86574\n",
            "Epoch: 19/30, step: 109/364, loss: 0.33070, accuracy: 0.86583\n",
            "Epoch: 19/30, step: 110/364, loss: 0.33120, accuracy: 0.86563\n",
            "Epoch: 19/30, step: 111/364, loss: 0.33169, accuracy: 0.86543\n",
            "Epoch: 19/30, step: 112/364, loss: 0.33091, accuracy: 0.86579\n",
            "Epoch: 19/30, step: 113/364, loss: 0.33123, accuracy: 0.86532\n",
            "Epoch: 19/30, step: 114/364, loss: 0.33129, accuracy: 0.86486\n",
            "Epoch: 19/30, step: 115/364, loss: 0.33132, accuracy: 0.86454\n",
            "Epoch: 19/30, step: 116/364, loss: 0.33059, accuracy: 0.86503\n",
            "Epoch: 19/30, step: 117/364, loss: 0.33066, accuracy: 0.86472\n",
            "Epoch: 19/30, step: 118/364, loss: 0.32993, accuracy: 0.86520\n",
            "Epoch: 19/30, step: 119/364, loss: 0.32984, accuracy: 0.86555\n",
            "Epoch: 19/30, step: 120/364, loss: 0.33044, accuracy: 0.86497\n",
            "Epoch: 19/30, step: 121/364, loss: 0.33117, accuracy: 0.86441\n",
            "Epoch: 19/30, step: 122/364, loss: 0.33353, accuracy: 0.86309\n",
            "Epoch: 19/30, step: 123/364, loss: 0.33386, accuracy: 0.86319\n",
            "Epoch: 19/30, step: 124/364, loss: 0.33367, accuracy: 0.86366\n",
            "Epoch: 19/30, step: 125/364, loss: 0.33368, accuracy: 0.86350\n",
            "Epoch: 19/30, step: 126/364, loss: 0.33292, accuracy: 0.86434\n",
            "Epoch: 19/30, step: 127/364, loss: 0.33272, accuracy: 0.86454\n",
            "Epoch: 19/30, step: 128/364, loss: 0.33247, accuracy: 0.86487\n",
            "Epoch: 19/30, step: 129/364, loss: 0.33200, accuracy: 0.86495\n",
            "Epoch: 19/30, step: 130/364, loss: 0.33262, accuracy: 0.86502\n",
            "Epoch: 19/30, step: 131/364, loss: 0.33321, accuracy: 0.86462\n",
            "Epoch: 19/30, step: 132/364, loss: 0.33356, accuracy: 0.86458\n",
            "Epoch: 19/30, step: 133/364, loss: 0.33381, accuracy: 0.86443\n",
            "Epoch: 19/30, step: 134/364, loss: 0.33392, accuracy: 0.86416\n",
            "Epoch: 19/30, step: 135/364, loss: 0.33509, accuracy: 0.86366\n",
            "Epoch: 19/30, step: 136/364, loss: 0.33596, accuracy: 0.86317\n",
            "Epoch: 19/30, step: 137/364, loss: 0.33588, accuracy: 0.86302\n",
            "Epoch: 19/30, step: 138/364, loss: 0.33635, accuracy: 0.86277\n",
            "Epoch: 19/30, step: 139/364, loss: 0.33578, accuracy: 0.86308\n",
            "Epoch: 19/30, step: 140/364, loss: 0.33518, accuracy: 0.86328\n",
            "Epoch: 19/30, step: 141/364, loss: 0.33481, accuracy: 0.86336\n",
            "Epoch: 19/30, step: 142/364, loss: 0.33462, accuracy: 0.86367\n",
            "Epoch: 19/30, step: 143/364, loss: 0.33449, accuracy: 0.86364\n",
            "Epoch: 19/30, step: 144/364, loss: 0.33447, accuracy: 0.86372\n",
            "Epoch: 19/30, step: 145/364, loss: 0.33413, accuracy: 0.86390\n",
            "Epoch: 19/30, step: 146/364, loss: 0.33380, accuracy: 0.86419\n",
            "Epoch: 19/30, step: 147/364, loss: 0.33311, accuracy: 0.86469\n",
            "Epoch: 19/30, step: 148/364, loss: 0.33279, accuracy: 0.86465\n",
            "Epoch: 19/30, step: 149/364, loss: 0.33305, accuracy: 0.86430\n",
            "Epoch: 19/30, step: 150/364, loss: 0.33260, accuracy: 0.86458\n",
            "Epoch: 19/30, step: 151/364, loss: 0.33243, accuracy: 0.86465\n",
            "Epoch: 19/30, step: 152/364, loss: 0.33242, accuracy: 0.86472\n",
            "Epoch: 19/30, step: 153/364, loss: 0.33192, accuracy: 0.86499\n",
            "Epoch: 19/30, step: 154/364, loss: 0.33219, accuracy: 0.86465\n",
            "Epoch: 19/30, step: 155/364, loss: 0.33214, accuracy: 0.86482\n",
            "Epoch: 19/30, step: 156/364, loss: 0.33213, accuracy: 0.86478\n",
            "Epoch: 19/30, step: 157/364, loss: 0.33261, accuracy: 0.86445\n",
            "Epoch: 19/30, step: 158/364, loss: 0.33260, accuracy: 0.86422\n",
            "Epoch: 19/30, step: 159/364, loss: 0.33224, accuracy: 0.86468\n",
            "Epoch: 19/30, step: 160/364, loss: 0.33304, accuracy: 0.86416\n",
            "Epoch: 19/30, step: 161/364, loss: 0.33265, accuracy: 0.86442\n",
            "Epoch: 19/30, step: 162/364, loss: 0.33242, accuracy: 0.86468\n",
            "Epoch: 19/30, step: 163/364, loss: 0.33231, accuracy: 0.86474\n",
            "Epoch: 19/30, step: 164/364, loss: 0.33244, accuracy: 0.86442\n",
            "Epoch: 19/30, step: 165/364, loss: 0.33206, accuracy: 0.86468\n",
            "Epoch: 19/30, step: 166/364, loss: 0.33220, accuracy: 0.86455\n",
            "Epoch: 19/30, step: 167/364, loss: 0.33217, accuracy: 0.86471\n",
            "Epoch: 19/30, step: 168/364, loss: 0.33207, accuracy: 0.86477\n",
            "Epoch: 19/30, step: 169/364, loss: 0.33157, accuracy: 0.86548\n",
            "Epoch: 19/30, step: 170/364, loss: 0.33214, accuracy: 0.86507\n",
            "Epoch: 19/30, step: 171/364, loss: 0.33269, accuracy: 0.86458\n",
            "Epoch: 19/30, step: 172/364, loss: 0.33287, accuracy: 0.86446\n",
            "Epoch: 19/30, step: 173/364, loss: 0.33272, accuracy: 0.86461\n",
            "Epoch: 19/30, step: 174/364, loss: 0.33258, accuracy: 0.86458\n",
            "Epoch: 19/30, step: 175/364, loss: 0.33321, accuracy: 0.86420\n",
            "Epoch: 19/30, step: 176/364, loss: 0.33326, accuracy: 0.86426\n",
            "Epoch: 19/30, step: 177/364, loss: 0.33363, accuracy: 0.86414\n",
            "Epoch: 19/30, step: 178/364, loss: 0.33323, accuracy: 0.86447\n",
            "Epoch: 19/30, step: 179/364, loss: 0.33312, accuracy: 0.86435\n",
            "Epoch: 19/30, step: 180/364, loss: 0.33263, accuracy: 0.86467\n",
            "Epoch: 19/30, step: 181/364, loss: 0.33293, accuracy: 0.86438\n",
            "Epoch: 19/30, step: 182/364, loss: 0.33324, accuracy: 0.86410\n",
            "Epoch: 19/30, step: 183/364, loss: 0.33259, accuracy: 0.86458\n",
            "Epoch: 19/30, step: 184/364, loss: 0.33252, accuracy: 0.86456\n",
            "Epoch: 19/30, step: 185/364, loss: 0.33320, accuracy: 0.86419\n",
            "Epoch: 19/30, step: 186/364, loss: 0.33282, accuracy: 0.86433\n",
            "Epoch: 19/30, step: 187/364, loss: 0.33249, accuracy: 0.86456\n",
            "Epoch: 19/30, step: 188/364, loss: 0.33254, accuracy: 0.86469\n",
            "Epoch: 19/30, step: 189/364, loss: 0.33224, accuracy: 0.86483\n",
            "Epoch: 19/30, step: 190/364, loss: 0.33253, accuracy: 0.86447\n",
            "Epoch: 19/30, step: 191/364, loss: 0.33275, accuracy: 0.86445\n",
            "Epoch: 19/30, step: 192/364, loss: 0.33263, accuracy: 0.86466\n",
            "Epoch: 19/30, step: 193/364, loss: 0.33254, accuracy: 0.86448\n",
            "Epoch: 19/30, step: 194/364, loss: 0.33251, accuracy: 0.86469\n",
            "Epoch: 19/30, step: 195/364, loss: 0.33233, accuracy: 0.86466\n",
            "Epoch: 19/30, step: 196/364, loss: 0.33234, accuracy: 0.86448\n",
            "Epoch: 19/30, step: 197/364, loss: 0.33215, accuracy: 0.86477\n",
            "Epoch: 19/30, step: 198/364, loss: 0.33177, accuracy: 0.86490\n",
            "Epoch: 19/30, step: 199/364, loss: 0.33200, accuracy: 0.86471\n",
            "Epoch: 19/30, step: 200/364, loss: 0.33164, accuracy: 0.86492\n",
            "Epoch: 19/30, step: 201/364, loss: 0.33179, accuracy: 0.86497\n",
            "Epoch: 19/30, step: 202/364, loss: 0.33182, accuracy: 0.86479\n",
            "Epoch: 19/30, step: 203/364, loss: 0.33178, accuracy: 0.86461\n",
            "Epoch: 19/30, step: 204/364, loss: 0.33160, accuracy: 0.86474\n",
            "Epoch: 19/30, step: 205/364, loss: 0.33162, accuracy: 0.86456\n",
            "Epoch: 19/30, step: 206/364, loss: 0.33141, accuracy: 0.86476\n",
            "Epoch: 19/30, step: 207/364, loss: 0.33180, accuracy: 0.86443\n",
            "Epoch: 19/30, step: 208/364, loss: 0.33217, accuracy: 0.86418\n",
            "Epoch: 19/30, step: 209/364, loss: 0.33204, accuracy: 0.86416\n",
            "Epoch: 19/30, step: 210/364, loss: 0.33213, accuracy: 0.86399\n",
            "Epoch: 19/30, step: 211/364, loss: 0.33195, accuracy: 0.86411\n",
            "Epoch: 19/30, step: 212/364, loss: 0.33157, accuracy: 0.86446\n",
            "Epoch: 19/30, step: 213/364, loss: 0.33126, accuracy: 0.86466\n",
            "Epoch: 19/30, step: 214/364, loss: 0.33142, accuracy: 0.86471\n",
            "Epoch: 19/30, step: 215/364, loss: 0.33139, accuracy: 0.86461\n",
            "Epoch: 19/30, step: 216/364, loss: 0.33111, accuracy: 0.86480\n",
            "Epoch: 19/30, step: 217/364, loss: 0.33101, accuracy: 0.86485\n",
            "Epoch: 19/30, step: 218/364, loss: 0.33143, accuracy: 0.86468\n",
            "Epoch: 19/30, step: 219/364, loss: 0.33139, accuracy: 0.86480\n",
            "Epoch: 19/30, step: 220/364, loss: 0.33162, accuracy: 0.86442\n",
            "Epoch: 19/30, step: 221/364, loss: 0.33104, accuracy: 0.86468\n",
            "Epoch: 19/30, step: 222/364, loss: 0.33093, accuracy: 0.86486\n",
            "Epoch: 19/30, step: 223/364, loss: 0.33086, accuracy: 0.86498\n",
            "Epoch: 19/30, step: 224/364, loss: 0.33018, accuracy: 0.86551\n",
            "Epoch: 19/30, step: 225/364, loss: 0.33004, accuracy: 0.86556\n",
            "Epoch: 19/30, step: 226/364, loss: 0.32997, accuracy: 0.86574\n",
            "Epoch: 19/30, step: 227/364, loss: 0.32984, accuracy: 0.86585\n",
            "Epoch: 19/30, step: 228/364, loss: 0.32968, accuracy: 0.86589\n",
            "Epoch: 19/30, step: 229/364, loss: 0.32994, accuracy: 0.86572\n",
            "Epoch: 19/30, step: 230/364, loss: 0.32948, accuracy: 0.86610\n",
            "Epoch: 19/30, step: 231/364, loss: 0.32959, accuracy: 0.86594\n",
            "Epoch: 19/30, step: 232/364, loss: 0.32965, accuracy: 0.86598\n",
            "Epoch: 19/30, step: 233/364, loss: 0.33000, accuracy: 0.86581\n",
            "Epoch: 19/30, step: 234/364, loss: 0.33007, accuracy: 0.86558\n",
            "Epoch: 19/30, step: 235/364, loss: 0.33023, accuracy: 0.86543\n",
            "Epoch: 19/30, step: 236/364, loss: 0.33009, accuracy: 0.86553\n",
            "Epoch: 19/30, step: 237/364, loss: 0.33091, accuracy: 0.86511\n",
            "Epoch: 19/30, step: 238/364, loss: 0.33043, accuracy: 0.86535\n",
            "Epoch: 19/30, step: 239/364, loss: 0.33026, accuracy: 0.86552\n",
            "Epoch: 19/30, step: 240/364, loss: 0.32996, accuracy: 0.86576\n",
            "Epoch: 19/30, step: 241/364, loss: 0.33006, accuracy: 0.86573\n",
            "Epoch: 19/30, step: 242/364, loss: 0.33031, accuracy: 0.86570\n",
            "Epoch: 19/30, step: 243/364, loss: 0.33037, accuracy: 0.86581\n",
            "Epoch: 19/30, step: 244/364, loss: 0.33013, accuracy: 0.86610\n",
            "Epoch: 19/30, step: 245/364, loss: 0.33074, accuracy: 0.86582\n",
            "Epoch: 19/30, step: 246/364, loss: 0.33049, accuracy: 0.86598\n",
            "Epoch: 19/30, step: 247/364, loss: 0.33022, accuracy: 0.86627\n",
            "Epoch: 19/30, step: 248/364, loss: 0.33021, accuracy: 0.86637\n",
            "Epoch: 19/30, step: 249/364, loss: 0.32998, accuracy: 0.86647\n",
            "Epoch: 19/30, step: 250/364, loss: 0.32946, accuracy: 0.86687\n",
            "Epoch: 19/30, step: 251/364, loss: 0.32951, accuracy: 0.86660\n",
            "Epoch: 19/30, step: 252/364, loss: 0.32923, accuracy: 0.86663\n",
            "Epoch: 19/30, step: 253/364, loss: 0.32963, accuracy: 0.86629\n",
            "Epoch: 19/30, step: 254/364, loss: 0.32971, accuracy: 0.86620\n",
            "Epoch: 19/30, step: 255/364, loss: 0.33001, accuracy: 0.86605\n",
            "Epoch: 19/30, step: 256/364, loss: 0.32974, accuracy: 0.86627\n",
            "Epoch: 19/30, step: 257/364, loss: 0.32975, accuracy: 0.86637\n",
            "Epoch: 19/30, step: 258/364, loss: 0.32983, accuracy: 0.86628\n",
            "Epoch: 19/30, step: 259/364, loss: 0.32965, accuracy: 0.86655\n",
            "Epoch: 19/30, step: 260/364, loss: 0.32991, accuracy: 0.86641\n",
            "Epoch: 19/30, step: 261/364, loss: 0.32963, accuracy: 0.86656\n",
            "Epoch: 19/30, step: 262/364, loss: 0.32949, accuracy: 0.86653\n",
            "Epoch: 19/30, step: 263/364, loss: 0.32998, accuracy: 0.86603\n",
            "Epoch: 19/30, step: 264/364, loss: 0.32994, accuracy: 0.86606\n",
            "Epoch: 19/30, step: 265/364, loss: 0.33021, accuracy: 0.86586\n",
            "Epoch: 19/30, step: 266/364, loss: 0.33072, accuracy: 0.86554\n",
            "Epoch: 19/30, step: 267/364, loss: 0.33070, accuracy: 0.86558\n",
            "Epoch: 19/30, step: 268/364, loss: 0.33095, accuracy: 0.86544\n",
            "Epoch: 19/30, step: 269/364, loss: 0.33138, accuracy: 0.86495\n",
            "Epoch: 19/30, step: 270/364, loss: 0.33168, accuracy: 0.86464\n",
            "Epoch: 19/30, step: 271/364, loss: 0.33187, accuracy: 0.86451\n",
            "Epoch: 19/30, step: 272/364, loss: 0.33203, accuracy: 0.86432\n",
            "Epoch: 19/30, step: 273/364, loss: 0.33234, accuracy: 0.86401\n",
            "Epoch: 19/30, step: 274/364, loss: 0.33241, accuracy: 0.86388\n",
            "Epoch: 19/30, step: 275/364, loss: 0.33229, accuracy: 0.86386\n",
            "Epoch: 19/30, step: 276/364, loss: 0.33225, accuracy: 0.86402\n",
            "Epoch: 19/30, step: 277/364, loss: 0.33231, accuracy: 0.86389\n",
            "Epoch: 19/30, step: 278/364, loss: 0.33205, accuracy: 0.86415\n",
            "Epoch: 19/30, step: 279/364, loss: 0.33179, accuracy: 0.86430\n",
            "Epoch: 19/30, step: 280/364, loss: 0.33191, accuracy: 0.86423\n",
            "Epoch: 19/30, step: 281/364, loss: 0.33180, accuracy: 0.86427\n",
            "Epoch: 19/30, step: 282/364, loss: 0.33183, accuracy: 0.86436\n",
            "Epoch: 19/30, step: 283/364, loss: 0.33179, accuracy: 0.86434\n",
            "Epoch: 19/30, step: 284/364, loss: 0.33212, accuracy: 0.86411\n",
            "Epoch: 19/30, step: 285/364, loss: 0.33221, accuracy: 0.86398\n",
            "Epoch: 19/30, step: 286/364, loss: 0.33219, accuracy: 0.86402\n",
            "Epoch: 19/30, step: 287/364, loss: 0.33179, accuracy: 0.86427\n",
            "Epoch: 19/30, step: 288/364, loss: 0.33178, accuracy: 0.86426\n",
            "Epoch: 19/30, step: 289/364, loss: 0.33192, accuracy: 0.86419\n",
            "Epoch: 19/30, step: 290/364, loss: 0.33198, accuracy: 0.86422\n",
            "Epoch: 19/30, step: 291/364, loss: 0.33181, accuracy: 0.86427\n",
            "Epoch: 19/30, train loss: 0.33181, train accuracy: 0.86427, valid loss: 0.66013, valid accuracy: 0.68723\n",
            "Epoch: 20/30, step: 1/364, loss: 0.31898, accuracy: 0.87500\n",
            "Epoch: 20/30, step: 2/364, loss: 0.31503, accuracy: 0.89062\n",
            "Epoch: 20/30, step: 3/364, loss: 0.30372, accuracy: 0.89062\n",
            "Epoch: 20/30, step: 4/364, loss: 0.29290, accuracy: 0.89453\n",
            "Epoch: 20/30, step: 5/364, loss: 0.29854, accuracy: 0.88437\n",
            "Epoch: 20/30, step: 6/364, loss: 0.31098, accuracy: 0.86979\n",
            "Epoch: 20/30, step: 7/364, loss: 0.31312, accuracy: 0.87054\n",
            "Epoch: 20/30, step: 8/364, loss: 0.31571, accuracy: 0.87109\n",
            "Epoch: 20/30, step: 9/364, loss: 0.30982, accuracy: 0.87500\n",
            "Epoch: 20/30, step: 10/364, loss: 0.30546, accuracy: 0.88125\n",
            "Epoch: 20/30, step: 11/364, loss: 0.30084, accuracy: 0.88494\n",
            "Epoch: 20/30, step: 12/364, loss: 0.30354, accuracy: 0.88281\n",
            "Epoch: 20/30, step: 13/364, loss: 0.30778, accuracy: 0.87981\n",
            "Epoch: 20/30, step: 14/364, loss: 0.31875, accuracy: 0.87165\n",
            "Epoch: 20/30, step: 15/364, loss: 0.31946, accuracy: 0.87083\n",
            "Epoch: 20/30, step: 16/364, loss: 0.31699, accuracy: 0.87305\n",
            "Epoch: 20/30, step: 17/364, loss: 0.31745, accuracy: 0.87316\n",
            "Epoch: 20/30, step: 18/364, loss: 0.31961, accuracy: 0.87240\n",
            "Epoch: 20/30, step: 19/364, loss: 0.31750, accuracy: 0.87418\n",
            "Epoch: 20/30, step: 20/364, loss: 0.31447, accuracy: 0.87500\n",
            "Epoch: 20/30, step: 21/364, loss: 0.31314, accuracy: 0.87723\n",
            "Epoch: 20/30, step: 22/364, loss: 0.31249, accuracy: 0.87713\n",
            "Epoch: 20/30, step: 23/364, loss: 0.31497, accuracy: 0.87636\n",
            "Epoch: 20/30, step: 24/364, loss: 0.31790, accuracy: 0.87240\n",
            "Epoch: 20/30, step: 25/364, loss: 0.32036, accuracy: 0.87187\n",
            "Epoch: 20/30, step: 26/364, loss: 0.32037, accuracy: 0.86959\n",
            "Epoch: 20/30, step: 27/364, loss: 0.32051, accuracy: 0.86979\n",
            "Epoch: 20/30, step: 28/364, loss: 0.32099, accuracy: 0.86942\n",
            "Epoch: 20/30, step: 29/364, loss: 0.32044, accuracy: 0.86907\n",
            "Epoch: 20/30, step: 30/364, loss: 0.31864, accuracy: 0.86979\n",
            "Epoch: 20/30, step: 31/364, loss: 0.31603, accuracy: 0.87198\n",
            "Epoch: 20/30, step: 32/364, loss: 0.31431, accuracy: 0.87305\n",
            "Epoch: 20/30, step: 33/364, loss: 0.31414, accuracy: 0.87358\n",
            "Epoch: 20/30, step: 34/364, loss: 0.31494, accuracy: 0.87178\n",
            "Epoch: 20/30, step: 35/364, loss: 0.31412, accuracy: 0.87277\n",
            "Epoch: 20/30, step: 36/364, loss: 0.31466, accuracy: 0.87240\n",
            "Epoch: 20/30, step: 37/364, loss: 0.31843, accuracy: 0.86993\n",
            "Epoch: 20/30, step: 38/364, loss: 0.31756, accuracy: 0.87171\n",
            "Epoch: 20/30, step: 39/364, loss: 0.31668, accuracy: 0.87179\n",
            "Epoch: 20/30, step: 40/364, loss: 0.31565, accuracy: 0.87227\n",
            "Epoch: 20/30, step: 41/364, loss: 0.31459, accuracy: 0.87271\n",
            "Epoch: 20/30, step: 42/364, loss: 0.31465, accuracy: 0.87277\n",
            "Epoch: 20/30, step: 43/364, loss: 0.31281, accuracy: 0.87464\n",
            "Epoch: 20/30, step: 44/364, loss: 0.31296, accuracy: 0.87429\n",
            "Epoch: 20/30, step: 45/364, loss: 0.31190, accuracy: 0.87465\n",
            "Epoch: 20/30, step: 46/364, loss: 0.31185, accuracy: 0.87500\n",
            "Epoch: 20/30, step: 47/364, loss: 0.31155, accuracy: 0.87467\n",
            "Epoch: 20/30, step: 48/364, loss: 0.31039, accuracy: 0.87565\n",
            "Epoch: 20/30, step: 49/364, loss: 0.31024, accuracy: 0.87500\n",
            "Epoch: 20/30, step: 50/364, loss: 0.30876, accuracy: 0.87625\n",
            "Epoch: 20/30, step: 51/364, loss: 0.31002, accuracy: 0.87500\n",
            "Epoch: 20/30, step: 52/364, loss: 0.31073, accuracy: 0.87440\n",
            "Epoch: 20/30, step: 53/364, loss: 0.31132, accuracy: 0.87382\n",
            "Epoch: 20/30, step: 54/364, loss: 0.31075, accuracy: 0.87413\n",
            "Epoch: 20/30, step: 55/364, loss: 0.30922, accuracy: 0.87472\n",
            "Epoch: 20/30, step: 56/364, loss: 0.30856, accuracy: 0.87444\n",
            "Epoch: 20/30, step: 57/364, loss: 0.30813, accuracy: 0.87445\n",
            "Epoch: 20/30, step: 58/364, loss: 0.30925, accuracy: 0.87446\n",
            "Epoch: 20/30, step: 59/364, loss: 0.30944, accuracy: 0.87368\n",
            "Epoch: 20/30, step: 60/364, loss: 0.30926, accuracy: 0.87448\n",
            "Epoch: 20/30, step: 61/364, loss: 0.30832, accuracy: 0.87551\n",
            "Epoch: 20/30, step: 62/364, loss: 0.30787, accuracy: 0.87525\n",
            "Epoch: 20/30, step: 63/364, loss: 0.30816, accuracy: 0.87525\n",
            "Epoch: 20/30, step: 64/364, loss: 0.30745, accuracy: 0.87573\n",
            "Epoch: 20/30, step: 65/364, loss: 0.30595, accuracy: 0.87692\n",
            "Epoch: 20/30, step: 66/364, loss: 0.30555, accuracy: 0.87760\n",
            "Epoch: 20/30, step: 67/364, loss: 0.30456, accuracy: 0.87780\n",
            "Epoch: 20/30, step: 68/364, loss: 0.30429, accuracy: 0.87868\n",
            "Epoch: 20/30, step: 69/364, loss: 0.30526, accuracy: 0.87817\n",
            "Epoch: 20/30, step: 70/364, loss: 0.30491, accuracy: 0.87813\n",
            "Epoch: 20/30, step: 71/364, loss: 0.30487, accuracy: 0.87786\n",
            "Epoch: 20/30, step: 72/364, loss: 0.30462, accuracy: 0.87804\n",
            "Epoch: 20/30, step: 73/364, loss: 0.30526, accuracy: 0.87757\n",
            "Epoch: 20/30, step: 74/364, loss: 0.30605, accuracy: 0.87711\n",
            "Epoch: 20/30, step: 75/364, loss: 0.30587, accuracy: 0.87729\n",
            "Epoch: 20/30, step: 76/364, loss: 0.30634, accuracy: 0.87685\n",
            "Epoch: 20/30, step: 77/364, loss: 0.30544, accuracy: 0.87723\n",
            "Epoch: 20/30, step: 78/364, loss: 0.30436, accuracy: 0.87800\n",
            "Epoch: 20/30, step: 79/364, loss: 0.30326, accuracy: 0.87876\n",
            "Epoch: 20/30, step: 80/364, loss: 0.30302, accuracy: 0.87910\n",
            "Epoch: 20/30, step: 81/364, loss: 0.30371, accuracy: 0.87828\n",
            "Epoch: 20/30, step: 82/364, loss: 0.30237, accuracy: 0.87938\n",
            "Epoch: 20/30, step: 83/364, loss: 0.30375, accuracy: 0.87801\n",
            "Epoch: 20/30, step: 84/364, loss: 0.30317, accuracy: 0.87853\n",
            "Epoch: 20/30, step: 85/364, loss: 0.30260, accuracy: 0.87904\n",
            "Epoch: 20/30, step: 86/364, loss: 0.30409, accuracy: 0.87845\n",
            "Epoch: 20/30, step: 87/364, loss: 0.30354, accuracy: 0.87895\n",
            "Epoch: 20/30, step: 88/364, loss: 0.30370, accuracy: 0.87873\n",
            "Epoch: 20/30, step: 89/364, loss: 0.30394, accuracy: 0.87834\n",
            "Epoch: 20/30, step: 90/364, loss: 0.30378, accuracy: 0.87847\n",
            "Epoch: 20/30, step: 91/364, loss: 0.30315, accuracy: 0.87929\n",
            "Epoch: 20/30, step: 92/364, loss: 0.30356, accuracy: 0.87908\n",
            "Epoch: 20/30, step: 93/364, loss: 0.30321, accuracy: 0.87920\n",
            "Epoch: 20/30, step: 94/364, loss: 0.30299, accuracy: 0.87965\n",
            "Epoch: 20/30, step: 95/364, loss: 0.30254, accuracy: 0.87961\n",
            "Epoch: 20/30, step: 96/364, loss: 0.30401, accuracy: 0.87891\n",
            "Epoch: 20/30, step: 97/364, loss: 0.30422, accuracy: 0.87870\n",
            "Epoch: 20/30, step: 98/364, loss: 0.30504, accuracy: 0.87787\n",
            "Epoch: 20/30, step: 99/364, loss: 0.30569, accuracy: 0.87753\n",
            "Epoch: 20/30, step: 100/364, loss: 0.30664, accuracy: 0.87656\n",
            "Epoch: 20/30, step: 101/364, loss: 0.30658, accuracy: 0.87639\n",
            "Epoch: 20/30, step: 102/364, loss: 0.30692, accuracy: 0.87607\n",
            "Epoch: 20/30, step: 103/364, loss: 0.30716, accuracy: 0.87591\n",
            "Epoch: 20/30, step: 104/364, loss: 0.30703, accuracy: 0.87650\n",
            "Epoch: 20/30, step: 105/364, loss: 0.30791, accuracy: 0.87560\n",
            "Epoch: 20/30, step: 106/364, loss: 0.30839, accuracy: 0.87544\n",
            "Epoch: 20/30, step: 107/364, loss: 0.30790, accuracy: 0.87602\n",
            "Epoch: 20/30, step: 108/364, loss: 0.30891, accuracy: 0.87543\n",
            "Epoch: 20/30, step: 109/364, loss: 0.30927, accuracy: 0.87514\n",
            "Epoch: 20/30, step: 110/364, loss: 0.31000, accuracy: 0.87528\n",
            "Epoch: 20/30, step: 111/364, loss: 0.30994, accuracy: 0.87514\n",
            "Epoch: 20/30, step: 112/364, loss: 0.31171, accuracy: 0.87388\n",
            "Epoch: 20/30, step: 113/364, loss: 0.31139, accuracy: 0.87445\n",
            "Epoch: 20/30, step: 114/364, loss: 0.31053, accuracy: 0.87514\n",
            "Epoch: 20/30, step: 115/364, loss: 0.31051, accuracy: 0.87486\n",
            "Epoch: 20/30, step: 116/364, loss: 0.31045, accuracy: 0.87500\n",
            "Epoch: 20/30, step: 117/364, loss: 0.31087, accuracy: 0.87500\n",
            "Epoch: 20/30, step: 118/364, loss: 0.31113, accuracy: 0.87513\n",
            "Epoch: 20/30, step: 119/364, loss: 0.31090, accuracy: 0.87513\n",
            "Epoch: 20/30, step: 120/364, loss: 0.31201, accuracy: 0.87435\n",
            "Epoch: 20/30, step: 121/364, loss: 0.31102, accuracy: 0.87448\n",
            "Epoch: 20/30, step: 122/364, loss: 0.31112, accuracy: 0.87462\n",
            "Epoch: 20/30, step: 123/364, loss: 0.31175, accuracy: 0.87411\n",
            "Epoch: 20/30, step: 124/364, loss: 0.31261, accuracy: 0.87361\n",
            "Epoch: 20/30, step: 125/364, loss: 0.31283, accuracy: 0.87375\n",
            "Epoch: 20/30, step: 126/364, loss: 0.31340, accuracy: 0.87339\n",
            "Epoch: 20/30, step: 127/364, loss: 0.31317, accuracy: 0.87365\n",
            "Epoch: 20/30, step: 128/364, loss: 0.31349, accuracy: 0.87354\n",
            "Epoch: 20/30, step: 129/364, loss: 0.31374, accuracy: 0.87343\n",
            "Epoch: 20/30, step: 130/364, loss: 0.31349, accuracy: 0.87356\n",
            "Epoch: 20/30, step: 131/364, loss: 0.31349, accuracy: 0.87345\n",
            "Epoch: 20/30, step: 132/364, loss: 0.31319, accuracy: 0.87370\n",
            "Epoch: 20/30, step: 133/364, loss: 0.31376, accuracy: 0.87336\n",
            "Epoch: 20/30, step: 134/364, loss: 0.31398, accuracy: 0.87302\n",
            "Epoch: 20/30, step: 135/364, loss: 0.31413, accuracy: 0.87326\n",
            "Epoch: 20/30, step: 136/364, loss: 0.31443, accuracy: 0.87305\n",
            "Epoch: 20/30, step: 137/364, loss: 0.31477, accuracy: 0.87272\n",
            "Epoch: 20/30, step: 138/364, loss: 0.31429, accuracy: 0.87296\n",
            "Epoch: 20/30, step: 139/364, loss: 0.31414, accuracy: 0.87309\n",
            "Epoch: 20/30, step: 140/364, loss: 0.31399, accuracy: 0.87333\n",
            "Epoch: 20/30, step: 141/364, loss: 0.31334, accuracy: 0.87378\n",
            "Epoch: 20/30, step: 142/364, loss: 0.31345, accuracy: 0.87357\n",
            "Epoch: 20/30, step: 143/364, loss: 0.31357, accuracy: 0.87325\n",
            "Epoch: 20/30, step: 144/364, loss: 0.31320, accuracy: 0.87370\n",
            "Epoch: 20/30, step: 145/364, loss: 0.31303, accuracy: 0.87381\n",
            "Epoch: 20/30, step: 146/364, loss: 0.31352, accuracy: 0.87329\n",
            "Epoch: 20/30, step: 147/364, loss: 0.31396, accuracy: 0.87277\n",
            "Epoch: 20/30, step: 148/364, loss: 0.31373, accuracy: 0.87299\n",
            "Epoch: 20/30, step: 149/364, loss: 0.31363, accuracy: 0.87311\n",
            "Epoch: 20/30, step: 150/364, loss: 0.31291, accuracy: 0.87365\n",
            "Epoch: 20/30, step: 151/364, loss: 0.31288, accuracy: 0.87376\n",
            "Epoch: 20/30, step: 152/364, loss: 0.31224, accuracy: 0.87407\n",
            "Epoch: 20/30, step: 153/364, loss: 0.31226, accuracy: 0.87357\n",
            "Epoch: 20/30, step: 154/364, loss: 0.31253, accuracy: 0.87348\n",
            "Epoch: 20/30, step: 155/364, loss: 0.31223, accuracy: 0.87359\n",
            "Epoch: 20/30, step: 156/364, loss: 0.31217, accuracy: 0.87350\n",
            "Epoch: 20/30, step: 157/364, loss: 0.31205, accuracy: 0.87371\n",
            "Epoch: 20/30, step: 158/364, loss: 0.31185, accuracy: 0.87381\n",
            "Epoch: 20/30, step: 159/364, loss: 0.31199, accuracy: 0.87372\n",
            "Epoch: 20/30, step: 160/364, loss: 0.31166, accuracy: 0.87373\n",
            "Epoch: 20/30, step: 161/364, loss: 0.31169, accuracy: 0.87354\n",
            "Epoch: 20/30, step: 162/364, loss: 0.31152, accuracy: 0.87365\n",
            "Epoch: 20/30, step: 163/364, loss: 0.31156, accuracy: 0.87356\n",
            "Epoch: 20/30, step: 164/364, loss: 0.31173, accuracy: 0.87348\n",
            "Epoch: 20/30, step: 165/364, loss: 0.31173, accuracy: 0.87339\n",
            "Epoch: 20/30, step: 166/364, loss: 0.31182, accuracy: 0.87321\n",
            "Epoch: 20/30, step: 167/364, loss: 0.31200, accuracy: 0.87304\n",
            "Epoch: 20/30, step: 168/364, loss: 0.31214, accuracy: 0.87305\n",
            "Epoch: 20/30, step: 169/364, loss: 0.31183, accuracy: 0.87324\n",
            "Epoch: 20/30, step: 170/364, loss: 0.31140, accuracy: 0.87344\n",
            "Epoch: 20/30, step: 171/364, loss: 0.31119, accuracy: 0.87372\n",
            "Epoch: 20/30, step: 172/364, loss: 0.31136, accuracy: 0.87346\n",
            "Epoch: 20/30, step: 173/364, loss: 0.31124, accuracy: 0.87355\n",
            "Epoch: 20/30, step: 174/364, loss: 0.31132, accuracy: 0.87356\n",
            "Epoch: 20/30, step: 175/364, loss: 0.31114, accuracy: 0.87357\n",
            "Epoch: 20/30, step: 176/364, loss: 0.31086, accuracy: 0.87367\n",
            "Epoch: 20/30, step: 177/364, loss: 0.31083, accuracy: 0.87359\n",
            "Epoch: 20/30, step: 178/364, loss: 0.31075, accuracy: 0.87333\n",
            "Epoch: 20/30, step: 179/364, loss: 0.31067, accuracy: 0.87308\n",
            "Epoch: 20/30, step: 180/364, loss: 0.31052, accuracy: 0.87309\n",
            "Epoch: 20/30, step: 181/364, loss: 0.31065, accuracy: 0.87310\n",
            "Epoch: 20/30, step: 182/364, loss: 0.31027, accuracy: 0.87328\n",
            "Epoch: 20/30, step: 183/364, loss: 0.31019, accuracy: 0.87338\n",
            "Epoch: 20/30, step: 184/364, loss: 0.31017, accuracy: 0.87330\n",
            "Epoch: 20/30, step: 185/364, loss: 0.31043, accuracy: 0.87306\n",
            "Epoch: 20/30, step: 186/364, loss: 0.30995, accuracy: 0.87340\n",
            "Epoch: 20/30, step: 187/364, loss: 0.30969, accuracy: 0.87375\n",
            "Epoch: 20/30, step: 188/364, loss: 0.30954, accuracy: 0.87400\n",
            "Epoch: 20/30, step: 189/364, loss: 0.30961, accuracy: 0.87401\n",
            "Epoch: 20/30, step: 190/364, loss: 0.30954, accuracy: 0.87401\n",
            "Epoch: 20/30, step: 191/364, loss: 0.30945, accuracy: 0.87426\n",
            "Epoch: 20/30, step: 192/364, loss: 0.30958, accuracy: 0.87402\n",
            "Epoch: 20/30, step: 193/364, loss: 0.30967, accuracy: 0.87395\n",
            "Epoch: 20/30, step: 194/364, loss: 0.30940, accuracy: 0.87428\n",
            "Epoch: 20/30, step: 195/364, loss: 0.30925, accuracy: 0.87428\n",
            "Epoch: 20/30, step: 196/364, loss: 0.30947, accuracy: 0.87412\n",
            "Epoch: 20/30, step: 197/364, loss: 0.30917, accuracy: 0.87437\n",
            "Epoch: 20/30, step: 198/364, loss: 0.30892, accuracy: 0.87453\n",
            "Epoch: 20/30, step: 199/364, loss: 0.30847, accuracy: 0.87508\n",
            "Epoch: 20/30, step: 200/364, loss: 0.30805, accuracy: 0.87523\n",
            "Epoch: 20/30, step: 201/364, loss: 0.30750, accuracy: 0.87562\n",
            "Epoch: 20/30, step: 202/364, loss: 0.30748, accuracy: 0.87570\n",
            "Epoch: 20/30, step: 203/364, loss: 0.30719, accuracy: 0.87585\n",
            "Epoch: 20/30, step: 204/364, loss: 0.30746, accuracy: 0.87554\n",
            "Epoch: 20/30, step: 205/364, loss: 0.30766, accuracy: 0.87546\n",
            "Epoch: 20/30, step: 206/364, loss: 0.30775, accuracy: 0.87538\n",
            "Epoch: 20/30, step: 207/364, loss: 0.30754, accuracy: 0.87553\n",
            "Epoch: 20/30, step: 208/364, loss: 0.30764, accuracy: 0.87545\n",
            "Epoch: 20/30, step: 209/364, loss: 0.30741, accuracy: 0.87575\n",
            "Epoch: 20/30, step: 210/364, loss: 0.30807, accuracy: 0.87537\n",
            "Epoch: 20/30, step: 211/364, loss: 0.30805, accuracy: 0.87544\n",
            "Epoch: 20/30, step: 212/364, loss: 0.30854, accuracy: 0.87507\n",
            "Epoch: 20/30, step: 213/364, loss: 0.30873, accuracy: 0.87500\n",
            "Epoch: 20/30, step: 214/364, loss: 0.30829, accuracy: 0.87529\n",
            "Epoch: 20/30, step: 215/364, loss: 0.30838, accuracy: 0.87515\n",
            "Epoch: 20/30, step: 216/364, loss: 0.30833, accuracy: 0.87500\n",
            "Epoch: 20/30, step: 217/364, loss: 0.30837, accuracy: 0.87486\n",
            "Epoch: 20/30, step: 218/364, loss: 0.30840, accuracy: 0.87493\n",
            "Epoch: 20/30, step: 219/364, loss: 0.30832, accuracy: 0.87493\n",
            "Epoch: 20/30, step: 220/364, loss: 0.30873, accuracy: 0.87479\n",
            "Epoch: 20/30, step: 221/364, loss: 0.30884, accuracy: 0.87472\n",
            "Epoch: 20/30, step: 222/364, loss: 0.30848, accuracy: 0.87479\n",
            "Epoch: 20/30, step: 223/364, loss: 0.30824, accuracy: 0.87493\n",
            "Epoch: 20/30, step: 224/364, loss: 0.30773, accuracy: 0.87514\n",
            "Epoch: 20/30, step: 225/364, loss: 0.30748, accuracy: 0.87535\n",
            "Epoch: 20/30, step: 226/364, loss: 0.30726, accuracy: 0.87555\n",
            "Epoch: 20/30, step: 227/364, loss: 0.30681, accuracy: 0.87576\n",
            "Epoch: 20/30, step: 228/364, loss: 0.30681, accuracy: 0.87569\n",
            "Epoch: 20/30, step: 229/364, loss: 0.30625, accuracy: 0.87623\n",
            "Epoch: 20/30, step: 230/364, loss: 0.30634, accuracy: 0.87588\n",
            "Epoch: 20/30, step: 231/364, loss: 0.30630, accuracy: 0.87581\n",
            "Epoch: 20/30, step: 232/364, loss: 0.30653, accuracy: 0.87594\n",
            "Epoch: 20/30, step: 233/364, loss: 0.30652, accuracy: 0.87601\n",
            "Epoch: 20/30, step: 234/364, loss: 0.30614, accuracy: 0.87640\n",
            "Epoch: 20/30, step: 235/364, loss: 0.30605, accuracy: 0.87646\n",
            "Epoch: 20/30, step: 236/364, loss: 0.30616, accuracy: 0.87646\n",
            "Epoch: 20/30, step: 237/364, loss: 0.30643, accuracy: 0.87632\n",
            "Epoch: 20/30, step: 238/364, loss: 0.30642, accuracy: 0.87638\n",
            "Epoch: 20/30, step: 239/364, loss: 0.30614, accuracy: 0.87657\n",
            "Epoch: 20/30, step: 240/364, loss: 0.30602, accuracy: 0.87669\n",
            "Epoch: 20/30, step: 241/364, loss: 0.30620, accuracy: 0.87649\n",
            "Epoch: 20/30, step: 242/364, loss: 0.30633, accuracy: 0.87649\n",
            "Epoch: 20/30, step: 243/364, loss: 0.30640, accuracy: 0.87654\n",
            "Epoch: 20/30, step: 244/364, loss: 0.30589, accuracy: 0.87692\n",
            "Epoch: 20/30, step: 245/364, loss: 0.30576, accuracy: 0.87698\n",
            "Epoch: 20/30, step: 246/364, loss: 0.30627, accuracy: 0.87678\n",
            "Epoch: 20/30, step: 247/364, loss: 0.30668, accuracy: 0.87652\n",
            "Epoch: 20/30, step: 248/364, loss: 0.30689, accuracy: 0.87632\n",
            "Epoch: 20/30, step: 249/364, loss: 0.30701, accuracy: 0.87632\n",
            "Epoch: 20/30, step: 250/364, loss: 0.30699, accuracy: 0.87644\n",
            "Epoch: 20/30, step: 251/364, loss: 0.30709, accuracy: 0.87643\n",
            "Epoch: 20/30, step: 252/364, loss: 0.30748, accuracy: 0.87624\n",
            "Epoch: 20/30, step: 253/364, loss: 0.30759, accuracy: 0.87611\n",
            "Epoch: 20/30, step: 254/364, loss: 0.30748, accuracy: 0.87611\n",
            "Epoch: 20/30, step: 255/364, loss: 0.30720, accuracy: 0.87616\n",
            "Epoch: 20/30, step: 256/364, loss: 0.30685, accuracy: 0.87628\n",
            "Epoch: 20/30, step: 257/364, loss: 0.30686, accuracy: 0.87640\n",
            "Epoch: 20/30, step: 258/364, loss: 0.30713, accuracy: 0.87627\n",
            "Epoch: 20/30, step: 259/364, loss: 0.30684, accuracy: 0.87651\n",
            "Epoch: 20/30, step: 260/364, loss: 0.30690, accuracy: 0.87650\n",
            "Epoch: 20/30, step: 261/364, loss: 0.30684, accuracy: 0.87668\n",
            "Epoch: 20/30, step: 262/364, loss: 0.30695, accuracy: 0.87667\n",
            "Epoch: 20/30, step: 263/364, loss: 0.30697, accuracy: 0.87660\n",
            "Epoch: 20/30, step: 264/364, loss: 0.30690, accuracy: 0.87678\n",
            "Epoch: 20/30, step: 265/364, loss: 0.30663, accuracy: 0.87700\n",
            "Epoch: 20/30, step: 266/364, loss: 0.30628, accuracy: 0.87723\n",
            "Epoch: 20/30, step: 267/364, loss: 0.30626, accuracy: 0.87722\n",
            "Epoch: 20/30, step: 268/364, loss: 0.30638, accuracy: 0.87722\n",
            "Epoch: 20/30, step: 269/364, loss: 0.30671, accuracy: 0.87709\n",
            "Epoch: 20/30, step: 270/364, loss: 0.30666, accuracy: 0.87691\n",
            "Epoch: 20/30, step: 271/364, loss: 0.30660, accuracy: 0.87690\n",
            "Epoch: 20/30, step: 272/364, loss: 0.30653, accuracy: 0.87690\n",
            "Epoch: 20/30, step: 273/364, loss: 0.30649, accuracy: 0.87706\n",
            "Epoch: 20/30, step: 274/364, loss: 0.30649, accuracy: 0.87711\n",
            "Epoch: 20/30, step: 275/364, loss: 0.30639, accuracy: 0.87716\n",
            "Epoch: 20/30, step: 276/364, loss: 0.30644, accuracy: 0.87721\n",
            "Epoch: 20/30, step: 277/364, loss: 0.30685, accuracy: 0.87697\n",
            "Epoch: 20/30, step: 278/364, loss: 0.30674, accuracy: 0.87708\n",
            "Epoch: 20/30, step: 279/364, loss: 0.30647, accuracy: 0.87730\n",
            "Epoch: 20/30, step: 280/364, loss: 0.30665, accuracy: 0.87701\n",
            "Epoch: 20/30, step: 281/364, loss: 0.30674, accuracy: 0.87683\n",
            "Epoch: 20/30, step: 282/364, loss: 0.30654, accuracy: 0.87694\n",
            "Epoch: 20/30, step: 283/364, loss: 0.30644, accuracy: 0.87704\n",
            "Epoch: 20/30, step: 284/364, loss: 0.30707, accuracy: 0.87665\n",
            "Epoch: 20/30, step: 285/364, loss: 0.30748, accuracy: 0.87654\n",
            "Epoch: 20/30, step: 286/364, loss: 0.30716, accuracy: 0.87680\n",
            "Epoch: 20/30, step: 287/364, loss: 0.30710, accuracy: 0.87685\n",
            "Epoch: 20/30, step: 288/364, loss: 0.30689, accuracy: 0.87690\n",
            "Epoch: 20/30, step: 289/364, loss: 0.30687, accuracy: 0.87695\n",
            "Epoch: 20/30, step: 290/364, loss: 0.30660, accuracy: 0.87716\n",
            "Epoch: 20/30, step: 291/364, loss: 0.30649, accuracy: 0.87716\n",
            "Epoch: 20/30, train loss: 0.30649, train accuracy: 0.87716, valid loss: 0.65482, valid accuracy: 0.69003\n",
            "Epoch: 21/30, step: 1/364, loss: 0.40752, accuracy: 0.79688\n",
            "Epoch: 21/30, step: 2/364, loss: 0.34411, accuracy: 0.85156\n",
            "Epoch: 21/30, step: 3/364, loss: 0.31965, accuracy: 0.85938\n",
            "Epoch: 21/30, step: 4/364, loss: 0.32476, accuracy: 0.86328\n",
            "Epoch: 21/30, step: 5/364, loss: 0.34274, accuracy: 0.85312\n",
            "Epoch: 21/30, step: 6/364, loss: 0.34055, accuracy: 0.85677\n",
            "Epoch: 21/30, step: 7/364, loss: 0.33402, accuracy: 0.85714\n",
            "Epoch: 21/30, step: 8/364, loss: 0.32204, accuracy: 0.86523\n",
            "Epoch: 21/30, step: 9/364, loss: 0.32008, accuracy: 0.86806\n",
            "Epoch: 21/30, step: 10/364, loss: 0.31781, accuracy: 0.86875\n",
            "Epoch: 21/30, step: 11/364, loss: 0.31973, accuracy: 0.86648\n",
            "Epoch: 21/30, step: 12/364, loss: 0.31485, accuracy: 0.86589\n",
            "Epoch: 21/30, step: 13/364, loss: 0.31949, accuracy: 0.86298\n",
            "Epoch: 21/30, step: 14/364, loss: 0.32291, accuracy: 0.86161\n",
            "Epoch: 21/30, step: 15/364, loss: 0.32619, accuracy: 0.86146\n",
            "Epoch: 21/30, step: 16/364, loss: 0.32387, accuracy: 0.86621\n",
            "Epoch: 21/30, step: 17/364, loss: 0.32030, accuracy: 0.86949\n",
            "Epoch: 21/30, step: 18/364, loss: 0.32332, accuracy: 0.86892\n",
            "Epoch: 21/30, step: 19/364, loss: 0.32012, accuracy: 0.87089\n",
            "Epoch: 21/30, step: 20/364, loss: 0.31615, accuracy: 0.87344\n",
            "Epoch: 21/30, step: 21/364, loss: 0.31114, accuracy: 0.87649\n",
            "Epoch: 21/30, step: 22/364, loss: 0.31372, accuracy: 0.87500\n",
            "Epoch: 21/30, step: 23/364, loss: 0.31374, accuracy: 0.87364\n",
            "Epoch: 21/30, step: 24/364, loss: 0.30894, accuracy: 0.87760\n",
            "Epoch: 21/30, step: 25/364, loss: 0.30648, accuracy: 0.87875\n",
            "Epoch: 21/30, step: 26/364, loss: 0.30873, accuracy: 0.87740\n",
            "Epoch: 21/30, step: 27/364, loss: 0.30481, accuracy: 0.88021\n",
            "Epoch: 21/30, step: 28/364, loss: 0.30305, accuracy: 0.88170\n",
            "Epoch: 21/30, step: 29/364, loss: 0.30321, accuracy: 0.88147\n",
            "Epoch: 21/30, step: 30/364, loss: 0.30155, accuracy: 0.88437\n",
            "Epoch: 21/30, step: 31/364, loss: 0.30077, accuracy: 0.88357\n",
            "Epoch: 21/30, step: 32/364, loss: 0.30205, accuracy: 0.88086\n",
            "Epoch: 21/30, step: 33/364, loss: 0.30233, accuracy: 0.87973\n",
            "Epoch: 21/30, step: 34/364, loss: 0.30302, accuracy: 0.87914\n",
            "Epoch: 21/30, step: 35/364, loss: 0.30104, accuracy: 0.87991\n",
            "Epoch: 21/30, step: 36/364, loss: 0.30105, accuracy: 0.88021\n",
            "Epoch: 21/30, step: 37/364, loss: 0.29938, accuracy: 0.88091\n",
            "Epoch: 21/30, step: 38/364, loss: 0.30075, accuracy: 0.88117\n",
            "Epoch: 21/30, step: 39/364, loss: 0.30293, accuracy: 0.88021\n",
            "Epoch: 21/30, step: 40/364, loss: 0.30032, accuracy: 0.88086\n",
            "Epoch: 21/30, step: 41/364, loss: 0.29981, accuracy: 0.88224\n",
            "Epoch: 21/30, step: 42/364, loss: 0.29962, accuracy: 0.88207\n",
            "Epoch: 21/30, step: 43/364, loss: 0.29994, accuracy: 0.88154\n",
            "Epoch: 21/30, step: 44/364, loss: 0.29987, accuracy: 0.88139\n",
            "Epoch: 21/30, step: 45/364, loss: 0.30044, accuracy: 0.88056\n",
            "Epoch: 21/30, step: 46/364, loss: 0.30058, accuracy: 0.88043\n",
            "Epoch: 21/30, step: 47/364, loss: 0.30234, accuracy: 0.87799\n",
            "Epoch: 21/30, step: 48/364, loss: 0.30230, accuracy: 0.87760\n",
            "Epoch: 21/30, step: 49/364, loss: 0.30097, accuracy: 0.87883\n",
            "Epoch: 21/30, step: 50/364, loss: 0.30062, accuracy: 0.87906\n",
            "Epoch: 21/30, step: 51/364, loss: 0.29822, accuracy: 0.88113\n",
            "Epoch: 21/30, step: 52/364, loss: 0.29735, accuracy: 0.88191\n",
            "Epoch: 21/30, step: 53/364, loss: 0.29705, accuracy: 0.88237\n",
            "Epoch: 21/30, step: 54/364, loss: 0.29613, accuracy: 0.88223\n",
            "Epoch: 21/30, step: 55/364, loss: 0.29716, accuracy: 0.88125\n",
            "Epoch: 21/30, step: 56/364, loss: 0.29591, accuracy: 0.88281\n",
            "Epoch: 21/30, step: 57/364, loss: 0.29628, accuracy: 0.88185\n",
            "Epoch: 21/30, step: 58/364, loss: 0.29699, accuracy: 0.88147\n",
            "Epoch: 21/30, step: 59/364, loss: 0.29692, accuracy: 0.88189\n",
            "Epoch: 21/30, step: 60/364, loss: 0.29733, accuracy: 0.88177\n",
            "Epoch: 21/30, step: 61/364, loss: 0.29718, accuracy: 0.88243\n",
            "Epoch: 21/30, step: 62/364, loss: 0.29816, accuracy: 0.88180\n",
            "Epoch: 21/30, step: 63/364, loss: 0.29917, accuracy: 0.88021\n",
            "Epoch: 21/30, step: 64/364, loss: 0.29804, accuracy: 0.88062\n",
            "Epoch: 21/30, step: 65/364, loss: 0.29687, accuracy: 0.88125\n",
            "Epoch: 21/30, step: 66/364, loss: 0.29644, accuracy: 0.88139\n",
            "Epoch: 21/30, step: 67/364, loss: 0.29611, accuracy: 0.88176\n",
            "Epoch: 21/30, step: 68/364, loss: 0.29776, accuracy: 0.88051\n",
            "Epoch: 21/30, step: 69/364, loss: 0.29749, accuracy: 0.88111\n",
            "Epoch: 21/30, step: 70/364, loss: 0.29651, accuracy: 0.88170\n",
            "Epoch: 21/30, step: 71/364, loss: 0.29613, accuracy: 0.88160\n",
            "Epoch: 21/30, step: 72/364, loss: 0.29569, accuracy: 0.88216\n",
            "Epoch: 21/30, step: 73/364, loss: 0.29520, accuracy: 0.88164\n",
            "Epoch: 21/30, step: 74/364, loss: 0.29439, accuracy: 0.88218\n",
            "Epoch: 21/30, step: 75/364, loss: 0.29381, accuracy: 0.88250\n",
            "Epoch: 21/30, step: 76/364, loss: 0.29305, accuracy: 0.88302\n",
            "Epoch: 21/30, step: 77/364, loss: 0.29297, accuracy: 0.88332\n",
            "Epoch: 21/30, step: 78/364, loss: 0.29223, accuracy: 0.88361\n",
            "Epoch: 21/30, step: 79/364, loss: 0.29160, accuracy: 0.88430\n",
            "Epoch: 21/30, step: 80/364, loss: 0.29193, accuracy: 0.88379\n",
            "Epoch: 21/30, step: 81/364, loss: 0.29164, accuracy: 0.88407\n",
            "Epoch: 21/30, step: 82/364, loss: 0.29050, accuracy: 0.88472\n",
            "Epoch: 21/30, step: 83/364, loss: 0.29028, accuracy: 0.88479\n",
            "Epoch: 21/30, step: 84/364, loss: 0.28997, accuracy: 0.88523\n",
            "Epoch: 21/30, step: 85/364, loss: 0.29011, accuracy: 0.88529\n",
            "Epoch: 21/30, step: 86/364, loss: 0.29010, accuracy: 0.88536\n",
            "Epoch: 21/30, step: 87/364, loss: 0.28949, accuracy: 0.88614\n",
            "Epoch: 21/30, step: 88/364, loss: 0.28912, accuracy: 0.88601\n",
            "Epoch: 21/30, step: 89/364, loss: 0.28964, accuracy: 0.88588\n",
            "Epoch: 21/30, step: 90/364, loss: 0.29069, accuracy: 0.88524\n",
            "Epoch: 21/30, step: 91/364, loss: 0.28958, accuracy: 0.88633\n",
            "Epoch: 21/30, step: 92/364, loss: 0.28924, accuracy: 0.88621\n",
            "Epoch: 21/30, step: 93/364, loss: 0.28942, accuracy: 0.88592\n",
            "Epoch: 21/30, step: 94/364, loss: 0.28832, accuracy: 0.88664\n",
            "Epoch: 21/30, step: 95/364, loss: 0.28855, accuracy: 0.88635\n",
            "Epoch: 21/30, step: 96/364, loss: 0.28903, accuracy: 0.88574\n",
            "Epoch: 21/30, step: 97/364, loss: 0.28854, accuracy: 0.88595\n",
            "Epoch: 21/30, step: 98/364, loss: 0.28949, accuracy: 0.88520\n",
            "Epoch: 21/30, step: 99/364, loss: 0.28986, accuracy: 0.88526\n",
            "Epoch: 21/30, step: 100/364, loss: 0.28967, accuracy: 0.88547\n",
            "Epoch: 21/30, step: 101/364, loss: 0.28955, accuracy: 0.88567\n",
            "Epoch: 21/30, step: 102/364, loss: 0.28885, accuracy: 0.88603\n",
            "Epoch: 21/30, step: 103/364, loss: 0.28961, accuracy: 0.88592\n",
            "Epoch: 21/30, step: 104/364, loss: 0.28936, accuracy: 0.88612\n",
            "Epoch: 21/30, step: 105/364, loss: 0.28956, accuracy: 0.88601\n",
            "Epoch: 21/30, step: 106/364, loss: 0.28878, accuracy: 0.88650\n",
            "Epoch: 21/30, step: 107/364, loss: 0.29025, accuracy: 0.88566\n",
            "Epoch: 21/30, step: 108/364, loss: 0.28985, accuracy: 0.88542\n",
            "Epoch: 21/30, step: 109/364, loss: 0.28913, accuracy: 0.88575\n",
            "Epoch: 21/30, step: 110/364, loss: 0.28893, accuracy: 0.88565\n",
            "Epoch: 21/30, step: 111/364, loss: 0.28911, accuracy: 0.88570\n",
            "Epoch: 21/30, step: 112/364, loss: 0.28997, accuracy: 0.88518\n",
            "Epoch: 21/30, step: 113/364, loss: 0.29015, accuracy: 0.88551\n",
            "Epoch: 21/30, step: 114/364, loss: 0.29039, accuracy: 0.88569\n",
            "Epoch: 21/30, step: 115/364, loss: 0.29243, accuracy: 0.88478\n",
            "Epoch: 21/30, step: 116/364, loss: 0.29286, accuracy: 0.88483\n",
            "Epoch: 21/30, step: 117/364, loss: 0.29208, accuracy: 0.88528\n",
            "Epoch: 21/30, step: 118/364, loss: 0.29185, accuracy: 0.88546\n",
            "Epoch: 21/30, step: 119/364, loss: 0.29168, accuracy: 0.88564\n",
            "Epoch: 21/30, step: 120/364, loss: 0.29174, accuracy: 0.88568\n",
            "Epoch: 21/30, step: 121/364, loss: 0.29157, accuracy: 0.88559\n",
            "Epoch: 21/30, step: 122/364, loss: 0.29129, accuracy: 0.88589\n",
            "Epoch: 21/30, step: 123/364, loss: 0.29083, accuracy: 0.88618\n",
            "Epoch: 21/30, step: 124/364, loss: 0.29081, accuracy: 0.88621\n",
            "Epoch: 21/30, step: 125/364, loss: 0.29115, accuracy: 0.88613\n",
            "Epoch: 21/30, step: 126/364, loss: 0.29086, accuracy: 0.88604\n",
            "Epoch: 21/30, step: 127/364, loss: 0.29003, accuracy: 0.88656\n",
            "Epoch: 21/30, step: 128/364, loss: 0.28968, accuracy: 0.88672\n",
            "Epoch: 21/30, step: 129/364, loss: 0.29027, accuracy: 0.88639\n",
            "Epoch: 21/30, step: 130/364, loss: 0.29026, accuracy: 0.88666\n",
            "Epoch: 21/30, step: 131/364, loss: 0.29074, accuracy: 0.88645\n",
            "Epoch: 21/30, step: 132/364, loss: 0.29062, accuracy: 0.88672\n",
            "Epoch: 21/30, step: 133/364, loss: 0.29086, accuracy: 0.88651\n",
            "Epoch: 21/30, step: 134/364, loss: 0.29092, accuracy: 0.88608\n",
            "Epoch: 21/30, step: 135/364, loss: 0.29089, accuracy: 0.88600\n",
            "Epoch: 21/30, step: 136/364, loss: 0.29133, accuracy: 0.88580\n",
            "Epoch: 21/30, step: 137/364, loss: 0.29192, accuracy: 0.88515\n",
            "Epoch: 21/30, step: 138/364, loss: 0.29145, accuracy: 0.88542\n",
            "Epoch: 21/30, step: 139/364, loss: 0.29116, accuracy: 0.88590\n",
            "Epoch: 21/30, step: 140/364, loss: 0.29141, accuracy: 0.88549\n",
            "Epoch: 21/30, step: 141/364, loss: 0.29121, accuracy: 0.88542\n",
            "Epoch: 21/30, step: 142/364, loss: 0.29124, accuracy: 0.88545\n",
            "Epoch: 21/30, step: 143/364, loss: 0.29136, accuracy: 0.88549\n",
            "Epoch: 21/30, step: 144/364, loss: 0.29120, accuracy: 0.88574\n",
            "Epoch: 21/30, step: 145/364, loss: 0.29081, accuracy: 0.88599\n",
            "Epoch: 21/30, step: 146/364, loss: 0.29037, accuracy: 0.88624\n",
            "Epoch: 21/30, step: 147/364, loss: 0.29025, accuracy: 0.88616\n",
            "Epoch: 21/30, step: 148/364, loss: 0.28958, accuracy: 0.88682\n",
            "Epoch: 21/30, step: 149/364, loss: 0.29027, accuracy: 0.88622\n",
            "Epoch: 21/30, step: 150/364, loss: 0.29025, accuracy: 0.88635\n",
            "Epoch: 21/30, step: 151/364, loss: 0.29044, accuracy: 0.88618\n",
            "Epoch: 21/30, step: 152/364, loss: 0.29099, accuracy: 0.88610\n",
            "Epoch: 21/30, step: 153/364, loss: 0.29102, accuracy: 0.88623\n",
            "Epoch: 21/30, step: 154/364, loss: 0.29166, accuracy: 0.88586\n",
            "Epoch: 21/30, step: 155/364, loss: 0.29183, accuracy: 0.88569\n",
            "Epoch: 21/30, step: 156/364, loss: 0.29173, accuracy: 0.88592\n",
            "Epoch: 21/30, step: 157/364, loss: 0.29288, accuracy: 0.88525\n",
            "Epoch: 21/30, step: 158/364, loss: 0.29294, accuracy: 0.88519\n",
            "Epoch: 21/30, step: 159/364, loss: 0.29277, accuracy: 0.88532\n",
            "Epoch: 21/30, step: 160/364, loss: 0.29257, accuracy: 0.88535\n",
            "Epoch: 21/30, step: 161/364, loss: 0.29343, accuracy: 0.88480\n",
            "Epoch: 21/30, step: 162/364, loss: 0.29313, accuracy: 0.88484\n",
            "Epoch: 21/30, step: 163/364, loss: 0.29320, accuracy: 0.88459\n",
            "Epoch: 21/30, step: 164/364, loss: 0.29283, accuracy: 0.88472\n",
            "Epoch: 21/30, step: 165/364, loss: 0.29250, accuracy: 0.88475\n",
            "Epoch: 21/30, step: 166/364, loss: 0.29216, accuracy: 0.88498\n",
            "Epoch: 21/30, step: 167/364, loss: 0.29187, accuracy: 0.88520\n",
            "Epoch: 21/30, step: 168/364, loss: 0.29239, accuracy: 0.88514\n",
            "Epoch: 21/30, step: 169/364, loss: 0.29226, accuracy: 0.88526\n",
            "Epoch: 21/30, step: 170/364, loss: 0.29322, accuracy: 0.88474\n",
            "Epoch: 21/30, step: 171/364, loss: 0.29321, accuracy: 0.88478\n",
            "Epoch: 21/30, step: 172/364, loss: 0.29278, accuracy: 0.88490\n",
            "Epoch: 21/30, step: 173/364, loss: 0.29329, accuracy: 0.88430\n",
            "Epoch: 21/30, step: 174/364, loss: 0.29325, accuracy: 0.88434\n",
            "Epoch: 21/30, step: 175/364, loss: 0.29303, accuracy: 0.88437\n",
            "Epoch: 21/30, step: 176/364, loss: 0.29363, accuracy: 0.88370\n",
            "Epoch: 21/30, step: 177/364, loss: 0.29336, accuracy: 0.88365\n",
            "Epoch: 21/30, step: 178/364, loss: 0.29316, accuracy: 0.88360\n",
            "Epoch: 21/30, step: 179/364, loss: 0.29371, accuracy: 0.88329\n",
            "Epoch: 21/30, step: 180/364, loss: 0.29320, accuracy: 0.88359\n",
            "Epoch: 21/30, step: 181/364, loss: 0.29359, accuracy: 0.88320\n",
            "Epoch: 21/30, step: 182/364, loss: 0.29341, accuracy: 0.88333\n",
            "Epoch: 21/30, step: 183/364, loss: 0.29325, accuracy: 0.88345\n",
            "Epoch: 21/30, step: 184/364, loss: 0.29353, accuracy: 0.88341\n",
            "Epoch: 21/30, step: 185/364, loss: 0.29294, accuracy: 0.88378\n",
            "Epoch: 21/30, step: 186/364, loss: 0.29289, accuracy: 0.88390\n",
            "Epoch: 21/30, step: 187/364, loss: 0.29368, accuracy: 0.88344\n",
            "Epoch: 21/30, step: 188/364, loss: 0.29361, accuracy: 0.88356\n",
            "Epoch: 21/30, step: 189/364, loss: 0.29355, accuracy: 0.88368\n",
            "Epoch: 21/30, step: 190/364, loss: 0.29348, accuracy: 0.88355\n",
            "Epoch: 21/30, step: 191/364, loss: 0.29448, accuracy: 0.88294\n",
            "Epoch: 21/30, step: 192/364, loss: 0.29391, accuracy: 0.88330\n",
            "Epoch: 21/30, step: 193/364, loss: 0.29368, accuracy: 0.88342\n",
            "Epoch: 21/30, step: 194/364, loss: 0.29386, accuracy: 0.88313\n",
            "Epoch: 21/30, step: 195/364, loss: 0.29401, accuracy: 0.88293\n",
            "Epoch: 21/30, step: 196/364, loss: 0.29405, accuracy: 0.88281\n",
            "Epoch: 21/30, step: 197/364, loss: 0.29404, accuracy: 0.88277\n",
            "Epoch: 21/30, step: 198/364, loss: 0.29376, accuracy: 0.88297\n",
            "Epoch: 21/30, step: 199/364, loss: 0.29382, accuracy: 0.88285\n",
            "Epoch: 21/30, step: 200/364, loss: 0.29349, accuracy: 0.88305\n",
            "Epoch: 21/30, step: 201/364, loss: 0.29368, accuracy: 0.88308\n",
            "Epoch: 21/30, step: 202/364, loss: 0.29338, accuracy: 0.88335\n",
            "Epoch: 21/30, step: 203/364, loss: 0.29390, accuracy: 0.88308\n",
            "Epoch: 21/30, step: 204/364, loss: 0.29364, accuracy: 0.88320\n",
            "Epoch: 21/30, step: 205/364, loss: 0.29337, accuracy: 0.88323\n",
            "Epoch: 21/30, step: 206/364, loss: 0.29333, accuracy: 0.88312\n",
            "Epoch: 21/30, step: 207/364, loss: 0.29352, accuracy: 0.88300\n",
            "Epoch: 21/30, step: 208/364, loss: 0.29330, accuracy: 0.88319\n",
            "Epoch: 21/30, step: 209/364, loss: 0.29321, accuracy: 0.88322\n",
            "Epoch: 21/30, step: 210/364, loss: 0.29302, accuracy: 0.88318\n",
            "Epoch: 21/30, step: 211/364, loss: 0.29295, accuracy: 0.88322\n",
            "Epoch: 21/30, step: 212/364, loss: 0.29290, accuracy: 0.88325\n",
            "Epoch: 21/30, step: 213/364, loss: 0.29341, accuracy: 0.88285\n",
            "Epoch: 21/30, step: 214/364, loss: 0.29335, accuracy: 0.88289\n",
            "Epoch: 21/30, step: 215/364, loss: 0.29380, accuracy: 0.88270\n",
            "Epoch: 21/30, step: 216/364, loss: 0.29392, accuracy: 0.88267\n",
            "Epoch: 21/30, step: 217/364, loss: 0.29391, accuracy: 0.88263\n",
            "Epoch: 21/30, step: 218/364, loss: 0.29388, accuracy: 0.88267\n",
            "Epoch: 21/30, step: 219/364, loss: 0.29383, accuracy: 0.88263\n",
            "Epoch: 21/30, step: 220/364, loss: 0.29426, accuracy: 0.88253\n",
            "Epoch: 21/30, step: 221/364, loss: 0.29446, accuracy: 0.88242\n",
            "Epoch: 21/30, step: 222/364, loss: 0.29433, accuracy: 0.88246\n",
            "Epoch: 21/30, step: 223/364, loss: 0.29452, accuracy: 0.88236\n",
            "Epoch: 21/30, step: 224/364, loss: 0.29419, accuracy: 0.88253\n",
            "Epoch: 21/30, step: 225/364, loss: 0.29398, accuracy: 0.88271\n",
            "Epoch: 21/30, step: 226/364, loss: 0.29357, accuracy: 0.88295\n",
            "Epoch: 21/30, step: 227/364, loss: 0.29337, accuracy: 0.88298\n",
            "Epoch: 21/30, step: 228/364, loss: 0.29343, accuracy: 0.88274\n",
            "Epoch: 21/30, step: 229/364, loss: 0.29386, accuracy: 0.88264\n",
            "Epoch: 21/30, step: 230/364, loss: 0.29370, accuracy: 0.88261\n",
            "Epoch: 21/30, step: 231/364, loss: 0.29360, accuracy: 0.88271\n",
            "Epoch: 21/30, step: 232/364, loss: 0.29398, accuracy: 0.88248\n",
            "Epoch: 21/30, step: 233/364, loss: 0.29400, accuracy: 0.88231\n",
            "Epoch: 21/30, step: 234/364, loss: 0.29410, accuracy: 0.88228\n",
            "Epoch: 21/30, step: 235/364, loss: 0.29407, accuracy: 0.88231\n",
            "Epoch: 21/30, step: 236/364, loss: 0.29386, accuracy: 0.88242\n",
            "Epoch: 21/30, step: 237/364, loss: 0.29417, accuracy: 0.88205\n",
            "Epoch: 21/30, step: 238/364, loss: 0.29428, accuracy: 0.88189\n",
            "Epoch: 21/30, step: 239/364, loss: 0.29432, accuracy: 0.88180\n",
            "Epoch: 21/30, step: 240/364, loss: 0.29397, accuracy: 0.88197\n",
            "Epoch: 21/30, step: 241/364, loss: 0.29416, accuracy: 0.88187\n",
            "Epoch: 21/30, step: 242/364, loss: 0.29444, accuracy: 0.88165\n",
            "Epoch: 21/30, step: 243/364, loss: 0.29433, accuracy: 0.88188\n",
            "Epoch: 21/30, step: 244/364, loss: 0.29411, accuracy: 0.88204\n",
            "Epoch: 21/30, step: 245/364, loss: 0.29399, accuracy: 0.88227\n",
            "Epoch: 21/30, step: 246/364, loss: 0.29396, accuracy: 0.88230\n",
            "Epoch: 21/30, step: 247/364, loss: 0.29394, accuracy: 0.88227\n",
            "Epoch: 21/30, step: 248/364, loss: 0.29403, accuracy: 0.88225\n",
            "Epoch: 21/30, step: 249/364, loss: 0.29352, accuracy: 0.88253\n",
            "Epoch: 21/30, step: 250/364, loss: 0.29365, accuracy: 0.88269\n",
            "Epoch: 21/30, step: 251/364, loss: 0.29335, accuracy: 0.88297\n",
            "Epoch: 21/30, step: 252/364, loss: 0.29294, accuracy: 0.88325\n",
            "Epoch: 21/30, step: 253/364, loss: 0.29287, accuracy: 0.88346\n",
            "Epoch: 21/30, step: 254/364, loss: 0.29288, accuracy: 0.88355\n",
            "Epoch: 21/30, step: 255/364, loss: 0.29315, accuracy: 0.88327\n",
            "Epoch: 21/30, step: 256/364, loss: 0.29325, accuracy: 0.88287\n",
            "Epoch: 21/30, step: 257/364, loss: 0.29306, accuracy: 0.88290\n",
            "Epoch: 21/30, step: 258/364, loss: 0.29356, accuracy: 0.88239\n",
            "Epoch: 21/30, step: 259/364, loss: 0.29312, accuracy: 0.88260\n",
            "Epoch: 21/30, step: 260/364, loss: 0.29307, accuracy: 0.88239\n",
            "Epoch: 21/30, step: 261/364, loss: 0.29306, accuracy: 0.88236\n",
            "Epoch: 21/30, step: 262/364, loss: 0.29301, accuracy: 0.88240\n",
            "Epoch: 21/30, step: 263/364, loss: 0.29284, accuracy: 0.88249\n",
            "Epoch: 21/30, step: 264/364, loss: 0.29272, accuracy: 0.88263\n",
            "Epoch: 21/30, step: 265/364, loss: 0.29246, accuracy: 0.88267\n",
            "Epoch: 21/30, step: 266/364, loss: 0.29236, accuracy: 0.88281\n",
            "Epoch: 21/30, step: 267/364, loss: 0.29181, accuracy: 0.88313\n",
            "Epoch: 21/30, step: 268/364, loss: 0.29189, accuracy: 0.88328\n",
            "Epoch: 21/30, step: 269/364, loss: 0.29194, accuracy: 0.88313\n",
            "Epoch: 21/30, step: 270/364, loss: 0.29201, accuracy: 0.88310\n",
            "Epoch: 21/30, step: 271/364, loss: 0.29189, accuracy: 0.88324\n",
            "Epoch: 21/30, step: 272/364, loss: 0.29173, accuracy: 0.88327\n",
            "Epoch: 21/30, step: 273/364, loss: 0.29156, accuracy: 0.88324\n",
            "Epoch: 21/30, step: 274/364, loss: 0.29152, accuracy: 0.88327\n",
            "Epoch: 21/30, step: 275/364, loss: 0.29161, accuracy: 0.88324\n",
            "Epoch: 21/30, step: 276/364, loss: 0.29135, accuracy: 0.88338\n",
            "Epoch: 21/30, step: 277/364, loss: 0.29104, accuracy: 0.88363\n",
            "Epoch: 21/30, step: 278/364, loss: 0.29112, accuracy: 0.88371\n",
            "Epoch: 21/30, step: 279/364, loss: 0.29111, accuracy: 0.88368\n",
            "Epoch: 21/30, step: 280/364, loss: 0.29092, accuracy: 0.88376\n",
            "Epoch: 21/30, step: 281/364, loss: 0.29111, accuracy: 0.88362\n",
            "Epoch: 21/30, step: 282/364, loss: 0.29119, accuracy: 0.88375\n",
            "Epoch: 21/30, step: 283/364, loss: 0.29117, accuracy: 0.88361\n",
            "Epoch: 21/30, step: 284/364, loss: 0.29141, accuracy: 0.88336\n",
            "Epoch: 21/30, step: 285/364, loss: 0.29157, accuracy: 0.88317\n",
            "Epoch: 21/30, step: 286/364, loss: 0.29169, accuracy: 0.88303\n",
            "Epoch: 21/30, step: 287/364, loss: 0.29208, accuracy: 0.88268\n",
            "Epoch: 21/30, step: 288/364, loss: 0.29189, accuracy: 0.88276\n",
            "Epoch: 21/30, step: 289/364, loss: 0.29169, accuracy: 0.88289\n",
            "Epoch: 21/30, step: 290/364, loss: 0.29197, accuracy: 0.88265\n",
            "Epoch: 21/30, step: 291/364, loss: 0.29187, accuracy: 0.88280\n",
            "Epoch: 21/30, train loss: 0.29187, train accuracy: 0.88280, valid loss: 0.72833, valid accuracy: 0.68078\n",
            "Epoch: 22/30, step: 1/364, loss: 0.26456, accuracy: 0.89062\n",
            "Epoch: 22/30, step: 2/364, loss: 0.23581, accuracy: 0.91406\n",
            "Epoch: 22/30, step: 3/364, loss: 0.23860, accuracy: 0.91146\n",
            "Epoch: 22/30, step: 4/364, loss: 0.24751, accuracy: 0.90234\n",
            "Epoch: 22/30, step: 5/364, loss: 0.26281, accuracy: 0.90000\n",
            "Epoch: 22/30, step: 6/364, loss: 0.28060, accuracy: 0.89323\n",
            "Epoch: 22/30, step: 7/364, loss: 0.28862, accuracy: 0.88393\n",
            "Epoch: 22/30, step: 8/364, loss: 0.28315, accuracy: 0.88867\n",
            "Epoch: 22/30, step: 9/364, loss: 0.27219, accuracy: 0.89931\n",
            "Epoch: 22/30, step: 10/364, loss: 0.26655, accuracy: 0.90312\n",
            "Epoch: 22/30, step: 11/364, loss: 0.26138, accuracy: 0.90625\n",
            "Epoch: 22/30, step: 12/364, loss: 0.26895, accuracy: 0.89974\n",
            "Epoch: 22/30, step: 13/364, loss: 0.27363, accuracy: 0.89663\n",
            "Epoch: 22/30, step: 14/364, loss: 0.27610, accuracy: 0.89621\n",
            "Epoch: 22/30, step: 15/364, loss: 0.27811, accuracy: 0.89479\n",
            "Epoch: 22/30, step: 16/364, loss: 0.28594, accuracy: 0.89160\n",
            "Epoch: 22/30, step: 17/364, loss: 0.29326, accuracy: 0.88787\n",
            "Epoch: 22/30, step: 18/364, loss: 0.28760, accuracy: 0.89236\n",
            "Epoch: 22/30, step: 19/364, loss: 0.29034, accuracy: 0.88898\n",
            "Epoch: 22/30, step: 20/364, loss: 0.28962, accuracy: 0.88984\n",
            "Epoch: 22/30, step: 21/364, loss: 0.28715, accuracy: 0.89062\n",
            "Epoch: 22/30, step: 22/364, loss: 0.28467, accuracy: 0.89062\n",
            "Epoch: 22/30, step: 23/364, loss: 0.28544, accuracy: 0.88859\n",
            "Epoch: 22/30, step: 24/364, loss: 0.28583, accuracy: 0.88802\n",
            "Epoch: 22/30, step: 25/364, loss: 0.28172, accuracy: 0.88937\n",
            "Epoch: 22/30, step: 26/364, loss: 0.28369, accuracy: 0.88882\n",
            "Epoch: 22/30, step: 27/364, loss: 0.28264, accuracy: 0.88947\n",
            "Epoch: 22/30, step: 28/364, loss: 0.28125, accuracy: 0.89007\n",
            "Epoch: 22/30, step: 29/364, loss: 0.27883, accuracy: 0.89116\n",
            "Epoch: 22/30, step: 30/364, loss: 0.27748, accuracy: 0.89167\n",
            "Epoch: 22/30, step: 31/364, loss: 0.28097, accuracy: 0.88911\n",
            "Epoch: 22/30, step: 32/364, loss: 0.28028, accuracy: 0.89111\n",
            "Epoch: 22/30, step: 33/364, loss: 0.28184, accuracy: 0.89015\n",
            "Epoch: 22/30, step: 34/364, loss: 0.28065, accuracy: 0.89200\n",
            "Epoch: 22/30, step: 35/364, loss: 0.28018, accuracy: 0.89196\n",
            "Epoch: 22/30, step: 36/364, loss: 0.28077, accuracy: 0.89149\n",
            "Epoch: 22/30, step: 37/364, loss: 0.28073, accuracy: 0.89147\n",
            "Epoch: 22/30, step: 38/364, loss: 0.28150, accuracy: 0.89186\n",
            "Epoch: 22/30, step: 39/364, loss: 0.28281, accuracy: 0.89143\n",
            "Epoch: 22/30, step: 40/364, loss: 0.28177, accuracy: 0.89141\n",
            "Epoch: 22/30, step: 41/364, loss: 0.27960, accuracy: 0.89329\n",
            "Epoch: 22/30, step: 42/364, loss: 0.27994, accuracy: 0.89323\n",
            "Epoch: 22/30, step: 43/364, loss: 0.28025, accuracy: 0.89353\n",
            "Epoch: 22/30, step: 44/364, loss: 0.28099, accuracy: 0.89347\n",
            "Epoch: 22/30, step: 45/364, loss: 0.27891, accuracy: 0.89479\n",
            "Epoch: 22/30, step: 46/364, loss: 0.27931, accuracy: 0.89470\n",
            "Epoch: 22/30, step: 47/364, loss: 0.27796, accuracy: 0.89461\n",
            "Epoch: 22/30, step: 48/364, loss: 0.27846, accuracy: 0.89388\n",
            "Epoch: 22/30, step: 49/364, loss: 0.27953, accuracy: 0.89222\n",
            "Epoch: 22/30, step: 50/364, loss: 0.27915, accuracy: 0.89312\n",
            "Epoch: 22/30, step: 51/364, loss: 0.27759, accuracy: 0.89369\n",
            "Epoch: 22/30, step: 52/364, loss: 0.27805, accuracy: 0.89333\n",
            "Epoch: 22/30, step: 53/364, loss: 0.27681, accuracy: 0.89416\n",
            "Epoch: 22/30, step: 54/364, loss: 0.27607, accuracy: 0.89497\n",
            "Epoch: 22/30, step: 55/364, loss: 0.27565, accuracy: 0.89489\n",
            "Epoch: 22/30, step: 56/364, loss: 0.27362, accuracy: 0.89621\n",
            "Epoch: 22/30, step: 57/364, loss: 0.27344, accuracy: 0.89666\n",
            "Epoch: 22/30, step: 58/364, loss: 0.27306, accuracy: 0.89655\n",
            "Epoch: 22/30, step: 59/364, loss: 0.27288, accuracy: 0.89645\n",
            "Epoch: 22/30, step: 60/364, loss: 0.27126, accuracy: 0.89766\n",
            "Epoch: 22/30, step: 61/364, loss: 0.27073, accuracy: 0.89805\n",
            "Epoch: 22/30, step: 62/364, loss: 0.27099, accuracy: 0.89793\n",
            "Epoch: 22/30, step: 63/364, loss: 0.27051, accuracy: 0.89831\n",
            "Epoch: 22/30, step: 64/364, loss: 0.27013, accuracy: 0.89795\n",
            "Epoch: 22/30, step: 65/364, loss: 0.26906, accuracy: 0.89856\n",
            "Epoch: 22/30, step: 66/364, loss: 0.26900, accuracy: 0.89891\n",
            "Epoch: 22/30, step: 67/364, loss: 0.26878, accuracy: 0.89879\n",
            "Epoch: 22/30, step: 68/364, loss: 0.26864, accuracy: 0.89867\n",
            "Epoch: 22/30, step: 69/364, loss: 0.26753, accuracy: 0.89946\n",
            "Epoch: 22/30, step: 70/364, loss: 0.26772, accuracy: 0.89911\n",
            "Epoch: 22/30, step: 71/364, loss: 0.26801, accuracy: 0.89899\n",
            "Epoch: 22/30, step: 72/364, loss: 0.26807, accuracy: 0.89822\n",
            "Epoch: 22/30, step: 73/364, loss: 0.26946, accuracy: 0.89726\n",
            "Epoch: 22/30, step: 74/364, loss: 0.26846, accuracy: 0.89802\n",
            "Epoch: 22/30, step: 75/364, loss: 0.26897, accuracy: 0.89708\n",
            "Epoch: 22/30, step: 76/364, loss: 0.26849, accuracy: 0.89720\n",
            "Epoch: 22/30, step: 77/364, loss: 0.26783, accuracy: 0.89752\n",
            "Epoch: 22/30, step: 78/364, loss: 0.26740, accuracy: 0.89744\n",
            "Epoch: 22/30, step: 79/364, loss: 0.26968, accuracy: 0.89636\n",
            "Epoch: 22/30, step: 80/364, loss: 0.27083, accuracy: 0.89570\n",
            "Epoch: 22/30, step: 81/364, loss: 0.27127, accuracy: 0.89545\n",
            "Epoch: 22/30, step: 82/364, loss: 0.27175, accuracy: 0.89482\n",
            "Epoch: 22/30, step: 83/364, loss: 0.27233, accuracy: 0.89477\n",
            "Epoch: 22/30, step: 84/364, loss: 0.27188, accuracy: 0.89472\n",
            "Epoch: 22/30, step: 85/364, loss: 0.27172, accuracy: 0.89504\n",
            "Epoch: 22/30, step: 86/364, loss: 0.27106, accuracy: 0.89553\n",
            "Epoch: 22/30, step: 87/364, loss: 0.26976, accuracy: 0.89637\n",
            "Epoch: 22/30, step: 88/364, loss: 0.26970, accuracy: 0.89595\n",
            "Epoch: 22/30, step: 89/364, loss: 0.27083, accuracy: 0.89572\n",
            "Epoch: 22/30, step: 90/364, loss: 0.27099, accuracy: 0.89566\n",
            "Epoch: 22/30, step: 91/364, loss: 0.27056, accuracy: 0.89578\n",
            "Epoch: 22/30, step: 92/364, loss: 0.27150, accuracy: 0.89521\n",
            "Epoch: 22/30, step: 93/364, loss: 0.27250, accuracy: 0.89415\n",
            "Epoch: 22/30, step: 94/364, loss: 0.27242, accuracy: 0.89395\n",
            "Epoch: 22/30, step: 95/364, loss: 0.27272, accuracy: 0.89391\n",
            "Epoch: 22/30, step: 96/364, loss: 0.27348, accuracy: 0.89323\n",
            "Epoch: 22/30, step: 97/364, loss: 0.27388, accuracy: 0.89272\n",
            "Epoch: 22/30, step: 98/364, loss: 0.27508, accuracy: 0.89238\n",
            "Epoch: 22/30, step: 99/364, loss: 0.27493, accuracy: 0.89283\n",
            "Epoch: 22/30, step: 100/364, loss: 0.27433, accuracy: 0.89312\n",
            "Epoch: 22/30, step: 101/364, loss: 0.27375, accuracy: 0.89356\n",
            "Epoch: 22/30, step: 102/364, loss: 0.27304, accuracy: 0.89400\n",
            "Epoch: 22/30, step: 103/364, loss: 0.27376, accuracy: 0.89366\n",
            "Epoch: 22/30, step: 104/364, loss: 0.27444, accuracy: 0.89348\n",
            "Epoch: 22/30, step: 105/364, loss: 0.27467, accuracy: 0.89360\n",
            "Epoch: 22/30, step: 106/364, loss: 0.27494, accuracy: 0.89387\n",
            "Epoch: 22/30, step: 107/364, loss: 0.27610, accuracy: 0.89296\n",
            "Epoch: 22/30, step: 108/364, loss: 0.27539, accuracy: 0.89381\n",
            "Epoch: 22/30, step: 109/364, loss: 0.27531, accuracy: 0.89392\n",
            "Epoch: 22/30, step: 110/364, loss: 0.27481, accuracy: 0.89432\n",
            "Epoch: 22/30, step: 111/364, loss: 0.27488, accuracy: 0.89443\n",
            "Epoch: 22/30, step: 112/364, loss: 0.27475, accuracy: 0.89453\n",
            "Epoch: 22/30, step: 113/364, loss: 0.27544, accuracy: 0.89422\n",
            "Epoch: 22/30, step: 114/364, loss: 0.27599, accuracy: 0.89391\n",
            "Epoch: 22/30, step: 115/364, loss: 0.27595, accuracy: 0.89389\n",
            "Epoch: 22/30, step: 116/364, loss: 0.27604, accuracy: 0.89440\n",
            "Epoch: 22/30, step: 117/364, loss: 0.27557, accuracy: 0.89490\n",
            "Epoch: 22/30, step: 118/364, loss: 0.27559, accuracy: 0.89460\n",
            "Epoch: 22/30, step: 119/364, loss: 0.27626, accuracy: 0.89430\n",
            "Epoch: 22/30, step: 120/364, loss: 0.27663, accuracy: 0.89414\n",
            "Epoch: 22/30, step: 121/364, loss: 0.27586, accuracy: 0.89476\n",
            "Epoch: 22/30, step: 122/364, loss: 0.27596, accuracy: 0.89472\n",
            "Epoch: 22/30, step: 123/364, loss: 0.27531, accuracy: 0.89507\n",
            "Epoch: 22/30, step: 124/364, loss: 0.27454, accuracy: 0.89554\n",
            "Epoch: 22/30, step: 125/364, loss: 0.27454, accuracy: 0.89538\n",
            "Epoch: 22/30, step: 126/364, loss: 0.27392, accuracy: 0.89596\n",
            "Epoch: 22/30, step: 127/364, loss: 0.27575, accuracy: 0.89481\n",
            "Epoch: 22/30, step: 128/364, loss: 0.27593, accuracy: 0.89417\n",
            "Epoch: 22/30, step: 129/364, loss: 0.27627, accuracy: 0.89377\n",
            "Epoch: 22/30, step: 130/364, loss: 0.27710, accuracy: 0.89327\n",
            "Epoch: 22/30, step: 131/364, loss: 0.27650, accuracy: 0.89385\n",
            "Epoch: 22/30, step: 132/364, loss: 0.27627, accuracy: 0.89382\n",
            "Epoch: 22/30, step: 133/364, loss: 0.27739, accuracy: 0.89286\n",
            "Epoch: 22/30, step: 134/364, loss: 0.27731, accuracy: 0.89272\n",
            "Epoch: 22/30, step: 135/364, loss: 0.27707, accuracy: 0.89294\n",
            "Epoch: 22/30, step: 136/364, loss: 0.27622, accuracy: 0.89350\n",
            "Epoch: 22/30, step: 137/364, loss: 0.27599, accuracy: 0.89336\n",
            "Epoch: 22/30, step: 138/364, loss: 0.27640, accuracy: 0.89300\n",
            "Epoch: 22/30, step: 139/364, loss: 0.27599, accuracy: 0.89310\n",
            "Epoch: 22/30, step: 140/364, loss: 0.27565, accuracy: 0.89319\n",
            "Epoch: 22/30, step: 141/364, loss: 0.27580, accuracy: 0.89306\n",
            "Epoch: 22/30, step: 142/364, loss: 0.27561, accuracy: 0.89316\n",
            "Epoch: 22/30, step: 143/364, loss: 0.27506, accuracy: 0.89358\n",
            "Epoch: 22/30, step: 144/364, loss: 0.27520, accuracy: 0.89345\n",
            "Epoch: 22/30, step: 145/364, loss: 0.27492, accuracy: 0.89353\n",
            "Epoch: 22/30, step: 146/364, loss: 0.27453, accuracy: 0.89373\n",
            "Epoch: 22/30, step: 147/364, loss: 0.27497, accuracy: 0.89349\n",
            "Epoch: 22/30, step: 148/364, loss: 0.27520, accuracy: 0.89305\n",
            "Epoch: 22/30, step: 149/364, loss: 0.27498, accuracy: 0.89325\n",
            "Epoch: 22/30, step: 150/364, loss: 0.27524, accuracy: 0.89333\n",
            "Epoch: 22/30, step: 151/364, loss: 0.27529, accuracy: 0.89300\n",
            "Epoch: 22/30, step: 152/364, loss: 0.27495, accuracy: 0.89319\n",
            "Epoch: 22/30, step: 153/364, loss: 0.27506, accuracy: 0.89328\n",
            "Epoch: 22/30, step: 154/364, loss: 0.27551, accuracy: 0.89296\n",
            "Epoch: 22/30, step: 155/364, loss: 0.27504, accuracy: 0.89335\n",
            "Epoch: 22/30, step: 156/364, loss: 0.27465, accuracy: 0.89353\n",
            "Epoch: 22/30, step: 157/364, loss: 0.27423, accuracy: 0.89381\n",
            "Epoch: 22/30, step: 158/364, loss: 0.27369, accuracy: 0.89409\n",
            "Epoch: 22/30, step: 159/364, loss: 0.27407, accuracy: 0.89426\n",
            "Epoch: 22/30, step: 160/364, loss: 0.27401, accuracy: 0.89414\n",
            "Epoch: 22/30, step: 161/364, loss: 0.27368, accuracy: 0.89441\n",
            "Epoch: 22/30, step: 162/364, loss: 0.27447, accuracy: 0.89323\n",
            "Epoch: 22/30, step: 163/364, loss: 0.27399, accuracy: 0.89340\n",
            "Epoch: 22/30, step: 164/364, loss: 0.27454, accuracy: 0.89320\n",
            "Epoch: 22/30, step: 165/364, loss: 0.27466, accuracy: 0.89290\n",
            "Epoch: 22/30, step: 166/364, loss: 0.27458, accuracy: 0.89288\n",
            "Epoch: 22/30, step: 167/364, loss: 0.27442, accuracy: 0.89278\n",
            "Epoch: 22/30, step: 168/364, loss: 0.27510, accuracy: 0.89239\n",
            "Epoch: 22/30, step: 169/364, loss: 0.27532, accuracy: 0.89220\n",
            "Epoch: 22/30, step: 170/364, loss: 0.27509, accuracy: 0.89228\n",
            "Epoch: 22/30, step: 171/364, loss: 0.27495, accuracy: 0.89227\n",
            "Epoch: 22/30, step: 172/364, loss: 0.27466, accuracy: 0.89244\n",
            "Epoch: 22/30, step: 173/364, loss: 0.27497, accuracy: 0.89225\n",
            "Epoch: 22/30, step: 174/364, loss: 0.27488, accuracy: 0.89224\n",
            "Epoch: 22/30, step: 175/364, loss: 0.27492, accuracy: 0.89223\n",
            "Epoch: 22/30, step: 176/364, loss: 0.27495, accuracy: 0.89231\n",
            "Epoch: 22/30, step: 177/364, loss: 0.27507, accuracy: 0.89221\n",
            "Epoch: 22/30, step: 178/364, loss: 0.27533, accuracy: 0.89203\n",
            "Epoch: 22/30, step: 179/364, loss: 0.27537, accuracy: 0.89193\n",
            "Epoch: 22/30, step: 180/364, loss: 0.27553, accuracy: 0.89201\n",
            "Epoch: 22/30, step: 181/364, loss: 0.27574, accuracy: 0.89166\n",
            "Epoch: 22/30, step: 182/364, loss: 0.27536, accuracy: 0.89191\n",
            "Epoch: 22/30, step: 183/364, loss: 0.27534, accuracy: 0.89165\n",
            "Epoch: 22/30, step: 184/364, loss: 0.27559, accuracy: 0.89147\n",
            "Epoch: 22/30, step: 185/364, loss: 0.27578, accuracy: 0.89147\n",
            "Epoch: 22/30, step: 186/364, loss: 0.27530, accuracy: 0.89180\n",
            "Epoch: 22/30, step: 187/364, loss: 0.27565, accuracy: 0.89163\n",
            "Epoch: 22/30, step: 188/364, loss: 0.27557, accuracy: 0.89146\n",
            "Epoch: 22/30, step: 189/364, loss: 0.27554, accuracy: 0.89145\n",
            "Epoch: 22/30, step: 190/364, loss: 0.27511, accuracy: 0.89178\n",
            "Epoch: 22/30, step: 191/364, loss: 0.27472, accuracy: 0.89193\n",
            "Epoch: 22/30, step: 192/364, loss: 0.27520, accuracy: 0.89152\n",
            "Epoch: 22/30, step: 193/364, loss: 0.27559, accuracy: 0.89127\n",
            "Epoch: 22/30, step: 194/364, loss: 0.27537, accuracy: 0.89143\n",
            "Epoch: 22/30, step: 195/364, loss: 0.27582, accuracy: 0.89127\n",
            "Epoch: 22/30, step: 196/364, loss: 0.27566, accuracy: 0.89126\n",
            "Epoch: 22/30, step: 197/364, loss: 0.27563, accuracy: 0.89118\n",
            "Epoch: 22/30, step: 198/364, loss: 0.27530, accuracy: 0.89141\n",
            "Epoch: 22/30, step: 199/364, loss: 0.27555, accuracy: 0.89102\n",
            "Epoch: 22/30, step: 200/364, loss: 0.27580, accuracy: 0.89078\n",
            "Epoch: 22/30, step: 201/364, loss: 0.27553, accuracy: 0.89109\n",
            "Epoch: 22/30, step: 202/364, loss: 0.27511, accuracy: 0.89148\n",
            "Epoch: 22/30, step: 203/364, loss: 0.27526, accuracy: 0.89147\n",
            "Epoch: 22/30, step: 204/364, loss: 0.27477, accuracy: 0.89185\n",
            "Epoch: 22/30, step: 205/364, loss: 0.27496, accuracy: 0.89192\n",
            "Epoch: 22/30, step: 206/364, loss: 0.27503, accuracy: 0.89214\n",
            "Epoch: 22/30, step: 207/364, loss: 0.27531, accuracy: 0.89198\n",
            "Epoch: 22/30, step: 208/364, loss: 0.27526, accuracy: 0.89190\n",
            "Epoch: 22/30, step: 209/364, loss: 0.27546, accuracy: 0.89190\n",
            "Epoch: 22/30, step: 210/364, loss: 0.27531, accuracy: 0.89196\n",
            "Epoch: 22/30, step: 211/364, loss: 0.27500, accuracy: 0.89218\n",
            "Epoch: 22/30, step: 212/364, loss: 0.27501, accuracy: 0.89225\n",
            "Epoch: 22/30, step: 213/364, loss: 0.27479, accuracy: 0.89231\n",
            "Epoch: 22/30, step: 214/364, loss: 0.27480, accuracy: 0.89223\n",
            "Epoch: 22/30, step: 215/364, loss: 0.27475, accuracy: 0.89222\n",
            "Epoch: 22/30, step: 216/364, loss: 0.27496, accuracy: 0.89214\n",
            "Epoch: 22/30, step: 217/364, loss: 0.27572, accuracy: 0.89142\n",
            "Epoch: 22/30, step: 218/364, loss: 0.27538, accuracy: 0.89163\n",
            "Epoch: 22/30, step: 219/364, loss: 0.27519, accuracy: 0.89177\n",
            "Epoch: 22/30, step: 220/364, loss: 0.27523, accuracy: 0.89169\n",
            "Epoch: 22/30, step: 221/364, loss: 0.27532, accuracy: 0.89161\n",
            "Epoch: 22/30, step: 222/364, loss: 0.27518, accuracy: 0.89168\n",
            "Epoch: 22/30, step: 223/364, loss: 0.27520, accuracy: 0.89161\n",
            "Epoch: 22/30, step: 224/364, loss: 0.27520, accuracy: 0.89153\n",
            "Epoch: 22/30, step: 225/364, loss: 0.27466, accuracy: 0.89181\n",
            "Epoch: 22/30, step: 226/364, loss: 0.27471, accuracy: 0.89173\n",
            "Epoch: 22/30, step: 227/364, loss: 0.27419, accuracy: 0.89214\n",
            "Epoch: 22/30, step: 228/364, loss: 0.27414, accuracy: 0.89220\n",
            "Epoch: 22/30, step: 229/364, loss: 0.27363, accuracy: 0.89254\n",
            "Epoch: 22/30, step: 230/364, loss: 0.27363, accuracy: 0.89253\n",
            "Epoch: 22/30, step: 231/364, loss: 0.27406, accuracy: 0.89238\n",
            "Epoch: 22/30, step: 232/364, loss: 0.27422, accuracy: 0.89251\n",
            "Epoch: 22/30, step: 233/364, loss: 0.27442, accuracy: 0.89230\n",
            "Epoch: 22/30, step: 234/364, loss: 0.27407, accuracy: 0.89263\n",
            "Epoch: 22/30, step: 235/364, loss: 0.27386, accuracy: 0.89262\n",
            "Epoch: 22/30, step: 236/364, loss: 0.27392, accuracy: 0.89255\n",
            "Epoch: 22/30, step: 237/364, loss: 0.27411, accuracy: 0.89247\n",
            "Epoch: 22/30, step: 238/364, loss: 0.27360, accuracy: 0.89286\n",
            "Epoch: 22/30, step: 239/364, loss: 0.27395, accuracy: 0.89252\n",
            "Epoch: 22/30, step: 240/364, loss: 0.27390, accuracy: 0.89251\n",
            "Epoch: 22/30, step: 241/364, loss: 0.27367, accuracy: 0.89263\n",
            "Epoch: 22/30, step: 242/364, loss: 0.27411, accuracy: 0.89237\n",
            "Epoch: 22/30, step: 243/364, loss: 0.27435, accuracy: 0.89217\n",
            "Epoch: 22/30, step: 244/364, loss: 0.27444, accuracy: 0.89197\n",
            "Epoch: 22/30, step: 245/364, loss: 0.27445, accuracy: 0.89184\n",
            "Epoch: 22/30, step: 246/364, loss: 0.27435, accuracy: 0.89183\n",
            "Epoch: 22/30, step: 247/364, loss: 0.27431, accuracy: 0.89189\n",
            "Epoch: 22/30, step: 248/364, loss: 0.27412, accuracy: 0.89195\n",
            "Epoch: 22/30, step: 249/364, loss: 0.27409, accuracy: 0.89207\n",
            "Epoch: 22/30, step: 250/364, loss: 0.27397, accuracy: 0.89213\n",
            "Epoch: 22/30, step: 251/364, loss: 0.27393, accuracy: 0.89224\n",
            "Epoch: 22/30, step: 252/364, loss: 0.27390, accuracy: 0.89230\n",
            "Epoch: 22/30, step: 253/364, loss: 0.27401, accuracy: 0.89217\n",
            "Epoch: 22/30, step: 254/364, loss: 0.27458, accuracy: 0.89179\n",
            "Epoch: 22/30, step: 255/364, loss: 0.27468, accuracy: 0.89167\n",
            "Epoch: 22/30, step: 256/364, loss: 0.27479, accuracy: 0.89166\n",
            "Epoch: 22/30, step: 257/364, loss: 0.27439, accuracy: 0.89196\n",
            "Epoch: 22/30, step: 258/364, loss: 0.27404, accuracy: 0.89214\n",
            "Epoch: 22/30, step: 259/364, loss: 0.27386, accuracy: 0.89225\n",
            "Epoch: 22/30, step: 260/364, loss: 0.27399, accuracy: 0.89219\n",
            "Epoch: 22/30, step: 261/364, loss: 0.27394, accuracy: 0.89230\n",
            "Epoch: 22/30, step: 262/364, loss: 0.27389, accuracy: 0.89235\n",
            "Epoch: 22/30, step: 263/364, loss: 0.27376, accuracy: 0.89247\n",
            "Epoch: 22/30, step: 264/364, loss: 0.27348, accuracy: 0.89276\n",
            "Epoch: 22/30, step: 265/364, loss: 0.27324, accuracy: 0.89281\n",
            "Epoch: 22/30, step: 266/364, loss: 0.27365, accuracy: 0.89250\n",
            "Epoch: 22/30, step: 267/364, loss: 0.27386, accuracy: 0.89244\n",
            "Epoch: 22/30, step: 268/364, loss: 0.27355, accuracy: 0.89261\n",
            "Epoch: 22/30, step: 269/364, loss: 0.27358, accuracy: 0.89254\n",
            "Epoch: 22/30, step: 270/364, loss: 0.27356, accuracy: 0.89242\n",
            "Epoch: 22/30, step: 271/364, loss: 0.27314, accuracy: 0.89270\n",
            "Epoch: 22/30, step: 272/364, loss: 0.27332, accuracy: 0.89264\n",
            "Epoch: 22/30, step: 273/364, loss: 0.27298, accuracy: 0.89291\n",
            "Epoch: 22/30, step: 274/364, loss: 0.27302, accuracy: 0.89285\n",
            "Epoch: 22/30, step: 275/364, loss: 0.27299, accuracy: 0.89278\n",
            "Epoch: 22/30, step: 276/364, loss: 0.27304, accuracy: 0.89283\n",
            "Epoch: 22/30, step: 277/364, loss: 0.27277, accuracy: 0.89305\n",
            "Epoch: 22/30, step: 278/364, loss: 0.27331, accuracy: 0.89259\n",
            "Epoch: 22/30, step: 279/364, loss: 0.27333, accuracy: 0.89264\n",
            "Epoch: 22/30, step: 280/364, loss: 0.27361, accuracy: 0.89230\n",
            "Epoch: 22/30, step: 281/364, loss: 0.27374, accuracy: 0.89229\n",
            "Epoch: 22/30, step: 282/364, loss: 0.27358, accuracy: 0.89245\n",
            "Epoch: 22/30, step: 283/364, loss: 0.27375, accuracy: 0.89239\n",
            "Epoch: 22/30, step: 284/364, loss: 0.27359, accuracy: 0.89261\n",
            "Epoch: 22/30, step: 285/364, loss: 0.27351, accuracy: 0.89249\n",
            "Epoch: 22/30, step: 286/364, loss: 0.27320, accuracy: 0.89265\n",
            "Epoch: 22/30, step: 287/364, loss: 0.27324, accuracy: 0.89258\n",
            "Epoch: 22/30, step: 288/364, loss: 0.27345, accuracy: 0.89231\n",
            "Epoch: 22/30, step: 289/364, loss: 0.27334, accuracy: 0.89236\n",
            "Epoch: 22/30, step: 290/364, loss: 0.27310, accuracy: 0.89251\n",
            "Epoch: 22/30, step: 291/364, loss: 0.27299, accuracy: 0.89258\n",
            "Epoch: 22/30, train loss: 0.27299, train accuracy: 0.89258, valid loss: 0.70251, valid accuracy: 0.69347\n",
            "Epoch: 23/30, step: 1/364, loss: 0.20542, accuracy: 0.90625\n",
            "Epoch: 23/30, step: 2/364, loss: 0.28927, accuracy: 0.86719\n",
            "Epoch: 23/30, step: 3/364, loss: 0.25521, accuracy: 0.89062\n",
            "Epoch: 23/30, step: 4/364, loss: 0.26716, accuracy: 0.88281\n",
            "Epoch: 23/30, step: 5/364, loss: 0.27442, accuracy: 0.88125\n",
            "Epoch: 23/30, step: 6/364, loss: 0.28329, accuracy: 0.87500\n",
            "Epoch: 23/30, step: 7/364, loss: 0.27533, accuracy: 0.87946\n",
            "Epoch: 23/30, step: 8/364, loss: 0.27693, accuracy: 0.87891\n",
            "Epoch: 23/30, step: 9/364, loss: 0.26719, accuracy: 0.88542\n",
            "Epoch: 23/30, step: 10/364, loss: 0.27048, accuracy: 0.88594\n",
            "Epoch: 23/30, step: 11/364, loss: 0.26585, accuracy: 0.89062\n",
            "Epoch: 23/30, step: 12/364, loss: 0.26328, accuracy: 0.89583\n",
            "Epoch: 23/30, step: 13/364, loss: 0.25982, accuracy: 0.90024\n",
            "Epoch: 23/30, step: 14/364, loss: 0.25497, accuracy: 0.90290\n",
            "Epoch: 23/30, step: 15/364, loss: 0.25321, accuracy: 0.90312\n",
            "Epoch: 23/30, step: 16/364, loss: 0.25677, accuracy: 0.90137\n",
            "Epoch: 23/30, step: 17/364, loss: 0.24903, accuracy: 0.90625\n",
            "Epoch: 23/30, step: 18/364, loss: 0.25545, accuracy: 0.90104\n",
            "Epoch: 23/30, step: 19/364, loss: 0.25700, accuracy: 0.89967\n",
            "Epoch: 23/30, step: 20/364, loss: 0.25608, accuracy: 0.90000\n",
            "Epoch: 23/30, step: 21/364, loss: 0.25335, accuracy: 0.90179\n",
            "Epoch: 23/30, step: 22/364, loss: 0.25128, accuracy: 0.90270\n",
            "Epoch: 23/30, step: 23/364, loss: 0.25045, accuracy: 0.90217\n",
            "Epoch: 23/30, step: 24/364, loss: 0.24826, accuracy: 0.90365\n",
            "Epoch: 23/30, step: 25/364, loss: 0.25407, accuracy: 0.90125\n",
            "Epoch: 23/30, step: 26/364, loss: 0.25267, accuracy: 0.90264\n",
            "Epoch: 23/30, step: 27/364, loss: 0.25657, accuracy: 0.89931\n",
            "Epoch: 23/30, step: 28/364, loss: 0.25617, accuracy: 0.90011\n",
            "Epoch: 23/30, step: 29/364, loss: 0.25693, accuracy: 0.89978\n",
            "Epoch: 23/30, step: 30/364, loss: 0.26039, accuracy: 0.89896\n",
            "Epoch: 23/30, step: 31/364, loss: 0.26478, accuracy: 0.89768\n",
            "Epoch: 23/30, step: 32/364, loss: 0.26273, accuracy: 0.89844\n",
            "Epoch: 23/30, step: 33/364, loss: 0.26101, accuracy: 0.89915\n",
            "Epoch: 23/30, step: 34/364, loss: 0.26017, accuracy: 0.90028\n",
            "Epoch: 23/30, step: 35/364, loss: 0.26417, accuracy: 0.89866\n",
            "Epoch: 23/30, step: 36/364, loss: 0.26459, accuracy: 0.89714\n",
            "Epoch: 23/30, step: 37/364, loss: 0.26222, accuracy: 0.89907\n",
            "Epoch: 23/30, step: 38/364, loss: 0.26212, accuracy: 0.89844\n",
            "Epoch: 23/30, step: 39/364, loss: 0.26073, accuracy: 0.89904\n",
            "Epoch: 23/30, step: 40/364, loss: 0.25916, accuracy: 0.90000\n",
            "Epoch: 23/30, step: 41/364, loss: 0.26086, accuracy: 0.89901\n",
            "Epoch: 23/30, step: 42/364, loss: 0.25895, accuracy: 0.90030\n",
            "Epoch: 23/30, step: 43/364, loss: 0.25916, accuracy: 0.89935\n",
            "Epoch: 23/30, step: 44/364, loss: 0.25818, accuracy: 0.89986\n",
            "Epoch: 23/30, step: 45/364, loss: 0.25918, accuracy: 0.89931\n",
            "Epoch: 23/30, step: 46/364, loss: 0.25736, accuracy: 0.90014\n",
            "Epoch: 23/30, step: 47/364, loss: 0.25563, accuracy: 0.90126\n",
            "Epoch: 23/30, step: 48/364, loss: 0.25527, accuracy: 0.90202\n",
            "Epoch: 23/30, step: 49/364, loss: 0.25402, accuracy: 0.90210\n",
            "Epoch: 23/30, step: 50/364, loss: 0.25446, accuracy: 0.90188\n",
            "Epoch: 23/30, step: 51/364, loss: 0.25402, accuracy: 0.90196\n",
            "Epoch: 23/30, step: 52/364, loss: 0.25324, accuracy: 0.90294\n",
            "Epoch: 23/30, step: 53/364, loss: 0.25158, accuracy: 0.90389\n",
            "Epoch: 23/30, step: 54/364, loss: 0.25072, accuracy: 0.90509\n",
            "Epoch: 23/30, step: 55/364, loss: 0.25080, accuracy: 0.90483\n",
            "Epoch: 23/30, step: 56/364, loss: 0.25050, accuracy: 0.90485\n",
            "Epoch: 23/30, step: 57/364, loss: 0.24957, accuracy: 0.90625\n",
            "Epoch: 23/30, step: 58/364, loss: 0.25094, accuracy: 0.90517\n",
            "Epoch: 23/30, step: 59/364, loss: 0.25025, accuracy: 0.90546\n",
            "Epoch: 23/30, step: 60/364, loss: 0.24993, accuracy: 0.90625\n",
            "Epoch: 23/30, step: 61/364, loss: 0.25108, accuracy: 0.90497\n",
            "Epoch: 23/30, step: 62/364, loss: 0.25058, accuracy: 0.90524\n",
            "Epoch: 23/30, step: 63/364, loss: 0.25147, accuracy: 0.90451\n",
            "Epoch: 23/30, step: 64/364, loss: 0.25084, accuracy: 0.90503\n",
            "Epoch: 23/30, step: 65/364, loss: 0.25048, accuracy: 0.90505\n",
            "Epoch: 23/30, step: 66/364, loss: 0.25049, accuracy: 0.90459\n",
            "Epoch: 23/30, step: 67/364, loss: 0.24994, accuracy: 0.90485\n",
            "Epoch: 23/30, step: 68/364, loss: 0.25186, accuracy: 0.90349\n",
            "Epoch: 23/30, step: 69/364, loss: 0.25126, accuracy: 0.90353\n",
            "Epoch: 23/30, step: 70/364, loss: 0.25164, accuracy: 0.90335\n",
            "Epoch: 23/30, step: 71/364, loss: 0.25104, accuracy: 0.90383\n",
            "Epoch: 23/30, step: 72/364, loss: 0.25087, accuracy: 0.90430\n",
            "Epoch: 23/30, step: 73/364, loss: 0.25058, accuracy: 0.90432\n",
            "Epoch: 23/30, step: 74/364, loss: 0.25012, accuracy: 0.90435\n",
            "Epoch: 23/30, step: 75/364, loss: 0.25090, accuracy: 0.90417\n",
            "Epoch: 23/30, step: 76/364, loss: 0.25068, accuracy: 0.90440\n",
            "Epoch: 23/30, step: 77/364, loss: 0.25070, accuracy: 0.90361\n",
            "Epoch: 23/30, step: 78/364, loss: 0.25068, accuracy: 0.90365\n",
            "Epoch: 23/30, step: 79/364, loss: 0.25036, accuracy: 0.90368\n",
            "Epoch: 23/30, step: 80/364, loss: 0.25037, accuracy: 0.90391\n",
            "Epoch: 23/30, step: 81/364, loss: 0.25224, accuracy: 0.90297\n",
            "Epoch: 23/30, step: 82/364, loss: 0.25184, accuracy: 0.90339\n",
            "Epoch: 23/30, step: 83/364, loss: 0.25139, accuracy: 0.90361\n",
            "Epoch: 23/30, step: 84/364, loss: 0.25113, accuracy: 0.90327\n",
            "Epoch: 23/30, step: 85/364, loss: 0.25172, accuracy: 0.90294\n",
            "Epoch: 23/30, step: 86/364, loss: 0.25296, accuracy: 0.90262\n",
            "Epoch: 23/30, step: 87/364, loss: 0.25260, accuracy: 0.90302\n",
            "Epoch: 23/30, step: 88/364, loss: 0.25162, accuracy: 0.90359\n",
            "Epoch: 23/30, step: 89/364, loss: 0.25161, accuracy: 0.90379\n",
            "Epoch: 23/30, step: 90/364, loss: 0.25093, accuracy: 0.90417\n",
            "Epoch: 23/30, step: 91/364, loss: 0.25146, accuracy: 0.90367\n",
            "Epoch: 23/30, step: 92/364, loss: 0.25154, accuracy: 0.90404\n",
            "Epoch: 23/30, step: 93/364, loss: 0.25139, accuracy: 0.90390\n",
            "Epoch: 23/30, step: 94/364, loss: 0.25215, accuracy: 0.90359\n",
            "Epoch: 23/30, step: 95/364, loss: 0.25138, accuracy: 0.90428\n",
            "Epoch: 23/30, step: 96/364, loss: 0.25222, accuracy: 0.90397\n",
            "Epoch: 23/30, step: 97/364, loss: 0.25224, accuracy: 0.90432\n",
            "Epoch: 23/30, step: 98/364, loss: 0.25158, accuracy: 0.90482\n",
            "Epoch: 23/30, step: 99/364, loss: 0.25238, accuracy: 0.90420\n",
            "Epoch: 23/30, step: 100/364, loss: 0.25190, accuracy: 0.90453\n",
            "Epoch: 23/30, step: 101/364, loss: 0.25188, accuracy: 0.90470\n",
            "Epoch: 23/30, step: 102/364, loss: 0.25112, accuracy: 0.90518\n",
            "Epoch: 23/30, step: 103/364, loss: 0.25019, accuracy: 0.90549\n",
            "Epoch: 23/30, step: 104/364, loss: 0.25061, accuracy: 0.90520\n",
            "Epoch: 23/30, step: 105/364, loss: 0.25070, accuracy: 0.90536\n",
            "Epoch: 23/30, step: 106/364, loss: 0.25117, accuracy: 0.90537\n",
            "Epoch: 23/30, step: 107/364, loss: 0.25184, accuracy: 0.90494\n",
            "Epoch: 23/30, step: 108/364, loss: 0.25329, accuracy: 0.90422\n",
            "Epoch: 23/30, step: 109/364, loss: 0.25293, accuracy: 0.90439\n",
            "Epoch: 23/30, step: 110/364, loss: 0.25256, accuracy: 0.90483\n",
            "Epoch: 23/30, step: 111/364, loss: 0.25233, accuracy: 0.90470\n",
            "Epoch: 23/30, step: 112/364, loss: 0.25414, accuracy: 0.90374\n",
            "Epoch: 23/30, step: 113/364, loss: 0.25349, accuracy: 0.90404\n",
            "Epoch: 23/30, step: 114/364, loss: 0.25340, accuracy: 0.90419\n",
            "Epoch: 23/30, step: 115/364, loss: 0.25357, accuracy: 0.90408\n",
            "Epoch: 23/30, step: 116/364, loss: 0.25346, accuracy: 0.90396\n",
            "Epoch: 23/30, step: 117/364, loss: 0.25306, accuracy: 0.90398\n",
            "Epoch: 23/30, step: 118/364, loss: 0.25312, accuracy: 0.90400\n",
            "Epoch: 23/30, step: 119/364, loss: 0.25299, accuracy: 0.90415\n",
            "Epoch: 23/30, step: 120/364, loss: 0.25293, accuracy: 0.90417\n",
            "Epoch: 23/30, step: 121/364, loss: 0.25285, accuracy: 0.90418\n",
            "Epoch: 23/30, step: 122/364, loss: 0.25254, accuracy: 0.90446\n",
            "Epoch: 23/30, step: 123/364, loss: 0.25317, accuracy: 0.90434\n",
            "Epoch: 23/30, step: 124/364, loss: 0.25265, accuracy: 0.90449\n",
            "Epoch: 23/30, step: 125/364, loss: 0.25235, accuracy: 0.90487\n",
            "Epoch: 23/30, step: 126/364, loss: 0.25264, accuracy: 0.90464\n",
            "Epoch: 23/30, step: 127/364, loss: 0.25199, accuracy: 0.90502\n",
            "Epoch: 23/30, step: 128/364, loss: 0.25228, accuracy: 0.90491\n",
            "Epoch: 23/30, step: 129/364, loss: 0.25207, accuracy: 0.90468\n",
            "Epoch: 23/30, step: 130/364, loss: 0.25210, accuracy: 0.90493\n",
            "Epoch: 23/30, step: 131/364, loss: 0.25260, accuracy: 0.90458\n",
            "Epoch: 23/30, step: 132/364, loss: 0.25287, accuracy: 0.90388\n",
            "Epoch: 23/30, step: 133/364, loss: 0.25233, accuracy: 0.90437\n",
            "Epoch: 23/30, step: 134/364, loss: 0.25200, accuracy: 0.90462\n",
            "Epoch: 23/30, step: 135/364, loss: 0.25190, accuracy: 0.90451\n",
            "Epoch: 23/30, step: 136/364, loss: 0.25230, accuracy: 0.90418\n",
            "Epoch: 23/30, step: 137/364, loss: 0.25246, accuracy: 0.90420\n",
            "Epoch: 23/30, step: 138/364, loss: 0.25293, accuracy: 0.90376\n",
            "Epoch: 23/30, step: 139/364, loss: 0.25322, accuracy: 0.90333\n",
            "Epoch: 23/30, step: 140/364, loss: 0.25296, accuracy: 0.90357\n",
            "Epoch: 23/30, step: 141/364, loss: 0.25412, accuracy: 0.90315\n",
            "Epoch: 23/30, step: 142/364, loss: 0.25440, accuracy: 0.90317\n",
            "Epoch: 23/30, step: 143/364, loss: 0.25392, accuracy: 0.90352\n",
            "Epoch: 23/30, step: 144/364, loss: 0.25327, accuracy: 0.90375\n",
            "Epoch: 23/30, step: 145/364, loss: 0.25342, accuracy: 0.90377\n",
            "Epoch: 23/30, step: 146/364, loss: 0.25391, accuracy: 0.90379\n",
            "Epoch: 23/30, step: 147/364, loss: 0.25413, accuracy: 0.90359\n",
            "Epoch: 23/30, step: 148/364, loss: 0.25461, accuracy: 0.90319\n",
            "Epoch: 23/30, step: 149/364, loss: 0.25445, accuracy: 0.90331\n",
            "Epoch: 23/30, step: 150/364, loss: 0.25465, accuracy: 0.90292\n",
            "Epoch: 23/30, step: 151/364, loss: 0.25439, accuracy: 0.90294\n",
            "Epoch: 23/30, step: 152/364, loss: 0.25401, accuracy: 0.90327\n",
            "Epoch: 23/30, step: 153/364, loss: 0.25438, accuracy: 0.90298\n",
            "Epoch: 23/30, step: 154/364, loss: 0.25434, accuracy: 0.90290\n",
            "Epoch: 23/30, step: 155/364, loss: 0.25382, accuracy: 0.90333\n",
            "Epoch: 23/30, step: 156/364, loss: 0.25395, accuracy: 0.90304\n",
            "Epoch: 23/30, step: 157/364, loss: 0.25311, accuracy: 0.90366\n",
            "Epoch: 23/30, step: 158/364, loss: 0.25291, accuracy: 0.90378\n",
            "Epoch: 23/30, step: 159/364, loss: 0.25268, accuracy: 0.90369\n",
            "Epoch: 23/30, step: 160/364, loss: 0.25228, accuracy: 0.90391\n",
            "Epoch: 23/30, step: 161/364, loss: 0.25222, accuracy: 0.90392\n",
            "Epoch: 23/30, step: 162/364, loss: 0.25216, accuracy: 0.90394\n",
            "Epoch: 23/30, step: 163/364, loss: 0.25232, accuracy: 0.90395\n",
            "Epoch: 23/30, step: 164/364, loss: 0.25210, accuracy: 0.90377\n",
            "Epoch: 23/30, step: 165/364, loss: 0.25209, accuracy: 0.90350\n",
            "Epoch: 23/30, step: 166/364, loss: 0.25195, accuracy: 0.90380\n",
            "Epoch: 23/30, step: 167/364, loss: 0.25185, accuracy: 0.90391\n",
            "Epoch: 23/30, step: 168/364, loss: 0.25203, accuracy: 0.90392\n",
            "Epoch: 23/30, step: 169/364, loss: 0.25189, accuracy: 0.90422\n",
            "Epoch: 23/30, step: 170/364, loss: 0.25189, accuracy: 0.90414\n",
            "Epoch: 23/30, step: 171/364, loss: 0.25212, accuracy: 0.90406\n",
            "Epoch: 23/30, step: 172/364, loss: 0.25192, accuracy: 0.90407\n",
            "Epoch: 23/30, step: 173/364, loss: 0.25198, accuracy: 0.90408\n",
            "Epoch: 23/30, step: 174/364, loss: 0.25159, accuracy: 0.90427\n",
            "Epoch: 23/30, step: 175/364, loss: 0.25174, accuracy: 0.90411\n",
            "Epoch: 23/30, step: 176/364, loss: 0.25134, accuracy: 0.90430\n",
            "Epoch: 23/30, step: 177/364, loss: 0.25156, accuracy: 0.90422\n",
            "Epoch: 23/30, step: 178/364, loss: 0.25198, accuracy: 0.90388\n",
            "Epoch: 23/30, step: 179/364, loss: 0.25186, accuracy: 0.90372\n",
            "Epoch: 23/30, step: 180/364, loss: 0.25138, accuracy: 0.90408\n",
            "Epoch: 23/30, step: 181/364, loss: 0.25144, accuracy: 0.90409\n",
            "Epoch: 23/30, step: 182/364, loss: 0.25120, accuracy: 0.90419\n",
            "Epoch: 23/30, step: 183/364, loss: 0.25089, accuracy: 0.90446\n",
            "Epoch: 23/30, step: 184/364, loss: 0.25113, accuracy: 0.90438\n",
            "Epoch: 23/30, step: 185/364, loss: 0.25110, accuracy: 0.90414\n",
            "Epoch: 23/30, step: 186/364, loss: 0.25090, accuracy: 0.90423\n",
            "Epoch: 23/30, step: 187/364, loss: 0.25141, accuracy: 0.90416\n",
            "Epoch: 23/30, step: 188/364, loss: 0.25109, accuracy: 0.90442\n",
            "Epoch: 23/30, step: 189/364, loss: 0.25086, accuracy: 0.90468\n",
            "Epoch: 23/30, step: 190/364, loss: 0.25121, accuracy: 0.90469\n",
            "Epoch: 23/30, step: 191/364, loss: 0.25126, accuracy: 0.90445\n",
            "Epoch: 23/30, step: 192/364, loss: 0.25107, accuracy: 0.90462\n",
            "Epoch: 23/30, step: 193/364, loss: 0.25176, accuracy: 0.90406\n",
            "Epoch: 23/30, step: 194/364, loss: 0.25141, accuracy: 0.90432\n",
            "Epoch: 23/30, step: 195/364, loss: 0.25125, accuracy: 0.90441\n",
            "Epoch: 23/30, step: 196/364, loss: 0.25129, accuracy: 0.90426\n",
            "Epoch: 23/30, step: 197/364, loss: 0.25187, accuracy: 0.90387\n",
            "Epoch: 23/30, step: 198/364, loss: 0.25186, accuracy: 0.90388\n",
            "Epoch: 23/30, step: 199/364, loss: 0.25150, accuracy: 0.90421\n",
            "Epoch: 23/30, step: 200/364, loss: 0.25168, accuracy: 0.90414\n",
            "Epoch: 23/30, step: 201/364, loss: 0.25197, accuracy: 0.90392\n",
            "Epoch: 23/30, step: 202/364, loss: 0.25161, accuracy: 0.90416\n",
            "Epoch: 23/30, step: 203/364, loss: 0.25140, accuracy: 0.90417\n",
            "Epoch: 23/30, step: 204/364, loss: 0.25124, accuracy: 0.90434\n",
            "Epoch: 23/30, step: 205/364, loss: 0.25167, accuracy: 0.90419\n",
            "Epoch: 23/30, step: 206/364, loss: 0.25156, accuracy: 0.90435\n",
            "Epoch: 23/30, step: 207/364, loss: 0.25153, accuracy: 0.90444\n",
            "Epoch: 23/30, step: 208/364, loss: 0.25092, accuracy: 0.90482\n",
            "Epoch: 23/30, step: 209/364, loss: 0.25071, accuracy: 0.90498\n",
            "Epoch: 23/30, step: 210/364, loss: 0.25042, accuracy: 0.90506\n",
            "Epoch: 23/30, step: 211/364, loss: 0.25053, accuracy: 0.90484\n",
            "Epoch: 23/30, step: 212/364, loss: 0.25088, accuracy: 0.90455\n",
            "Epoch: 23/30, step: 213/364, loss: 0.25118, accuracy: 0.90456\n",
            "Epoch: 23/30, step: 214/364, loss: 0.25097, accuracy: 0.90472\n",
            "Epoch: 23/30, step: 215/364, loss: 0.25122, accuracy: 0.90451\n",
            "Epoch: 23/30, step: 216/364, loss: 0.25124, accuracy: 0.90451\n",
            "Epoch: 23/30, step: 217/364, loss: 0.25119, accuracy: 0.90445\n",
            "Epoch: 23/30, step: 218/364, loss: 0.25097, accuracy: 0.90467\n",
            "Epoch: 23/30, step: 219/364, loss: 0.25110, accuracy: 0.90468\n",
            "Epoch: 23/30, step: 220/364, loss: 0.25108, accuracy: 0.90462\n",
            "Epoch: 23/30, step: 221/364, loss: 0.25085, accuracy: 0.90469\n",
            "Epoch: 23/30, step: 222/364, loss: 0.25044, accuracy: 0.90498\n",
            "Epoch: 23/30, step: 223/364, loss: 0.25015, accuracy: 0.90513\n",
            "Epoch: 23/30, step: 224/364, loss: 0.25022, accuracy: 0.90492\n",
            "Epoch: 23/30, step: 225/364, loss: 0.25026, accuracy: 0.90493\n",
            "Epoch: 23/30, step: 226/364, loss: 0.24977, accuracy: 0.90528\n",
            "Epoch: 23/30, step: 227/364, loss: 0.24948, accuracy: 0.90549\n",
            "Epoch: 23/30, step: 228/364, loss: 0.24934, accuracy: 0.90556\n",
            "Epoch: 23/30, step: 229/364, loss: 0.24960, accuracy: 0.90550\n",
            "Epoch: 23/30, step: 230/364, loss: 0.24973, accuracy: 0.90537\n",
            "Epoch: 23/30, step: 231/364, loss: 0.24962, accuracy: 0.90544\n",
            "Epoch: 23/30, step: 232/364, loss: 0.24975, accuracy: 0.90544\n",
            "Epoch: 23/30, step: 233/364, loss: 0.24977, accuracy: 0.90545\n",
            "Epoch: 23/30, step: 234/364, loss: 0.25002, accuracy: 0.90505\n",
            "Epoch: 23/30, step: 235/364, loss: 0.25037, accuracy: 0.90492\n",
            "Epoch: 23/30, step: 236/364, loss: 0.25053, accuracy: 0.90466\n",
            "Epoch: 23/30, step: 237/364, loss: 0.25033, accuracy: 0.90487\n",
            "Epoch: 23/30, step: 238/364, loss: 0.25013, accuracy: 0.90500\n",
            "Epoch: 23/30, step: 239/364, loss: 0.24995, accuracy: 0.90527\n",
            "Epoch: 23/30, step: 240/364, loss: 0.24964, accuracy: 0.90534\n",
            "Epoch: 23/30, step: 241/364, loss: 0.24980, accuracy: 0.90534\n",
            "Epoch: 23/30, step: 242/364, loss: 0.24944, accuracy: 0.90554\n",
            "Epoch: 23/30, step: 243/364, loss: 0.24933, accuracy: 0.90567\n",
            "Epoch: 23/30, step: 244/364, loss: 0.24916, accuracy: 0.90580\n",
            "Epoch: 23/30, step: 245/364, loss: 0.24898, accuracy: 0.90593\n",
            "Epoch: 23/30, step: 246/364, loss: 0.24922, accuracy: 0.90593\n",
            "Epoch: 23/30, step: 247/364, loss: 0.24952, accuracy: 0.90581\n",
            "Epoch: 23/30, step: 248/364, loss: 0.24961, accuracy: 0.90556\n",
            "Epoch: 23/30, step: 249/364, loss: 0.24907, accuracy: 0.90594\n",
            "Epoch: 23/30, step: 250/364, loss: 0.24942, accuracy: 0.90562\n",
            "Epoch: 23/30, step: 251/364, loss: 0.24912, accuracy: 0.90588\n",
            "Epoch: 23/30, step: 252/364, loss: 0.24889, accuracy: 0.90594\n",
            "Epoch: 23/30, step: 253/364, loss: 0.24919, accuracy: 0.90576\n",
            "Epoch: 23/30, step: 254/364, loss: 0.24901, accuracy: 0.90588\n",
            "Epoch: 23/30, step: 255/364, loss: 0.24882, accuracy: 0.90594\n",
            "Epoch: 23/30, step: 256/364, loss: 0.24888, accuracy: 0.90607\n",
            "Epoch: 23/30, step: 257/364, loss: 0.24944, accuracy: 0.90595\n",
            "Epoch: 23/30, step: 258/364, loss: 0.24961, accuracy: 0.90583\n",
            "Epoch: 23/30, step: 259/364, loss: 0.24946, accuracy: 0.90577\n",
            "Epoch: 23/30, step: 260/364, loss: 0.24957, accuracy: 0.90565\n",
            "Epoch: 23/30, step: 261/364, loss: 0.24959, accuracy: 0.90571\n",
            "Epoch: 23/30, step: 262/364, loss: 0.24970, accuracy: 0.90559\n",
            "Epoch: 23/30, step: 263/364, loss: 0.24958, accuracy: 0.90566\n",
            "Epoch: 23/30, step: 264/364, loss: 0.24920, accuracy: 0.90589\n",
            "Epoch: 23/30, step: 265/364, loss: 0.24916, accuracy: 0.90596\n",
            "Epoch: 23/30, step: 266/364, loss: 0.24899, accuracy: 0.90607\n",
            "Epoch: 23/30, step: 267/364, loss: 0.24923, accuracy: 0.90602\n",
            "Epoch: 23/30, step: 268/364, loss: 0.24936, accuracy: 0.90590\n",
            "Epoch: 23/30, step: 269/364, loss: 0.24981, accuracy: 0.90549\n",
            "Epoch: 23/30, step: 270/364, loss: 0.24946, accuracy: 0.90567\n",
            "Epoch: 23/30, step: 271/364, loss: 0.24934, accuracy: 0.90567\n",
            "Epoch: 23/30, step: 272/364, loss: 0.24913, accuracy: 0.90573\n",
            "Epoch: 23/30, step: 273/364, loss: 0.24899, accuracy: 0.90591\n",
            "Epoch: 23/30, step: 274/364, loss: 0.24928, accuracy: 0.90568\n",
            "Epoch: 23/30, step: 275/364, loss: 0.24943, accuracy: 0.90551\n",
            "Epoch: 23/30, step: 276/364, loss: 0.24961, accuracy: 0.90517\n",
            "Epoch: 23/30, step: 277/364, loss: 0.24958, accuracy: 0.90523\n",
            "Epoch: 23/30, step: 278/364, loss: 0.24935, accuracy: 0.90541\n",
            "Epoch: 23/30, step: 279/364, loss: 0.24936, accuracy: 0.90547\n",
            "Epoch: 23/30, step: 280/364, loss: 0.24904, accuracy: 0.90569\n",
            "Epoch: 23/30, step: 281/364, loss: 0.24894, accuracy: 0.90569\n",
            "Epoch: 23/30, step: 282/364, loss: 0.24935, accuracy: 0.90531\n",
            "Epoch: 23/30, step: 283/364, loss: 0.24921, accuracy: 0.90542\n",
            "Epoch: 23/30, step: 284/364, loss: 0.24900, accuracy: 0.90564\n",
            "Epoch: 23/30, step: 285/364, loss: 0.24890, accuracy: 0.90576\n",
            "Epoch: 23/30, step: 286/364, loss: 0.24899, accuracy: 0.90559\n",
            "Epoch: 23/30, step: 287/364, loss: 0.24905, accuracy: 0.90554\n",
            "Epoch: 23/30, step: 288/364, loss: 0.24920, accuracy: 0.90544\n",
            "Epoch: 23/30, step: 289/364, loss: 0.24910, accuracy: 0.90544\n",
            "Epoch: 23/30, step: 290/364, loss: 0.24892, accuracy: 0.90560\n",
            "Epoch: 23/30, step: 291/364, loss: 0.24885, accuracy: 0.90559\n",
            "Epoch: 23/30, train loss: 0.24885, train accuracy: 0.90559, valid loss: 0.70721, valid accuracy: 0.68745\n",
            "Epoch: 24/30, step: 1/364, loss: 0.25537, accuracy: 0.87500\n",
            "Epoch: 24/30, step: 2/364, loss: 0.21877, accuracy: 0.91406\n",
            "Epoch: 24/30, step: 3/364, loss: 0.27098, accuracy: 0.88021\n",
            "Epoch: 24/30, step: 4/364, loss: 0.23753, accuracy: 0.90625\n",
            "Epoch: 24/30, step: 5/364, loss: 0.24620, accuracy: 0.90312\n",
            "Epoch: 24/30, step: 6/364, loss: 0.25424, accuracy: 0.89844\n",
            "Epoch: 24/30, step: 7/364, loss: 0.25246, accuracy: 0.90402\n",
            "Epoch: 24/30, step: 8/364, loss: 0.25913, accuracy: 0.89648\n",
            "Epoch: 24/30, step: 9/364, loss: 0.25710, accuracy: 0.89583\n",
            "Epoch: 24/30, step: 10/364, loss: 0.24720, accuracy: 0.90156\n",
            "Epoch: 24/30, step: 11/364, loss: 0.24574, accuracy: 0.90057\n",
            "Epoch: 24/30, step: 12/364, loss: 0.24839, accuracy: 0.89974\n",
            "Epoch: 24/30, step: 13/364, loss: 0.24740, accuracy: 0.90144\n",
            "Epoch: 24/30, step: 14/364, loss: 0.24661, accuracy: 0.90290\n",
            "Epoch: 24/30, step: 15/364, loss: 0.24550, accuracy: 0.90417\n",
            "Epoch: 24/30, step: 16/364, loss: 0.24231, accuracy: 0.90527\n",
            "Epoch: 24/30, step: 17/364, loss: 0.23728, accuracy: 0.90901\n",
            "Epoch: 24/30, step: 18/364, loss: 0.23257, accuracy: 0.91146\n",
            "Epoch: 24/30, step: 19/364, loss: 0.23290, accuracy: 0.91283\n",
            "Epoch: 24/30, step: 20/364, loss: 0.23882, accuracy: 0.90859\n",
            "Epoch: 24/30, step: 21/364, loss: 0.23600, accuracy: 0.91146\n",
            "Epoch: 24/30, step: 22/364, loss: 0.23400, accuracy: 0.91335\n",
            "Epoch: 24/30, step: 23/364, loss: 0.23127, accuracy: 0.91576\n",
            "Epoch: 24/30, step: 24/364, loss: 0.22912, accuracy: 0.91667\n",
            "Epoch: 24/30, step: 25/364, loss: 0.22804, accuracy: 0.91750\n",
            "Epoch: 24/30, step: 26/364, loss: 0.22759, accuracy: 0.91707\n",
            "Epoch: 24/30, step: 27/364, loss: 0.22961, accuracy: 0.91493\n",
            "Epoch: 24/30, step: 28/364, loss: 0.22867, accuracy: 0.91462\n",
            "Epoch: 24/30, step: 29/364, loss: 0.22701, accuracy: 0.91541\n",
            "Epoch: 24/30, step: 30/364, loss: 0.22841, accuracy: 0.91562\n",
            "Epoch: 24/30, step: 31/364, loss: 0.22991, accuracy: 0.91431\n",
            "Epoch: 24/30, step: 32/364, loss: 0.23259, accuracy: 0.91260\n",
            "Epoch: 24/30, step: 33/364, loss: 0.23662, accuracy: 0.90909\n",
            "Epoch: 24/30, step: 34/364, loss: 0.23410, accuracy: 0.91085\n",
            "Epoch: 24/30, step: 35/364, loss: 0.23353, accuracy: 0.91071\n",
            "Epoch: 24/30, step: 36/364, loss: 0.23203, accuracy: 0.91146\n",
            "Epoch: 24/30, step: 37/364, loss: 0.23046, accuracy: 0.91258\n",
            "Epoch: 24/30, step: 38/364, loss: 0.23122, accuracy: 0.91201\n",
            "Epoch: 24/30, step: 39/364, loss: 0.22992, accuracy: 0.91306\n",
            "Epoch: 24/30, step: 40/364, loss: 0.23152, accuracy: 0.91211\n",
            "Epoch: 24/30, step: 41/364, loss: 0.23136, accuracy: 0.91273\n",
            "Epoch: 24/30, step: 42/364, loss: 0.23332, accuracy: 0.91071\n",
            "Epoch: 24/30, step: 43/364, loss: 0.23813, accuracy: 0.90807\n",
            "Epoch: 24/30, step: 44/364, loss: 0.23720, accuracy: 0.90909\n",
            "Epoch: 24/30, step: 45/364, loss: 0.23527, accuracy: 0.91042\n",
            "Epoch: 24/30, step: 46/364, loss: 0.23475, accuracy: 0.91067\n",
            "Epoch: 24/30, step: 47/364, loss: 0.23476, accuracy: 0.91057\n",
            "Epoch: 24/30, step: 48/364, loss: 0.23453, accuracy: 0.91113\n",
            "Epoch: 24/30, step: 49/364, loss: 0.23339, accuracy: 0.91199\n",
            "Epoch: 24/30, step: 50/364, loss: 0.23326, accuracy: 0.91219\n",
            "Epoch: 24/30, step: 51/364, loss: 0.23377, accuracy: 0.91207\n",
            "Epoch: 24/30, step: 52/364, loss: 0.23368, accuracy: 0.91226\n",
            "Epoch: 24/30, step: 53/364, loss: 0.23214, accuracy: 0.91333\n",
            "Epoch: 24/30, step: 54/364, loss: 0.23124, accuracy: 0.91377\n",
            "Epoch: 24/30, step: 55/364, loss: 0.23109, accuracy: 0.91364\n",
            "Epoch: 24/30, step: 56/364, loss: 0.22940, accuracy: 0.91406\n",
            "Epoch: 24/30, step: 57/364, loss: 0.22973, accuracy: 0.91447\n",
            "Epoch: 24/30, step: 58/364, loss: 0.23180, accuracy: 0.91379\n",
            "Epoch: 24/30, step: 59/364, loss: 0.23239, accuracy: 0.91340\n",
            "Epoch: 24/30, step: 60/364, loss: 0.23246, accuracy: 0.91276\n",
            "Epoch: 24/30, step: 61/364, loss: 0.23125, accuracy: 0.91368\n",
            "Epoch: 24/30, step: 62/364, loss: 0.23273, accuracy: 0.91255\n",
            "Epoch: 24/30, step: 63/364, loss: 0.23230, accuracy: 0.91295\n",
            "Epoch: 24/30, step: 64/364, loss: 0.23376, accuracy: 0.91138\n",
            "Epoch: 24/30, step: 65/364, loss: 0.23434, accuracy: 0.91034\n",
            "Epoch: 24/30, step: 66/364, loss: 0.23517, accuracy: 0.91075\n",
            "Epoch: 24/30, step: 67/364, loss: 0.23510, accuracy: 0.91068\n",
            "Epoch: 24/30, step: 68/364, loss: 0.23789, accuracy: 0.90901\n",
            "Epoch: 24/30, step: 69/364, loss: 0.24077, accuracy: 0.90761\n",
            "Epoch: 24/30, step: 70/364, loss: 0.24183, accuracy: 0.90714\n",
            "Epoch: 24/30, step: 71/364, loss: 0.24074, accuracy: 0.90757\n",
            "Epoch: 24/30, step: 72/364, loss: 0.24075, accuracy: 0.90734\n",
            "Epoch: 24/30, step: 73/364, loss: 0.24306, accuracy: 0.90625\n",
            "Epoch: 24/30, step: 74/364, loss: 0.24232, accuracy: 0.90709\n",
            "Epoch: 24/30, step: 75/364, loss: 0.24096, accuracy: 0.90750\n",
            "Epoch: 24/30, step: 76/364, loss: 0.24022, accuracy: 0.90810\n",
            "Epoch: 24/30, step: 77/364, loss: 0.23998, accuracy: 0.90808\n",
            "Epoch: 24/30, step: 78/364, loss: 0.23966, accuracy: 0.90825\n",
            "Epoch: 24/30, step: 79/364, loss: 0.23969, accuracy: 0.90843\n",
            "Epoch: 24/30, step: 80/364, loss: 0.24022, accuracy: 0.90781\n",
            "Epoch: 24/30, step: 81/364, loss: 0.23950, accuracy: 0.90856\n",
            "Epoch: 24/30, step: 82/364, loss: 0.23894, accuracy: 0.90854\n",
            "Epoch: 24/30, step: 83/364, loss: 0.23868, accuracy: 0.90870\n",
            "Epoch: 24/30, step: 84/364, loss: 0.24006, accuracy: 0.90755\n",
            "Epoch: 24/30, step: 85/364, loss: 0.23951, accuracy: 0.90809\n",
            "Epoch: 24/30, step: 86/364, loss: 0.23918, accuracy: 0.90807\n",
            "Epoch: 24/30, step: 87/364, loss: 0.23888, accuracy: 0.90823\n",
            "Epoch: 24/30, step: 88/364, loss: 0.23796, accuracy: 0.90874\n",
            "Epoch: 24/30, step: 89/364, loss: 0.23859, accuracy: 0.90818\n",
            "Epoch: 24/30, step: 90/364, loss: 0.23816, accuracy: 0.90851\n",
            "Epoch: 24/30, step: 91/364, loss: 0.23897, accuracy: 0.90848\n",
            "Epoch: 24/30, step: 92/364, loss: 0.23867, accuracy: 0.90846\n",
            "Epoch: 24/30, step: 93/364, loss: 0.23862, accuracy: 0.90827\n",
            "Epoch: 24/30, step: 94/364, loss: 0.23965, accuracy: 0.90741\n",
            "Epoch: 24/30, step: 95/364, loss: 0.23930, accuracy: 0.90789\n",
            "Epoch: 24/30, step: 96/364, loss: 0.23890, accuracy: 0.90837\n",
            "Epoch: 24/30, step: 97/364, loss: 0.23900, accuracy: 0.90818\n",
            "Epoch: 24/30, step: 98/364, loss: 0.23984, accuracy: 0.90721\n",
            "Epoch: 24/30, step: 99/364, loss: 0.23950, accuracy: 0.90720\n",
            "Epoch: 24/30, step: 100/364, loss: 0.24020, accuracy: 0.90688\n",
            "Epoch: 24/30, step: 101/364, loss: 0.23925, accuracy: 0.90749\n",
            "Epoch: 24/30, step: 102/364, loss: 0.23924, accuracy: 0.90732\n",
            "Epoch: 24/30, step: 103/364, loss: 0.23913, accuracy: 0.90716\n",
            "Epoch: 24/30, step: 104/364, loss: 0.23870, accuracy: 0.90745\n",
            "Epoch: 24/30, step: 105/364, loss: 0.23894, accuracy: 0.90729\n",
            "Epoch: 24/30, step: 106/364, loss: 0.23897, accuracy: 0.90728\n",
            "Epoch: 24/30, step: 107/364, loss: 0.23916, accuracy: 0.90683\n",
            "Epoch: 24/30, step: 108/364, loss: 0.23871, accuracy: 0.90726\n",
            "Epoch: 24/30, step: 109/364, loss: 0.23891, accuracy: 0.90725\n",
            "Epoch: 24/30, step: 110/364, loss: 0.23869, accuracy: 0.90753\n",
            "Epoch: 24/30, step: 111/364, loss: 0.23833, accuracy: 0.90752\n",
            "Epoch: 24/30, step: 112/364, loss: 0.23781, accuracy: 0.90792\n",
            "Epoch: 24/30, step: 113/364, loss: 0.23796, accuracy: 0.90791\n",
            "Epoch: 24/30, step: 114/364, loss: 0.23758, accuracy: 0.90817\n",
            "Epoch: 24/30, step: 115/364, loss: 0.23800, accuracy: 0.90829\n",
            "Epoch: 24/30, step: 116/364, loss: 0.23757, accuracy: 0.90867\n",
            "Epoch: 24/30, step: 117/364, loss: 0.23746, accuracy: 0.90892\n",
            "Epoch: 24/30, step: 118/364, loss: 0.23837, accuracy: 0.90810\n",
            "Epoch: 24/30, step: 119/364, loss: 0.23898, accuracy: 0.90743\n",
            "Epoch: 24/30, step: 120/364, loss: 0.24002, accuracy: 0.90690\n",
            "Epoch: 24/30, step: 121/364, loss: 0.23949, accuracy: 0.90754\n",
            "Epoch: 24/30, step: 122/364, loss: 0.23936, accuracy: 0.90740\n",
            "Epoch: 24/30, step: 123/364, loss: 0.23932, accuracy: 0.90739\n",
            "Epoch: 24/30, step: 124/364, loss: 0.23963, accuracy: 0.90751\n",
            "Epoch: 24/30, step: 125/364, loss: 0.23977, accuracy: 0.90737\n",
            "Epoch: 24/30, step: 126/364, loss: 0.23936, accuracy: 0.90737\n",
            "Epoch: 24/30, step: 127/364, loss: 0.23915, accuracy: 0.90773\n",
            "Epoch: 24/30, step: 128/364, loss: 0.23911, accuracy: 0.90784\n",
            "Epoch: 24/30, step: 129/364, loss: 0.23891, accuracy: 0.90782\n",
            "Epoch: 24/30, step: 130/364, loss: 0.23898, accuracy: 0.90781\n",
            "Epoch: 24/30, step: 131/364, loss: 0.23955, accuracy: 0.90732\n",
            "Epoch: 24/30, step: 132/364, loss: 0.23897, accuracy: 0.90767\n",
            "Epoch: 24/30, step: 133/364, loss: 0.23953, accuracy: 0.90754\n",
            "Epoch: 24/30, step: 134/364, loss: 0.23937, accuracy: 0.90753\n",
            "Epoch: 24/30, step: 135/364, loss: 0.23851, accuracy: 0.90810\n",
            "Epoch: 24/30, step: 136/364, loss: 0.23802, accuracy: 0.90855\n",
            "Epoch: 24/30, step: 137/364, loss: 0.23772, accuracy: 0.90887\n",
            "Epoch: 24/30, step: 138/364, loss: 0.23836, accuracy: 0.90851\n",
            "Epoch: 24/30, step: 139/364, loss: 0.23826, accuracy: 0.90850\n",
            "Epoch: 24/30, step: 140/364, loss: 0.23812, accuracy: 0.90848\n",
            "Epoch: 24/30, step: 141/364, loss: 0.23783, accuracy: 0.90869\n",
            "Epoch: 24/30, step: 142/364, loss: 0.23739, accuracy: 0.90867\n",
            "Epoch: 24/30, step: 143/364, loss: 0.23680, accuracy: 0.90898\n",
            "Epoch: 24/30, step: 144/364, loss: 0.23688, accuracy: 0.90885\n",
            "Epoch: 24/30, step: 145/364, loss: 0.23706, accuracy: 0.90873\n",
            "Epoch: 24/30, step: 146/364, loss: 0.23677, accuracy: 0.90860\n",
            "Epoch: 24/30, step: 147/364, loss: 0.23655, accuracy: 0.90848\n",
            "Epoch: 24/30, step: 148/364, loss: 0.23601, accuracy: 0.90878\n",
            "Epoch: 24/30, step: 149/364, loss: 0.23592, accuracy: 0.90898\n",
            "Epoch: 24/30, step: 150/364, loss: 0.23614, accuracy: 0.90875\n",
            "Epoch: 24/30, step: 151/364, loss: 0.23622, accuracy: 0.90904\n",
            "Epoch: 24/30, step: 152/364, loss: 0.23623, accuracy: 0.90903\n",
            "Epoch: 24/30, step: 153/364, loss: 0.23578, accuracy: 0.90931\n",
            "Epoch: 24/30, step: 154/364, loss: 0.23564, accuracy: 0.90950\n",
            "Epoch: 24/30, step: 155/364, loss: 0.23537, accuracy: 0.90978\n",
            "Epoch: 24/30, step: 156/364, loss: 0.23567, accuracy: 0.90976\n",
            "Epoch: 24/30, step: 157/364, loss: 0.23536, accuracy: 0.90973\n",
            "Epoch: 24/30, step: 158/364, loss: 0.23548, accuracy: 0.90961\n",
            "Epoch: 24/30, step: 159/364, loss: 0.23491, accuracy: 0.90998\n",
            "Epoch: 24/30, step: 160/364, loss: 0.23492, accuracy: 0.91006\n",
            "Epoch: 24/30, step: 161/364, loss: 0.23466, accuracy: 0.91003\n",
            "Epoch: 24/30, step: 162/364, loss: 0.23493, accuracy: 0.90992\n",
            "Epoch: 24/30, step: 163/364, loss: 0.23483, accuracy: 0.90999\n",
            "Epoch: 24/30, step: 164/364, loss: 0.23495, accuracy: 0.91006\n",
            "Epoch: 24/30, step: 165/364, loss: 0.23453, accuracy: 0.91051\n",
            "Epoch: 24/30, step: 166/364, loss: 0.23452, accuracy: 0.91039\n",
            "Epoch: 24/30, step: 167/364, loss: 0.23418, accuracy: 0.91074\n",
            "Epoch: 24/30, step: 168/364, loss: 0.23450, accuracy: 0.91044\n",
            "Epoch: 24/30, step: 169/364, loss: 0.23411, accuracy: 0.91078\n",
            "Epoch: 24/30, step: 170/364, loss: 0.23407, accuracy: 0.91103\n",
            "Epoch: 24/30, step: 171/364, loss: 0.23404, accuracy: 0.91091\n",
            "Epoch: 24/30, step: 172/364, loss: 0.23380, accuracy: 0.91106\n",
            "Epoch: 24/30, step: 173/364, loss: 0.23421, accuracy: 0.91086\n",
            "Epoch: 24/30, step: 174/364, loss: 0.23411, accuracy: 0.91083\n",
            "Epoch: 24/30, step: 175/364, loss: 0.23388, accuracy: 0.91089\n",
            "Epoch: 24/30, step: 176/364, loss: 0.23424, accuracy: 0.91033\n",
            "Epoch: 24/30, step: 177/364, loss: 0.23399, accuracy: 0.91040\n",
            "Epoch: 24/30, step: 178/364, loss: 0.23377, accuracy: 0.91055\n",
            "Epoch: 24/30, step: 179/364, loss: 0.23368, accuracy: 0.91053\n",
            "Epoch: 24/30, step: 180/364, loss: 0.23417, accuracy: 0.90998\n",
            "Epoch: 24/30, step: 181/364, loss: 0.23425, accuracy: 0.91005\n",
            "Epoch: 24/30, step: 182/364, loss: 0.23413, accuracy: 0.91020\n",
            "Epoch: 24/30, step: 183/364, loss: 0.23432, accuracy: 0.91009\n",
            "Epoch: 24/30, step: 184/364, loss: 0.23399, accuracy: 0.91024\n",
            "Epoch: 24/30, step: 185/364, loss: 0.23385, accuracy: 0.91022\n",
            "Epoch: 24/30, step: 186/364, loss: 0.23411, accuracy: 0.90995\n",
            "Epoch: 24/30, step: 187/364, loss: 0.23400, accuracy: 0.90976\n",
            "Epoch: 24/30, step: 188/364, loss: 0.23380, accuracy: 0.90999\n",
            "Epoch: 24/30, step: 189/364, loss: 0.23387, accuracy: 0.90989\n",
            "Epoch: 24/30, step: 190/364, loss: 0.23429, accuracy: 0.90946\n",
            "Epoch: 24/30, step: 191/364, loss: 0.23432, accuracy: 0.90944\n",
            "Epoch: 24/30, step: 192/364, loss: 0.23452, accuracy: 0.90926\n",
            "Epoch: 24/30, step: 193/364, loss: 0.23415, accuracy: 0.90949\n",
            "Epoch: 24/30, step: 194/364, loss: 0.23398, accuracy: 0.90963\n",
            "Epoch: 24/30, step: 195/364, loss: 0.23403, accuracy: 0.90954\n",
            "Epoch: 24/30, step: 196/364, loss: 0.23411, accuracy: 0.90952\n",
            "Epoch: 24/30, step: 197/364, loss: 0.23401, accuracy: 0.90966\n",
            "Epoch: 24/30, step: 198/364, loss: 0.23405, accuracy: 0.90964\n",
            "Epoch: 24/30, step: 199/364, loss: 0.23405, accuracy: 0.90947\n",
            "Epoch: 24/30, step: 200/364, loss: 0.23381, accuracy: 0.90977\n",
            "Epoch: 24/30, step: 201/364, loss: 0.23357, accuracy: 0.91014\n",
            "Epoch: 24/30, step: 202/364, loss: 0.23389, accuracy: 0.91004\n",
            "Epoch: 24/30, step: 203/364, loss: 0.23404, accuracy: 0.90987\n",
            "Epoch: 24/30, step: 204/364, loss: 0.23362, accuracy: 0.91000\n",
            "Epoch: 24/30, step: 205/364, loss: 0.23404, accuracy: 0.90968\n",
            "Epoch: 24/30, step: 206/364, loss: 0.23411, accuracy: 0.90966\n",
            "Epoch: 24/30, step: 207/364, loss: 0.23395, accuracy: 0.90972\n",
            "Epoch: 24/30, step: 208/364, loss: 0.23357, accuracy: 0.90993\n",
            "Epoch: 24/30, step: 209/364, loss: 0.23324, accuracy: 0.91021\n",
            "Epoch: 24/30, step: 210/364, loss: 0.23317, accuracy: 0.91004\n",
            "Epoch: 24/30, step: 211/364, loss: 0.23280, accuracy: 0.91032\n",
            "Epoch: 24/30, step: 212/364, loss: 0.23293, accuracy: 0.91030\n",
            "Epoch: 24/30, step: 213/364, loss: 0.23344, accuracy: 0.91006\n",
            "Epoch: 24/30, step: 214/364, loss: 0.23333, accuracy: 0.91012\n",
            "Epoch: 24/30, step: 215/364, loss: 0.23304, accuracy: 0.91047\n",
            "Epoch: 24/30, step: 216/364, loss: 0.23320, accuracy: 0.91037\n",
            "Epoch: 24/30, step: 217/364, loss: 0.23302, accuracy: 0.91043\n",
            "Epoch: 24/30, step: 218/364, loss: 0.23339, accuracy: 0.91012\n",
            "Epoch: 24/30, step: 219/364, loss: 0.23305, accuracy: 0.91017\n",
            "Epoch: 24/30, step: 220/364, loss: 0.23296, accuracy: 0.91030\n",
            "Epoch: 24/30, step: 221/364, loss: 0.23247, accuracy: 0.91070\n",
            "Epoch: 24/30, step: 222/364, loss: 0.23203, accuracy: 0.91097\n",
            "Epoch: 24/30, step: 223/364, loss: 0.23184, accuracy: 0.91108\n",
            "Epoch: 24/30, step: 224/364, loss: 0.23203, accuracy: 0.91092\n",
            "Epoch: 24/30, step: 225/364, loss: 0.23206, accuracy: 0.91097\n",
            "Epoch: 24/30, step: 226/364, loss: 0.23170, accuracy: 0.91123\n",
            "Epoch: 24/30, step: 227/364, loss: 0.23160, accuracy: 0.91121\n",
            "Epoch: 24/30, step: 228/364, loss: 0.23129, accuracy: 0.91139\n",
            "Epoch: 24/30, step: 229/364, loss: 0.23143, accuracy: 0.91137\n",
            "Epoch: 24/30, step: 230/364, loss: 0.23118, accuracy: 0.91155\n",
            "Epoch: 24/30, step: 231/364, loss: 0.23145, accuracy: 0.91146\n",
            "Epoch: 24/30, step: 232/364, loss: 0.23167, accuracy: 0.91123\n",
            "Epoch: 24/30, step: 233/364, loss: 0.23157, accuracy: 0.91135\n",
            "Epoch: 24/30, step: 234/364, loss: 0.23133, accuracy: 0.91146\n",
            "Epoch: 24/30, step: 235/364, loss: 0.23108, accuracy: 0.91170\n",
            "Epoch: 24/30, step: 236/364, loss: 0.23128, accuracy: 0.91155\n",
            "Epoch: 24/30, step: 237/364, loss: 0.23126, accuracy: 0.91152\n",
            "Epoch: 24/30, step: 238/364, loss: 0.23126, accuracy: 0.91157\n",
            "Epoch: 24/30, step: 239/364, loss: 0.23121, accuracy: 0.91155\n",
            "Epoch: 24/30, step: 240/364, loss: 0.23097, accuracy: 0.91185\n",
            "Epoch: 24/30, step: 241/364, loss: 0.23096, accuracy: 0.91189\n",
            "Epoch: 24/30, step: 242/364, loss: 0.23065, accuracy: 0.91213\n",
            "Epoch: 24/30, step: 243/364, loss: 0.23058, accuracy: 0.91204\n",
            "Epoch: 24/30, step: 244/364, loss: 0.23104, accuracy: 0.91182\n",
            "Epoch: 24/30, step: 245/364, loss: 0.23069, accuracy: 0.91199\n",
            "Epoch: 24/30, step: 246/364, loss: 0.23115, accuracy: 0.91178\n",
            "Epoch: 24/30, step: 247/364, loss: 0.23103, accuracy: 0.91182\n",
            "Epoch: 24/30, step: 248/364, loss: 0.23070, accuracy: 0.91205\n",
            "Epoch: 24/30, step: 249/364, loss: 0.23093, accuracy: 0.91196\n",
            "Epoch: 24/30, step: 250/364, loss: 0.23090, accuracy: 0.91200\n",
            "Epoch: 24/30, step: 251/364, loss: 0.23089, accuracy: 0.91198\n",
            "Epoch: 24/30, step: 252/364, loss: 0.23111, accuracy: 0.91195\n",
            "Epoch: 24/30, step: 253/364, loss: 0.23088, accuracy: 0.91206\n",
            "Epoch: 24/30, step: 254/364, loss: 0.23099, accuracy: 0.91197\n",
            "Epoch: 24/30, step: 255/364, loss: 0.23066, accuracy: 0.91225\n",
            "Epoch: 24/30, step: 256/364, loss: 0.23029, accuracy: 0.91248\n",
            "Epoch: 24/30, step: 257/364, loss: 0.23004, accuracy: 0.91263\n",
            "Epoch: 24/30, step: 258/364, loss: 0.22975, accuracy: 0.91279\n",
            "Epoch: 24/30, step: 259/364, loss: 0.22953, accuracy: 0.91283\n",
            "Epoch: 24/30, step: 260/364, loss: 0.22952, accuracy: 0.91292\n",
            "Epoch: 24/30, step: 261/364, loss: 0.22934, accuracy: 0.91301\n",
            "Epoch: 24/30, step: 262/364, loss: 0.22930, accuracy: 0.91305\n",
            "Epoch: 24/30, step: 263/364, loss: 0.22908, accuracy: 0.91314\n",
            "Epoch: 24/30, step: 264/364, loss: 0.22872, accuracy: 0.91335\n",
            "Epoch: 24/30, step: 265/364, loss: 0.22890, accuracy: 0.91315\n",
            "Epoch: 24/30, step: 266/364, loss: 0.22872, accuracy: 0.91318\n",
            "Epoch: 24/30, step: 267/364, loss: 0.22893, accuracy: 0.91304\n",
            "Epoch: 24/30, step: 268/364, loss: 0.22907, accuracy: 0.91295\n",
            "Epoch: 24/30, step: 269/364, loss: 0.22898, accuracy: 0.91293\n",
            "Epoch: 24/30, step: 270/364, loss: 0.22877, accuracy: 0.91302\n",
            "Epoch: 24/30, step: 271/364, loss: 0.22908, accuracy: 0.91282\n",
            "Epoch: 24/30, step: 272/364, loss: 0.22912, accuracy: 0.91280\n",
            "Epoch: 24/30, step: 273/364, loss: 0.22869, accuracy: 0.91306\n",
            "Epoch: 24/30, step: 274/364, loss: 0.22867, accuracy: 0.91304\n",
            "Epoch: 24/30, step: 275/364, loss: 0.22835, accuracy: 0.91330\n",
            "Epoch: 24/30, step: 276/364, loss: 0.22870, accuracy: 0.91316\n",
            "Epoch: 24/30, step: 277/364, loss: 0.22880, accuracy: 0.91308\n",
            "Epoch: 24/30, step: 278/364, loss: 0.22885, accuracy: 0.91288\n",
            "Epoch: 24/30, step: 279/364, loss: 0.22906, accuracy: 0.91286\n",
            "Epoch: 24/30, step: 280/364, loss: 0.22908, accuracy: 0.91283\n",
            "Epoch: 24/30, step: 281/364, loss: 0.22912, accuracy: 0.91287\n",
            "Epoch: 24/30, step: 282/364, loss: 0.22883, accuracy: 0.91301\n",
            "Epoch: 24/30, step: 283/364, loss: 0.22873, accuracy: 0.91310\n",
            "Epoch: 24/30, step: 284/364, loss: 0.22869, accuracy: 0.91318\n",
            "Epoch: 24/30, step: 285/364, loss: 0.22888, accuracy: 0.91321\n",
            "Epoch: 24/30, step: 286/364, loss: 0.22898, accuracy: 0.91313\n",
            "Epoch: 24/30, step: 287/364, loss: 0.22865, accuracy: 0.91327\n",
            "Epoch: 24/30, step: 288/364, loss: 0.22872, accuracy: 0.91336\n",
            "Epoch: 24/30, step: 289/364, loss: 0.22886, accuracy: 0.91333\n",
            "Epoch: 24/30, step: 290/364, loss: 0.22879, accuracy: 0.91331\n",
            "Epoch: 24/30, step: 291/364, loss: 0.22854, accuracy: 0.91338\n",
            "Epoch: 24/30, train loss: 0.22854, train accuracy: 0.91338, valid loss: 0.83050, valid accuracy: 0.66122\n",
            "Epoch: 25/30, step: 1/364, loss: 0.17894, accuracy: 0.93750\n",
            "Epoch: 25/30, step: 2/364, loss: 0.18495, accuracy: 0.92969\n",
            "Epoch: 25/30, step: 3/364, loss: 0.23819, accuracy: 0.89062\n",
            "Epoch: 25/30, step: 4/364, loss: 0.24496, accuracy: 0.90234\n",
            "Epoch: 25/30, step: 5/364, loss: 0.23556, accuracy: 0.90312\n",
            "Epoch: 25/30, step: 6/364, loss: 0.24752, accuracy: 0.90625\n",
            "Epoch: 25/30, step: 7/364, loss: 0.24753, accuracy: 0.90848\n",
            "Epoch: 25/30, step: 8/364, loss: 0.23916, accuracy: 0.91406\n",
            "Epoch: 25/30, step: 9/364, loss: 0.23168, accuracy: 0.91667\n",
            "Epoch: 25/30, step: 10/364, loss: 0.23468, accuracy: 0.91406\n",
            "Epoch: 25/30, step: 11/364, loss: 0.23139, accuracy: 0.91619\n",
            "Epoch: 25/30, step: 12/364, loss: 0.22589, accuracy: 0.91927\n",
            "Epoch: 25/30, step: 13/364, loss: 0.22450, accuracy: 0.91827\n",
            "Epoch: 25/30, step: 14/364, loss: 0.22367, accuracy: 0.91964\n",
            "Epoch: 25/30, step: 15/364, loss: 0.22457, accuracy: 0.91771\n",
            "Epoch: 25/30, step: 16/364, loss: 0.22274, accuracy: 0.91797\n",
            "Epoch: 25/30, step: 17/364, loss: 0.22142, accuracy: 0.91912\n",
            "Epoch: 25/30, step: 18/364, loss: 0.22665, accuracy: 0.91753\n",
            "Epoch: 25/30, step: 19/364, loss: 0.22126, accuracy: 0.92105\n",
            "Epoch: 25/30, step: 20/364, loss: 0.22021, accuracy: 0.92031\n",
            "Epoch: 25/30, step: 21/364, loss: 0.22110, accuracy: 0.92039\n",
            "Epoch: 25/30, step: 22/364, loss: 0.21706, accuracy: 0.92259\n",
            "Epoch: 25/30, step: 23/364, loss: 0.22302, accuracy: 0.91780\n",
            "Epoch: 25/30, step: 24/364, loss: 0.22253, accuracy: 0.91667\n",
            "Epoch: 25/30, step: 25/364, loss: 0.22189, accuracy: 0.91812\n",
            "Epoch: 25/30, step: 26/364, loss: 0.21926, accuracy: 0.92007\n",
            "Epoch: 25/30, step: 27/364, loss: 0.22003, accuracy: 0.92014\n",
            "Epoch: 25/30, step: 28/364, loss: 0.21780, accuracy: 0.92076\n",
            "Epoch: 25/30, step: 29/364, loss: 0.21627, accuracy: 0.92134\n",
            "Epoch: 25/30, step: 30/364, loss: 0.21831, accuracy: 0.91979\n",
            "Epoch: 25/30, step: 31/364, loss: 0.22279, accuracy: 0.91734\n",
            "Epoch: 25/30, step: 32/364, loss: 0.22117, accuracy: 0.91748\n",
            "Epoch: 25/30, step: 33/364, loss: 0.22152, accuracy: 0.91714\n",
            "Epoch: 25/30, step: 34/364, loss: 0.21932, accuracy: 0.91866\n",
            "Epoch: 25/30, step: 35/364, loss: 0.21998, accuracy: 0.91786\n",
            "Epoch: 25/30, step: 36/364, loss: 0.21973, accuracy: 0.91840\n",
            "Epoch: 25/30, step: 37/364, loss: 0.22036, accuracy: 0.91807\n",
            "Epoch: 25/30, step: 38/364, loss: 0.22189, accuracy: 0.91735\n",
            "Epoch: 25/30, step: 39/364, loss: 0.22214, accuracy: 0.91627\n",
            "Epoch: 25/30, step: 40/364, loss: 0.22129, accuracy: 0.91758\n",
            "Epoch: 25/30, step: 41/364, loss: 0.21937, accuracy: 0.91921\n",
            "Epoch: 25/30, step: 42/364, loss: 0.21783, accuracy: 0.91964\n",
            "Epoch: 25/30, step: 43/364, loss: 0.22048, accuracy: 0.91897\n",
            "Epoch: 25/30, step: 44/364, loss: 0.22202, accuracy: 0.91903\n",
            "Epoch: 25/30, step: 45/364, loss: 0.22274, accuracy: 0.91840\n",
            "Epoch: 25/30, step: 46/364, loss: 0.22353, accuracy: 0.91848\n",
            "Epoch: 25/30, step: 47/364, loss: 0.22308, accuracy: 0.91822\n",
            "Epoch: 25/30, step: 48/364, loss: 0.22189, accuracy: 0.91862\n",
            "Epoch: 25/30, step: 49/364, loss: 0.22050, accuracy: 0.91964\n",
            "Epoch: 25/30, step: 50/364, loss: 0.22029, accuracy: 0.92000\n",
            "Epoch: 25/30, step: 51/364, loss: 0.21913, accuracy: 0.92096\n",
            "Epoch: 25/30, step: 52/364, loss: 0.21868, accuracy: 0.92127\n",
            "Epoch: 25/30, step: 53/364, loss: 0.21797, accuracy: 0.92188\n",
            "Epoch: 25/30, step: 54/364, loss: 0.21902, accuracy: 0.92159\n",
            "Epoch: 25/30, step: 55/364, loss: 0.21797, accuracy: 0.92188\n",
            "Epoch: 25/30, step: 56/364, loss: 0.21803, accuracy: 0.92160\n",
            "Epoch: 25/30, step: 57/364, loss: 0.22026, accuracy: 0.92050\n",
            "Epoch: 25/30, step: 58/364, loss: 0.22141, accuracy: 0.92053\n",
            "Epoch: 25/30, step: 59/364, loss: 0.22175, accuracy: 0.92002\n",
            "Epoch: 25/30, step: 60/364, loss: 0.22205, accuracy: 0.91979\n",
            "Epoch: 25/30, step: 61/364, loss: 0.22165, accuracy: 0.91983\n",
            "Epoch: 25/30, step: 62/364, loss: 0.21982, accuracy: 0.92087\n",
            "Epoch: 25/30, step: 63/364, loss: 0.21922, accuracy: 0.92113\n",
            "Epoch: 25/30, step: 64/364, loss: 0.22004, accuracy: 0.92017\n",
            "Epoch: 25/30, step: 65/364, loss: 0.21947, accuracy: 0.92043\n",
            "Epoch: 25/30, step: 66/364, loss: 0.21920, accuracy: 0.92045\n",
            "Epoch: 25/30, step: 67/364, loss: 0.21925, accuracy: 0.92071\n",
            "Epoch: 25/30, step: 68/364, loss: 0.21866, accuracy: 0.92119\n",
            "Epoch: 25/30, step: 69/364, loss: 0.21939, accuracy: 0.92074\n",
            "Epoch: 25/30, step: 70/364, loss: 0.21995, accuracy: 0.92054\n",
            "Epoch: 25/30, step: 71/364, loss: 0.21930, accuracy: 0.92099\n",
            "Epoch: 25/30, step: 72/364, loss: 0.21918, accuracy: 0.92101\n",
            "Epoch: 25/30, step: 73/364, loss: 0.21805, accuracy: 0.92166\n",
            "Epoch: 25/30, step: 74/364, loss: 0.22018, accuracy: 0.92082\n",
            "Epoch: 25/30, step: 75/364, loss: 0.21982, accuracy: 0.92083\n",
            "Epoch: 25/30, step: 76/364, loss: 0.21943, accuracy: 0.92085\n",
            "Epoch: 25/30, step: 77/364, loss: 0.22146, accuracy: 0.91985\n",
            "Epoch: 25/30, step: 78/364, loss: 0.22186, accuracy: 0.91967\n",
            "Epoch: 25/30, step: 79/364, loss: 0.22174, accuracy: 0.91950\n",
            "Epoch: 25/30, step: 80/364, loss: 0.22144, accuracy: 0.91914\n",
            "Epoch: 25/30, step: 81/364, loss: 0.22160, accuracy: 0.91898\n",
            "Epoch: 25/30, step: 82/364, loss: 0.22302, accuracy: 0.91845\n",
            "Epoch: 25/30, step: 83/364, loss: 0.22353, accuracy: 0.91830\n",
            "Epoch: 25/30, step: 84/364, loss: 0.22513, accuracy: 0.91741\n",
            "Epoch: 25/30, step: 85/364, loss: 0.22475, accuracy: 0.91746\n",
            "Epoch: 25/30, step: 86/364, loss: 0.22476, accuracy: 0.91751\n",
            "Epoch: 25/30, step: 87/364, loss: 0.22396, accuracy: 0.91792\n",
            "Epoch: 25/30, step: 88/364, loss: 0.22321, accuracy: 0.91850\n",
            "Epoch: 25/30, step: 89/364, loss: 0.22214, accuracy: 0.91907\n",
            "Epoch: 25/30, step: 90/364, loss: 0.22130, accuracy: 0.91979\n",
            "Epoch: 25/30, step: 91/364, loss: 0.22233, accuracy: 0.91930\n",
            "Epoch: 25/30, step: 92/364, loss: 0.22277, accuracy: 0.91916\n",
            "Epoch: 25/30, step: 93/364, loss: 0.22296, accuracy: 0.91868\n",
            "Epoch: 25/30, step: 94/364, loss: 0.22279, accuracy: 0.91872\n",
            "Epoch: 25/30, step: 95/364, loss: 0.22239, accuracy: 0.91891\n",
            "Epoch: 25/30, step: 96/364, loss: 0.22206, accuracy: 0.91927\n",
            "Epoch: 25/30, step: 97/364, loss: 0.22154, accuracy: 0.91978\n",
            "Epoch: 25/30, step: 98/364, loss: 0.22145, accuracy: 0.91980\n",
            "Epoch: 25/30, step: 99/364, loss: 0.22169, accuracy: 0.91967\n",
            "Epoch: 25/30, step: 100/364, loss: 0.22235, accuracy: 0.91875\n",
            "Epoch: 25/30, step: 101/364, loss: 0.22420, accuracy: 0.91739\n",
            "Epoch: 25/30, step: 102/364, loss: 0.22397, accuracy: 0.91759\n",
            "Epoch: 25/30, step: 103/364, loss: 0.22340, accuracy: 0.91808\n",
            "Epoch: 25/30, step: 104/364, loss: 0.22252, accuracy: 0.91857\n",
            "Epoch: 25/30, step: 105/364, loss: 0.22173, accuracy: 0.91890\n",
            "Epoch: 25/30, step: 106/364, loss: 0.22299, accuracy: 0.91819\n",
            "Epoch: 25/30, step: 107/364, loss: 0.22182, accuracy: 0.91895\n",
            "Epoch: 25/30, step: 108/364, loss: 0.22229, accuracy: 0.91869\n",
            "Epoch: 25/30, step: 109/364, loss: 0.22244, accuracy: 0.91843\n",
            "Epoch: 25/30, step: 110/364, loss: 0.22257, accuracy: 0.91804\n",
            "Epoch: 25/30, step: 111/364, loss: 0.22324, accuracy: 0.91765\n",
            "Epoch: 25/30, step: 112/364, loss: 0.22260, accuracy: 0.91797\n",
            "Epoch: 25/30, step: 113/364, loss: 0.22229, accuracy: 0.91814\n",
            "Epoch: 25/30, step: 114/364, loss: 0.22201, accuracy: 0.91831\n",
            "Epoch: 25/30, step: 115/364, loss: 0.22095, accuracy: 0.91902\n",
            "Epoch: 25/30, step: 116/364, loss: 0.22050, accuracy: 0.91959\n",
            "Epoch: 25/30, step: 117/364, loss: 0.22133, accuracy: 0.91907\n",
            "Epoch: 25/30, step: 118/364, loss: 0.22046, accuracy: 0.91949\n",
            "Epoch: 25/30, step: 119/364, loss: 0.22033, accuracy: 0.91938\n",
            "Epoch: 25/30, step: 120/364, loss: 0.21994, accuracy: 0.91953\n",
            "Epoch: 25/30, step: 121/364, loss: 0.21978, accuracy: 0.91981\n",
            "Epoch: 25/30, step: 122/364, loss: 0.21998, accuracy: 0.91970\n",
            "Epoch: 25/30, step: 123/364, loss: 0.21990, accuracy: 0.91946\n",
            "Epoch: 25/30, step: 124/364, loss: 0.22026, accuracy: 0.91935\n",
            "Epoch: 25/30, step: 125/364, loss: 0.21977, accuracy: 0.91975\n",
            "Epoch: 25/30, step: 126/364, loss: 0.21983, accuracy: 0.91964\n",
            "Epoch: 25/30, step: 127/364, loss: 0.21969, accuracy: 0.91978\n",
            "Epoch: 25/30, step: 128/364, loss: 0.21949, accuracy: 0.91956\n",
            "Epoch: 25/30, step: 129/364, loss: 0.21944, accuracy: 0.91957\n",
            "Epoch: 25/30, step: 130/364, loss: 0.22009, accuracy: 0.91911\n",
            "Epoch: 25/30, step: 131/364, loss: 0.21988, accuracy: 0.91937\n",
            "Epoch: 25/30, step: 132/364, loss: 0.22171, accuracy: 0.91809\n",
            "Epoch: 25/30, step: 133/364, loss: 0.22092, accuracy: 0.91859\n",
            "Epoch: 25/30, step: 134/364, loss: 0.22040, accuracy: 0.91884\n",
            "Epoch: 25/30, step: 135/364, loss: 0.22077, accuracy: 0.91887\n",
            "Epoch: 25/30, step: 136/364, loss: 0.22078, accuracy: 0.91877\n",
            "Epoch: 25/30, step: 137/364, loss: 0.22099, accuracy: 0.91868\n",
            "Epoch: 25/30, step: 138/364, loss: 0.22238, accuracy: 0.91791\n",
            "Epoch: 25/30, step: 139/364, loss: 0.22371, accuracy: 0.91693\n",
            "Epoch: 25/30, step: 140/364, loss: 0.22315, accuracy: 0.91719\n",
            "Epoch: 25/30, step: 141/364, loss: 0.22383, accuracy: 0.91689\n",
            "Epoch: 25/30, step: 142/364, loss: 0.22378, accuracy: 0.91714\n",
            "Epoch: 25/30, step: 143/364, loss: 0.22415, accuracy: 0.91685\n",
            "Epoch: 25/30, step: 144/364, loss: 0.22493, accuracy: 0.91645\n",
            "Epoch: 25/30, step: 145/364, loss: 0.22471, accuracy: 0.91692\n",
            "Epoch: 25/30, step: 146/364, loss: 0.22415, accuracy: 0.91717\n",
            "Epoch: 25/30, step: 147/364, loss: 0.22368, accuracy: 0.91752\n",
            "Epoch: 25/30, step: 148/364, loss: 0.22314, accuracy: 0.91797\n",
            "Epoch: 25/30, step: 149/364, loss: 0.22287, accuracy: 0.91810\n",
            "Epoch: 25/30, step: 150/364, loss: 0.22247, accuracy: 0.91812\n",
            "Epoch: 25/30, step: 151/364, loss: 0.22212, accuracy: 0.91815\n",
            "Epoch: 25/30, step: 152/364, loss: 0.22162, accuracy: 0.91828\n",
            "Epoch: 25/30, step: 153/364, loss: 0.22140, accuracy: 0.91820\n",
            "Epoch: 25/30, step: 154/364, loss: 0.22136, accuracy: 0.91812\n",
            "Epoch: 25/30, step: 155/364, loss: 0.22190, accuracy: 0.91754\n",
            "Epoch: 25/30, step: 156/364, loss: 0.22233, accuracy: 0.91727\n",
            "Epoch: 25/30, step: 157/364, loss: 0.22214, accuracy: 0.91740\n",
            "Epoch: 25/30, step: 158/364, loss: 0.22197, accuracy: 0.91742\n",
            "Epoch: 25/30, step: 159/364, loss: 0.22201, accuracy: 0.91735\n",
            "Epoch: 25/30, step: 160/364, loss: 0.22334, accuracy: 0.91641\n",
            "Epoch: 25/30, step: 161/364, loss: 0.22430, accuracy: 0.91586\n",
            "Epoch: 25/30, step: 162/364, loss: 0.22403, accuracy: 0.91609\n",
            "Epoch: 25/30, step: 163/364, loss: 0.22452, accuracy: 0.91584\n",
            "Epoch: 25/30, step: 164/364, loss: 0.22448, accuracy: 0.91568\n",
            "Epoch: 25/30, step: 165/364, loss: 0.22425, accuracy: 0.91572\n",
            "Epoch: 25/30, step: 166/364, loss: 0.22445, accuracy: 0.91547\n",
            "Epoch: 25/30, step: 167/364, loss: 0.22476, accuracy: 0.91533\n",
            "Epoch: 25/30, step: 168/364, loss: 0.22465, accuracy: 0.91536\n",
            "Epoch: 25/30, step: 169/364, loss: 0.22454, accuracy: 0.91522\n",
            "Epoch: 25/30, step: 170/364, loss: 0.22431, accuracy: 0.91526\n",
            "Epoch: 25/30, step: 171/364, loss: 0.22501, accuracy: 0.91493\n",
            "Epoch: 25/30, step: 172/364, loss: 0.22497, accuracy: 0.91515\n",
            "Epoch: 25/30, step: 173/364, loss: 0.22471, accuracy: 0.91537\n",
            "Epoch: 25/30, step: 174/364, loss: 0.22451, accuracy: 0.91550\n",
            "Epoch: 25/30, step: 175/364, loss: 0.22428, accuracy: 0.91554\n",
            "Epoch: 25/30, step: 176/364, loss: 0.22415, accuracy: 0.91548\n",
            "Epoch: 25/30, step: 177/364, loss: 0.22425, accuracy: 0.91570\n",
            "Epoch: 25/30, step: 178/364, loss: 0.22426, accuracy: 0.91555\n",
            "Epoch: 25/30, step: 179/364, loss: 0.22431, accuracy: 0.91550\n",
            "Epoch: 25/30, step: 180/364, loss: 0.22458, accuracy: 0.91536\n",
            "Epoch: 25/30, step: 181/364, loss: 0.22467, accuracy: 0.91531\n",
            "Epoch: 25/30, step: 182/364, loss: 0.22450, accuracy: 0.91526\n",
            "Epoch: 25/30, step: 183/364, loss: 0.22476, accuracy: 0.91504\n",
            "Epoch: 25/30, step: 184/364, loss: 0.22467, accuracy: 0.91517\n",
            "Epoch: 25/30, step: 185/364, loss: 0.22439, accuracy: 0.91520\n",
            "Epoch: 25/30, step: 186/364, loss: 0.22404, accuracy: 0.91541\n",
            "Epoch: 25/30, step: 187/364, loss: 0.22366, accuracy: 0.91561\n",
            "Epoch: 25/30, step: 188/364, loss: 0.22310, accuracy: 0.91589\n",
            "Epoch: 25/30, step: 189/364, loss: 0.22321, accuracy: 0.91584\n",
            "Epoch: 25/30, step: 190/364, loss: 0.22266, accuracy: 0.91604\n",
            "Epoch: 25/30, step: 191/364, loss: 0.22242, accuracy: 0.91607\n",
            "Epoch: 25/30, step: 192/364, loss: 0.22285, accuracy: 0.91569\n",
            "Epoch: 25/30, step: 193/364, loss: 0.22261, accuracy: 0.91580\n",
            "Epoch: 25/30, step: 194/364, loss: 0.22227, accuracy: 0.91600\n",
            "Epoch: 25/30, step: 195/364, loss: 0.22182, accuracy: 0.91619\n",
            "Epoch: 25/30, step: 196/364, loss: 0.22217, accuracy: 0.91590\n",
            "Epoch: 25/30, step: 197/364, loss: 0.22241, accuracy: 0.91569\n",
            "Epoch: 25/30, step: 198/364, loss: 0.22205, accuracy: 0.91596\n",
            "Epoch: 25/30, step: 199/364, loss: 0.22245, accuracy: 0.91575\n",
            "Epoch: 25/30, step: 200/364, loss: 0.22243, accuracy: 0.91578\n",
            "Epoch: 25/30, step: 201/364, loss: 0.22237, accuracy: 0.91573\n",
            "Epoch: 25/30, step: 202/364, loss: 0.22305, accuracy: 0.91545\n",
            "Epoch: 25/30, step: 203/364, loss: 0.22330, accuracy: 0.91533\n",
            "Epoch: 25/30, step: 204/364, loss: 0.22296, accuracy: 0.91544\n",
            "Epoch: 25/30, step: 205/364, loss: 0.22278, accuracy: 0.91562\n",
            "Epoch: 25/30, step: 206/364, loss: 0.22280, accuracy: 0.91550\n",
            "Epoch: 25/30, step: 207/364, loss: 0.22258, accuracy: 0.91569\n",
            "Epoch: 25/30, step: 208/364, loss: 0.22234, accuracy: 0.91587\n",
            "Epoch: 25/30, step: 209/364, loss: 0.22193, accuracy: 0.91619\n",
            "Epoch: 25/30, step: 210/364, loss: 0.22157, accuracy: 0.91644\n",
            "Epoch: 25/30, step: 211/364, loss: 0.22153, accuracy: 0.91640\n",
            "Epoch: 25/30, step: 212/364, loss: 0.22149, accuracy: 0.91627\n",
            "Epoch: 25/30, step: 213/364, loss: 0.22125, accuracy: 0.91645\n",
            "Epoch: 25/30, step: 214/364, loss: 0.22147, accuracy: 0.91633\n",
            "Epoch: 25/30, step: 215/364, loss: 0.22139, accuracy: 0.91642\n",
            "Epoch: 25/30, step: 216/364, loss: 0.22141, accuracy: 0.91638\n",
            "Epoch: 25/30, step: 217/364, loss: 0.22151, accuracy: 0.91626\n",
            "Epoch: 25/30, step: 218/364, loss: 0.22121, accuracy: 0.91657\n",
            "Epoch: 25/30, step: 219/364, loss: 0.22163, accuracy: 0.91624\n",
            "Epoch: 25/30, step: 220/364, loss: 0.22146, accuracy: 0.91648\n",
            "Epoch: 25/30, step: 221/364, loss: 0.22178, accuracy: 0.91622\n",
            "Epoch: 25/30, step: 222/364, loss: 0.22154, accuracy: 0.91631\n",
            "Epoch: 25/30, step: 223/364, loss: 0.22184, accuracy: 0.91627\n",
            "Epoch: 25/30, step: 224/364, loss: 0.22224, accuracy: 0.91595\n",
            "Epoch: 25/30, step: 225/364, loss: 0.22249, accuracy: 0.91583\n",
            "Epoch: 25/30, step: 226/364, loss: 0.22299, accuracy: 0.91545\n",
            "Epoch: 25/30, step: 227/364, loss: 0.22317, accuracy: 0.91540\n",
            "Epoch: 25/30, step: 228/364, loss: 0.22313, accuracy: 0.91536\n",
            "Epoch: 25/30, step: 229/364, loss: 0.22259, accuracy: 0.91573\n",
            "Epoch: 25/30, step: 230/364, loss: 0.22258, accuracy: 0.91569\n",
            "Epoch: 25/30, step: 231/364, loss: 0.22276, accuracy: 0.91538\n",
            "Epoch: 25/30, step: 232/364, loss: 0.22257, accuracy: 0.91534\n",
            "Epoch: 25/30, step: 233/364, loss: 0.22248, accuracy: 0.91537\n",
            "Epoch: 25/30, step: 234/364, loss: 0.22278, accuracy: 0.91526\n",
            "Epoch: 25/30, step: 235/364, loss: 0.22303, accuracy: 0.91529\n",
            "Epoch: 25/30, step: 236/364, loss: 0.22288, accuracy: 0.91525\n",
            "Epoch: 25/30, step: 237/364, loss: 0.22304, accuracy: 0.91528\n",
            "Epoch: 25/30, step: 238/364, loss: 0.22306, accuracy: 0.91524\n",
            "Epoch: 25/30, step: 239/364, loss: 0.22289, accuracy: 0.91521\n",
            "Epoch: 25/30, step: 240/364, loss: 0.22294, accuracy: 0.91523\n",
            "Epoch: 25/30, step: 241/364, loss: 0.22263, accuracy: 0.91539\n",
            "Epoch: 25/30, step: 242/364, loss: 0.22252, accuracy: 0.91542\n",
            "Epoch: 25/30, step: 243/364, loss: 0.22290, accuracy: 0.91519\n",
            "Epoch: 25/30, step: 244/364, loss: 0.22245, accuracy: 0.91541\n",
            "Epoch: 25/30, step: 245/364, loss: 0.22228, accuracy: 0.91550\n",
            "Epoch: 25/30, step: 246/364, loss: 0.22243, accuracy: 0.91540\n",
            "Epoch: 25/30, step: 247/364, loss: 0.22212, accuracy: 0.91549\n",
            "Epoch: 25/30, step: 248/364, loss: 0.22210, accuracy: 0.91545\n",
            "Epoch: 25/30, step: 249/364, loss: 0.22165, accuracy: 0.91566\n",
            "Epoch: 25/30, step: 250/364, loss: 0.22161, accuracy: 0.91575\n",
            "Epoch: 25/30, step: 251/364, loss: 0.22141, accuracy: 0.91590\n",
            "Epoch: 25/30, step: 252/364, loss: 0.22135, accuracy: 0.91592\n",
            "Epoch: 25/30, step: 253/364, loss: 0.22139, accuracy: 0.91595\n",
            "Epoch: 25/30, step: 254/364, loss: 0.22129, accuracy: 0.91603\n",
            "Epoch: 25/30, step: 255/364, loss: 0.22106, accuracy: 0.91612\n",
            "Epoch: 25/30, step: 256/364, loss: 0.22081, accuracy: 0.91620\n",
            "Epoch: 25/30, step: 257/364, loss: 0.22084, accuracy: 0.91616\n",
            "Epoch: 25/30, step: 258/364, loss: 0.22090, accuracy: 0.91594\n",
            "Epoch: 25/30, step: 259/364, loss: 0.22091, accuracy: 0.91596\n",
            "Epoch: 25/30, step: 260/364, loss: 0.22125, accuracy: 0.91556\n",
            "Epoch: 25/30, step: 261/364, loss: 0.22091, accuracy: 0.91571\n",
            "Epoch: 25/30, step: 262/364, loss: 0.22056, accuracy: 0.91585\n",
            "Epoch: 25/30, step: 263/364, loss: 0.22060, accuracy: 0.91576\n",
            "Epoch: 25/30, step: 264/364, loss: 0.22071, accuracy: 0.91572\n",
            "Epoch: 25/30, step: 265/364, loss: 0.22088, accuracy: 0.91574\n",
            "Epoch: 25/30, step: 266/364, loss: 0.22105, accuracy: 0.91565\n",
            "Epoch: 25/30, step: 267/364, loss: 0.22101, accuracy: 0.91567\n",
            "Epoch: 25/30, step: 268/364, loss: 0.22084, accuracy: 0.91569\n",
            "Epoch: 25/30, step: 269/364, loss: 0.22076, accuracy: 0.91583\n",
            "Epoch: 25/30, step: 270/364, loss: 0.22095, accuracy: 0.91586\n",
            "Epoch: 25/30, step: 271/364, loss: 0.22112, accuracy: 0.91576\n",
            "Epoch: 25/30, step: 272/364, loss: 0.22137, accuracy: 0.91561\n",
            "Epoch: 25/30, step: 273/364, loss: 0.22128, accuracy: 0.91552\n",
            "Epoch: 25/30, step: 274/364, loss: 0.22118, accuracy: 0.91572\n",
            "Epoch: 25/30, step: 275/364, loss: 0.22140, accuracy: 0.91545\n",
            "Epoch: 25/30, step: 276/364, loss: 0.22131, accuracy: 0.91542\n",
            "Epoch: 25/30, step: 277/364, loss: 0.22123, accuracy: 0.91556\n",
            "Epoch: 25/30, step: 278/364, loss: 0.22117, accuracy: 0.91569\n",
            "Epoch: 25/30, step: 279/364, loss: 0.22131, accuracy: 0.91549\n",
            "Epoch: 25/30, step: 280/364, loss: 0.22097, accuracy: 0.91562\n",
            "Epoch: 25/30, step: 281/364, loss: 0.22081, accuracy: 0.91570\n",
            "Epoch: 25/30, step: 282/364, loss: 0.22058, accuracy: 0.91578\n",
            "Epoch: 25/30, step: 283/364, loss: 0.22049, accuracy: 0.91575\n",
            "Epoch: 25/30, step: 284/364, loss: 0.22021, accuracy: 0.91593\n",
            "Epoch: 25/30, step: 285/364, loss: 0.21984, accuracy: 0.91606\n",
            "Epoch: 25/30, step: 286/364, loss: 0.21963, accuracy: 0.91625\n",
            "Epoch: 25/30, step: 287/364, loss: 0.21935, accuracy: 0.91643\n",
            "Epoch: 25/30, step: 288/364, loss: 0.21927, accuracy: 0.91661\n",
            "Epoch: 25/30, step: 289/364, loss: 0.21952, accuracy: 0.91647\n",
            "Epoch: 25/30, step: 290/364, loss: 0.21979, accuracy: 0.91643\n",
            "Epoch: 25/30, step: 291/364, loss: 0.21965, accuracy: 0.91655\n",
            "Epoch: 25/30, train loss: 0.21965, train accuracy: 0.91655, valid loss: 0.75167, valid accuracy: 0.68766\n",
            "Epoch: 26/30, step: 1/364, loss: 0.15354, accuracy: 0.96875\n",
            "Epoch: 26/30, step: 2/364, loss: 0.18273, accuracy: 0.94531\n",
            "Epoch: 26/30, step: 3/364, loss: 0.16899, accuracy: 0.95312\n",
            "Epoch: 26/30, step: 4/364, loss: 0.20744, accuracy: 0.91797\n",
            "Epoch: 26/30, step: 5/364, loss: 0.20632, accuracy: 0.92188\n",
            "Epoch: 26/30, step: 6/364, loss: 0.19007, accuracy: 0.93229\n",
            "Epoch: 26/30, step: 7/364, loss: 0.18340, accuracy: 0.94196\n",
            "Epoch: 26/30, step: 8/364, loss: 0.18125, accuracy: 0.94336\n",
            "Epoch: 26/30, step: 9/364, loss: 0.18316, accuracy: 0.94271\n",
            "Epoch: 26/30, step: 10/364, loss: 0.19333, accuracy: 0.93281\n",
            "Epoch: 26/30, step: 11/364, loss: 0.18979, accuracy: 0.93892\n",
            "Epoch: 26/30, step: 12/364, loss: 0.19136, accuracy: 0.93880\n",
            "Epoch: 26/30, step: 13/364, loss: 0.18918, accuracy: 0.94111\n",
            "Epoch: 26/30, step: 14/364, loss: 0.18677, accuracy: 0.94308\n",
            "Epoch: 26/30, step: 15/364, loss: 0.18544, accuracy: 0.94375\n",
            "Epoch: 26/30, step: 16/364, loss: 0.18294, accuracy: 0.94531\n",
            "Epoch: 26/30, step: 17/364, loss: 0.18141, accuracy: 0.94669\n",
            "Epoch: 26/30, step: 18/364, loss: 0.17853, accuracy: 0.94965\n",
            "Epoch: 26/30, step: 19/364, loss: 0.18062, accuracy: 0.94737\n",
            "Epoch: 26/30, step: 20/364, loss: 0.18625, accuracy: 0.94453\n",
            "Epoch: 26/30, step: 21/364, loss: 0.18441, accuracy: 0.94420\n",
            "Epoch: 26/30, step: 22/364, loss: 0.19560, accuracy: 0.93821\n",
            "Epoch: 26/30, step: 23/364, loss: 0.20011, accuracy: 0.93274\n",
            "Epoch: 26/30, step: 24/364, loss: 0.19805, accuracy: 0.93359\n",
            "Epoch: 26/30, step: 25/364, loss: 0.19674, accuracy: 0.93437\n",
            "Epoch: 26/30, step: 26/364, loss: 0.19705, accuracy: 0.93450\n",
            "Epoch: 26/30, step: 27/364, loss: 0.19683, accuracy: 0.93461\n",
            "Epoch: 26/30, step: 28/364, loss: 0.19368, accuracy: 0.93638\n",
            "Epoch: 26/30, step: 29/364, loss: 0.19247, accuracy: 0.93588\n",
            "Epoch: 26/30, step: 30/364, loss: 0.19040, accuracy: 0.93750\n",
            "Epoch: 26/30, step: 31/364, loss: 0.18758, accuracy: 0.93901\n",
            "Epoch: 26/30, step: 32/364, loss: 0.18564, accuracy: 0.93994\n",
            "Epoch: 26/30, step: 33/364, loss: 0.19019, accuracy: 0.93750\n",
            "Epoch: 26/30, step: 34/364, loss: 0.18936, accuracy: 0.93750\n",
            "Epoch: 26/30, step: 35/364, loss: 0.19149, accuracy: 0.93661\n",
            "Epoch: 26/30, step: 36/364, loss: 0.19244, accuracy: 0.93533\n",
            "Epoch: 26/30, step: 37/364, loss: 0.19258, accuracy: 0.93539\n",
            "Epoch: 26/30, step: 38/364, loss: 0.19648, accuracy: 0.93215\n",
            "Epoch: 26/30, step: 39/364, loss: 0.19549, accuracy: 0.93269\n",
            "Epoch: 26/30, step: 40/364, loss: 0.19549, accuracy: 0.93281\n",
            "Epoch: 26/30, step: 41/364, loss: 0.19411, accuracy: 0.93369\n",
            "Epoch: 26/30, step: 42/364, loss: 0.19798, accuracy: 0.93192\n",
            "Epoch: 26/30, step: 43/364, loss: 0.19792, accuracy: 0.93205\n",
            "Epoch: 26/30, step: 44/364, loss: 0.19698, accuracy: 0.93288\n",
            "Epoch: 26/30, step: 45/364, loss: 0.19676, accuracy: 0.93299\n",
            "Epoch: 26/30, step: 46/364, loss: 0.19683, accuracy: 0.93308\n",
            "Epoch: 26/30, step: 47/364, loss: 0.20011, accuracy: 0.93085\n",
            "Epoch: 26/30, step: 48/364, loss: 0.20062, accuracy: 0.93034\n",
            "Epoch: 26/30, step: 49/364, loss: 0.20001, accuracy: 0.93017\n",
            "Epoch: 26/30, step: 50/364, loss: 0.20062, accuracy: 0.92875\n",
            "Epoch: 26/30, step: 51/364, loss: 0.20044, accuracy: 0.92892\n",
            "Epoch: 26/30, step: 52/364, loss: 0.20057, accuracy: 0.92939\n",
            "Epoch: 26/30, step: 53/364, loss: 0.19983, accuracy: 0.93013\n",
            "Epoch: 26/30, step: 54/364, loss: 0.19873, accuracy: 0.93084\n",
            "Epoch: 26/30, step: 55/364, loss: 0.19883, accuracy: 0.93068\n",
            "Epoch: 26/30, step: 56/364, loss: 0.19759, accuracy: 0.93108\n",
            "Epoch: 26/30, step: 57/364, loss: 0.19846, accuracy: 0.92982\n",
            "Epoch: 26/30, step: 58/364, loss: 0.19750, accuracy: 0.93077\n",
            "Epoch: 26/30, step: 59/364, loss: 0.19690, accuracy: 0.93114\n",
            "Epoch: 26/30, step: 60/364, loss: 0.19573, accuracy: 0.93203\n",
            "Epoch: 26/30, step: 61/364, loss: 0.19660, accuracy: 0.93058\n",
            "Epoch: 26/30, step: 62/364, loss: 0.19785, accuracy: 0.92969\n",
            "Epoch: 26/30, step: 63/364, loss: 0.19742, accuracy: 0.92981\n",
            "Epoch: 26/30, step: 64/364, loss: 0.19731, accuracy: 0.92969\n",
            "Epoch: 26/30, step: 65/364, loss: 0.19741, accuracy: 0.92957\n",
            "Epoch: 26/30, step: 66/364, loss: 0.19735, accuracy: 0.92921\n",
            "Epoch: 26/30, step: 67/364, loss: 0.19651, accuracy: 0.92980\n",
            "Epoch: 26/30, step: 68/364, loss: 0.19558, accuracy: 0.93038\n",
            "Epoch: 26/30, step: 69/364, loss: 0.19458, accuracy: 0.93048\n",
            "Epoch: 26/30, step: 70/364, loss: 0.19524, accuracy: 0.93013\n",
            "Epoch: 26/30, step: 71/364, loss: 0.19528, accuracy: 0.93002\n",
            "Epoch: 26/30, step: 72/364, loss: 0.19556, accuracy: 0.92947\n",
            "Epoch: 26/30, step: 73/364, loss: 0.19507, accuracy: 0.92979\n",
            "Epoch: 26/30, step: 74/364, loss: 0.19429, accuracy: 0.93011\n",
            "Epoch: 26/30, step: 75/364, loss: 0.19556, accuracy: 0.92958\n",
            "Epoch: 26/30, step: 76/364, loss: 0.19532, accuracy: 0.92989\n",
            "Epoch: 26/30, step: 77/364, loss: 0.19517, accuracy: 0.93019\n",
            "Epoch: 26/30, step: 78/364, loss: 0.19589, accuracy: 0.92989\n",
            "Epoch: 26/30, step: 79/364, loss: 0.19576, accuracy: 0.92959\n",
            "Epoch: 26/30, step: 80/364, loss: 0.19672, accuracy: 0.92949\n",
            "Epoch: 26/30, step: 81/364, loss: 0.19625, accuracy: 0.92998\n",
            "Epoch: 26/30, step: 82/364, loss: 0.19638, accuracy: 0.92969\n",
            "Epoch: 26/30, step: 83/364, loss: 0.19667, accuracy: 0.92922\n",
            "Epoch: 26/30, step: 84/364, loss: 0.19691, accuracy: 0.92932\n",
            "Epoch: 26/30, step: 85/364, loss: 0.19724, accuracy: 0.92923\n",
            "Epoch: 26/30, step: 86/364, loss: 0.19785, accuracy: 0.92914\n",
            "Epoch: 26/30, step: 87/364, loss: 0.19795, accuracy: 0.92906\n",
            "Epoch: 26/30, step: 88/364, loss: 0.20005, accuracy: 0.92773\n",
            "Epoch: 26/30, step: 89/364, loss: 0.20191, accuracy: 0.92644\n",
            "Epoch: 26/30, step: 90/364, loss: 0.20127, accuracy: 0.92691\n",
            "Epoch: 26/30, step: 91/364, loss: 0.20127, accuracy: 0.92703\n",
            "Epoch: 26/30, step: 92/364, loss: 0.20048, accuracy: 0.92731\n",
            "Epoch: 26/30, step: 93/364, loss: 0.20005, accuracy: 0.92759\n",
            "Epoch: 26/30, step: 94/364, loss: 0.20062, accuracy: 0.92719\n",
            "Epoch: 26/30, step: 95/364, loss: 0.20076, accuracy: 0.92714\n",
            "Epoch: 26/30, step: 96/364, loss: 0.20097, accuracy: 0.92708\n",
            "Epoch: 26/30, step: 97/364, loss: 0.20087, accuracy: 0.92735\n",
            "Epoch: 26/30, step: 98/364, loss: 0.20028, accuracy: 0.92761\n",
            "Epoch: 26/30, step: 99/364, loss: 0.20043, accuracy: 0.92756\n",
            "Epoch: 26/30, step: 100/364, loss: 0.20200, accuracy: 0.92641\n",
            "Epoch: 26/30, step: 101/364, loss: 0.20199, accuracy: 0.92605\n",
            "Epoch: 26/30, step: 102/364, loss: 0.20230, accuracy: 0.92601\n",
            "Epoch: 26/30, step: 103/364, loss: 0.20221, accuracy: 0.92597\n",
            "Epoch: 26/30, step: 104/364, loss: 0.20225, accuracy: 0.92593\n",
            "Epoch: 26/30, step: 105/364, loss: 0.20286, accuracy: 0.92560\n",
            "Epoch: 26/30, step: 106/364, loss: 0.20259, accuracy: 0.92600\n",
            "Epoch: 26/30, step: 107/364, loss: 0.20289, accuracy: 0.92582\n",
            "Epoch: 26/30, step: 108/364, loss: 0.20283, accuracy: 0.92593\n",
            "Epoch: 26/30, step: 109/364, loss: 0.20270, accuracy: 0.92589\n",
            "Epoch: 26/30, step: 110/364, loss: 0.20198, accuracy: 0.92656\n",
            "Epoch: 26/30, step: 111/364, loss: 0.20194, accuracy: 0.92666\n",
            "Epoch: 26/30, step: 112/364, loss: 0.20217, accuracy: 0.92620\n",
            "Epoch: 26/30, step: 113/364, loss: 0.20215, accuracy: 0.92630\n",
            "Epoch: 26/30, step: 114/364, loss: 0.20182, accuracy: 0.92640\n",
            "Epoch: 26/30, step: 115/364, loss: 0.20141, accuracy: 0.92677\n",
            "Epoch: 26/30, step: 116/364, loss: 0.20128, accuracy: 0.92672\n",
            "Epoch: 26/30, step: 117/364, loss: 0.20116, accuracy: 0.92682\n",
            "Epoch: 26/30, step: 118/364, loss: 0.20102, accuracy: 0.92691\n",
            "Epoch: 26/30, step: 119/364, loss: 0.20167, accuracy: 0.92673\n",
            "Epoch: 26/30, step: 120/364, loss: 0.20130, accuracy: 0.92695\n",
            "Epoch: 26/30, step: 121/364, loss: 0.20251, accuracy: 0.92652\n",
            "Epoch: 26/30, step: 122/364, loss: 0.20188, accuracy: 0.92674\n",
            "Epoch: 26/30, step: 123/364, loss: 0.20234, accuracy: 0.92619\n",
            "Epoch: 26/30, step: 124/364, loss: 0.20176, accuracy: 0.92666\n",
            "Epoch: 26/30, step: 125/364, loss: 0.20183, accuracy: 0.92675\n",
            "Epoch: 26/30, step: 126/364, loss: 0.20245, accuracy: 0.92634\n",
            "Epoch: 26/30, step: 127/364, loss: 0.20200, accuracy: 0.92655\n",
            "Epoch: 26/30, step: 128/364, loss: 0.20146, accuracy: 0.92688\n",
            "Epoch: 26/30, step: 129/364, loss: 0.20116, accuracy: 0.92720\n",
            "Epoch: 26/30, step: 130/364, loss: 0.20139, accuracy: 0.92704\n",
            "Epoch: 26/30, step: 131/364, loss: 0.20055, accuracy: 0.92748\n",
            "Epoch: 26/30, step: 132/364, loss: 0.20009, accuracy: 0.92779\n",
            "Epoch: 26/30, step: 133/364, loss: 0.19973, accuracy: 0.92787\n",
            "Epoch: 26/30, step: 134/364, loss: 0.19905, accuracy: 0.92829\n",
            "Epoch: 26/30, step: 135/364, loss: 0.19870, accuracy: 0.92847\n",
            "Epoch: 26/30, step: 136/364, loss: 0.19918, accuracy: 0.92796\n",
            "Epoch: 26/30, step: 137/364, loss: 0.19971, accuracy: 0.92735\n",
            "Epoch: 26/30, step: 138/364, loss: 0.19937, accuracy: 0.92754\n",
            "Epoch: 26/30, step: 139/364, loss: 0.19962, accuracy: 0.92705\n",
            "Epoch: 26/30, step: 140/364, loss: 0.19940, accuracy: 0.92723\n",
            "Epoch: 26/30, step: 141/364, loss: 0.19956, accuracy: 0.92697\n",
            "Epoch: 26/30, step: 142/364, loss: 0.19974, accuracy: 0.92683\n",
            "Epoch: 26/30, step: 143/364, loss: 0.19959, accuracy: 0.92690\n",
            "Epoch: 26/30, step: 144/364, loss: 0.20019, accuracy: 0.92632\n",
            "Epoch: 26/30, step: 145/364, loss: 0.20017, accuracy: 0.92651\n",
            "Epoch: 26/30, step: 146/364, loss: 0.20030, accuracy: 0.92626\n",
            "Epoch: 26/30, step: 147/364, loss: 0.20165, accuracy: 0.92570\n",
            "Epoch: 26/30, step: 148/364, loss: 0.20121, accuracy: 0.92599\n",
            "Epoch: 26/30, step: 149/364, loss: 0.20098, accuracy: 0.92628\n",
            "Epoch: 26/30, step: 150/364, loss: 0.20066, accuracy: 0.92635\n",
            "Epoch: 26/30, step: 151/364, loss: 0.20055, accuracy: 0.92622\n",
            "Epoch: 26/30, step: 152/364, loss: 0.20100, accuracy: 0.92599\n",
            "Epoch: 26/30, step: 153/364, loss: 0.20095, accuracy: 0.92616\n",
            "Epoch: 26/30, step: 154/364, loss: 0.20112, accuracy: 0.92603\n",
            "Epoch: 26/30, step: 155/364, loss: 0.20081, accuracy: 0.92631\n",
            "Epoch: 26/30, step: 156/364, loss: 0.20048, accuracy: 0.92668\n",
            "Epoch: 26/30, step: 157/364, loss: 0.20040, accuracy: 0.92695\n",
            "Epoch: 26/30, step: 158/364, loss: 0.20007, accuracy: 0.92712\n",
            "Epoch: 26/30, step: 159/364, loss: 0.19982, accuracy: 0.92718\n",
            "Epoch: 26/30, step: 160/364, loss: 0.20076, accuracy: 0.92676\n",
            "Epoch: 26/30, step: 161/364, loss: 0.20136, accuracy: 0.92644\n",
            "Epoch: 26/30, step: 162/364, loss: 0.20156, accuracy: 0.92631\n",
            "Epoch: 26/30, step: 163/364, loss: 0.20168, accuracy: 0.92609\n",
            "Epoch: 26/30, step: 164/364, loss: 0.20114, accuracy: 0.92635\n",
            "Epoch: 26/30, step: 165/364, loss: 0.20112, accuracy: 0.92623\n",
            "Epoch: 26/30, step: 166/364, loss: 0.20145, accuracy: 0.92611\n",
            "Epoch: 26/30, step: 167/364, loss: 0.20163, accuracy: 0.92609\n",
            "Epoch: 26/30, step: 168/364, loss: 0.20272, accuracy: 0.92550\n",
            "Epoch: 26/30, step: 169/364, loss: 0.20303, accuracy: 0.92530\n",
            "Epoch: 26/30, step: 170/364, loss: 0.20275, accuracy: 0.92546\n",
            "Epoch: 26/30, step: 171/364, loss: 0.20298, accuracy: 0.92516\n",
            "Epoch: 26/30, step: 172/364, loss: 0.20266, accuracy: 0.92524\n",
            "Epoch: 26/30, step: 173/364, loss: 0.20353, accuracy: 0.92486\n",
            "Epoch: 26/30, step: 174/364, loss: 0.20387, accuracy: 0.92466\n",
            "Epoch: 26/30, step: 175/364, loss: 0.20371, accuracy: 0.92473\n",
            "Epoch: 26/30, step: 176/364, loss: 0.20345, accuracy: 0.92498\n",
            "Epoch: 26/30, step: 177/364, loss: 0.20311, accuracy: 0.92514\n",
            "Epoch: 26/30, step: 178/364, loss: 0.20390, accuracy: 0.92477\n",
            "Epoch: 26/30, step: 179/364, loss: 0.20401, accuracy: 0.92484\n",
            "Epoch: 26/30, step: 180/364, loss: 0.20340, accuracy: 0.92509\n",
            "Epoch: 26/30, step: 181/364, loss: 0.20392, accuracy: 0.92481\n",
            "Epoch: 26/30, step: 182/364, loss: 0.20467, accuracy: 0.92454\n",
            "Epoch: 26/30, step: 183/364, loss: 0.20562, accuracy: 0.92392\n",
            "Epoch: 26/30, step: 184/364, loss: 0.20562, accuracy: 0.92383\n",
            "Epoch: 26/30, step: 185/364, loss: 0.20553, accuracy: 0.92365\n",
            "Epoch: 26/30, step: 186/364, loss: 0.20591, accuracy: 0.92347\n",
            "Epoch: 26/30, step: 187/364, loss: 0.20585, accuracy: 0.92355\n",
            "Epoch: 26/30, step: 188/364, loss: 0.20520, accuracy: 0.92387\n",
            "Epoch: 26/30, step: 189/364, loss: 0.20594, accuracy: 0.92361\n",
            "Epoch: 26/30, step: 190/364, loss: 0.20582, accuracy: 0.92385\n",
            "Epoch: 26/30, step: 191/364, loss: 0.20583, accuracy: 0.92384\n",
            "Epoch: 26/30, step: 192/364, loss: 0.20558, accuracy: 0.92391\n",
            "Epoch: 26/30, step: 193/364, loss: 0.20569, accuracy: 0.92382\n",
            "Epoch: 26/30, step: 194/364, loss: 0.20531, accuracy: 0.92389\n",
            "Epoch: 26/30, step: 195/364, loss: 0.20545, accuracy: 0.92364\n",
            "Epoch: 26/30, step: 196/364, loss: 0.20507, accuracy: 0.92379\n",
            "Epoch: 26/30, step: 197/364, loss: 0.20467, accuracy: 0.92402\n",
            "Epoch: 26/30, step: 198/364, loss: 0.20461, accuracy: 0.92408\n",
            "Epoch: 26/30, step: 199/364, loss: 0.20411, accuracy: 0.92431\n",
            "Epoch: 26/30, step: 200/364, loss: 0.20412, accuracy: 0.92430\n",
            "Epoch: 26/30, step: 201/364, loss: 0.20408, accuracy: 0.92428\n",
            "Epoch: 26/30, step: 202/364, loss: 0.20353, accuracy: 0.92458\n",
            "Epoch: 26/30, step: 203/364, loss: 0.20359, accuracy: 0.92449\n",
            "Epoch: 26/30, step: 204/364, loss: 0.20350, accuracy: 0.92456\n",
            "Epoch: 26/30, step: 205/364, loss: 0.20304, accuracy: 0.92485\n",
            "Epoch: 26/30, step: 206/364, loss: 0.20297, accuracy: 0.92483\n",
            "Epoch: 26/30, step: 207/364, loss: 0.20282, accuracy: 0.92474\n",
            "Epoch: 26/30, step: 208/364, loss: 0.20256, accuracy: 0.92495\n",
            "Epoch: 26/30, step: 209/364, loss: 0.20277, accuracy: 0.92487\n",
            "Epoch: 26/30, step: 210/364, loss: 0.20263, accuracy: 0.92478\n",
            "Epoch: 26/30, step: 211/364, loss: 0.20228, accuracy: 0.92491\n",
            "Epoch: 26/30, step: 212/364, loss: 0.20208, accuracy: 0.92490\n",
            "Epoch: 26/30, step: 213/364, loss: 0.20170, accuracy: 0.92510\n",
            "Epoch: 26/30, step: 214/364, loss: 0.20159, accuracy: 0.92509\n",
            "Epoch: 26/30, step: 215/364, loss: 0.20170, accuracy: 0.92507\n",
            "Epoch: 26/30, step: 216/364, loss: 0.20165, accuracy: 0.92506\n",
            "Epoch: 26/30, step: 217/364, loss: 0.20132, accuracy: 0.92519\n",
            "Epoch: 26/30, step: 218/364, loss: 0.20114, accuracy: 0.92532\n",
            "Epoch: 26/30, step: 219/364, loss: 0.20151, accuracy: 0.92501\n",
            "Epoch: 26/30, step: 220/364, loss: 0.20132, accuracy: 0.92521\n",
            "Epoch: 26/30, step: 221/364, loss: 0.20163, accuracy: 0.92499\n",
            "Epoch: 26/30, step: 222/364, loss: 0.20150, accuracy: 0.92511\n",
            "Epoch: 26/30, step: 223/364, loss: 0.20114, accuracy: 0.92531\n",
            "Epoch: 26/30, step: 224/364, loss: 0.20118, accuracy: 0.92522\n",
            "Epoch: 26/30, step: 225/364, loss: 0.20110, accuracy: 0.92528\n",
            "Epoch: 26/30, step: 226/364, loss: 0.20176, accuracy: 0.92478\n",
            "Epoch: 26/30, step: 227/364, loss: 0.20160, accuracy: 0.92477\n",
            "Epoch: 26/30, step: 228/364, loss: 0.20148, accuracy: 0.92482\n",
            "Epoch: 26/30, step: 229/364, loss: 0.20119, accuracy: 0.92495\n",
            "Epoch: 26/30, step: 230/364, loss: 0.20097, accuracy: 0.92507\n",
            "Epoch: 26/30, step: 231/364, loss: 0.20108, accuracy: 0.92485\n",
            "Epoch: 26/30, step: 232/364, loss: 0.20088, accuracy: 0.92491\n",
            "Epoch: 26/30, step: 233/364, loss: 0.20087, accuracy: 0.92503\n",
            "Epoch: 26/30, step: 234/364, loss: 0.20076, accuracy: 0.92495\n",
            "Epoch: 26/30, step: 235/364, loss: 0.20049, accuracy: 0.92520\n",
            "Epoch: 26/30, step: 236/364, loss: 0.20024, accuracy: 0.92538\n",
            "Epoch: 26/30, step: 237/364, loss: 0.20044, accuracy: 0.92524\n",
            "Epoch: 26/30, step: 238/364, loss: 0.20014, accuracy: 0.92542\n",
            "Epoch: 26/30, step: 239/364, loss: 0.20000, accuracy: 0.92554\n",
            "Epoch: 26/30, step: 240/364, loss: 0.20015, accuracy: 0.92533\n",
            "Epoch: 26/30, step: 241/364, loss: 0.19990, accuracy: 0.92544\n",
            "Epoch: 26/30, step: 242/364, loss: 0.19976, accuracy: 0.92549\n",
            "Epoch: 26/30, step: 243/364, loss: 0.19985, accuracy: 0.92535\n",
            "Epoch: 26/30, step: 244/364, loss: 0.19981, accuracy: 0.92540\n",
            "Epoch: 26/30, step: 245/364, loss: 0.20003, accuracy: 0.92519\n",
            "Epoch: 26/30, step: 246/364, loss: 0.19971, accuracy: 0.92537\n",
            "Epoch: 26/30, step: 247/364, loss: 0.19960, accuracy: 0.92529\n",
            "Epoch: 26/30, step: 248/364, loss: 0.19933, accuracy: 0.92534\n",
            "Epoch: 26/30, step: 249/364, loss: 0.19971, accuracy: 0.92482\n",
            "Epoch: 26/30, step: 250/364, loss: 0.19955, accuracy: 0.92494\n",
            "Epoch: 26/30, step: 251/364, loss: 0.19921, accuracy: 0.92511\n",
            "Epoch: 26/30, step: 252/364, loss: 0.19914, accuracy: 0.92522\n",
            "Epoch: 26/30, step: 253/364, loss: 0.19900, accuracy: 0.92540\n",
            "Epoch: 26/30, step: 254/364, loss: 0.19911, accuracy: 0.92526\n",
            "Epoch: 26/30, step: 255/364, loss: 0.19906, accuracy: 0.92531\n",
            "Epoch: 26/30, step: 256/364, loss: 0.19903, accuracy: 0.92529\n",
            "Epoch: 26/30, step: 257/364, loss: 0.19916, accuracy: 0.92540\n",
            "Epoch: 26/30, step: 258/364, loss: 0.19936, accuracy: 0.92533\n",
            "Epoch: 26/30, step: 259/364, loss: 0.19915, accuracy: 0.92549\n",
            "Epoch: 26/30, step: 260/364, loss: 0.19902, accuracy: 0.92566\n",
            "Epoch: 26/30, step: 261/364, loss: 0.19869, accuracy: 0.92589\n",
            "Epoch: 26/30, step: 262/364, loss: 0.19874, accuracy: 0.92587\n",
            "Epoch: 26/30, step: 263/364, loss: 0.19864, accuracy: 0.92597\n",
            "Epoch: 26/30, step: 264/364, loss: 0.19838, accuracy: 0.92614\n",
            "Epoch: 26/30, step: 265/364, loss: 0.19940, accuracy: 0.92571\n",
            "Epoch: 26/30, step: 266/364, loss: 0.19937, accuracy: 0.92563\n",
            "Epoch: 26/30, step: 267/364, loss: 0.19950, accuracy: 0.92556\n",
            "Epoch: 26/30, step: 268/364, loss: 0.19962, accuracy: 0.92543\n",
            "Epoch: 26/30, step: 269/364, loss: 0.19988, accuracy: 0.92524\n",
            "Epoch: 26/30, step: 270/364, loss: 0.19961, accuracy: 0.92541\n",
            "Epoch: 26/30, step: 271/364, loss: 0.19949, accuracy: 0.92551\n",
            "Epoch: 26/30, step: 272/364, loss: 0.19929, accuracy: 0.92561\n",
            "Epoch: 26/30, step: 273/364, loss: 0.19934, accuracy: 0.92560\n",
            "Epoch: 26/30, step: 274/364, loss: 0.19919, accuracy: 0.92570\n",
            "Epoch: 26/30, step: 275/364, loss: 0.19900, accuracy: 0.92585\n",
            "Epoch: 26/30, step: 276/364, loss: 0.19881, accuracy: 0.92595\n",
            "Epoch: 26/30, step: 277/364, loss: 0.19895, accuracy: 0.92582\n",
            "Epoch: 26/30, step: 278/364, loss: 0.19880, accuracy: 0.92592\n",
            "Epoch: 26/30, step: 279/364, loss: 0.19858, accuracy: 0.92613\n",
            "Epoch: 26/30, step: 280/364, loss: 0.19851, accuracy: 0.92612\n",
            "Epoch: 26/30, step: 281/364, loss: 0.19851, accuracy: 0.92616\n",
            "Epoch: 26/30, step: 282/364, loss: 0.19833, accuracy: 0.92620\n",
            "Epoch: 26/30, step: 283/364, loss: 0.19847, accuracy: 0.92607\n",
            "Epoch: 26/30, step: 284/364, loss: 0.19826, accuracy: 0.92622\n",
            "Epoch: 26/30, step: 285/364, loss: 0.19817, accuracy: 0.92632\n",
            "Epoch: 26/30, step: 286/364, loss: 0.19792, accuracy: 0.92646\n",
            "Epoch: 26/30, step: 287/364, loss: 0.19811, accuracy: 0.92623\n",
            "Epoch: 26/30, step: 288/364, loss: 0.19816, accuracy: 0.92622\n",
            "Epoch: 26/30, step: 289/364, loss: 0.19819, accuracy: 0.92615\n",
            "Epoch: 26/30, step: 290/364, loss: 0.19803, accuracy: 0.92619\n",
            "Epoch: 26/30, step: 291/364, loss: 0.19849, accuracy: 0.92606\n",
            "Epoch: 26/30, train loss: 0.19849, train accuracy: 0.92606, valid loss: 0.78135, valid accuracy: 0.68637\n",
            "Epoch: 27/30, step: 1/364, loss: 0.21012, accuracy: 0.87500\n",
            "Epoch: 27/30, step: 2/364, loss: 0.15297, accuracy: 0.92969\n",
            "Epoch: 27/30, step: 3/364, loss: 0.16653, accuracy: 0.92708\n",
            "Epoch: 27/30, step: 4/364, loss: 0.16550, accuracy: 0.92969\n",
            "Epoch: 27/30, step: 5/364, loss: 0.16391, accuracy: 0.93750\n",
            "Epoch: 27/30, step: 6/364, loss: 0.17027, accuracy: 0.93490\n",
            "Epoch: 27/30, step: 7/364, loss: 0.16709, accuracy: 0.93973\n",
            "Epoch: 27/30, step: 8/364, loss: 0.16399, accuracy: 0.94336\n",
            "Epoch: 27/30, step: 9/364, loss: 0.17009, accuracy: 0.94097\n",
            "Epoch: 27/30, step: 10/364, loss: 0.17236, accuracy: 0.93906\n",
            "Epoch: 27/30, step: 11/364, loss: 0.16511, accuracy: 0.94460\n",
            "Epoch: 27/30, step: 12/364, loss: 0.16321, accuracy: 0.94531\n",
            "Epoch: 27/30, step: 13/364, loss: 0.16336, accuracy: 0.94471\n",
            "Epoch: 27/30, step: 14/364, loss: 0.15944, accuracy: 0.94754\n",
            "Epoch: 27/30, step: 15/364, loss: 0.16511, accuracy: 0.94271\n",
            "Epoch: 27/30, step: 16/364, loss: 0.16364, accuracy: 0.94531\n",
            "Epoch: 27/30, step: 17/364, loss: 0.16464, accuracy: 0.94393\n",
            "Epoch: 27/30, step: 18/364, loss: 0.16656, accuracy: 0.94358\n",
            "Epoch: 27/30, step: 19/364, loss: 0.16636, accuracy: 0.94490\n",
            "Epoch: 27/30, step: 20/364, loss: 0.16400, accuracy: 0.94687\n",
            "Epoch: 27/30, step: 21/364, loss: 0.16472, accuracy: 0.94568\n",
            "Epoch: 27/30, step: 22/364, loss: 0.16334, accuracy: 0.94673\n",
            "Epoch: 27/30, step: 23/364, loss: 0.16733, accuracy: 0.94565\n",
            "Epoch: 27/30, step: 24/364, loss: 0.16881, accuracy: 0.94401\n",
            "Epoch: 27/30, step: 25/364, loss: 0.17209, accuracy: 0.94375\n",
            "Epoch: 27/30, step: 26/364, loss: 0.17064, accuracy: 0.94411\n",
            "Epoch: 27/30, step: 27/364, loss: 0.17506, accuracy: 0.94097\n",
            "Epoch: 27/30, step: 28/364, loss: 0.17326, accuracy: 0.94196\n",
            "Epoch: 27/30, step: 29/364, loss: 0.17875, accuracy: 0.94073\n",
            "Epoch: 27/30, step: 30/364, loss: 0.18227, accuracy: 0.93958\n",
            "Epoch: 27/30, step: 31/364, loss: 0.18046, accuracy: 0.94052\n",
            "Epoch: 27/30, step: 32/364, loss: 0.18201, accuracy: 0.93896\n",
            "Epoch: 27/30, step: 33/364, loss: 0.18191, accuracy: 0.93939\n",
            "Epoch: 27/30, step: 34/364, loss: 0.18278, accuracy: 0.93888\n",
            "Epoch: 27/30, step: 35/364, loss: 0.18117, accuracy: 0.93884\n",
            "Epoch: 27/30, step: 36/364, loss: 0.18005, accuracy: 0.93967\n",
            "Epoch: 27/30, step: 37/364, loss: 0.17842, accuracy: 0.94046\n",
            "Epoch: 27/30, step: 38/364, loss: 0.17808, accuracy: 0.94079\n",
            "Epoch: 27/30, step: 39/364, loss: 0.17670, accuracy: 0.94191\n",
            "Epoch: 27/30, step: 40/364, loss: 0.17624, accuracy: 0.94180\n",
            "Epoch: 27/30, step: 41/364, loss: 0.17423, accuracy: 0.94322\n",
            "Epoch: 27/30, step: 42/364, loss: 0.17360, accuracy: 0.94308\n",
            "Epoch: 27/30, step: 43/364, loss: 0.17214, accuracy: 0.94404\n",
            "Epoch: 27/30, step: 44/364, loss: 0.17074, accuracy: 0.94425\n",
            "Epoch: 27/30, step: 45/364, loss: 0.17030, accuracy: 0.94444\n",
            "Epoch: 27/30, step: 46/364, loss: 0.17038, accuracy: 0.94463\n",
            "Epoch: 27/30, step: 47/364, loss: 0.17033, accuracy: 0.94515\n",
            "Epoch: 27/30, step: 48/364, loss: 0.17062, accuracy: 0.94531\n",
            "Epoch: 27/30, step: 49/364, loss: 0.17063, accuracy: 0.94611\n",
            "Epoch: 27/30, step: 50/364, loss: 0.17042, accuracy: 0.94656\n",
            "Epoch: 27/30, step: 51/364, loss: 0.17005, accuracy: 0.94700\n",
            "Epoch: 27/30, step: 52/364, loss: 0.16981, accuracy: 0.94712\n",
            "Epoch: 27/30, step: 53/364, loss: 0.17015, accuracy: 0.94664\n",
            "Epoch: 27/30, step: 54/364, loss: 0.17049, accuracy: 0.94647\n",
            "Epoch: 27/30, step: 55/364, loss: 0.17097, accuracy: 0.94517\n",
            "Epoch: 27/30, step: 56/364, loss: 0.17103, accuracy: 0.94531\n",
            "Epoch: 27/30, step: 57/364, loss: 0.17380, accuracy: 0.94353\n",
            "Epoch: 27/30, step: 58/364, loss: 0.17414, accuracy: 0.94316\n",
            "Epoch: 27/30, step: 59/364, loss: 0.17465, accuracy: 0.94227\n",
            "Epoch: 27/30, step: 60/364, loss: 0.17425, accuracy: 0.94245\n",
            "Epoch: 27/30, step: 61/364, loss: 0.17445, accuracy: 0.94262\n",
            "Epoch: 27/30, step: 62/364, loss: 0.17625, accuracy: 0.94078\n",
            "Epoch: 27/30, step: 63/364, loss: 0.17820, accuracy: 0.93948\n",
            "Epoch: 27/30, step: 64/364, loss: 0.17861, accuracy: 0.93872\n",
            "Epoch: 27/30, step: 65/364, loss: 0.17777, accuracy: 0.93942\n",
            "Epoch: 27/30, step: 66/364, loss: 0.17659, accuracy: 0.94010\n",
            "Epoch: 27/30, step: 67/364, loss: 0.17617, accuracy: 0.94053\n",
            "Epoch: 27/30, step: 68/364, loss: 0.17524, accuracy: 0.94072\n",
            "Epoch: 27/30, step: 69/364, loss: 0.17474, accuracy: 0.94112\n",
            "Epoch: 27/30, step: 70/364, loss: 0.17482, accuracy: 0.94107\n",
            "Epoch: 27/30, step: 71/364, loss: 0.17482, accuracy: 0.94102\n",
            "Epoch: 27/30, step: 72/364, loss: 0.17414, accuracy: 0.94141\n",
            "Epoch: 27/30, step: 73/364, loss: 0.17341, accuracy: 0.94199\n",
            "Epoch: 27/30, step: 74/364, loss: 0.17283, accuracy: 0.94278\n",
            "Epoch: 27/30, step: 75/364, loss: 0.17268, accuracy: 0.94250\n",
            "Epoch: 27/30, step: 76/364, loss: 0.17216, accuracy: 0.94285\n",
            "Epoch: 27/30, step: 77/364, loss: 0.17275, accuracy: 0.94237\n",
            "Epoch: 27/30, step: 78/364, loss: 0.17287, accuracy: 0.94231\n",
            "Epoch: 27/30, step: 79/364, loss: 0.17371, accuracy: 0.94185\n",
            "Epoch: 27/30, step: 80/364, loss: 0.17324, accuracy: 0.94199\n",
            "Epoch: 27/30, step: 81/364, loss: 0.17268, accuracy: 0.94232\n",
            "Epoch: 27/30, step: 82/364, loss: 0.17263, accuracy: 0.94264\n",
            "Epoch: 27/30, step: 83/364, loss: 0.17317, accuracy: 0.94202\n",
            "Epoch: 27/30, step: 84/364, loss: 0.17346, accuracy: 0.94178\n",
            "Epoch: 27/30, step: 85/364, loss: 0.17433, accuracy: 0.94136\n",
            "Epoch: 27/30, step: 86/364, loss: 0.17516, accuracy: 0.94095\n",
            "Epoch: 27/30, step: 87/364, loss: 0.17558, accuracy: 0.94055\n",
            "Epoch: 27/30, step: 88/364, loss: 0.17578, accuracy: 0.94034\n",
            "Epoch: 27/30, step: 89/364, loss: 0.17675, accuracy: 0.93978\n",
            "Epoch: 27/30, step: 90/364, loss: 0.17746, accuracy: 0.93924\n",
            "Epoch: 27/30, step: 91/364, loss: 0.17693, accuracy: 0.93956\n",
            "Epoch: 27/30, step: 92/364, loss: 0.17710, accuracy: 0.93954\n",
            "Epoch: 27/30, step: 93/364, loss: 0.17726, accuracy: 0.93968\n",
            "Epoch: 27/30, step: 94/364, loss: 0.17666, accuracy: 0.93999\n",
            "Epoch: 27/30, step: 95/364, loss: 0.17597, accuracy: 0.94013\n",
            "Epoch: 27/30, step: 96/364, loss: 0.17612, accuracy: 0.93994\n",
            "Epoch: 27/30, step: 97/364, loss: 0.17639, accuracy: 0.93992\n",
            "Epoch: 27/30, step: 98/364, loss: 0.17578, accuracy: 0.94021\n",
            "Epoch: 27/30, step: 99/364, loss: 0.17532, accuracy: 0.94050\n",
            "Epoch: 27/30, step: 100/364, loss: 0.17479, accuracy: 0.94078\n",
            "Epoch: 27/30, step: 101/364, loss: 0.17481, accuracy: 0.94075\n",
            "Epoch: 27/30, step: 102/364, loss: 0.17409, accuracy: 0.94133\n",
            "Epoch: 27/30, step: 103/364, loss: 0.17356, accuracy: 0.94144\n",
            "Epoch: 27/30, step: 104/364, loss: 0.17341, accuracy: 0.94126\n",
            "Epoch: 27/30, step: 105/364, loss: 0.17482, accuracy: 0.94033\n",
            "Epoch: 27/30, step: 106/364, loss: 0.17404, accuracy: 0.94074\n",
            "Epoch: 27/30, step: 107/364, loss: 0.17389, accuracy: 0.94086\n",
            "Epoch: 27/30, step: 108/364, loss: 0.17403, accuracy: 0.94068\n",
            "Epoch: 27/30, step: 109/364, loss: 0.17400, accuracy: 0.94065\n",
            "Epoch: 27/30, step: 110/364, loss: 0.17358, accuracy: 0.94077\n",
            "Epoch: 27/30, step: 111/364, loss: 0.17306, accuracy: 0.94088\n",
            "Epoch: 27/30, step: 112/364, loss: 0.17317, accuracy: 0.94113\n",
            "Epoch: 27/30, step: 113/364, loss: 0.17380, accuracy: 0.94110\n",
            "Epoch: 27/30, step: 114/364, loss: 0.17417, accuracy: 0.94079\n",
            "Epoch: 27/30, step: 115/364, loss: 0.17640, accuracy: 0.93940\n",
            "Epoch: 27/30, step: 116/364, loss: 0.17643, accuracy: 0.93925\n",
            "Epoch: 27/30, step: 117/364, loss: 0.17639, accuracy: 0.93924\n",
            "Epoch: 27/30, step: 118/364, loss: 0.17625, accuracy: 0.93935\n",
            "Epoch: 27/30, step: 119/364, loss: 0.17667, accuracy: 0.93908\n",
            "Epoch: 27/30, step: 120/364, loss: 0.17638, accuracy: 0.93919\n",
            "Epoch: 27/30, step: 121/364, loss: 0.17746, accuracy: 0.93853\n",
            "Epoch: 27/30, step: 122/364, loss: 0.17753, accuracy: 0.93840\n",
            "Epoch: 27/30, step: 123/364, loss: 0.17703, accuracy: 0.93877\n",
            "Epoch: 27/30, step: 124/364, loss: 0.17671, accuracy: 0.93889\n",
            "Epoch: 27/30, step: 125/364, loss: 0.17770, accuracy: 0.93825\n",
            "Epoch: 27/30, step: 126/364, loss: 0.17779, accuracy: 0.93824\n",
            "Epoch: 27/30, step: 127/364, loss: 0.17715, accuracy: 0.93873\n",
            "Epoch: 27/30, step: 128/364, loss: 0.17784, accuracy: 0.93823\n",
            "Epoch: 27/30, step: 129/364, loss: 0.17760, accuracy: 0.93847\n",
            "Epoch: 27/30, step: 130/364, loss: 0.17713, accuracy: 0.93870\n",
            "Epoch: 27/30, step: 131/364, loss: 0.17737, accuracy: 0.93857\n",
            "Epoch: 27/30, step: 132/364, loss: 0.17701, accuracy: 0.93857\n",
            "Epoch: 27/30, step: 133/364, loss: 0.17786, accuracy: 0.93820\n",
            "Epoch: 27/30, step: 134/364, loss: 0.17780, accuracy: 0.93832\n",
            "Epoch: 27/30, step: 135/364, loss: 0.17768, accuracy: 0.93843\n",
            "Epoch: 27/30, step: 136/364, loss: 0.17803, accuracy: 0.93830\n",
            "Epoch: 27/30, step: 137/364, loss: 0.17912, accuracy: 0.93784\n",
            "Epoch: 27/30, step: 138/364, loss: 0.17940, accuracy: 0.93784\n",
            "Epoch: 27/30, step: 139/364, loss: 0.17917, accuracy: 0.93784\n",
            "Epoch: 27/30, step: 140/364, loss: 0.17903, accuracy: 0.93783\n",
            "Epoch: 27/30, step: 141/364, loss: 0.17859, accuracy: 0.93805\n",
            "Epoch: 27/30, step: 142/364, loss: 0.17800, accuracy: 0.93827\n",
            "Epoch: 27/30, step: 143/364, loss: 0.17750, accuracy: 0.93848\n",
            "Epoch: 27/30, step: 144/364, loss: 0.17761, accuracy: 0.93826\n",
            "Epoch: 27/30, step: 145/364, loss: 0.17725, accuracy: 0.93836\n",
            "Epoch: 27/30, step: 146/364, loss: 0.17706, accuracy: 0.93825\n",
            "Epoch: 27/30, step: 147/364, loss: 0.17733, accuracy: 0.93824\n",
            "Epoch: 27/30, step: 148/364, loss: 0.17779, accuracy: 0.93792\n",
            "Epoch: 27/30, step: 149/364, loss: 0.17820, accuracy: 0.93771\n",
            "Epoch: 27/30, step: 150/364, loss: 0.17824, accuracy: 0.93760\n",
            "Epoch: 27/30, step: 151/364, loss: 0.17877, accuracy: 0.93719\n",
            "Epoch: 27/30, step: 152/364, loss: 0.17814, accuracy: 0.93750\n",
            "Epoch: 27/30, step: 153/364, loss: 0.17788, accuracy: 0.93760\n",
            "Epoch: 27/30, step: 154/364, loss: 0.17841, accuracy: 0.93730\n",
            "Epoch: 27/30, step: 155/364, loss: 0.17815, accuracy: 0.93760\n",
            "Epoch: 27/30, step: 156/364, loss: 0.17831, accuracy: 0.93780\n",
            "Epoch: 27/30, step: 157/364, loss: 0.18014, accuracy: 0.93710\n",
            "Epoch: 27/30, step: 158/364, loss: 0.18064, accuracy: 0.93661\n",
            "Epoch: 27/30, step: 159/364, loss: 0.18024, accuracy: 0.93681\n",
            "Epoch: 27/30, step: 160/364, loss: 0.18059, accuracy: 0.93662\n",
            "Epoch: 27/30, step: 161/364, loss: 0.18086, accuracy: 0.93624\n",
            "Epoch: 27/30, step: 162/364, loss: 0.18069, accuracy: 0.93634\n",
            "Epoch: 27/30, step: 163/364, loss: 0.18055, accuracy: 0.93654\n",
            "Epoch: 27/30, step: 164/364, loss: 0.18011, accuracy: 0.93664\n",
            "Epoch: 27/30, step: 165/364, loss: 0.18085, accuracy: 0.93636\n",
            "Epoch: 27/30, step: 166/364, loss: 0.18089, accuracy: 0.93637\n",
            "Epoch: 27/30, step: 167/364, loss: 0.18061, accuracy: 0.93656\n",
            "Epoch: 27/30, step: 168/364, loss: 0.18040, accuracy: 0.93676\n",
            "Epoch: 27/30, step: 169/364, loss: 0.18087, accuracy: 0.93639\n",
            "Epoch: 27/30, step: 170/364, loss: 0.18034, accuracy: 0.93667\n",
            "Epoch: 27/30, step: 171/364, loss: 0.18077, accuracy: 0.93640\n",
            "Epoch: 27/30, step: 172/364, loss: 0.18064, accuracy: 0.93650\n",
            "Epoch: 27/30, step: 173/364, loss: 0.18091, accuracy: 0.93651\n",
            "Epoch: 27/30, step: 174/364, loss: 0.18074, accuracy: 0.93660\n",
            "Epoch: 27/30, step: 175/364, loss: 0.18126, accuracy: 0.93634\n",
            "Epoch: 27/30, step: 176/364, loss: 0.18109, accuracy: 0.93626\n",
            "Epoch: 27/30, step: 177/364, loss: 0.18098, accuracy: 0.93635\n",
            "Epoch: 27/30, step: 178/364, loss: 0.18094, accuracy: 0.93627\n",
            "Epoch: 27/30, step: 179/364, loss: 0.18109, accuracy: 0.93619\n",
            "Epoch: 27/30, step: 180/364, loss: 0.18121, accuracy: 0.93620\n",
            "Epoch: 27/30, step: 181/364, loss: 0.18067, accuracy: 0.93655\n",
            "Epoch: 27/30, step: 182/364, loss: 0.18086, accuracy: 0.93647\n",
            "Epoch: 27/30, step: 183/364, loss: 0.18048, accuracy: 0.93673\n",
            "Epoch: 27/30, step: 184/364, loss: 0.18065, accuracy: 0.93674\n",
            "Epoch: 27/30, step: 185/364, loss: 0.18098, accuracy: 0.93640\n",
            "Epoch: 27/30, step: 186/364, loss: 0.18166, accuracy: 0.93599\n",
            "Epoch: 27/30, step: 187/364, loss: 0.18175, accuracy: 0.93600\n",
            "Epoch: 27/30, step: 188/364, loss: 0.18132, accuracy: 0.93634\n",
            "Epoch: 27/30, step: 189/364, loss: 0.18109, accuracy: 0.93651\n",
            "Epoch: 27/30, step: 190/364, loss: 0.18109, accuracy: 0.93660\n",
            "Epoch: 27/30, step: 191/364, loss: 0.18122, accuracy: 0.93660\n",
            "Epoch: 27/30, step: 192/364, loss: 0.18126, accuracy: 0.93669\n",
            "Epoch: 27/30, step: 193/364, loss: 0.18143, accuracy: 0.93653\n",
            "Epoch: 27/30, step: 194/364, loss: 0.18096, accuracy: 0.93678\n",
            "Epoch: 27/30, step: 195/364, loss: 0.18078, accuracy: 0.93678\n",
            "Epoch: 27/30, step: 196/364, loss: 0.18087, accuracy: 0.93678\n",
            "Epoch: 27/30, step: 197/364, loss: 0.18067, accuracy: 0.93694\n",
            "Epoch: 27/30, step: 198/364, loss: 0.18076, accuracy: 0.93687\n",
            "Epoch: 27/30, step: 199/364, loss: 0.18084, accuracy: 0.93687\n",
            "Epoch: 27/30, step: 200/364, loss: 0.18064, accuracy: 0.93703\n",
            "Epoch: 27/30, step: 201/364, loss: 0.18023, accuracy: 0.93727\n",
            "Epoch: 27/30, step: 202/364, loss: 0.18069, accuracy: 0.93680\n",
            "Epoch: 27/30, step: 203/364, loss: 0.18093, accuracy: 0.93665\n",
            "Epoch: 27/30, step: 204/364, loss: 0.18137, accuracy: 0.93635\n",
            "Epoch: 27/30, step: 205/364, loss: 0.18225, accuracy: 0.93605\n",
            "Epoch: 27/30, step: 206/364, loss: 0.18266, accuracy: 0.93598\n",
            "Epoch: 27/30, step: 207/364, loss: 0.18264, accuracy: 0.93599\n",
            "Epoch: 27/30, step: 208/364, loss: 0.18251, accuracy: 0.93600\n",
            "Epoch: 27/30, step: 209/364, loss: 0.18206, accuracy: 0.93615\n",
            "Epoch: 27/30, step: 210/364, loss: 0.18222, accuracy: 0.93586\n",
            "Epoch: 27/30, step: 211/364, loss: 0.18219, accuracy: 0.93594\n",
            "Epoch: 27/30, step: 212/364, loss: 0.18190, accuracy: 0.93603\n",
            "Epoch: 27/30, step: 213/364, loss: 0.18187, accuracy: 0.93603\n",
            "Epoch: 27/30, step: 214/364, loss: 0.18157, accuracy: 0.93619\n",
            "Epoch: 27/30, step: 215/364, loss: 0.18199, accuracy: 0.93612\n",
            "Epoch: 27/30, step: 216/364, loss: 0.18166, accuracy: 0.93627\n",
            "Epoch: 27/30, step: 217/364, loss: 0.18135, accuracy: 0.93642\n",
            "Epoch: 27/30, step: 218/364, loss: 0.18151, accuracy: 0.93650\n",
            "Epoch: 27/30, step: 219/364, loss: 0.18190, accuracy: 0.93643\n",
            "Epoch: 27/30, step: 220/364, loss: 0.18165, accuracy: 0.93643\n",
            "Epoch: 27/30, step: 221/364, loss: 0.18140, accuracy: 0.93665\n",
            "Epoch: 27/30, step: 222/364, loss: 0.18164, accuracy: 0.93651\n",
            "Epoch: 27/30, step: 223/364, loss: 0.18169, accuracy: 0.93652\n",
            "Epoch: 27/30, step: 224/364, loss: 0.18140, accuracy: 0.93673\n",
            "Epoch: 27/30, step: 225/364, loss: 0.18147, accuracy: 0.93674\n",
            "Epoch: 27/30, step: 226/364, loss: 0.18136, accuracy: 0.93674\n",
            "Epoch: 27/30, step: 227/364, loss: 0.18158, accuracy: 0.93654\n",
            "Epoch: 27/30, step: 228/364, loss: 0.18145, accuracy: 0.93668\n",
            "Epoch: 27/30, step: 229/364, loss: 0.18130, accuracy: 0.93675\n",
            "Epoch: 27/30, step: 230/364, loss: 0.18111, accuracy: 0.93689\n",
            "Epoch: 27/30, step: 231/364, loss: 0.18146, accuracy: 0.93655\n",
            "Epoch: 27/30, step: 232/364, loss: 0.18153, accuracy: 0.93636\n",
            "Epoch: 27/30, step: 233/364, loss: 0.18162, accuracy: 0.93629\n",
            "Epoch: 27/30, step: 234/364, loss: 0.18132, accuracy: 0.93650\n",
            "Epoch: 27/30, step: 235/364, loss: 0.18114, accuracy: 0.93650\n",
            "Epoch: 27/30, step: 236/364, loss: 0.18080, accuracy: 0.93677\n",
            "Epoch: 27/30, step: 237/364, loss: 0.18059, accuracy: 0.93691\n",
            "Epoch: 27/30, step: 238/364, loss: 0.18063, accuracy: 0.93691\n",
            "Epoch: 27/30, step: 239/364, loss: 0.18065, accuracy: 0.93678\n",
            "Epoch: 27/30, step: 240/364, loss: 0.18054, accuracy: 0.93678\n",
            "Epoch: 27/30, step: 241/364, loss: 0.18085, accuracy: 0.93659\n",
            "Epoch: 27/30, step: 242/364, loss: 0.18087, accuracy: 0.93660\n",
            "Epoch: 27/30, step: 243/364, loss: 0.18101, accuracy: 0.93654\n",
            "Epoch: 27/30, step: 244/364, loss: 0.18142, accuracy: 0.93622\n",
            "Epoch: 27/30, step: 245/364, loss: 0.18126, accuracy: 0.93635\n",
            "Epoch: 27/30, step: 246/364, loss: 0.18131, accuracy: 0.93623\n",
            "Epoch: 27/30, step: 247/364, loss: 0.18178, accuracy: 0.93592\n",
            "Epoch: 27/30, step: 248/364, loss: 0.18179, accuracy: 0.93586\n",
            "Epoch: 27/30, step: 249/364, loss: 0.18186, accuracy: 0.93574\n",
            "Epoch: 27/30, step: 250/364, loss: 0.18198, accuracy: 0.93569\n",
            "Epoch: 27/30, step: 251/364, loss: 0.18214, accuracy: 0.93557\n",
            "Epoch: 27/30, step: 252/364, loss: 0.18254, accuracy: 0.93545\n",
            "Epoch: 27/30, step: 253/364, loss: 0.18233, accuracy: 0.93546\n",
            "Epoch: 27/30, step: 254/364, loss: 0.18195, accuracy: 0.93565\n",
            "Epoch: 27/30, step: 255/364, loss: 0.18170, accuracy: 0.93578\n",
            "Epoch: 27/30, step: 256/364, loss: 0.18192, accuracy: 0.93561\n",
            "Epoch: 27/30, step: 257/364, loss: 0.18190, accuracy: 0.93574\n",
            "Epoch: 27/30, step: 258/364, loss: 0.18205, accuracy: 0.93568\n",
            "Epoch: 27/30, step: 259/364, loss: 0.18204, accuracy: 0.93569\n",
            "Epoch: 27/30, step: 260/364, loss: 0.18261, accuracy: 0.93546\n",
            "Epoch: 27/30, step: 261/364, loss: 0.18241, accuracy: 0.93552\n",
            "Epoch: 27/30, step: 262/364, loss: 0.18258, accuracy: 0.93547\n",
            "Epoch: 27/30, step: 263/364, loss: 0.18260, accuracy: 0.93542\n",
            "Epoch: 27/30, step: 264/364, loss: 0.18262, accuracy: 0.93543\n",
            "Epoch: 27/30, step: 265/364, loss: 0.18276, accuracy: 0.93544\n",
            "Epoch: 27/30, step: 266/364, loss: 0.18271, accuracy: 0.93544\n",
            "Epoch: 27/30, step: 267/364, loss: 0.18273, accuracy: 0.93539\n",
            "Epoch: 27/30, step: 268/364, loss: 0.18291, accuracy: 0.93540\n",
            "Epoch: 27/30, step: 269/364, loss: 0.18289, accuracy: 0.93541\n",
            "Epoch: 27/30, step: 270/364, loss: 0.18326, accuracy: 0.93524\n",
            "Epoch: 27/30, step: 271/364, loss: 0.18339, accuracy: 0.93525\n",
            "Epoch: 27/30, step: 272/364, loss: 0.18333, accuracy: 0.93532\n",
            "Epoch: 27/30, step: 273/364, loss: 0.18355, accuracy: 0.93504\n",
            "Epoch: 27/30, step: 274/364, loss: 0.18364, accuracy: 0.93499\n",
            "Epoch: 27/30, step: 275/364, loss: 0.18367, accuracy: 0.93500\n",
            "Epoch: 27/30, step: 276/364, loss: 0.18344, accuracy: 0.93512\n",
            "Epoch: 27/30, step: 277/364, loss: 0.18331, accuracy: 0.93513\n",
            "Epoch: 27/30, step: 278/364, loss: 0.18425, accuracy: 0.93463\n",
            "Epoch: 27/30, step: 279/364, loss: 0.18457, accuracy: 0.93448\n",
            "Epoch: 27/30, step: 280/364, loss: 0.18442, accuracy: 0.93460\n",
            "Epoch: 27/30, step: 281/364, loss: 0.18425, accuracy: 0.93466\n",
            "Epoch: 27/30, step: 282/364, loss: 0.18406, accuracy: 0.93473\n",
            "Epoch: 27/30, step: 283/364, loss: 0.18413, accuracy: 0.93474\n",
            "Epoch: 27/30, step: 284/364, loss: 0.18460, accuracy: 0.93453\n",
            "Epoch: 27/30, step: 285/364, loss: 0.18432, accuracy: 0.93476\n",
            "Epoch: 27/30, step: 286/364, loss: 0.18439, accuracy: 0.93466\n",
            "Epoch: 27/30, step: 287/364, loss: 0.18451, accuracy: 0.93472\n",
            "Epoch: 27/30, step: 288/364, loss: 0.18427, accuracy: 0.93490\n",
            "Epoch: 27/30, step: 289/364, loss: 0.18433, accuracy: 0.93490\n",
            "Epoch: 27/30, step: 290/364, loss: 0.18422, accuracy: 0.93502\n",
            "Epoch: 27/30, step: 291/364, loss: 0.18438, accuracy: 0.93498\n",
            "Epoch: 27/30, train loss: 0.18438, train accuracy: 0.93498, valid loss: 0.81060, valid accuracy: 0.68422\n",
            "Epoch: 28/30, step: 1/364, loss: 0.14280, accuracy: 0.92188\n",
            "Epoch: 28/30, step: 2/364, loss: 0.13964, accuracy: 0.93750\n",
            "Epoch: 28/30, step: 3/364, loss: 0.19449, accuracy: 0.92188\n",
            "Epoch: 28/30, step: 4/364, loss: 0.17340, accuracy: 0.93750\n",
            "Epoch: 28/30, step: 5/364, loss: 0.18471, accuracy: 0.93750\n",
            "Epoch: 28/30, step: 6/364, loss: 0.17888, accuracy: 0.94010\n",
            "Epoch: 28/30, step: 7/364, loss: 0.19687, accuracy: 0.93527\n",
            "Epoch: 28/30, step: 8/364, loss: 0.19891, accuracy: 0.93555\n",
            "Epoch: 28/30, step: 9/364, loss: 0.19107, accuracy: 0.93924\n",
            "Epoch: 28/30, step: 10/364, loss: 0.18684, accuracy: 0.94063\n",
            "Epoch: 28/30, step: 11/364, loss: 0.18611, accuracy: 0.94034\n",
            "Epoch: 28/30, step: 12/364, loss: 0.19422, accuracy: 0.93750\n",
            "Epoch: 28/30, step: 13/364, loss: 0.18777, accuracy: 0.93990\n",
            "Epoch: 28/30, step: 14/364, loss: 0.18833, accuracy: 0.93862\n",
            "Epoch: 28/30, step: 15/364, loss: 0.18410, accuracy: 0.94063\n",
            "Epoch: 28/30, step: 16/364, loss: 0.18131, accuracy: 0.94141\n",
            "Epoch: 28/30, step: 17/364, loss: 0.17978, accuracy: 0.94026\n",
            "Epoch: 28/30, step: 18/364, loss: 0.17494, accuracy: 0.94271\n",
            "Epoch: 28/30, step: 19/364, loss: 0.17431, accuracy: 0.94243\n",
            "Epoch: 28/30, step: 20/364, loss: 0.16949, accuracy: 0.94453\n",
            "Epoch: 28/30, step: 21/364, loss: 0.16855, accuracy: 0.94420\n",
            "Epoch: 28/30, step: 22/364, loss: 0.17122, accuracy: 0.94247\n",
            "Epoch: 28/30, step: 23/364, loss: 0.17153, accuracy: 0.94293\n",
            "Epoch: 28/30, step: 24/364, loss: 0.16757, accuracy: 0.94466\n",
            "Epoch: 28/30, step: 25/364, loss: 0.16371, accuracy: 0.94687\n",
            "Epoch: 28/30, step: 26/364, loss: 0.16274, accuracy: 0.94832\n",
            "Epoch: 28/30, step: 27/364, loss: 0.16096, accuracy: 0.94792\n",
            "Epoch: 28/30, step: 28/364, loss: 0.16079, accuracy: 0.94810\n",
            "Epoch: 28/30, step: 29/364, loss: 0.16010, accuracy: 0.94828\n",
            "Epoch: 28/30, step: 30/364, loss: 0.16114, accuracy: 0.94792\n",
            "Epoch: 28/30, step: 31/364, loss: 0.15900, accuracy: 0.94859\n",
            "Epoch: 28/30, step: 32/364, loss: 0.15732, accuracy: 0.94971\n",
            "Epoch: 28/30, step: 33/364, loss: 0.15898, accuracy: 0.94886\n",
            "Epoch: 28/30, step: 34/364, loss: 0.15932, accuracy: 0.94853\n",
            "Epoch: 28/30, step: 35/364, loss: 0.15764, accuracy: 0.94955\n",
            "Epoch: 28/30, step: 36/364, loss: 0.15692, accuracy: 0.95009\n",
            "Epoch: 28/30, step: 37/364, loss: 0.15678, accuracy: 0.95017\n",
            "Epoch: 28/30, step: 38/364, loss: 0.15533, accuracy: 0.95066\n",
            "Epoch: 28/30, step: 39/364, loss: 0.15460, accuracy: 0.95072\n",
            "Epoch: 28/30, step: 40/364, loss: 0.15413, accuracy: 0.95078\n",
            "Epoch: 28/30, step: 41/364, loss: 0.15810, accuracy: 0.94970\n",
            "Epoch: 28/30, step: 42/364, loss: 0.15806, accuracy: 0.94978\n",
            "Epoch: 28/30, step: 43/364, loss: 0.16085, accuracy: 0.94731\n",
            "Epoch: 28/30, step: 44/364, loss: 0.15988, accuracy: 0.94780\n",
            "Epoch: 28/30, step: 45/364, loss: 0.16075, accuracy: 0.94792\n",
            "Epoch: 28/30, step: 46/364, loss: 0.16123, accuracy: 0.94769\n",
            "Epoch: 28/30, step: 47/364, loss: 0.16325, accuracy: 0.94747\n",
            "Epoch: 28/30, step: 48/364, loss: 0.16363, accuracy: 0.94694\n",
            "Epoch: 28/30, step: 49/364, loss: 0.16348, accuracy: 0.94707\n",
            "Epoch: 28/30, step: 50/364, loss: 0.16619, accuracy: 0.94531\n",
            "Epoch: 28/30, step: 51/364, loss: 0.16518, accuracy: 0.94577\n",
            "Epoch: 28/30, step: 52/364, loss: 0.16458, accuracy: 0.94621\n",
            "Epoch: 28/30, step: 53/364, loss: 0.16534, accuracy: 0.94605\n",
            "Epoch: 28/30, step: 54/364, loss: 0.16513, accuracy: 0.94618\n",
            "Epoch: 28/30, step: 55/364, loss: 0.16853, accuracy: 0.94432\n",
            "Epoch: 28/30, step: 56/364, loss: 0.16780, accuracy: 0.94448\n",
            "Epoch: 28/30, step: 57/364, loss: 0.16857, accuracy: 0.94408\n",
            "Epoch: 28/30, step: 58/364, loss: 0.16836, accuracy: 0.94423\n",
            "Epoch: 28/30, step: 59/364, loss: 0.16814, accuracy: 0.94386\n",
            "Epoch: 28/30, step: 60/364, loss: 0.16800, accuracy: 0.94349\n",
            "Epoch: 28/30, step: 61/364, loss: 0.16811, accuracy: 0.94339\n",
            "Epoch: 28/30, step: 62/364, loss: 0.16704, accuracy: 0.94405\n",
            "Epoch: 28/30, step: 63/364, loss: 0.16676, accuracy: 0.94395\n",
            "Epoch: 28/30, step: 64/364, loss: 0.16796, accuracy: 0.94287\n",
            "Epoch: 28/30, step: 65/364, loss: 0.16718, accuracy: 0.94327\n",
            "Epoch: 28/30, step: 66/364, loss: 0.16671, accuracy: 0.94318\n",
            "Epoch: 28/30, step: 67/364, loss: 0.16806, accuracy: 0.94193\n",
            "Epoch: 28/30, step: 68/364, loss: 0.16690, accuracy: 0.94233\n",
            "Epoch: 28/30, step: 69/364, loss: 0.16721, accuracy: 0.94180\n",
            "Epoch: 28/30, step: 70/364, loss: 0.16650, accuracy: 0.94219\n",
            "Epoch: 28/30, step: 71/364, loss: 0.16624, accuracy: 0.94190\n",
            "Epoch: 28/30, step: 72/364, loss: 0.16584, accuracy: 0.94184\n",
            "Epoch: 28/30, step: 73/364, loss: 0.16471, accuracy: 0.94221\n",
            "Epoch: 28/30, step: 74/364, loss: 0.16378, accuracy: 0.94278\n",
            "Epoch: 28/30, step: 75/364, loss: 0.16324, accuracy: 0.94313\n",
            "Epoch: 28/30, step: 76/364, loss: 0.16476, accuracy: 0.94202\n",
            "Epoch: 28/30, step: 77/364, loss: 0.16496, accuracy: 0.94196\n",
            "Epoch: 28/30, step: 78/364, loss: 0.16638, accuracy: 0.94111\n",
            "Epoch: 28/30, step: 79/364, loss: 0.16601, accuracy: 0.94106\n",
            "Epoch: 28/30, step: 80/364, loss: 0.16594, accuracy: 0.94102\n",
            "Epoch: 28/30, step: 81/364, loss: 0.16670, accuracy: 0.94059\n",
            "Epoch: 28/30, step: 82/364, loss: 0.16641, accuracy: 0.94055\n",
            "Epoch: 28/30, step: 83/364, loss: 0.16611, accuracy: 0.94051\n",
            "Epoch: 28/30, step: 84/364, loss: 0.16591, accuracy: 0.94103\n",
            "Epoch: 28/30, step: 85/364, loss: 0.16585, accuracy: 0.94118\n",
            "Epoch: 28/30, step: 86/364, loss: 0.16631, accuracy: 0.94113\n",
            "Epoch: 28/30, step: 87/364, loss: 0.16656, accuracy: 0.94091\n",
            "Epoch: 28/30, step: 88/364, loss: 0.16565, accuracy: 0.94141\n",
            "Epoch: 28/30, step: 89/364, loss: 0.16504, accuracy: 0.94189\n",
            "Epoch: 28/30, step: 90/364, loss: 0.16447, accuracy: 0.94236\n",
            "Epoch: 28/30, step: 91/364, loss: 0.16439, accuracy: 0.94214\n",
            "Epoch: 28/30, step: 92/364, loss: 0.16393, accuracy: 0.94226\n",
            "Epoch: 28/30, step: 93/364, loss: 0.16348, accuracy: 0.94254\n",
            "Epoch: 28/30, step: 94/364, loss: 0.16299, accuracy: 0.94299\n",
            "Epoch: 28/30, step: 95/364, loss: 0.16196, accuracy: 0.94342\n",
            "Epoch: 28/30, step: 96/364, loss: 0.16224, accuracy: 0.94320\n",
            "Epoch: 28/30, step: 97/364, loss: 0.16226, accuracy: 0.94298\n",
            "Epoch: 28/30, step: 98/364, loss: 0.16208, accuracy: 0.94324\n",
            "Epoch: 28/30, step: 99/364, loss: 0.16304, accuracy: 0.94318\n",
            "Epoch: 28/30, step: 100/364, loss: 0.16230, accuracy: 0.94375\n",
            "Epoch: 28/30, step: 101/364, loss: 0.16300, accuracy: 0.94369\n",
            "Epoch: 28/30, step: 102/364, loss: 0.16301, accuracy: 0.94363\n",
            "Epoch: 28/30, step: 103/364, loss: 0.16232, accuracy: 0.94417\n",
            "Epoch: 28/30, step: 104/364, loss: 0.16256, accuracy: 0.94441\n",
            "Epoch: 28/30, step: 105/364, loss: 0.16213, accuracy: 0.94479\n",
            "Epoch: 28/30, step: 106/364, loss: 0.16172, accuracy: 0.94487\n",
            "Epoch: 28/30, step: 107/364, loss: 0.16193, accuracy: 0.94436\n",
            "Epoch: 28/30, step: 108/364, loss: 0.16475, accuracy: 0.94285\n",
            "Epoch: 28/30, step: 109/364, loss: 0.16541, accuracy: 0.94237\n",
            "Epoch: 28/30, step: 110/364, loss: 0.16570, accuracy: 0.94219\n",
            "Epoch: 28/30, step: 111/364, loss: 0.16636, accuracy: 0.94186\n",
            "Epoch: 28/30, step: 112/364, loss: 0.16670, accuracy: 0.94141\n",
            "Epoch: 28/30, step: 113/364, loss: 0.16591, accuracy: 0.94192\n",
            "Epoch: 28/30, step: 114/364, loss: 0.16593, accuracy: 0.94189\n",
            "Epoch: 28/30, step: 115/364, loss: 0.16621, accuracy: 0.94171\n",
            "Epoch: 28/30, step: 116/364, loss: 0.16603, accuracy: 0.94208\n",
            "Epoch: 28/30, step: 117/364, loss: 0.16564, accuracy: 0.94204\n",
            "Epoch: 28/30, step: 118/364, loss: 0.16552, accuracy: 0.94227\n",
            "Epoch: 28/30, step: 119/364, loss: 0.16538, accuracy: 0.94223\n",
            "Epoch: 28/30, step: 120/364, loss: 0.16595, accuracy: 0.94193\n",
            "Epoch: 28/30, step: 121/364, loss: 0.16674, accuracy: 0.94137\n",
            "Epoch: 28/30, step: 122/364, loss: 0.16678, accuracy: 0.94134\n",
            "Epoch: 28/30, step: 123/364, loss: 0.16624, accuracy: 0.94169\n",
            "Epoch: 28/30, step: 124/364, loss: 0.16588, accuracy: 0.94191\n",
            "Epoch: 28/30, step: 125/364, loss: 0.16605, accuracy: 0.94200\n",
            "Epoch: 28/30, step: 126/364, loss: 0.16590, accuracy: 0.94209\n",
            "Epoch: 28/30, step: 127/364, loss: 0.16560, accuracy: 0.94230\n",
            "Epoch: 28/30, step: 128/364, loss: 0.16557, accuracy: 0.94238\n",
            "Epoch: 28/30, step: 129/364, loss: 0.16572, accuracy: 0.94222\n",
            "Epoch: 28/30, step: 130/364, loss: 0.16602, accuracy: 0.94195\n",
            "Epoch: 28/30, step: 131/364, loss: 0.16607, accuracy: 0.94203\n",
            "Epoch: 28/30, step: 132/364, loss: 0.16613, accuracy: 0.94212\n",
            "Epoch: 28/30, step: 133/364, loss: 0.16606, accuracy: 0.94208\n",
            "Epoch: 28/30, step: 134/364, loss: 0.16731, accuracy: 0.94135\n",
            "Epoch: 28/30, step: 135/364, loss: 0.16713, accuracy: 0.94144\n",
            "Epoch: 28/30, step: 136/364, loss: 0.16683, accuracy: 0.94175\n",
            "Epoch: 28/30, step: 137/364, loss: 0.16636, accuracy: 0.94195\n",
            "Epoch: 28/30, step: 138/364, loss: 0.16664, accuracy: 0.94192\n",
            "Epoch: 28/30, step: 139/364, loss: 0.16660, accuracy: 0.94200\n",
            "Epoch: 28/30, step: 140/364, loss: 0.16678, accuracy: 0.94174\n",
            "Epoch: 28/30, step: 141/364, loss: 0.16669, accuracy: 0.94193\n",
            "Epoch: 28/30, step: 142/364, loss: 0.16706, accuracy: 0.94168\n",
            "Epoch: 28/30, step: 143/364, loss: 0.16780, accuracy: 0.94111\n",
            "Epoch: 28/30, step: 144/364, loss: 0.16764, accuracy: 0.94108\n",
            "Epoch: 28/30, step: 145/364, loss: 0.16703, accuracy: 0.94138\n",
            "Epoch: 28/30, step: 146/364, loss: 0.16707, accuracy: 0.94157\n",
            "Epoch: 28/30, step: 147/364, loss: 0.16726, accuracy: 0.94133\n",
            "Epoch: 28/30, step: 148/364, loss: 0.16684, accuracy: 0.94172\n",
            "Epoch: 28/30, step: 149/364, loss: 0.16716, accuracy: 0.94169\n",
            "Epoch: 28/30, step: 150/364, loss: 0.16741, accuracy: 0.94146\n",
            "Epoch: 28/30, step: 151/364, loss: 0.16711, accuracy: 0.94154\n",
            "Epoch: 28/30, step: 152/364, loss: 0.16691, accuracy: 0.94161\n",
            "Epoch: 28/30, step: 153/364, loss: 0.16690, accuracy: 0.94169\n",
            "Epoch: 28/30, step: 154/364, loss: 0.16668, accuracy: 0.94176\n",
            "Epoch: 28/30, step: 155/364, loss: 0.16704, accuracy: 0.94143\n",
            "Epoch: 28/30, step: 156/364, loss: 0.16685, accuracy: 0.94141\n",
            "Epoch: 28/30, step: 157/364, loss: 0.16729, accuracy: 0.94108\n",
            "Epoch: 28/30, step: 158/364, loss: 0.16716, accuracy: 0.94106\n",
            "Epoch: 28/30, step: 159/364, loss: 0.16680, accuracy: 0.94104\n",
            "Epoch: 28/30, step: 160/364, loss: 0.16663, accuracy: 0.94111\n",
            "Epoch: 28/30, step: 161/364, loss: 0.16641, accuracy: 0.94109\n",
            "Epoch: 28/30, step: 162/364, loss: 0.16668, accuracy: 0.94088\n",
            "Epoch: 28/30, step: 163/364, loss: 0.16677, accuracy: 0.94076\n",
            "Epoch: 28/30, step: 164/364, loss: 0.16631, accuracy: 0.94103\n",
            "Epoch: 28/30, step: 165/364, loss: 0.16625, accuracy: 0.94119\n",
            "Epoch: 28/30, step: 166/364, loss: 0.16587, accuracy: 0.94155\n",
            "Epoch: 28/30, step: 167/364, loss: 0.16632, accuracy: 0.94115\n",
            "Epoch: 28/30, step: 168/364, loss: 0.16619, accuracy: 0.94122\n",
            "Epoch: 28/30, step: 169/364, loss: 0.16588, accuracy: 0.94138\n",
            "Epoch: 28/30, step: 170/364, loss: 0.16559, accuracy: 0.94145\n",
            "Epoch: 28/30, step: 171/364, loss: 0.16561, accuracy: 0.94143\n",
            "Epoch: 28/30, step: 172/364, loss: 0.16542, accuracy: 0.94150\n",
            "Epoch: 28/30, step: 173/364, loss: 0.16509, accuracy: 0.94165\n",
            "Epoch: 28/30, step: 174/364, loss: 0.16471, accuracy: 0.94190\n",
            "Epoch: 28/30, step: 175/364, loss: 0.16412, accuracy: 0.94223\n",
            "Epoch: 28/30, step: 176/364, loss: 0.16392, accuracy: 0.94238\n",
            "Epoch: 28/30, step: 177/364, loss: 0.16333, accuracy: 0.94262\n",
            "Epoch: 28/30, step: 178/364, loss: 0.16341, accuracy: 0.94250\n",
            "Epoch: 28/30, step: 179/364, loss: 0.16299, accuracy: 0.94282\n",
            "Epoch: 28/30, step: 180/364, loss: 0.16333, accuracy: 0.94253\n",
            "Epoch: 28/30, step: 181/364, loss: 0.16343, accuracy: 0.94259\n",
            "Epoch: 28/30, step: 182/364, loss: 0.16327, accuracy: 0.94274\n",
            "Epoch: 28/30, step: 183/364, loss: 0.16301, accuracy: 0.94288\n",
            "Epoch: 28/30, step: 184/364, loss: 0.16277, accuracy: 0.94293\n",
            "Epoch: 28/30, step: 185/364, loss: 0.16337, accuracy: 0.94223\n",
            "Epoch: 28/30, step: 186/364, loss: 0.16365, accuracy: 0.94212\n",
            "Epoch: 28/30, step: 187/364, loss: 0.16342, accuracy: 0.94235\n",
            "Epoch: 28/30, step: 188/364, loss: 0.16318, accuracy: 0.94240\n",
            "Epoch: 28/30, step: 189/364, loss: 0.16357, accuracy: 0.94221\n",
            "Epoch: 28/30, step: 190/364, loss: 0.16330, accuracy: 0.94235\n",
            "Epoch: 28/30, step: 191/364, loss: 0.16366, accuracy: 0.94200\n",
            "Epoch: 28/30, step: 192/364, loss: 0.16448, accuracy: 0.94157\n",
            "Epoch: 28/30, step: 193/364, loss: 0.16487, accuracy: 0.94139\n",
            "Epoch: 28/30, step: 194/364, loss: 0.16531, accuracy: 0.94104\n",
            "Epoch: 28/30, step: 195/364, loss: 0.16627, accuracy: 0.94054\n",
            "Epoch: 28/30, step: 196/364, loss: 0.16589, accuracy: 0.94069\n",
            "Epoch: 28/30, step: 197/364, loss: 0.16609, accuracy: 0.94051\n",
            "Epoch: 28/30, step: 198/364, loss: 0.16586, accuracy: 0.94066\n",
            "Epoch: 28/30, step: 199/364, loss: 0.16575, accuracy: 0.94072\n",
            "Epoch: 28/30, step: 200/364, loss: 0.16632, accuracy: 0.94039\n",
            "Epoch: 28/30, step: 201/364, loss: 0.16669, accuracy: 0.94007\n",
            "Epoch: 28/30, step: 202/364, loss: 0.16684, accuracy: 0.93998\n",
            "Epoch: 28/30, step: 203/364, loss: 0.16708, accuracy: 0.93981\n",
            "Epoch: 28/30, step: 204/364, loss: 0.16690, accuracy: 0.93987\n",
            "Epoch: 28/30, step: 205/364, loss: 0.16647, accuracy: 0.94017\n",
            "Epoch: 28/30, step: 206/364, loss: 0.16615, accuracy: 0.94038\n",
            "Epoch: 28/30, step: 207/364, loss: 0.16578, accuracy: 0.94067\n",
            "Epoch: 28/30, step: 208/364, loss: 0.16582, accuracy: 0.94058\n",
            "Epoch: 28/30, step: 209/364, loss: 0.16548, accuracy: 0.94071\n",
            "Epoch: 28/30, step: 210/364, loss: 0.16534, accuracy: 0.94085\n",
            "Epoch: 28/30, step: 211/364, loss: 0.16514, accuracy: 0.94098\n",
            "Epoch: 28/30, step: 212/364, loss: 0.16511, accuracy: 0.94089\n",
            "Epoch: 28/30, step: 213/364, loss: 0.16485, accuracy: 0.94102\n",
            "Epoch: 28/30, step: 214/364, loss: 0.16444, accuracy: 0.94130\n",
            "Epoch: 28/30, step: 215/364, loss: 0.16416, accuracy: 0.94157\n",
            "Epoch: 28/30, step: 216/364, loss: 0.16412, accuracy: 0.94148\n",
            "Epoch: 28/30, step: 217/364, loss: 0.16389, accuracy: 0.94160\n",
            "Epoch: 28/30, step: 218/364, loss: 0.16441, accuracy: 0.94130\n",
            "Epoch: 28/30, step: 219/364, loss: 0.16414, accuracy: 0.94135\n",
            "Epoch: 28/30, step: 220/364, loss: 0.16434, accuracy: 0.94105\n",
            "Epoch: 28/30, step: 221/364, loss: 0.16537, accuracy: 0.94054\n",
            "Epoch: 28/30, step: 222/364, loss: 0.16503, accuracy: 0.94074\n",
            "Epoch: 28/30, step: 223/364, loss: 0.16494, accuracy: 0.94072\n",
            "Epoch: 28/30, step: 224/364, loss: 0.16495, accuracy: 0.94071\n",
            "Epoch: 28/30, step: 225/364, loss: 0.16478, accuracy: 0.94083\n",
            "Epoch: 28/30, step: 226/364, loss: 0.16474, accuracy: 0.94096\n",
            "Epoch: 28/30, step: 227/364, loss: 0.16450, accuracy: 0.94108\n",
            "Epoch: 28/30, step: 228/364, loss: 0.16425, accuracy: 0.94120\n",
            "Epoch: 28/30, step: 229/364, loss: 0.16404, accuracy: 0.94132\n",
            "Epoch: 28/30, step: 230/364, loss: 0.16437, accuracy: 0.94110\n",
            "Epoch: 28/30, step: 231/364, loss: 0.16400, accuracy: 0.94129\n",
            "Epoch: 28/30, step: 232/364, loss: 0.16405, accuracy: 0.94127\n",
            "Epoch: 28/30, step: 233/364, loss: 0.16368, accuracy: 0.94146\n",
            "Epoch: 28/30, step: 234/364, loss: 0.16427, accuracy: 0.94097\n",
            "Epoch: 28/30, step: 235/364, loss: 0.16477, accuracy: 0.94069\n",
            "Epoch: 28/30, step: 236/364, loss: 0.16499, accuracy: 0.94048\n",
            "Epoch: 28/30, step: 237/364, loss: 0.16492, accuracy: 0.94066\n",
            "Epoch: 28/30, step: 238/364, loss: 0.16462, accuracy: 0.94078\n",
            "Epoch: 28/30, step: 239/364, loss: 0.16456, accuracy: 0.94083\n",
            "Epoch: 28/30, step: 240/364, loss: 0.16429, accuracy: 0.94095\n",
            "Epoch: 28/30, step: 241/364, loss: 0.16556, accuracy: 0.94042\n",
            "Epoch: 28/30, step: 242/364, loss: 0.16515, accuracy: 0.94066\n",
            "Epoch: 28/30, step: 243/364, loss: 0.16530, accuracy: 0.94046\n",
            "Epoch: 28/30, step: 244/364, loss: 0.16516, accuracy: 0.94064\n",
            "Epoch: 28/30, step: 245/364, loss: 0.16504, accuracy: 0.94069\n",
            "Epoch: 28/30, step: 246/364, loss: 0.16523, accuracy: 0.94061\n",
            "Epoch: 28/30, step: 247/364, loss: 0.16495, accuracy: 0.94073\n",
            "Epoch: 28/30, step: 248/364, loss: 0.16513, accuracy: 0.94071\n",
            "Epoch: 28/30, step: 249/364, loss: 0.16498, accuracy: 0.94083\n",
            "Epoch: 28/30, step: 250/364, loss: 0.16491, accuracy: 0.94087\n",
            "Epoch: 28/30, step: 251/364, loss: 0.16491, accuracy: 0.94092\n",
            "Epoch: 28/30, step: 252/364, loss: 0.16456, accuracy: 0.94116\n",
            "Epoch: 28/30, step: 253/364, loss: 0.16471, accuracy: 0.94114\n",
            "Epoch: 28/30, step: 254/364, loss: 0.16451, accuracy: 0.94138\n",
            "Epoch: 28/30, step: 255/364, loss: 0.16447, accuracy: 0.94148\n",
            "Epoch: 28/30, step: 256/364, loss: 0.16432, accuracy: 0.94165\n",
            "Epoch: 28/30, step: 257/364, loss: 0.16408, accuracy: 0.94176\n",
            "Epoch: 28/30, step: 258/364, loss: 0.16447, accuracy: 0.94150\n",
            "Epoch: 28/30, step: 259/364, loss: 0.16412, accuracy: 0.94172\n",
            "Epoch: 28/30, step: 260/364, loss: 0.16373, accuracy: 0.94195\n",
            "Epoch: 28/30, step: 261/364, loss: 0.16391, accuracy: 0.94193\n",
            "Epoch: 28/30, step: 262/364, loss: 0.16397, accuracy: 0.94197\n",
            "Epoch: 28/30, step: 263/364, loss: 0.16445, accuracy: 0.94178\n",
            "Epoch: 28/30, step: 264/364, loss: 0.16455, accuracy: 0.94170\n",
            "Epoch: 28/30, step: 265/364, loss: 0.16477, accuracy: 0.94163\n",
            "Epoch: 28/30, step: 266/364, loss: 0.16491, accuracy: 0.94161\n",
            "Epoch: 28/30, step: 267/364, loss: 0.16521, accuracy: 0.94142\n",
            "Epoch: 28/30, step: 268/364, loss: 0.16526, accuracy: 0.94146\n",
            "Epoch: 28/30, step: 269/364, loss: 0.16554, accuracy: 0.94133\n",
            "Epoch: 28/30, step: 270/364, loss: 0.16552, accuracy: 0.94138\n",
            "Epoch: 28/30, step: 271/364, loss: 0.16525, accuracy: 0.94154\n",
            "Epoch: 28/30, step: 272/364, loss: 0.16508, accuracy: 0.94158\n",
            "Epoch: 28/30, step: 273/364, loss: 0.16502, accuracy: 0.94168\n",
            "Epoch: 28/30, step: 274/364, loss: 0.16479, accuracy: 0.94183\n",
            "Epoch: 28/30, step: 275/364, loss: 0.16483, accuracy: 0.94182\n",
            "Epoch: 28/30, step: 276/364, loss: 0.16466, accuracy: 0.94192\n",
            "Epoch: 28/30, step: 277/364, loss: 0.16453, accuracy: 0.94201\n",
            "Epoch: 28/30, step: 278/364, loss: 0.16426, accuracy: 0.94211\n",
            "Epoch: 28/30, step: 279/364, loss: 0.16405, accuracy: 0.94220\n",
            "Epoch: 28/30, step: 280/364, loss: 0.16418, accuracy: 0.94219\n",
            "Epoch: 28/30, step: 281/364, loss: 0.16419, accuracy: 0.94212\n",
            "Epoch: 28/30, step: 282/364, loss: 0.16402, accuracy: 0.94221\n",
            "Epoch: 28/30, step: 283/364, loss: 0.16425, accuracy: 0.94208\n",
            "Epoch: 28/30, step: 284/364, loss: 0.16433, accuracy: 0.94207\n",
            "Epoch: 28/30, step: 285/364, loss: 0.16441, accuracy: 0.94205\n",
            "Epoch: 28/30, step: 286/364, loss: 0.16433, accuracy: 0.94209\n",
            "Epoch: 28/30, step: 287/364, loss: 0.16438, accuracy: 0.94196\n",
            "Epoch: 28/30, step: 288/364, loss: 0.16455, accuracy: 0.94173\n",
            "Epoch: 28/30, step: 289/364, loss: 0.16468, accuracy: 0.94155\n",
            "Epoch: 28/30, step: 290/364, loss: 0.16472, accuracy: 0.94154\n",
            "Epoch: 28/30, step: 291/364, loss: 0.16469, accuracy: 0.94148\n",
            "Epoch: 28/30, train loss: 0.16469, train accuracy: 0.94148, valid loss: 0.85869, valid accuracy: 0.68121\n",
            "Epoch: 29/30, step: 1/364, loss: 0.11407, accuracy: 0.95312\n",
            "Epoch: 29/30, step: 2/364, loss: 0.11924, accuracy: 0.94531\n",
            "Epoch: 29/30, step: 3/364, loss: 0.11644, accuracy: 0.95833\n",
            "Epoch: 29/30, step: 4/364, loss: 0.12025, accuracy: 0.96094\n",
            "Epoch: 29/30, step: 5/364, loss: 0.12762, accuracy: 0.95625\n",
            "Epoch: 29/30, step: 6/364, loss: 0.13397, accuracy: 0.94792\n",
            "Epoch: 29/30, step: 7/364, loss: 0.13676, accuracy: 0.94866\n",
            "Epoch: 29/30, step: 8/364, loss: 0.14449, accuracy: 0.94727\n",
            "Epoch: 29/30, step: 9/364, loss: 0.14610, accuracy: 0.94792\n",
            "Epoch: 29/30, step: 10/364, loss: 0.14045, accuracy: 0.95312\n",
            "Epoch: 29/30, step: 11/364, loss: 0.14405, accuracy: 0.95170\n",
            "Epoch: 29/30, step: 12/364, loss: 0.14152, accuracy: 0.95312\n",
            "Epoch: 29/30, step: 13/364, loss: 0.14075, accuracy: 0.95192\n",
            "Epoch: 29/30, step: 14/364, loss: 0.15839, accuracy: 0.94420\n",
            "Epoch: 29/30, step: 15/364, loss: 0.15580, accuracy: 0.94375\n",
            "Epoch: 29/30, step: 16/364, loss: 0.16185, accuracy: 0.94336\n",
            "Epoch: 29/30, step: 17/364, loss: 0.16023, accuracy: 0.94301\n",
            "Epoch: 29/30, step: 18/364, loss: 0.16378, accuracy: 0.94184\n",
            "Epoch: 29/30, step: 19/364, loss: 0.16055, accuracy: 0.94326\n",
            "Epoch: 29/30, step: 20/364, loss: 0.15911, accuracy: 0.94297\n",
            "Epoch: 29/30, step: 21/364, loss: 0.15722, accuracy: 0.94494\n",
            "Epoch: 29/30, step: 22/364, loss: 0.15635, accuracy: 0.94602\n",
            "Epoch: 29/30, step: 23/364, loss: 0.15283, accuracy: 0.94769\n",
            "Epoch: 29/30, step: 24/364, loss: 0.15257, accuracy: 0.94857\n",
            "Epoch: 29/30, step: 25/364, loss: 0.15049, accuracy: 0.95000\n",
            "Epoch: 29/30, step: 26/364, loss: 0.14892, accuracy: 0.95072\n",
            "Epoch: 29/30, step: 27/364, loss: 0.14899, accuracy: 0.95139\n",
            "Epoch: 29/30, step: 28/364, loss: 0.15319, accuracy: 0.95033\n",
            "Epoch: 29/30, step: 29/364, loss: 0.15122, accuracy: 0.95151\n",
            "Epoch: 29/30, step: 30/364, loss: 0.15024, accuracy: 0.95156\n",
            "Epoch: 29/30, step: 31/364, loss: 0.14906, accuracy: 0.95262\n",
            "Epoch: 29/30, step: 32/364, loss: 0.14739, accuracy: 0.95361\n",
            "Epoch: 29/30, step: 33/364, loss: 0.14741, accuracy: 0.95407\n",
            "Epoch: 29/30, step: 34/364, loss: 0.14657, accuracy: 0.95496\n",
            "Epoch: 29/30, step: 35/364, loss: 0.14645, accuracy: 0.95491\n",
            "Epoch: 29/30, step: 36/364, loss: 0.14794, accuracy: 0.95356\n",
            "Epoch: 29/30, step: 37/364, loss: 0.14859, accuracy: 0.95270\n",
            "Epoch: 29/30, step: 38/364, loss: 0.14797, accuracy: 0.95148\n",
            "Epoch: 29/30, step: 39/364, loss: 0.14628, accuracy: 0.95232\n",
            "Epoch: 29/30, step: 40/364, loss: 0.14632, accuracy: 0.95273\n",
            "Epoch: 29/30, step: 41/364, loss: 0.14516, accuracy: 0.95312\n",
            "Epoch: 29/30, step: 42/364, loss: 0.14767, accuracy: 0.95201\n",
            "Epoch: 29/30, step: 43/364, loss: 0.14788, accuracy: 0.95203\n",
            "Epoch: 29/30, step: 44/364, loss: 0.14665, accuracy: 0.95312\n",
            "Epoch: 29/30, step: 45/364, loss: 0.14508, accuracy: 0.95382\n",
            "Epoch: 29/30, step: 46/364, loss: 0.14521, accuracy: 0.95312\n",
            "Epoch: 29/30, step: 47/364, loss: 0.14439, accuracy: 0.95379\n",
            "Epoch: 29/30, step: 48/364, loss: 0.14269, accuracy: 0.95475\n",
            "Epoch: 29/30, step: 49/364, loss: 0.14387, accuracy: 0.95440\n",
            "Epoch: 29/30, step: 50/364, loss: 0.14325, accuracy: 0.95469\n",
            "Epoch: 29/30, step: 51/364, loss: 0.14227, accuracy: 0.95527\n",
            "Epoch: 29/30, step: 52/364, loss: 0.14248, accuracy: 0.95553\n",
            "Epoch: 29/30, step: 53/364, loss: 0.14199, accuracy: 0.95607\n",
            "Epoch: 29/30, step: 54/364, loss: 0.14443, accuracy: 0.95486\n",
            "Epoch: 29/30, step: 55/364, loss: 0.14499, accuracy: 0.95455\n",
            "Epoch: 29/30, step: 56/364, loss: 0.14345, accuracy: 0.95536\n",
            "Epoch: 29/30, step: 57/364, loss: 0.14288, accuracy: 0.95532\n",
            "Epoch: 29/30, step: 58/364, loss: 0.14401, accuracy: 0.95474\n",
            "Epoch: 29/30, step: 59/364, loss: 0.14410, accuracy: 0.95445\n",
            "Epoch: 29/30, step: 60/364, loss: 0.14357, accuracy: 0.95495\n",
            "Epoch: 29/30, step: 61/364, loss: 0.14566, accuracy: 0.95415\n",
            "Epoch: 29/30, step: 62/364, loss: 0.14522, accuracy: 0.95464\n",
            "Epoch: 29/30, step: 63/364, loss: 0.14526, accuracy: 0.95437\n",
            "Epoch: 29/30, step: 64/364, loss: 0.14418, accuracy: 0.95483\n",
            "Epoch: 29/30, step: 65/364, loss: 0.14329, accuracy: 0.95553\n",
            "Epoch: 29/30, step: 66/364, loss: 0.14308, accuracy: 0.95620\n",
            "Epoch: 29/30, step: 67/364, loss: 0.14306, accuracy: 0.95546\n",
            "Epoch: 29/30, step: 68/364, loss: 0.14225, accuracy: 0.95588\n",
            "Epoch: 29/30, step: 69/364, loss: 0.14142, accuracy: 0.95607\n",
            "Epoch: 29/30, step: 70/364, loss: 0.14189, accuracy: 0.95558\n",
            "Epoch: 29/30, step: 71/364, loss: 0.14312, accuracy: 0.95533\n",
            "Epoch: 29/30, step: 72/364, loss: 0.14365, accuracy: 0.95551\n",
            "Epoch: 29/30, step: 73/364, loss: 0.14393, accuracy: 0.95527\n",
            "Epoch: 29/30, step: 74/364, loss: 0.14438, accuracy: 0.95481\n",
            "Epoch: 29/30, step: 75/364, loss: 0.14425, accuracy: 0.95500\n",
            "Epoch: 29/30, step: 76/364, loss: 0.14434, accuracy: 0.95477\n",
            "Epoch: 29/30, step: 77/364, loss: 0.14395, accuracy: 0.95495\n",
            "Epoch: 29/30, step: 78/364, loss: 0.14360, accuracy: 0.95493\n",
            "Epoch: 29/30, step: 79/364, loss: 0.14347, accuracy: 0.95471\n",
            "Epoch: 29/30, step: 80/364, loss: 0.14262, accuracy: 0.95508\n",
            "Epoch: 29/30, step: 81/364, loss: 0.14231, accuracy: 0.95525\n",
            "Epoch: 29/30, step: 82/364, loss: 0.14265, accuracy: 0.95522\n",
            "Epoch: 29/30, step: 83/364, loss: 0.14466, accuracy: 0.95463\n",
            "Epoch: 29/30, step: 84/364, loss: 0.14425, accuracy: 0.95480\n",
            "Epoch: 29/30, step: 85/364, loss: 0.14346, accuracy: 0.95515\n",
            "Epoch: 29/30, step: 86/364, loss: 0.14427, accuracy: 0.95476\n",
            "Epoch: 29/30, step: 87/364, loss: 0.14559, accuracy: 0.95402\n",
            "Epoch: 29/30, step: 88/364, loss: 0.14702, accuracy: 0.95295\n",
            "Epoch: 29/30, step: 89/364, loss: 0.14759, accuracy: 0.95277\n",
            "Epoch: 29/30, step: 90/364, loss: 0.14722, accuracy: 0.95312\n",
            "Epoch: 29/30, step: 91/364, loss: 0.14693, accuracy: 0.95312\n",
            "Epoch: 29/30, step: 92/364, loss: 0.14728, accuracy: 0.95245\n",
            "Epoch: 29/30, step: 93/364, loss: 0.14642, accuracy: 0.95296\n",
            "Epoch: 29/30, step: 94/364, loss: 0.14560, accuracy: 0.95329\n",
            "Epoch: 29/30, step: 95/364, loss: 0.14538, accuracy: 0.95345\n",
            "Epoch: 29/30, step: 96/364, loss: 0.14512, accuracy: 0.95361\n",
            "Epoch: 29/30, step: 97/364, loss: 0.14446, accuracy: 0.95377\n",
            "Epoch: 29/30, step: 98/364, loss: 0.14415, accuracy: 0.95392\n",
            "Epoch: 29/30, step: 99/364, loss: 0.14516, accuracy: 0.95312\n",
            "Epoch: 29/30, step: 100/364, loss: 0.14509, accuracy: 0.95328\n",
            "Epoch: 29/30, step: 101/364, loss: 0.14438, accuracy: 0.95359\n",
            "Epoch: 29/30, step: 102/364, loss: 0.14494, accuracy: 0.95328\n",
            "Epoch: 29/30, step: 103/364, loss: 0.14522, accuracy: 0.95328\n",
            "Epoch: 29/30, step: 104/364, loss: 0.14591, accuracy: 0.95267\n",
            "Epoch: 29/30, step: 105/364, loss: 0.14541, accuracy: 0.95312\n",
            "Epoch: 29/30, step: 106/364, loss: 0.14479, accuracy: 0.95342\n",
            "Epoch: 29/30, step: 107/364, loss: 0.14495, accuracy: 0.95327\n",
            "Epoch: 29/30, step: 108/364, loss: 0.14487, accuracy: 0.95341\n",
            "Epoch: 29/30, step: 109/364, loss: 0.14524, accuracy: 0.95312\n",
            "Epoch: 29/30, step: 110/364, loss: 0.14529, accuracy: 0.95327\n",
            "Epoch: 29/30, step: 111/364, loss: 0.14505, accuracy: 0.95355\n",
            "Epoch: 29/30, step: 112/364, loss: 0.14576, accuracy: 0.95354\n",
            "Epoch: 29/30, step: 113/364, loss: 0.14545, accuracy: 0.95368\n",
            "Epoch: 29/30, step: 114/364, loss: 0.14510, accuracy: 0.95381\n",
            "Epoch: 29/30, step: 115/364, loss: 0.14492, accuracy: 0.95380\n",
            "Epoch: 29/30, step: 116/364, loss: 0.14537, accuracy: 0.95380\n",
            "Epoch: 29/30, step: 117/364, loss: 0.14541, accuracy: 0.95379\n",
            "Epoch: 29/30, step: 118/364, loss: 0.14472, accuracy: 0.95418\n",
            "Epoch: 29/30, step: 119/364, loss: 0.14534, accuracy: 0.95378\n",
            "Epoch: 29/30, step: 120/364, loss: 0.14574, accuracy: 0.95352\n",
            "Epoch: 29/30, step: 121/364, loss: 0.14640, accuracy: 0.95312\n",
            "Epoch: 29/30, step: 122/364, loss: 0.14607, accuracy: 0.95338\n",
            "Epoch: 29/30, step: 123/364, loss: 0.14582, accuracy: 0.95351\n",
            "Epoch: 29/30, step: 124/364, loss: 0.14667, accuracy: 0.95300\n",
            "Epoch: 29/30, step: 125/364, loss: 0.14705, accuracy: 0.95288\n",
            "Epoch: 29/30, step: 126/364, loss: 0.14637, accuracy: 0.95312\n",
            "Epoch: 29/30, step: 127/364, loss: 0.14634, accuracy: 0.95325\n",
            "Epoch: 29/30, step: 128/364, loss: 0.14605, accuracy: 0.95349\n",
            "Epoch: 29/30, step: 129/364, loss: 0.14612, accuracy: 0.95337\n",
            "Epoch: 29/30, step: 130/364, loss: 0.14570, accuracy: 0.95349\n",
            "Epoch: 29/30, step: 131/364, loss: 0.14639, accuracy: 0.95301\n",
            "Epoch: 29/30, step: 132/364, loss: 0.14607, accuracy: 0.95312\n",
            "Epoch: 29/30, step: 133/364, loss: 0.14637, accuracy: 0.95277\n",
            "Epoch: 29/30, step: 134/364, loss: 0.14619, accuracy: 0.95301\n",
            "Epoch: 29/30, step: 135/364, loss: 0.14592, accuracy: 0.95312\n",
            "Epoch: 29/30, step: 136/364, loss: 0.14548, accuracy: 0.95347\n",
            "Epoch: 29/30, step: 137/364, loss: 0.14519, accuracy: 0.95370\n",
            "Epoch: 29/30, step: 138/364, loss: 0.14511, accuracy: 0.95358\n",
            "Epoch: 29/30, step: 139/364, loss: 0.14527, accuracy: 0.95324\n",
            "Epoch: 29/30, step: 140/364, loss: 0.14631, accuracy: 0.95268\n",
            "Epoch: 29/30, step: 141/364, loss: 0.14661, accuracy: 0.95257\n",
            "Epoch: 29/30, step: 142/364, loss: 0.14603, accuracy: 0.95290\n",
            "Epoch: 29/30, step: 143/364, loss: 0.14559, accuracy: 0.95312\n",
            "Epoch: 29/30, step: 144/364, loss: 0.14518, accuracy: 0.95334\n",
            "Epoch: 29/30, step: 145/364, loss: 0.14540, accuracy: 0.95291\n",
            "Epoch: 29/30, step: 146/364, loss: 0.14535, accuracy: 0.95291\n",
            "Epoch: 29/30, step: 147/364, loss: 0.14697, accuracy: 0.95217\n",
            "Epoch: 29/30, step: 148/364, loss: 0.14727, accuracy: 0.95196\n",
            "Epoch: 29/30, step: 149/364, loss: 0.14718, accuracy: 0.95208\n",
            "Epoch: 29/30, step: 150/364, loss: 0.14681, accuracy: 0.95229\n",
            "Epoch: 29/30, step: 151/364, loss: 0.14643, accuracy: 0.95240\n",
            "Epoch: 29/30, step: 152/364, loss: 0.14636, accuracy: 0.95241\n",
            "Epoch: 29/30, step: 153/364, loss: 0.14614, accuracy: 0.95251\n",
            "Epoch: 29/30, step: 154/364, loss: 0.14688, accuracy: 0.95201\n",
            "Epoch: 29/30, step: 155/364, loss: 0.14706, accuracy: 0.95181\n",
            "Epoch: 29/30, step: 156/364, loss: 0.14755, accuracy: 0.95152\n",
            "Epoch: 29/30, step: 157/364, loss: 0.14772, accuracy: 0.95133\n",
            "Epoch: 29/30, step: 158/364, loss: 0.14754, accuracy: 0.95154\n",
            "Epoch: 29/30, step: 159/364, loss: 0.14734, accuracy: 0.95155\n",
            "Epoch: 29/30, step: 160/364, loss: 0.14701, accuracy: 0.95166\n",
            "Epoch: 29/30, step: 161/364, loss: 0.14712, accuracy: 0.95157\n",
            "Epoch: 29/30, step: 162/364, loss: 0.14725, accuracy: 0.95149\n",
            "Epoch: 29/30, step: 163/364, loss: 0.14705, accuracy: 0.95140\n",
            "Epoch: 29/30, step: 164/364, loss: 0.14697, accuracy: 0.95141\n",
            "Epoch: 29/30, step: 165/364, loss: 0.14668, accuracy: 0.95152\n",
            "Epoch: 29/30, step: 166/364, loss: 0.14747, accuracy: 0.95115\n",
            "Epoch: 29/30, step: 167/364, loss: 0.14748, accuracy: 0.95125\n",
            "Epoch: 29/30, step: 168/364, loss: 0.14718, accuracy: 0.95145\n",
            "Epoch: 29/30, step: 169/364, loss: 0.14677, accuracy: 0.95155\n",
            "Epoch: 29/30, step: 170/364, loss: 0.14869, accuracy: 0.95028\n",
            "Epoch: 29/30, step: 171/364, loss: 0.14844, accuracy: 0.95048\n",
            "Epoch: 29/30, step: 172/364, loss: 0.14858, accuracy: 0.95022\n",
            "Epoch: 29/30, step: 173/364, loss: 0.14910, accuracy: 0.94978\n",
            "Epoch: 29/30, step: 174/364, loss: 0.14894, accuracy: 0.94971\n",
            "Epoch: 29/30, step: 175/364, loss: 0.14881, accuracy: 0.94982\n",
            "Epoch: 29/30, step: 176/364, loss: 0.14865, accuracy: 0.94984\n",
            "Epoch: 29/30, step: 177/364, loss: 0.14902, accuracy: 0.94968\n",
            "Epoch: 29/30, step: 178/364, loss: 0.14922, accuracy: 0.94935\n",
            "Epoch: 29/30, step: 179/364, loss: 0.14950, accuracy: 0.94902\n",
            "Epoch: 29/30, step: 180/364, loss: 0.14939, accuracy: 0.94913\n",
            "Epoch: 29/30, step: 181/364, loss: 0.14916, accuracy: 0.94924\n",
            "Epoch: 29/30, step: 182/364, loss: 0.14899, accuracy: 0.94935\n",
            "Epoch: 29/30, step: 183/364, loss: 0.14882, accuracy: 0.94945\n",
            "Epoch: 29/30, step: 184/364, loss: 0.14870, accuracy: 0.94939\n",
            "Epoch: 29/30, step: 185/364, loss: 0.14891, accuracy: 0.94924\n",
            "Epoch: 29/30, step: 186/364, loss: 0.14931, accuracy: 0.94901\n",
            "Epoch: 29/30, step: 187/364, loss: 0.14914, accuracy: 0.94911\n",
            "Epoch: 29/30, step: 188/364, loss: 0.14904, accuracy: 0.94905\n",
            "Epoch: 29/30, step: 189/364, loss: 0.14887, accuracy: 0.94916\n",
            "Epoch: 29/30, step: 190/364, loss: 0.14876, accuracy: 0.94918\n",
            "Epoch: 29/30, step: 191/364, loss: 0.14858, accuracy: 0.94944\n",
            "Epoch: 29/30, step: 192/364, loss: 0.14853, accuracy: 0.94938\n",
            "Epoch: 29/30, step: 193/364, loss: 0.14838, accuracy: 0.94948\n",
            "Epoch: 29/30, step: 194/364, loss: 0.14816, accuracy: 0.94958\n",
            "Epoch: 29/30, step: 195/364, loss: 0.14870, accuracy: 0.94912\n",
            "Epoch: 29/30, step: 196/364, loss: 0.14906, accuracy: 0.94890\n",
            "Epoch: 29/30, step: 197/364, loss: 0.14884, accuracy: 0.94916\n",
            "Epoch: 29/30, step: 198/364, loss: 0.14867, accuracy: 0.94934\n",
            "Epoch: 29/30, step: 199/364, loss: 0.14840, accuracy: 0.94943\n",
            "Epoch: 29/30, step: 200/364, loss: 0.14821, accuracy: 0.94945\n",
            "Epoch: 29/30, step: 201/364, loss: 0.14799, accuracy: 0.94955\n",
            "Epoch: 29/30, step: 202/364, loss: 0.14770, accuracy: 0.94972\n",
            "Epoch: 29/30, step: 203/364, loss: 0.14779, accuracy: 0.94966\n",
            "Epoch: 29/30, step: 204/364, loss: 0.14758, accuracy: 0.94975\n",
            "Epoch: 29/30, step: 205/364, loss: 0.14781, accuracy: 0.94977\n",
            "Epoch: 29/30, step: 206/364, loss: 0.14757, accuracy: 0.94979\n",
            "Epoch: 29/30, step: 207/364, loss: 0.14739, accuracy: 0.94988\n",
            "Epoch: 29/30, step: 208/364, loss: 0.14718, accuracy: 0.94997\n",
            "Epoch: 29/30, step: 209/364, loss: 0.14725, accuracy: 0.94984\n",
            "Epoch: 29/30, step: 210/364, loss: 0.14704, accuracy: 0.94993\n",
            "Epoch: 29/30, step: 211/364, loss: 0.14698, accuracy: 0.95001\n",
            "Epoch: 29/30, step: 212/364, loss: 0.14670, accuracy: 0.95018\n",
            "Epoch: 29/30, step: 213/364, loss: 0.14673, accuracy: 0.95019\n",
            "Epoch: 29/30, step: 214/364, loss: 0.14675, accuracy: 0.95013\n",
            "Epoch: 29/30, step: 215/364, loss: 0.14664, accuracy: 0.95022\n",
            "Epoch: 29/30, step: 216/364, loss: 0.14693, accuracy: 0.95009\n",
            "Epoch: 29/30, step: 217/364, loss: 0.14680, accuracy: 0.95017\n",
            "Epoch: 29/30, step: 218/364, loss: 0.14680, accuracy: 0.95004\n",
            "Epoch: 29/30, step: 219/364, loss: 0.14660, accuracy: 0.95020\n",
            "Epoch: 29/30, step: 220/364, loss: 0.14653, accuracy: 0.95021\n",
            "Epoch: 29/30, step: 221/364, loss: 0.14623, accuracy: 0.95037\n",
            "Epoch: 29/30, step: 222/364, loss: 0.14617, accuracy: 0.95045\n",
            "Epoch: 29/30, step: 223/364, loss: 0.14636, accuracy: 0.95025\n",
            "Epoch: 29/30, step: 224/364, loss: 0.14616, accuracy: 0.95040\n",
            "Epoch: 29/30, step: 225/364, loss: 0.14605, accuracy: 0.95042\n",
            "Epoch: 29/30, step: 226/364, loss: 0.14618, accuracy: 0.95036\n",
            "Epoch: 29/30, step: 227/364, loss: 0.14599, accuracy: 0.95037\n",
            "Epoch: 29/30, step: 228/364, loss: 0.14668, accuracy: 0.95004\n",
            "Epoch: 29/30, step: 229/364, loss: 0.14651, accuracy: 0.95019\n",
            "Epoch: 29/30, step: 230/364, loss: 0.14635, accuracy: 0.95027\n",
            "Epoch: 29/30, step: 231/364, loss: 0.14635, accuracy: 0.95022\n",
            "Epoch: 29/30, step: 232/364, loss: 0.14692, accuracy: 0.95003\n",
            "Epoch: 29/30, step: 233/364, loss: 0.14696, accuracy: 0.94991\n",
            "Epoch: 29/30, step: 234/364, loss: 0.14682, accuracy: 0.94992\n",
            "Epoch: 29/30, step: 235/364, loss: 0.14655, accuracy: 0.95007\n",
            "Epoch: 29/30, step: 236/364, loss: 0.14639, accuracy: 0.95021\n",
            "Epoch: 29/30, step: 237/364, loss: 0.14620, accuracy: 0.95036\n",
            "Epoch: 29/30, step: 238/364, loss: 0.14598, accuracy: 0.95050\n",
            "Epoch: 29/30, step: 239/364, loss: 0.14575, accuracy: 0.95058\n",
            "Epoch: 29/30, step: 240/364, loss: 0.14564, accuracy: 0.95065\n",
            "Epoch: 29/30, step: 241/364, loss: 0.14562, accuracy: 0.95053\n",
            "Epoch: 29/30, step: 242/364, loss: 0.14561, accuracy: 0.95061\n",
            "Epoch: 29/30, step: 243/364, loss: 0.14566, accuracy: 0.95062\n",
            "Epoch: 29/30, step: 244/364, loss: 0.14542, accuracy: 0.95076\n",
            "Epoch: 29/30, step: 245/364, loss: 0.14583, accuracy: 0.95064\n",
            "Epoch: 29/30, step: 246/364, loss: 0.14588, accuracy: 0.95065\n",
            "Epoch: 29/30, step: 247/364, loss: 0.14573, accuracy: 0.95078\n",
            "Epoch: 29/30, step: 248/364, loss: 0.14632, accuracy: 0.95042\n",
            "Epoch: 29/30, step: 249/364, loss: 0.14607, accuracy: 0.95043\n",
            "Epoch: 29/30, step: 250/364, loss: 0.14617, accuracy: 0.95038\n",
            "Epoch: 29/30, step: 251/364, loss: 0.14646, accuracy: 0.95020\n",
            "Epoch: 29/30, step: 252/364, loss: 0.14634, accuracy: 0.95033\n",
            "Epoch: 29/30, step: 253/364, loss: 0.14692, accuracy: 0.95016\n",
            "Epoch: 29/30, step: 254/364, loss: 0.14679, accuracy: 0.95023\n",
            "Epoch: 29/30, step: 255/364, loss: 0.14689, accuracy: 0.95025\n",
            "Epoch: 29/30, step: 256/364, loss: 0.14678, accuracy: 0.95026\n",
            "Epoch: 29/30, step: 257/364, loss: 0.14720, accuracy: 0.95009\n",
            "Epoch: 29/30, step: 258/364, loss: 0.14712, accuracy: 0.95016\n",
            "Epoch: 29/30, step: 259/364, loss: 0.14702, accuracy: 0.95023\n",
            "Epoch: 29/30, step: 260/364, loss: 0.14678, accuracy: 0.95042\n",
            "Epoch: 29/30, step: 261/364, loss: 0.14663, accuracy: 0.95055\n",
            "Epoch: 29/30, step: 262/364, loss: 0.14647, accuracy: 0.95056\n",
            "Epoch: 29/30, step: 263/364, loss: 0.14677, accuracy: 0.95045\n",
            "Epoch: 29/30, step: 264/364, loss: 0.14675, accuracy: 0.95046\n",
            "Epoch: 29/30, step: 265/364, loss: 0.14666, accuracy: 0.95041\n",
            "Epoch: 29/30, step: 266/364, loss: 0.14736, accuracy: 0.94995\n",
            "Epoch: 29/30, step: 267/364, loss: 0.14766, accuracy: 0.94985\n",
            "Epoch: 29/30, step: 268/364, loss: 0.14759, accuracy: 0.94980\n",
            "Epoch: 29/30, step: 269/364, loss: 0.14740, accuracy: 0.94987\n",
            "Epoch: 29/30, step: 270/364, loss: 0.14758, accuracy: 0.94988\n",
            "Epoch: 29/30, step: 271/364, loss: 0.14775, accuracy: 0.94978\n",
            "Epoch: 29/30, step: 272/364, loss: 0.14795, accuracy: 0.94968\n",
            "Epoch: 29/30, step: 273/364, loss: 0.14773, accuracy: 0.94975\n",
            "Epoch: 29/30, step: 274/364, loss: 0.14782, accuracy: 0.94970\n",
            "Epoch: 29/30, step: 275/364, loss: 0.14771, accuracy: 0.94977\n",
            "Epoch: 29/30, step: 276/364, loss: 0.14820, accuracy: 0.94928\n",
            "Epoch: 29/30, step: 277/364, loss: 0.14799, accuracy: 0.94935\n",
            "Epoch: 29/30, step: 278/364, loss: 0.14798, accuracy: 0.94930\n",
            "Epoch: 29/30, step: 279/364, loss: 0.14784, accuracy: 0.94926\n",
            "Epoch: 29/30, step: 280/364, loss: 0.14775, accuracy: 0.94933\n",
            "Epoch: 29/30, step: 281/364, loss: 0.14783, accuracy: 0.94923\n",
            "Epoch: 29/30, step: 282/364, loss: 0.14775, accuracy: 0.94925\n",
            "Epoch: 29/30, step: 283/364, loss: 0.14761, accuracy: 0.94937\n",
            "Epoch: 29/30, step: 284/364, loss: 0.14742, accuracy: 0.94944\n",
            "Epoch: 29/30, step: 285/364, loss: 0.14753, accuracy: 0.94940\n",
            "Epoch: 29/30, step: 286/364, loss: 0.14730, accuracy: 0.94957\n",
            "Epoch: 29/30, step: 287/364, loss: 0.14725, accuracy: 0.94959\n",
            "Epoch: 29/30, step: 288/364, loss: 0.14726, accuracy: 0.94949\n",
            "Epoch: 29/30, step: 289/364, loss: 0.14696, accuracy: 0.94961\n",
            "Epoch: 29/30, step: 290/364, loss: 0.14688, accuracy: 0.94962\n",
            "Epoch: 29/30, step: 291/364, loss: 0.14671, accuracy: 0.94970\n",
            "Epoch: 29/30, train loss: 0.14671, train accuracy: 0.94970, valid loss: 0.85531, valid accuracy: 0.67842\n",
            "Epoch: 30/30, step: 1/364, loss: 0.07739, accuracy: 0.98438\n",
            "Epoch: 30/30, step: 2/364, loss: 0.13735, accuracy: 0.95312\n",
            "Epoch: 30/30, step: 3/364, loss: 0.13570, accuracy: 0.95312\n",
            "Epoch: 30/30, step: 4/364, loss: 0.13657, accuracy: 0.95312\n",
            "Epoch: 30/30, step: 5/364, loss: 0.13133, accuracy: 0.95625\n",
            "Epoch: 30/30, step: 6/364, loss: 0.13486, accuracy: 0.95052\n",
            "Epoch: 30/30, step: 7/364, loss: 0.13537, accuracy: 0.95089\n",
            "Epoch: 30/30, step: 8/364, loss: 0.13328, accuracy: 0.95312\n",
            "Epoch: 30/30, step: 9/364, loss: 0.13419, accuracy: 0.95139\n",
            "Epoch: 30/30, step: 10/364, loss: 0.13351, accuracy: 0.95312\n",
            "Epoch: 30/30, step: 11/364, loss: 0.13112, accuracy: 0.95312\n",
            "Epoch: 30/30, step: 12/364, loss: 0.12923, accuracy: 0.95443\n",
            "Epoch: 30/30, step: 13/364, loss: 0.12733, accuracy: 0.95553\n",
            "Epoch: 30/30, step: 14/364, loss: 0.12691, accuracy: 0.95536\n",
            "Epoch: 30/30, step: 15/364, loss: 0.12669, accuracy: 0.95625\n",
            "Epoch: 30/30, step: 16/364, loss: 0.12808, accuracy: 0.95703\n",
            "Epoch: 30/30, step: 17/364, loss: 0.12744, accuracy: 0.95772\n",
            "Epoch: 30/30, step: 18/364, loss: 0.12739, accuracy: 0.95747\n",
            "Epoch: 30/30, step: 19/364, loss: 0.12517, accuracy: 0.95970\n",
            "Epoch: 30/30, step: 20/364, loss: 0.12468, accuracy: 0.95938\n",
            "Epoch: 30/30, step: 21/364, loss: 0.12310, accuracy: 0.95982\n",
            "Epoch: 30/30, step: 22/364, loss: 0.12525, accuracy: 0.95881\n",
            "Epoch: 30/30, step: 23/364, loss: 0.12783, accuracy: 0.95720\n",
            "Epoch: 30/30, step: 24/364, loss: 0.12537, accuracy: 0.95898\n",
            "Epoch: 30/30, step: 25/364, loss: 0.12525, accuracy: 0.95938\n",
            "Epoch: 30/30, step: 26/364, loss: 0.12401, accuracy: 0.96034\n",
            "Epoch: 30/30, step: 27/364, loss: 0.12544, accuracy: 0.95949\n",
            "Epoch: 30/30, step: 28/364, loss: 0.12456, accuracy: 0.96094\n",
            "Epoch: 30/30, step: 29/364, loss: 0.12449, accuracy: 0.96121\n",
            "Epoch: 30/30, step: 30/364, loss: 0.12493, accuracy: 0.96094\n",
            "Epoch: 30/30, step: 31/364, loss: 0.12663, accuracy: 0.96069\n",
            "Epoch: 30/30, step: 32/364, loss: 0.12618, accuracy: 0.96094\n",
            "Epoch: 30/30, step: 33/364, loss: 0.12597, accuracy: 0.96117\n",
            "Epoch: 30/30, step: 34/364, loss: 0.12831, accuracy: 0.96002\n",
            "Epoch: 30/30, step: 35/364, loss: 0.12908, accuracy: 0.95893\n",
            "Epoch: 30/30, step: 36/364, loss: 0.12936, accuracy: 0.95920\n",
            "Epoch: 30/30, step: 37/364, loss: 0.13340, accuracy: 0.95608\n",
            "Epoch: 30/30, step: 38/364, loss: 0.13190, accuracy: 0.95724\n",
            "Epoch: 30/30, step: 39/364, loss: 0.13294, accuracy: 0.95793\n",
            "Epoch: 30/30, step: 40/364, loss: 0.13228, accuracy: 0.95859\n",
            "Epoch: 30/30, step: 41/364, loss: 0.13370, accuracy: 0.95732\n",
            "Epoch: 30/30, step: 42/364, loss: 0.13414, accuracy: 0.95759\n",
            "Epoch: 30/30, step: 43/364, loss: 0.13487, accuracy: 0.95749\n",
            "Epoch: 30/30, step: 44/364, loss: 0.13319, accuracy: 0.95845\n",
            "Epoch: 30/30, step: 45/364, loss: 0.13152, accuracy: 0.95938\n",
            "Epoch: 30/30, step: 46/364, loss: 0.13087, accuracy: 0.96026\n",
            "Epoch: 30/30, step: 47/364, loss: 0.13173, accuracy: 0.96011\n",
            "Epoch: 30/30, step: 48/364, loss: 0.13023, accuracy: 0.96061\n",
            "Epoch: 30/30, step: 49/364, loss: 0.13043, accuracy: 0.96014\n",
            "Epoch: 30/30, step: 50/364, loss: 0.13171, accuracy: 0.95906\n",
            "Epoch: 30/30, step: 51/364, loss: 0.13617, accuracy: 0.95650\n",
            "Epoch: 30/30, step: 52/364, loss: 0.13693, accuracy: 0.95553\n",
            "Epoch: 30/30, step: 53/364, loss: 0.13658, accuracy: 0.95578\n",
            "Epoch: 30/30, step: 54/364, loss: 0.13663, accuracy: 0.95631\n",
            "Epoch: 30/30, step: 55/364, loss: 0.13786, accuracy: 0.95568\n",
            "Epoch: 30/30, step: 56/364, loss: 0.13810, accuracy: 0.95536\n",
            "Epoch: 30/30, step: 57/364, loss: 0.13817, accuracy: 0.95587\n",
            "Epoch: 30/30, step: 58/364, loss: 0.13822, accuracy: 0.95582\n",
            "Epoch: 30/30, step: 59/364, loss: 0.13762, accuracy: 0.95604\n",
            "Epoch: 30/30, step: 60/364, loss: 0.13892, accuracy: 0.95495\n",
            "Epoch: 30/30, step: 61/364, loss: 0.13913, accuracy: 0.95441\n",
            "Epoch: 30/30, step: 62/364, loss: 0.13812, accuracy: 0.95514\n",
            "Epoch: 30/30, step: 63/364, loss: 0.13692, accuracy: 0.95585\n",
            "Epoch: 30/30, step: 64/364, loss: 0.13637, accuracy: 0.95630\n",
            "Epoch: 30/30, step: 65/364, loss: 0.13586, accuracy: 0.95697\n",
            "Epoch: 30/30, step: 66/364, loss: 0.13462, accuracy: 0.95762\n",
            "Epoch: 30/30, step: 67/364, loss: 0.13528, accuracy: 0.95756\n",
            "Epoch: 30/30, step: 68/364, loss: 0.13465, accuracy: 0.95795\n",
            "Epoch: 30/30, step: 69/364, loss: 0.13500, accuracy: 0.95811\n",
            "Epoch: 30/30, step: 70/364, loss: 0.13500, accuracy: 0.95826\n",
            "Epoch: 30/30, step: 71/364, loss: 0.13465, accuracy: 0.95819\n",
            "Epoch: 30/30, step: 72/364, loss: 0.13438, accuracy: 0.95812\n",
            "Epoch: 30/30, step: 73/364, loss: 0.13369, accuracy: 0.95826\n",
            "Epoch: 30/30, step: 74/364, loss: 0.13396, accuracy: 0.95840\n",
            "Epoch: 30/30, step: 75/364, loss: 0.13393, accuracy: 0.95854\n",
            "Epoch: 30/30, step: 76/364, loss: 0.13299, accuracy: 0.95909\n",
            "Epoch: 30/30, step: 77/364, loss: 0.13286, accuracy: 0.95901\n",
            "Epoch: 30/30, step: 78/364, loss: 0.13193, accuracy: 0.95954\n",
            "Epoch: 30/30, step: 79/364, loss: 0.13141, accuracy: 0.95985\n",
            "Epoch: 30/30, step: 80/364, loss: 0.13394, accuracy: 0.95898\n",
            "Epoch: 30/30, step: 81/364, loss: 0.13391, accuracy: 0.95853\n",
            "Epoch: 30/30, step: 82/364, loss: 0.13289, accuracy: 0.95903\n",
            "Epoch: 30/30, step: 83/364, loss: 0.13282, accuracy: 0.95934\n",
            "Epoch: 30/30, step: 84/364, loss: 0.13243, accuracy: 0.95926\n",
            "Epoch: 30/30, step: 85/364, loss: 0.13279, accuracy: 0.95938\n",
            "Epoch: 30/30, step: 86/364, loss: 0.13280, accuracy: 0.95948\n",
            "Epoch: 30/30, step: 87/364, loss: 0.13249, accuracy: 0.95941\n",
            "Epoch: 30/30, step: 88/364, loss: 0.13289, accuracy: 0.95898\n",
            "Epoch: 30/30, step: 89/364, loss: 0.13316, accuracy: 0.95874\n",
            "Epoch: 30/30, step: 90/364, loss: 0.13404, accuracy: 0.95816\n",
            "Epoch: 30/30, step: 91/364, loss: 0.13424, accuracy: 0.95793\n",
            "Epoch: 30/30, step: 92/364, loss: 0.13397, accuracy: 0.95805\n",
            "Epoch: 30/30, step: 93/364, loss: 0.13427, accuracy: 0.95783\n",
            "Epoch: 30/30, step: 94/364, loss: 0.13417, accuracy: 0.95795\n",
            "Epoch: 30/30, step: 95/364, loss: 0.13416, accuracy: 0.95789\n",
            "Epoch: 30/30, step: 96/364, loss: 0.13471, accuracy: 0.95752\n",
            "Epoch: 30/30, step: 97/364, loss: 0.13562, accuracy: 0.95683\n",
            "Epoch: 30/30, step: 98/364, loss: 0.13599, accuracy: 0.95663\n",
            "Epoch: 30/30, step: 99/364, loss: 0.13702, accuracy: 0.95628\n",
            "Epoch: 30/30, step: 100/364, loss: 0.13665, accuracy: 0.95672\n",
            "Epoch: 30/30, step: 101/364, loss: 0.13934, accuracy: 0.95591\n",
            "Epoch: 30/30, step: 102/364, loss: 0.14053, accuracy: 0.95558\n",
            "Epoch: 30/30, step: 103/364, loss: 0.14062, accuracy: 0.95555\n",
            "Epoch: 30/30, step: 104/364, loss: 0.14005, accuracy: 0.95583\n",
            "Epoch: 30/30, step: 105/364, loss: 0.14005, accuracy: 0.95551\n",
            "Epoch: 30/30, step: 106/364, loss: 0.13967, accuracy: 0.95563\n",
            "Epoch: 30/30, step: 107/364, loss: 0.13961, accuracy: 0.95546\n",
            "Epoch: 30/30, step: 108/364, loss: 0.13886, accuracy: 0.95587\n",
            "Epoch: 30/30, step: 109/364, loss: 0.13887, accuracy: 0.95571\n",
            "Epoch: 30/30, step: 110/364, loss: 0.13820, accuracy: 0.95582\n",
            "Epoch: 30/30, step: 111/364, loss: 0.13884, accuracy: 0.95552\n",
            "Epoch: 30/30, step: 112/364, loss: 0.13862, accuracy: 0.95578\n",
            "Epoch: 30/30, step: 113/364, loss: 0.13854, accuracy: 0.95589\n",
            "Epoch: 30/30, step: 114/364, loss: 0.13890, accuracy: 0.95573\n",
            "Epoch: 30/30, step: 115/364, loss: 0.13883, accuracy: 0.95598\n",
            "Epoch: 30/30, step: 116/364, loss: 0.13864, accuracy: 0.95609\n",
            "Epoch: 30/30, step: 117/364, loss: 0.13914, accuracy: 0.95580\n",
            "Epoch: 30/30, step: 118/364, loss: 0.13882, accuracy: 0.95577\n",
            "Epoch: 30/30, step: 119/364, loss: 0.13823, accuracy: 0.95588\n",
            "Epoch: 30/30, step: 120/364, loss: 0.13812, accuracy: 0.95599\n",
            "Epoch: 30/30, step: 121/364, loss: 0.13816, accuracy: 0.95610\n",
            "Epoch: 30/30, step: 122/364, loss: 0.13787, accuracy: 0.95620\n",
            "Epoch: 30/30, step: 123/364, loss: 0.13783, accuracy: 0.95630\n",
            "Epoch: 30/30, step: 124/364, loss: 0.13923, accuracy: 0.95590\n",
            "Epoch: 30/30, step: 125/364, loss: 0.13890, accuracy: 0.95600\n",
            "Epoch: 30/30, step: 126/364, loss: 0.13840, accuracy: 0.95623\n",
            "Epoch: 30/30, step: 127/364, loss: 0.13886, accuracy: 0.95583\n",
            "Epoch: 30/30, step: 128/364, loss: 0.13935, accuracy: 0.95532\n",
            "Epoch: 30/30, step: 129/364, loss: 0.13924, accuracy: 0.95531\n",
            "Epoch: 30/30, step: 130/364, loss: 0.13918, accuracy: 0.95541\n",
            "Epoch: 30/30, step: 131/364, loss: 0.13914, accuracy: 0.95551\n",
            "Epoch: 30/30, step: 132/364, loss: 0.13880, accuracy: 0.95561\n",
            "Epoch: 30/30, step: 133/364, loss: 0.13894, accuracy: 0.95559\n",
            "Epoch: 30/30, step: 134/364, loss: 0.13896, accuracy: 0.95569\n",
            "Epoch: 30/30, step: 135/364, loss: 0.13875, accuracy: 0.95579\n",
            "Epoch: 30/30, step: 136/364, loss: 0.13824, accuracy: 0.95600\n",
            "Epoch: 30/30, step: 137/364, loss: 0.13816, accuracy: 0.95609\n",
            "Epoch: 30/30, step: 138/364, loss: 0.13843, accuracy: 0.95596\n",
            "Epoch: 30/30, step: 139/364, loss: 0.13838, accuracy: 0.95582\n",
            "Epoch: 30/30, step: 140/364, loss: 0.13824, accuracy: 0.95592\n",
            "Epoch: 30/30, step: 141/364, loss: 0.13827, accuracy: 0.95601\n",
            "Epoch: 30/30, step: 142/364, loss: 0.13773, accuracy: 0.95632\n",
            "Epoch: 30/30, step: 143/364, loss: 0.13731, accuracy: 0.95651\n",
            "Epoch: 30/30, step: 144/364, loss: 0.13795, accuracy: 0.95605\n",
            "Epoch: 30/30, step: 145/364, loss: 0.13791, accuracy: 0.95614\n",
            "Epoch: 30/30, step: 146/364, loss: 0.13796, accuracy: 0.95612\n",
            "Epoch: 30/30, step: 147/364, loss: 0.13863, accuracy: 0.95568\n",
            "Epoch: 30/30, step: 148/364, loss: 0.13938, accuracy: 0.95513\n",
            "Epoch: 30/30, step: 149/364, loss: 0.13946, accuracy: 0.95522\n",
            "Epoch: 30/30, step: 150/364, loss: 0.13974, accuracy: 0.95510\n",
            "Epoch: 30/30, step: 151/364, loss: 0.13949, accuracy: 0.95519\n",
            "Epoch: 30/30, step: 152/364, loss: 0.13959, accuracy: 0.95508\n",
            "Epoch: 30/30, step: 153/364, loss: 0.14021, accuracy: 0.95476\n",
            "Epoch: 30/30, step: 154/364, loss: 0.14006, accuracy: 0.95485\n",
            "Epoch: 30/30, step: 155/364, loss: 0.13997, accuracy: 0.95504\n",
            "Epoch: 30/30, step: 156/364, loss: 0.13997, accuracy: 0.95503\n",
            "Epoch: 30/30, step: 157/364, loss: 0.14054, accuracy: 0.95462\n",
            "Epoch: 30/30, step: 158/364, loss: 0.14002, accuracy: 0.95481\n",
            "Epoch: 30/30, step: 159/364, loss: 0.13971, accuracy: 0.95499\n",
            "Epoch: 30/30, step: 160/364, loss: 0.13976, accuracy: 0.95498\n",
            "Epoch: 30/30, step: 161/364, loss: 0.13952, accuracy: 0.95516\n",
            "Epoch: 30/30, step: 162/364, loss: 0.13958, accuracy: 0.95496\n",
            "Epoch: 30/30, step: 163/364, loss: 0.13920, accuracy: 0.95514\n",
            "Epoch: 30/30, step: 164/364, loss: 0.13904, accuracy: 0.95522\n",
            "Epoch: 30/30, step: 165/364, loss: 0.13894, accuracy: 0.95540\n",
            "Epoch: 30/30, step: 166/364, loss: 0.13939, accuracy: 0.95529\n",
            "Epoch: 30/30, step: 167/364, loss: 0.13966, accuracy: 0.95500\n",
            "Epoch: 30/30, step: 168/364, loss: 0.13946, accuracy: 0.95517\n",
            "Epoch: 30/30, step: 169/364, loss: 0.13959, accuracy: 0.95497\n",
            "Epoch: 30/30, step: 170/364, loss: 0.13943, accuracy: 0.95506\n",
            "Epoch: 30/30, step: 171/364, loss: 0.13977, accuracy: 0.95495\n",
            "Epoch: 30/30, step: 172/364, loss: 0.13978, accuracy: 0.95494\n",
            "Epoch: 30/30, step: 173/364, loss: 0.13975, accuracy: 0.95493\n",
            "Epoch: 30/30, step: 174/364, loss: 0.14010, accuracy: 0.95474\n",
            "Epoch: 30/30, step: 175/364, loss: 0.14067, accuracy: 0.95446\n",
            "Epoch: 30/30, step: 176/364, loss: 0.14087, accuracy: 0.95437\n",
            "Epoch: 30/30, step: 177/364, loss: 0.14074, accuracy: 0.95436\n",
            "Epoch: 30/30, step: 178/364, loss: 0.14149, accuracy: 0.95391\n",
            "Epoch: 30/30, step: 179/364, loss: 0.14161, accuracy: 0.95382\n",
            "Epoch: 30/30, step: 180/364, loss: 0.14134, accuracy: 0.95399\n",
            "Epoch: 30/30, step: 181/364, loss: 0.14096, accuracy: 0.95416\n",
            "Epoch: 30/30, step: 182/364, loss: 0.14101, accuracy: 0.95407\n",
            "Epoch: 30/30, step: 183/364, loss: 0.14093, accuracy: 0.95415\n",
            "Epoch: 30/30, step: 184/364, loss: 0.14084, accuracy: 0.95406\n",
            "Epoch: 30/30, step: 185/364, loss: 0.14153, accuracy: 0.95363\n",
            "Epoch: 30/30, step: 186/364, loss: 0.14152, accuracy: 0.95355\n",
            "Epoch: 30/30, step: 187/364, loss: 0.14204, accuracy: 0.95312\n",
            "Epoch: 30/30, step: 188/364, loss: 0.14254, accuracy: 0.95296\n",
            "Epoch: 30/30, step: 189/364, loss: 0.14209, accuracy: 0.95321\n",
            "Epoch: 30/30, step: 190/364, loss: 0.14288, accuracy: 0.95263\n",
            "Epoch: 30/30, step: 191/364, loss: 0.14241, accuracy: 0.95288\n",
            "Epoch: 30/30, step: 192/364, loss: 0.14217, accuracy: 0.95296\n",
            "Epoch: 30/30, step: 193/364, loss: 0.14344, accuracy: 0.95215\n",
            "Epoch: 30/30, step: 194/364, loss: 0.14312, accuracy: 0.95240\n",
            "Epoch: 30/30, step: 195/364, loss: 0.14297, accuracy: 0.95256\n",
            "Epoch: 30/30, step: 196/364, loss: 0.14281, accuracy: 0.95273\n",
            "Epoch: 30/30, step: 197/364, loss: 0.14259, accuracy: 0.95289\n",
            "Epoch: 30/30, step: 198/364, loss: 0.14239, accuracy: 0.95305\n",
            "Epoch: 30/30, step: 199/364, loss: 0.14302, accuracy: 0.95273\n",
            "Epoch: 30/30, step: 200/364, loss: 0.14314, accuracy: 0.95266\n",
            "Epoch: 30/30, step: 201/364, loss: 0.14306, accuracy: 0.95258\n",
            "Epoch: 30/30, step: 202/364, loss: 0.14341, accuracy: 0.95227\n",
            "Epoch: 30/30, step: 203/364, loss: 0.14413, accuracy: 0.95182\n",
            "Epoch: 30/30, step: 204/364, loss: 0.14392, accuracy: 0.95190\n",
            "Epoch: 30/30, step: 205/364, loss: 0.14401, accuracy: 0.95183\n",
            "Epoch: 30/30, step: 206/364, loss: 0.14431, accuracy: 0.95176\n",
            "Epoch: 30/30, step: 207/364, loss: 0.14426, accuracy: 0.95169\n",
            "Epoch: 30/30, step: 208/364, loss: 0.14409, accuracy: 0.95170\n",
            "Epoch: 30/30, step: 209/364, loss: 0.14413, accuracy: 0.95178\n",
            "Epoch: 30/30, step: 210/364, loss: 0.14493, accuracy: 0.95134\n",
            "Epoch: 30/30, step: 211/364, loss: 0.14462, accuracy: 0.95150\n",
            "Epoch: 30/30, step: 212/364, loss: 0.14447, accuracy: 0.95158\n",
            "Epoch: 30/30, step: 213/364, loss: 0.14415, accuracy: 0.95180\n",
            "Epoch: 30/30, step: 214/364, loss: 0.14459, accuracy: 0.95145\n",
            "Epoch: 30/30, step: 215/364, loss: 0.14541, accuracy: 0.95116\n",
            "Epoch: 30/30, step: 216/364, loss: 0.14527, accuracy: 0.95124\n",
            "Epoch: 30/30, step: 217/364, loss: 0.14530, accuracy: 0.95118\n",
            "Epoch: 30/30, step: 218/364, loss: 0.14491, accuracy: 0.95140\n",
            "Epoch: 30/30, step: 219/364, loss: 0.14516, accuracy: 0.95120\n",
            "Epoch: 30/30, step: 220/364, loss: 0.14548, accuracy: 0.95092\n",
            "Epoch: 30/30, step: 221/364, loss: 0.14514, accuracy: 0.95115\n",
            "Epoch: 30/30, step: 222/364, loss: 0.14613, accuracy: 0.95080\n",
            "Epoch: 30/30, step: 223/364, loss: 0.14583, accuracy: 0.95088\n",
            "Epoch: 30/30, step: 224/364, loss: 0.14558, accuracy: 0.95089\n",
            "Epoch: 30/30, step: 225/364, loss: 0.14617, accuracy: 0.95049\n",
            "Epoch: 30/30, step: 226/364, loss: 0.14665, accuracy: 0.95015\n",
            "Epoch: 30/30, step: 227/364, loss: 0.14666, accuracy: 0.95010\n",
            "Epoch: 30/30, step: 228/364, loss: 0.14637, accuracy: 0.95018\n",
            "Epoch: 30/30, step: 229/364, loss: 0.14659, accuracy: 0.95026\n",
            "Epoch: 30/30, step: 230/364, loss: 0.14644, accuracy: 0.95034\n",
            "Epoch: 30/30, step: 231/364, loss: 0.14657, accuracy: 0.95028\n",
            "Epoch: 30/30, step: 232/364, loss: 0.14637, accuracy: 0.95036\n",
            "Epoch: 30/30, step: 233/364, loss: 0.14623, accuracy: 0.95031\n",
            "Epoch: 30/30, step: 234/364, loss: 0.14589, accuracy: 0.95052\n",
            "Epoch: 30/30, step: 235/364, loss: 0.14643, accuracy: 0.95013\n",
            "Epoch: 30/30, step: 236/364, loss: 0.14663, accuracy: 0.94995\n",
            "Epoch: 30/30, step: 237/364, loss: 0.14659, accuracy: 0.94996\n",
            "Epoch: 30/30, step: 238/364, loss: 0.14666, accuracy: 0.94997\n",
            "Epoch: 30/30, step: 239/364, loss: 0.14666, accuracy: 0.94992\n",
            "Epoch: 30/30, step: 240/364, loss: 0.14627, accuracy: 0.95013\n",
            "Epoch: 30/30, step: 241/364, loss: 0.14597, accuracy: 0.95027\n",
            "Epoch: 30/30, step: 242/364, loss: 0.14584, accuracy: 0.95022\n",
            "Epoch: 30/30, step: 243/364, loss: 0.14547, accuracy: 0.95042\n",
            "Epoch: 30/30, step: 244/364, loss: 0.14536, accuracy: 0.95037\n",
            "Epoch: 30/30, step: 245/364, loss: 0.14523, accuracy: 0.95045\n",
            "Epoch: 30/30, step: 246/364, loss: 0.14489, accuracy: 0.95065\n",
            "Epoch: 30/30, step: 247/364, loss: 0.14487, accuracy: 0.95059\n",
            "Epoch: 30/30, step: 248/364, loss: 0.14477, accuracy: 0.95048\n",
            "Epoch: 30/30, step: 249/364, loss: 0.14461, accuracy: 0.95061\n",
            "Epoch: 30/30, step: 250/364, loss: 0.14462, accuracy: 0.95056\n",
            "Epoch: 30/30, step: 251/364, loss: 0.14482, accuracy: 0.95045\n",
            "Epoch: 30/30, step: 252/364, loss: 0.14466, accuracy: 0.95052\n",
            "Epoch: 30/30, step: 253/364, loss: 0.14440, accuracy: 0.95065\n",
            "Epoch: 30/30, step: 254/364, loss: 0.14420, accuracy: 0.95079\n",
            "Epoch: 30/30, step: 255/364, loss: 0.14407, accuracy: 0.95086\n",
            "Epoch: 30/30, step: 256/364, loss: 0.14412, accuracy: 0.95081\n",
            "Epoch: 30/30, step: 257/364, loss: 0.14398, accuracy: 0.95094\n",
            "Epoch: 30/30, step: 258/364, loss: 0.14369, accuracy: 0.95113\n",
            "Epoch: 30/30, step: 259/364, loss: 0.14333, accuracy: 0.95132\n",
            "Epoch: 30/30, step: 260/364, loss: 0.14329, accuracy: 0.95132\n",
            "Epoch: 30/30, step: 261/364, loss: 0.14336, accuracy: 0.95133\n",
            "Epoch: 30/30, step: 262/364, loss: 0.14315, accuracy: 0.95146\n",
            "Epoch: 30/30, step: 263/364, loss: 0.14298, accuracy: 0.95158\n",
            "Epoch: 30/30, step: 264/364, loss: 0.14300, accuracy: 0.95153\n",
            "Epoch: 30/30, step: 265/364, loss: 0.14296, accuracy: 0.95153\n",
            "Epoch: 30/30, step: 266/364, loss: 0.14299, accuracy: 0.95154\n",
            "Epoch: 30/30, step: 267/364, loss: 0.14331, accuracy: 0.95143\n",
            "Epoch: 30/30, step: 268/364, loss: 0.14309, accuracy: 0.95161\n",
            "Epoch: 30/30, step: 269/364, loss: 0.14306, accuracy: 0.95161\n",
            "Epoch: 30/30, step: 270/364, loss: 0.14282, accuracy: 0.95168\n",
            "Epoch: 30/30, step: 271/364, loss: 0.14264, accuracy: 0.95180\n",
            "Epoch: 30/30, step: 272/364, loss: 0.14261, accuracy: 0.95175\n",
            "Epoch: 30/30, step: 273/364, loss: 0.14254, accuracy: 0.95175\n",
            "Epoch: 30/30, step: 274/364, loss: 0.14244, accuracy: 0.95176\n",
            "Epoch: 30/30, step: 275/364, loss: 0.14264, accuracy: 0.95159\n",
            "Epoch: 30/30, step: 276/364, loss: 0.14249, accuracy: 0.95160\n",
            "Epoch: 30/30, step: 277/364, loss: 0.14259, accuracy: 0.95155\n",
            "Epoch: 30/30, step: 278/364, loss: 0.14243, accuracy: 0.95161\n",
            "Epoch: 30/30, step: 279/364, loss: 0.14266, accuracy: 0.95139\n",
            "Epoch: 30/30, step: 280/364, loss: 0.14251, accuracy: 0.95151\n",
            "Epoch: 30/30, step: 281/364, loss: 0.14240, accuracy: 0.95157\n",
            "Epoch: 30/30, step: 282/364, loss: 0.14225, accuracy: 0.95168\n",
            "Epoch: 30/30, step: 283/364, loss: 0.14208, accuracy: 0.95174\n",
            "Epoch: 30/30, step: 284/364, loss: 0.14214, accuracy: 0.95164\n",
            "Epoch: 30/30, step: 285/364, loss: 0.14214, accuracy: 0.95170\n",
            "Epoch: 30/30, step: 286/364, loss: 0.14258, accuracy: 0.95143\n",
            "Epoch: 30/30, step: 287/364, loss: 0.14251, accuracy: 0.95149\n",
            "Epoch: 30/30, step: 288/364, loss: 0.14228, accuracy: 0.95166\n",
            "Epoch: 30/30, step: 289/364, loss: 0.14258, accuracy: 0.95139\n",
            "Epoch: 30/30, step: 290/364, loss: 0.14234, accuracy: 0.95145\n",
            "Epoch: 30/30, step: 291/364, loss: 0.14262, accuracy: 0.95132\n",
            "Epoch: 30/30, train loss: 0.14262, train accuracy: 0.95132, valid loss: 0.90616, valid accuracy: 0.67928\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(type='resnet34_2'):\n",
        "    if type == 'resenet50_2':\n",
        "        model = resnet_50_2()\n",
        "    else:\n",
        "        model = resnet_34()\n",
        "    model.build(input_shape=(None, 224, 224, 3))\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "model = get_model()\n",
        "\n",
        "import math\n",
        "\n",
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(images, training=True)\n",
        "        loss = loss_object(y_true=labels, y_pred=predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(grads_and_vars=zip(gradients, model.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(labels, predictions)\n",
        "\n",
        "@tf.function\n",
        "def valid_step(images, labels):\n",
        "    predictions = model(images, training=False)\n",
        "    v_loss = loss_object(labels, predictions)\n",
        "\n",
        "    valid_loss(v_loss)\n",
        "    valid_accuracy(labels, predictions)\n",
        "\n",
        "# start training\n",
        "for epoch in range(30):\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    valid_loss.reset_states()\n",
        "    valid_accuracy.reset_states()\n",
        "    step = 0\n",
        "    for images, labels in train_batches:\n",
        "        step += 1\n",
        "        train_step(images, labels)\n",
        "        print(\"Epoch: {}/{}, step: {}/{}, loss: {:.5f}, accuracy: {:.5f}\".format(epoch + 1,\n",
        "                                                                                    30,\n",
        "                                                                                    step,\n",
        "                                                                                    math.ceil(num_examples / 64),\n",
        "                                                                                    train_loss.result(),\n",
        "                                                                                    train_accuracy.result()))\n",
        "\n",
        "    for valid_images, valid_labels in validation_batches:\n",
        "        valid_step(valid_images, valid_labels)\n",
        "\n",
        "    print(\"Epoch: {}/{}, train loss: {:.5f}, train accuracy: {:.5f}, \"\n",
        "            \"valid loss: {:.5f}, valid accuracy: {:.5f}\".format(epoch + 1,\n",
        "                                                                30,\n",
        "                                                                train_loss.result(),\n",
        "                                                                train_accuracy.result(),\n",
        "                                                                valid_loss.result(),\n",
        "                                                                valid_accuracy.result()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xj7JlI2K3cL6",
        "outputId": "ee2fcf1d-3d50-4190-9c62-285dc31f760a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch: 13/30, step: 257/364, loss: 0.47082, accuracy: 0.78052\n",
            "Epoch: 13/30, step: 258/364, loss: 0.47073, accuracy: 0.78064\n",
            "Epoch: 13/30, step: 259/364, loss: 0.47076, accuracy: 0.78083\n",
            "Epoch: 13/30, step: 260/364, loss: 0.47075, accuracy: 0.78077\n",
            "Epoch: 13/30, step: 261/364, loss: 0.47058, accuracy: 0.78107\n",
            "Epoch: 13/30, step: 262/364, loss: 0.47037, accuracy: 0.78125\n",
            "Epoch: 13/30, step: 263/364, loss: 0.47040, accuracy: 0.78131\n",
            "Epoch: 13/30, step: 264/364, loss: 0.47024, accuracy: 0.78137\n",
            "Epoch: 13/30, step: 265/364, loss: 0.47001, accuracy: 0.78166\n",
            "Epoch: 13/30, step: 266/364, loss: 0.46973, accuracy: 0.78201\n",
            "Epoch: 13/30, step: 267/364, loss: 0.46964, accuracy: 0.78201\n",
            "Epoch: 13/30, step: 268/364, loss: 0.46982, accuracy: 0.78166\n",
            "Epoch: 13/30, step: 269/364, loss: 0.47017, accuracy: 0.78142\n",
            "Epoch: 13/30, step: 270/364, loss: 0.47009, accuracy: 0.78154\n",
            "Epoch: 13/30, step: 271/364, loss: 0.47010, accuracy: 0.78160\n",
            "Epoch: 13/30, step: 272/364, loss: 0.46976, accuracy: 0.78165\n",
            "Epoch: 13/30, step: 273/364, loss: 0.46965, accuracy: 0.78171\n",
            "Epoch: 13/30, step: 274/364, loss: 0.46946, accuracy: 0.78188\n",
            "Epoch: 13/30, step: 275/364, loss: 0.46947, accuracy: 0.78188\n",
            "Epoch: 13/30, step: 276/364, loss: 0.46924, accuracy: 0.78193\n",
            "Epoch: 13/30, step: 277/364, loss: 0.46945, accuracy: 0.78164\n",
            "Epoch: 13/30, step: 278/364, loss: 0.46947, accuracy: 0.78176\n",
            "Epoch: 13/30, step: 279/364, loss: 0.46923, accuracy: 0.78192\n",
            "Epoch: 13/30, step: 280/364, loss: 0.46929, accuracy: 0.78209\n",
            "Epoch: 13/30, step: 281/364, loss: 0.46899, accuracy: 0.78242\n",
            "Epoch: 13/30, step: 282/364, loss: 0.46914, accuracy: 0.78230\n",
            "Epoch: 13/30, step: 283/364, loss: 0.46900, accuracy: 0.78235\n",
            "Epoch: 13/30, step: 284/364, loss: 0.46901, accuracy: 0.78219\n",
            "Epoch: 13/30, step: 285/364, loss: 0.46922, accuracy: 0.78207\n",
            "Epoch: 13/30, step: 286/364, loss: 0.46923, accuracy: 0.78201\n",
            "Epoch: 13/30, step: 287/364, loss: 0.46889, accuracy: 0.78223\n",
            "Epoch: 13/30, step: 288/364, loss: 0.46898, accuracy: 0.78201\n",
            "Epoch: 13/30, step: 289/364, loss: 0.46894, accuracy: 0.78217\n",
            "Epoch: 13/30, step: 290/364, loss: 0.46902, accuracy: 0.78227\n",
            "Epoch: 13/30, step: 291/364, loss: 0.46872, accuracy: 0.78243\n",
            "Epoch: 13/30, train loss: 0.46872, train accuracy: 0.78243, valid loss: 0.60027, valid accuracy: 0.68938\n",
            "Epoch: 14/30, step: 1/364, loss: 0.53293, accuracy: 0.71875\n",
            "Epoch: 14/30, step: 2/364, loss: 0.46732, accuracy: 0.79688\n",
            "Epoch: 14/30, step: 3/364, loss: 0.42897, accuracy: 0.83854\n",
            "Epoch: 14/30, step: 4/364, loss: 0.43600, accuracy: 0.82812\n",
            "Epoch: 14/30, step: 5/364, loss: 0.43350, accuracy: 0.83438\n",
            "Epoch: 14/30, step: 6/364, loss: 0.42500, accuracy: 0.84115\n",
            "Epoch: 14/30, step: 7/364, loss: 0.42440, accuracy: 0.84821\n",
            "Epoch: 14/30, step: 8/364, loss: 0.42538, accuracy: 0.83984\n",
            "Epoch: 14/30, step: 9/364, loss: 0.44218, accuracy: 0.82292\n",
            "Epoch: 14/30, step: 10/364, loss: 0.44093, accuracy: 0.82031\n",
            "Epoch: 14/30, step: 11/364, loss: 0.43862, accuracy: 0.82244\n",
            "Epoch: 14/30, step: 12/364, loss: 0.43557, accuracy: 0.82161\n",
            "Epoch: 14/30, step: 13/364, loss: 0.43856, accuracy: 0.81731\n",
            "Epoch: 14/30, step: 14/364, loss: 0.44126, accuracy: 0.81362\n",
            "Epoch: 14/30, step: 15/364, loss: 0.44292, accuracy: 0.81250\n",
            "Epoch: 14/30, step: 16/364, loss: 0.44324, accuracy: 0.81152\n",
            "Epoch: 14/30, step: 17/364, loss: 0.44171, accuracy: 0.80790\n",
            "Epoch: 14/30, step: 18/364, loss: 0.44044, accuracy: 0.80816\n",
            "Epoch: 14/30, step: 19/364, loss: 0.43923, accuracy: 0.81086\n",
            "Epoch: 14/30, step: 20/364, loss: 0.43892, accuracy: 0.80937\n",
            "Epoch: 14/30, step: 21/364, loss: 0.44592, accuracy: 0.80283\n",
            "Epoch: 14/30, step: 22/364, loss: 0.44716, accuracy: 0.80114\n",
            "Epoch: 14/30, step: 23/364, loss: 0.44808, accuracy: 0.80095\n",
            "Epoch: 14/30, step: 24/364, loss: 0.44694, accuracy: 0.80339\n",
            "Epoch: 14/30, step: 25/364, loss: 0.44665, accuracy: 0.80313\n",
            "Epoch: 14/30, step: 26/364, loss: 0.44584, accuracy: 0.80469\n",
            "Epoch: 14/30, step: 27/364, loss: 0.44854, accuracy: 0.80208\n",
            "Epoch: 14/30, step: 28/364, loss: 0.44841, accuracy: 0.80190\n",
            "Epoch: 14/30, step: 29/364, loss: 0.45003, accuracy: 0.80119\n",
            "Epoch: 14/30, step: 30/364, loss: 0.44970, accuracy: 0.80260\n",
            "Epoch: 14/30, step: 31/364, loss: 0.45126, accuracy: 0.80343\n",
            "Epoch: 14/30, step: 32/364, loss: 0.45029, accuracy: 0.80469\n",
            "Epoch: 14/30, step: 33/364, loss: 0.45174, accuracy: 0.80398\n",
            "Epoch: 14/30, step: 34/364, loss: 0.45255, accuracy: 0.80285\n",
            "Epoch: 14/30, step: 35/364, loss: 0.45454, accuracy: 0.80134\n",
            "Epoch: 14/30, step: 36/364, loss: 0.45515, accuracy: 0.79905\n",
            "Epoch: 14/30, step: 37/364, loss: 0.45523, accuracy: 0.79899\n",
            "Epoch: 14/30, step: 38/364, loss: 0.45563, accuracy: 0.79852\n",
            "Epoch: 14/30, step: 39/364, loss: 0.45557, accuracy: 0.79808\n",
            "Epoch: 14/30, step: 40/364, loss: 0.45526, accuracy: 0.79805\n",
            "Epoch: 14/30, step: 41/364, loss: 0.45621, accuracy: 0.79688\n",
            "Epoch: 14/30, step: 42/364, loss: 0.45686, accuracy: 0.79688\n",
            "Epoch: 14/30, step: 43/364, loss: 0.45483, accuracy: 0.79797\n",
            "Epoch: 14/30, step: 44/364, loss: 0.45523, accuracy: 0.79723\n",
            "Epoch: 14/30, step: 45/364, loss: 0.45574, accuracy: 0.79688\n",
            "Epoch: 14/30, step: 46/364, loss: 0.45496, accuracy: 0.79654\n",
            "Epoch: 14/30, step: 47/364, loss: 0.45385, accuracy: 0.79820\n",
            "Epoch: 14/30, step: 48/364, loss: 0.45300, accuracy: 0.79948\n",
            "Epoch: 14/30, step: 49/364, loss: 0.45065, accuracy: 0.80166\n",
            "Epoch: 14/30, step: 50/364, loss: 0.45020, accuracy: 0.80125\n",
            "Epoch: 14/30, step: 51/364, loss: 0.45042, accuracy: 0.80086\n",
            "Epoch: 14/30, step: 52/364, loss: 0.45069, accuracy: 0.80108\n",
            "Epoch: 14/30, step: 53/364, loss: 0.45070, accuracy: 0.80041\n",
            "Epoch: 14/30, step: 54/364, loss: 0.45280, accuracy: 0.79774\n",
            "Epoch: 14/30, step: 55/364, loss: 0.45408, accuracy: 0.79659\n",
            "Epoch: 14/30, step: 56/364, loss: 0.45403, accuracy: 0.79715\n",
            "Epoch: 14/30, step: 57/364, loss: 0.45324, accuracy: 0.79797\n",
            "Epoch: 14/30, step: 58/364, loss: 0.45199, accuracy: 0.79930\n",
            "Epoch: 14/30, step: 59/364, loss: 0.45401, accuracy: 0.79820\n",
            "Epoch: 14/30, step: 60/364, loss: 0.45437, accuracy: 0.79661\n",
            "Epoch: 14/30, step: 61/364, loss: 0.45510, accuracy: 0.79611\n",
            "Epoch: 14/30, step: 62/364, loss: 0.45596, accuracy: 0.79536\n",
            "Epoch: 14/30, step: 63/364, loss: 0.45545, accuracy: 0.79613\n",
            "Epoch: 14/30, step: 64/364, loss: 0.45638, accuracy: 0.79492\n",
            "Epoch: 14/30, step: 65/364, loss: 0.45697, accuracy: 0.79495\n",
            "Epoch: 14/30, step: 66/364, loss: 0.45774, accuracy: 0.79522\n",
            "Epoch: 14/30, step: 67/364, loss: 0.45709, accuracy: 0.79548\n",
            "Epoch: 14/30, step: 68/364, loss: 0.45733, accuracy: 0.79504\n",
            "Epoch: 14/30, step: 69/364, loss: 0.45777, accuracy: 0.79416\n",
            "Epoch: 14/30, step: 70/364, loss: 0.45781, accuracy: 0.79420\n",
            "Epoch: 14/30, step: 71/364, loss: 0.45717, accuracy: 0.79489\n",
            "Epoch: 14/30, step: 72/364, loss: 0.45705, accuracy: 0.79449\n",
            "Epoch: 14/30, step: 73/364, loss: 0.45703, accuracy: 0.79473\n",
            "Epoch: 14/30, step: 74/364, loss: 0.45684, accuracy: 0.79497\n",
            "Epoch: 14/30, step: 75/364, loss: 0.45670, accuracy: 0.79521\n",
            "Epoch: 14/30, step: 76/364, loss: 0.45532, accuracy: 0.79626\n",
            "Epoch: 14/30, step: 77/364, loss: 0.45590, accuracy: 0.79606\n",
            "Epoch: 14/30, step: 78/364, loss: 0.45735, accuracy: 0.79487\n",
            "Epoch: 14/30, step: 79/364, loss: 0.45733, accuracy: 0.79470\n",
            "Epoch: 14/30, step: 80/364, loss: 0.45728, accuracy: 0.79473\n",
            "Epoch: 14/30, step: 81/364, loss: 0.45603, accuracy: 0.79572\n",
            "Epoch: 14/30, step: 82/364, loss: 0.45553, accuracy: 0.79611\n",
            "Epoch: 14/30, step: 83/364, loss: 0.45434, accuracy: 0.79763\n",
            "Epoch: 14/30, step: 84/364, loss: 0.45431, accuracy: 0.79725\n",
            "Epoch: 14/30, step: 85/364, loss: 0.45367, accuracy: 0.79798\n",
            "Epoch: 14/30, step: 86/364, loss: 0.45381, accuracy: 0.79778\n",
            "Epoch: 14/30, step: 87/364, loss: 0.45461, accuracy: 0.79705\n",
            "Epoch: 14/30, step: 88/364, loss: 0.45531, accuracy: 0.79616\n",
            "Epoch: 14/30, step: 89/364, loss: 0.45526, accuracy: 0.79617\n",
            "Epoch: 14/30, step: 90/364, loss: 0.45638, accuracy: 0.79514\n",
            "Epoch: 14/30, step: 91/364, loss: 0.45670, accuracy: 0.79430\n",
            "Epoch: 14/30, step: 92/364, loss: 0.45605, accuracy: 0.79450\n",
            "Epoch: 14/30, step: 93/364, loss: 0.45668, accuracy: 0.79385\n",
            "Epoch: 14/30, step: 94/364, loss: 0.45705, accuracy: 0.79372\n",
            "Epoch: 14/30, step: 95/364, loss: 0.45649, accuracy: 0.79424\n",
            "Epoch: 14/30, step: 96/364, loss: 0.45627, accuracy: 0.79508\n",
            "Epoch: 14/30, step: 97/364, loss: 0.45697, accuracy: 0.79414\n",
            "Epoch: 14/30, step: 98/364, loss: 0.45680, accuracy: 0.79432\n",
            "Epoch: 14/30, step: 99/364, loss: 0.45622, accuracy: 0.79498\n",
            "Epoch: 14/30, step: 100/364, loss: 0.45551, accuracy: 0.79562\n",
            "Epoch: 14/30, step: 101/364, loss: 0.45637, accuracy: 0.79486\n",
            "Epoch: 14/30, step: 102/364, loss: 0.45616, accuracy: 0.79488\n",
            "Epoch: 14/30, step: 103/364, loss: 0.45607, accuracy: 0.79460\n",
            "Epoch: 14/30, step: 104/364, loss: 0.45587, accuracy: 0.79447\n",
            "Epoch: 14/30, step: 105/364, loss: 0.45670, accuracy: 0.79390\n",
            "Epoch: 14/30, step: 106/364, loss: 0.45705, accuracy: 0.79334\n",
            "Epoch: 14/30, step: 107/364, loss: 0.45728, accuracy: 0.79279\n",
            "Epoch: 14/30, step: 108/364, loss: 0.45696, accuracy: 0.79297\n",
            "Epoch: 14/30, step: 109/364, loss: 0.45649, accuracy: 0.79358\n",
            "Epoch: 14/30, step: 110/364, loss: 0.45566, accuracy: 0.79418\n",
            "Epoch: 14/30, step: 111/364, loss: 0.45541, accuracy: 0.79462\n",
            "Epoch: 14/30, step: 112/364, loss: 0.45522, accuracy: 0.79506\n",
            "Epoch: 14/30, step: 113/364, loss: 0.45562, accuracy: 0.79452\n",
            "Epoch: 14/30, step: 114/364, loss: 0.45485, accuracy: 0.79509\n",
            "Epoch: 14/30, step: 115/364, loss: 0.45480, accuracy: 0.79511\n",
            "Epoch: 14/30, step: 116/364, loss: 0.45439, accuracy: 0.79553\n",
            "Epoch: 14/30, step: 117/364, loss: 0.45439, accuracy: 0.79581\n",
            "Epoch: 14/30, step: 118/364, loss: 0.45402, accuracy: 0.79621\n",
            "Epoch: 14/30, step: 119/364, loss: 0.45329, accuracy: 0.79661\n",
            "Epoch: 14/30, step: 120/364, loss: 0.45337, accuracy: 0.79648\n",
            "Epoch: 14/30, step: 121/364, loss: 0.45327, accuracy: 0.79623\n",
            "Epoch: 14/30, step: 122/364, loss: 0.45253, accuracy: 0.79700\n",
            "Epoch: 14/30, step: 123/364, loss: 0.45239, accuracy: 0.79688\n",
            "Epoch: 14/30, step: 124/364, loss: 0.45191, accuracy: 0.79688\n",
            "Epoch: 14/30, step: 125/364, loss: 0.45158, accuracy: 0.79712\n",
            "Epoch: 14/30, step: 126/364, loss: 0.45249, accuracy: 0.79650\n",
            "Epoch: 14/30, step: 127/364, loss: 0.45224, accuracy: 0.79663\n",
            "Epoch: 14/30, step: 128/364, loss: 0.45294, accuracy: 0.79590\n",
            "Epoch: 14/30, step: 129/364, loss: 0.45363, accuracy: 0.79494\n",
            "Epoch: 14/30, step: 130/364, loss: 0.45308, accuracy: 0.79507\n",
            "Epoch: 14/30, step: 131/364, loss: 0.45278, accuracy: 0.79532\n",
            "Epoch: 14/30, step: 132/364, loss: 0.45319, accuracy: 0.79439\n",
            "Epoch: 14/30, step: 133/364, loss: 0.45301, accuracy: 0.79511\n",
            "Epoch: 14/30, step: 134/364, loss: 0.45266, accuracy: 0.79466\n",
            "Epoch: 14/30, step: 135/364, loss: 0.45294, accuracy: 0.79433\n",
            "Epoch: 14/30, step: 136/364, loss: 0.45310, accuracy: 0.79423\n",
            "Epoch: 14/30, step: 137/364, loss: 0.45276, accuracy: 0.79459\n",
            "Epoch: 14/30, step: 138/364, loss: 0.45244, accuracy: 0.79495\n",
            "Epoch: 14/30, step: 139/364, loss: 0.45267, accuracy: 0.79429\n",
            "Epoch: 14/30, step: 140/364, loss: 0.45245, accuracy: 0.79453\n",
            "Epoch: 14/30, step: 141/364, loss: 0.45248, accuracy: 0.79388\n",
            "Epoch: 14/30, step: 142/364, loss: 0.45276, accuracy: 0.79357\n",
            "Epoch: 14/30, step: 143/364, loss: 0.45240, accuracy: 0.79382\n",
            "Epoch: 14/30, step: 144/364, loss: 0.45252, accuracy: 0.79373\n",
            "Epoch: 14/30, step: 145/364, loss: 0.45297, accuracy: 0.79353\n",
            "Epoch: 14/30, step: 146/364, loss: 0.45491, accuracy: 0.79217\n",
            "Epoch: 14/30, step: 147/364, loss: 0.45494, accuracy: 0.79199\n",
            "Epoch: 14/30, step: 148/364, loss: 0.45503, accuracy: 0.79181\n",
            "Epoch: 14/30, step: 149/364, loss: 0.45500, accuracy: 0.79195\n",
            "Epoch: 14/30, step: 150/364, loss: 0.45489, accuracy: 0.79198\n",
            "Epoch: 14/30, step: 151/364, loss: 0.45491, accuracy: 0.79191\n",
            "Epoch: 14/30, step: 152/364, loss: 0.45473, accuracy: 0.79215\n",
            "Epoch: 14/30, step: 153/364, loss: 0.45500, accuracy: 0.79187\n",
            "Epoch: 14/30, step: 154/364, loss: 0.45541, accuracy: 0.79099\n",
            "Epoch: 14/30, step: 155/364, loss: 0.45501, accuracy: 0.79143\n",
            "Epoch: 14/30, step: 156/364, loss: 0.45502, accuracy: 0.79147\n",
            "Epoch: 14/30, step: 157/364, loss: 0.45550, accuracy: 0.79130\n",
            "Epoch: 14/30, step: 158/364, loss: 0.45539, accuracy: 0.79134\n",
            "Epoch: 14/30, step: 159/364, loss: 0.45512, accuracy: 0.79157\n",
            "Epoch: 14/30, step: 160/364, loss: 0.45489, accuracy: 0.79180\n",
            "Epoch: 14/30, step: 161/364, loss: 0.45516, accuracy: 0.79163\n",
            "Epoch: 14/30, step: 162/364, loss: 0.45441, accuracy: 0.79244\n",
            "Epoch: 14/30, step: 163/364, loss: 0.45431, accuracy: 0.79256\n",
            "Epoch: 14/30, step: 164/364, loss: 0.45455, accuracy: 0.79221\n",
            "Epoch: 14/30, step: 165/364, loss: 0.45522, accuracy: 0.79138\n",
            "Epoch: 14/30, step: 166/364, loss: 0.45482, accuracy: 0.79170\n",
            "Epoch: 14/30, step: 167/364, loss: 0.45472, accuracy: 0.79192\n",
            "Epoch: 14/30, step: 168/364, loss: 0.45507, accuracy: 0.79139\n",
            "Epoch: 14/30, step: 169/364, loss: 0.45554, accuracy: 0.79087\n",
            "Epoch: 14/30, step: 170/364, loss: 0.45547, accuracy: 0.79108\n",
            "Epoch: 14/30, step: 171/364, loss: 0.45593, accuracy: 0.79084\n",
            "Epoch: 14/30, step: 172/364, loss: 0.45564, accuracy: 0.79124\n",
            "Epoch: 14/30, step: 173/364, loss: 0.45529, accuracy: 0.79155\n",
            "Epoch: 14/30, step: 174/364, loss: 0.45529, accuracy: 0.79167\n",
            "Epoch: 14/30, step: 175/364, loss: 0.45511, accuracy: 0.79179\n",
            "Epoch: 14/30, step: 176/364, loss: 0.45491, accuracy: 0.79199\n",
            "Epoch: 14/30, step: 177/364, loss: 0.45435, accuracy: 0.79246\n",
            "Epoch: 14/30, step: 178/364, loss: 0.45411, accuracy: 0.79301\n",
            "Epoch: 14/30, step: 179/364, loss: 0.45436, accuracy: 0.79303\n",
            "Epoch: 14/30, step: 180/364, loss: 0.45418, accuracy: 0.79323\n",
            "Epoch: 14/30, step: 181/364, loss: 0.45416, accuracy: 0.79308\n",
            "Epoch: 14/30, step: 182/364, loss: 0.45378, accuracy: 0.79353\n",
            "Epoch: 14/30, step: 183/364, loss: 0.45409, accuracy: 0.79303\n",
            "Epoch: 14/30, step: 184/364, loss: 0.45368, accuracy: 0.79322\n",
            "Epoch: 14/30, step: 185/364, loss: 0.45394, accuracy: 0.79291\n",
            "Epoch: 14/30, step: 186/364, loss: 0.45422, accuracy: 0.79267\n",
            "Epoch: 14/30, step: 187/364, loss: 0.45461, accuracy: 0.79220\n",
            "Epoch: 14/30, step: 188/364, loss: 0.45465, accuracy: 0.79230\n",
            "Epoch: 14/30, step: 189/364, loss: 0.45481, accuracy: 0.79208\n",
            "Epoch: 14/30, step: 190/364, loss: 0.45451, accuracy: 0.79219\n",
            "Epoch: 14/30, step: 191/364, loss: 0.45408, accuracy: 0.79238\n",
            "Epoch: 14/30, step: 192/364, loss: 0.45437, accuracy: 0.79224\n",
            "Epoch: 14/30, step: 193/364, loss: 0.45427, accuracy: 0.79242\n",
            "Epoch: 14/30, step: 194/364, loss: 0.45390, accuracy: 0.79293\n",
            "Epoch: 14/30, step: 195/364, loss: 0.45389, accuracy: 0.79303\n",
            "Epoch: 14/30, step: 196/364, loss: 0.45381, accuracy: 0.79337\n",
            "Epoch: 14/30, step: 197/364, loss: 0.45384, accuracy: 0.79354\n",
            "Epoch: 14/30, step: 198/364, loss: 0.45366, accuracy: 0.79388\n",
            "Epoch: 14/30, step: 199/364, loss: 0.45353, accuracy: 0.79413\n",
            "Epoch: 14/30, step: 200/364, loss: 0.45306, accuracy: 0.79445\n",
            "Epoch: 14/30, step: 201/364, loss: 0.45251, accuracy: 0.79485\n",
            "Epoch: 14/30, step: 202/364, loss: 0.45277, accuracy: 0.79455\n",
            "Epoch: 14/30, step: 203/364, loss: 0.45242, accuracy: 0.79495\n",
            "Epoch: 14/30, step: 204/364, loss: 0.45240, accuracy: 0.79488\n",
            "Epoch: 14/30, step: 205/364, loss: 0.45241, accuracy: 0.79520\n",
            "Epoch: 14/30, step: 206/364, loss: 0.45232, accuracy: 0.79505\n",
            "Epoch: 14/30, step: 207/364, loss: 0.45227, accuracy: 0.79514\n",
            "Epoch: 14/30, step: 208/364, loss: 0.45247, accuracy: 0.79462\n",
            "Epoch: 14/30, step: 209/364, loss: 0.45273, accuracy: 0.79441\n",
            "Epoch: 14/30, step: 210/364, loss: 0.45220, accuracy: 0.79472\n",
            "Epoch: 14/30, step: 211/364, loss: 0.45212, accuracy: 0.79480\n",
            "Epoch: 14/30, step: 212/364, loss: 0.45186, accuracy: 0.79503\n",
            "Epoch: 14/30, step: 213/364, loss: 0.45176, accuracy: 0.79533\n",
            "Epoch: 14/30, step: 214/364, loss: 0.45178, accuracy: 0.79534\n",
            "Epoch: 14/30, step: 215/364, loss: 0.45196, accuracy: 0.79506\n",
            "Epoch: 14/30, step: 216/364, loss: 0.45193, accuracy: 0.79514\n",
            "Epoch: 14/30, step: 217/364, loss: 0.45204, accuracy: 0.79529\n",
            "Epoch: 14/30, step: 218/364, loss: 0.45220, accuracy: 0.79501\n",
            "Epoch: 14/30, step: 219/364, loss: 0.45206, accuracy: 0.79523\n",
            "Epoch: 14/30, step: 220/364, loss: 0.45178, accuracy: 0.79553\n",
            "Epoch: 14/30, step: 221/364, loss: 0.45138, accuracy: 0.79574\n",
            "Epoch: 14/30, step: 222/364, loss: 0.45188, accuracy: 0.79533\n",
            "Epoch: 14/30, step: 223/364, loss: 0.45220, accuracy: 0.79484\n",
            "Epoch: 14/30, step: 224/364, loss: 0.45187, accuracy: 0.79513\n",
            "Epoch: 14/30, step: 225/364, loss: 0.45188, accuracy: 0.79507\n",
            "Epoch: 14/30, step: 226/364, loss: 0.45156, accuracy: 0.79528\n",
            "Epoch: 14/30, step: 227/364, loss: 0.45164, accuracy: 0.79522\n",
            "Epoch: 14/30, step: 228/364, loss: 0.45148, accuracy: 0.79523\n",
            "Epoch: 14/30, step: 229/364, loss: 0.45173, accuracy: 0.79483\n",
            "Epoch: 14/30, step: 230/364, loss: 0.45203, accuracy: 0.79450\n",
            "Epoch: 14/30, step: 231/364, loss: 0.45233, accuracy: 0.79444\n",
            "Epoch: 14/30, step: 232/364, loss: 0.45257, accuracy: 0.79432\n",
            "Epoch: 14/30, step: 233/364, loss: 0.45258, accuracy: 0.79433\n",
            "Epoch: 14/30, step: 234/364, loss: 0.45302, accuracy: 0.79380\n",
            "Epoch: 14/30, step: 235/364, loss: 0.45341, accuracy: 0.79362\n",
            "Epoch: 14/30, step: 236/364, loss: 0.45335, accuracy: 0.79370\n",
            "Epoch: 14/30, step: 237/364, loss: 0.45319, accuracy: 0.79391\n",
            "Epoch: 14/30, step: 238/364, loss: 0.45295, accuracy: 0.79399\n",
            "Epoch: 14/30, step: 239/364, loss: 0.45280, accuracy: 0.79406\n",
            "Epoch: 14/30, step: 240/364, loss: 0.45284, accuracy: 0.79395\n",
            "Epoch: 14/30, step: 241/364, loss: 0.45252, accuracy: 0.79409\n",
            "Epoch: 14/30, step: 242/364, loss: 0.45284, accuracy: 0.79384\n",
            "Epoch: 14/30, step: 243/364, loss: 0.45309, accuracy: 0.79366\n",
            "Epoch: 14/30, step: 244/364, loss: 0.45286, accuracy: 0.79393\n",
            "Epoch: 14/30, step: 245/364, loss: 0.45275, accuracy: 0.79388\n",
            "Epoch: 14/30, step: 246/364, loss: 0.45273, accuracy: 0.79408\n",
            "Epoch: 14/30, step: 247/364, loss: 0.45263, accuracy: 0.79428\n",
            "Epoch: 14/30, step: 248/364, loss: 0.45261, accuracy: 0.79429\n",
            "Epoch: 14/30, step: 249/364, loss: 0.45266, accuracy: 0.79411\n",
            "Epoch: 14/30, step: 250/364, loss: 0.45276, accuracy: 0.79406\n",
            "Epoch: 14/30, step: 251/364, loss: 0.45272, accuracy: 0.79426\n",
            "Epoch: 14/30, step: 252/364, loss: 0.45264, accuracy: 0.79408\n",
            "Epoch: 14/30, step: 253/364, loss: 0.45256, accuracy: 0.79403\n",
            "Epoch: 14/30, step: 254/364, loss: 0.45296, accuracy: 0.79349\n",
            "Epoch: 14/30, step: 255/364, loss: 0.45278, accuracy: 0.79369\n",
            "Epoch: 14/30, step: 256/364, loss: 0.45269, accuracy: 0.79376\n",
            "Epoch: 14/30, step: 257/364, loss: 0.45255, accuracy: 0.79371\n",
            "Epoch: 14/30, step: 258/364, loss: 0.45252, accuracy: 0.79379\n",
            "Epoch: 14/30, step: 259/364, loss: 0.45248, accuracy: 0.79374\n",
            "Epoch: 14/30, step: 260/364, loss: 0.45251, accuracy: 0.79375\n",
            "Epoch: 14/30, step: 261/364, loss: 0.45285, accuracy: 0.79352\n",
            "Epoch: 14/30, step: 262/364, loss: 0.45290, accuracy: 0.79336\n",
            "Epoch: 14/30, step: 263/364, loss: 0.45271, accuracy: 0.79361\n",
            "Epoch: 14/30, step: 264/364, loss: 0.45226, accuracy: 0.79403\n",
            "Epoch: 14/30, step: 265/364, loss: 0.45226, accuracy: 0.79387\n",
            "Epoch: 14/30, step: 266/364, loss: 0.45230, accuracy: 0.79388\n",
            "Epoch: 14/30, step: 267/364, loss: 0.45207, accuracy: 0.79407\n",
            "Epoch: 14/30, step: 268/364, loss: 0.45235, accuracy: 0.79390\n",
            "Epoch: 14/30, step: 269/364, loss: 0.45225, accuracy: 0.79397\n",
            "Epoch: 14/30, step: 270/364, loss: 0.45191, accuracy: 0.79439\n",
            "Epoch: 14/30, step: 271/364, loss: 0.45213, accuracy: 0.79417\n",
            "Epoch: 14/30, step: 272/364, loss: 0.45190, accuracy: 0.79418\n",
            "Epoch: 14/30, step: 273/364, loss: 0.45163, accuracy: 0.79436\n",
            "Epoch: 14/30, step: 274/364, loss: 0.45160, accuracy: 0.79437\n",
            "Epoch: 14/30, step: 275/364, loss: 0.45171, accuracy: 0.79432\n",
            "Epoch: 14/30, step: 276/364, loss: 0.45163, accuracy: 0.79427\n",
            "Epoch: 14/30, step: 277/364, loss: 0.45180, accuracy: 0.79428\n",
            "Epoch: 14/30, step: 278/364, loss: 0.45191, accuracy: 0.79412\n",
            "Epoch: 14/30, step: 279/364, loss: 0.45184, accuracy: 0.79424\n",
            "Epoch: 14/30, step: 280/364, loss: 0.45178, accuracy: 0.79425\n",
            "Epoch: 14/30, step: 281/364, loss: 0.45152, accuracy: 0.79443\n",
            "Epoch: 14/30, step: 282/364, loss: 0.45171, accuracy: 0.79438\n",
            "Epoch: 14/30, step: 283/364, loss: 0.45187, accuracy: 0.79411\n",
            "Epoch: 14/30, step: 284/364, loss: 0.45185, accuracy: 0.79401\n",
            "Epoch: 14/30, step: 285/364, loss: 0.45176, accuracy: 0.79408\n",
            "Epoch: 14/30, step: 286/364, loss: 0.45195, accuracy: 0.79387\n",
            "Epoch: 14/30, step: 287/364, loss: 0.45201, accuracy: 0.79383\n",
            "Epoch: 14/30, step: 288/364, loss: 0.45167, accuracy: 0.79400\n",
            "Epoch: 14/30, step: 289/364, loss: 0.45157, accuracy: 0.79433\n",
            "Epoch: 14/30, step: 290/364, loss: 0.45127, accuracy: 0.79450\n",
            "Epoch: 14/30, step: 291/364, loss: 0.45122, accuracy: 0.79463\n",
            "Epoch: 14/30, train loss: 0.45122, train accuracy: 0.79463, valid loss: 0.61290, valid accuracy: 0.68229\n",
            "Epoch: 15/30, step: 1/364, loss: 0.43778, accuracy: 0.81250\n",
            "Epoch: 15/30, step: 2/364, loss: 0.39049, accuracy: 0.87500\n",
            "Epoch: 15/30, step: 3/364, loss: 0.39532, accuracy: 0.87500\n",
            "Epoch: 15/30, step: 4/364, loss: 0.41293, accuracy: 0.85938\n",
            "Epoch: 15/30, step: 5/364, loss: 0.42673, accuracy: 0.84688\n",
            "Epoch: 15/30, step: 6/364, loss: 0.42123, accuracy: 0.84635\n",
            "Epoch: 15/30, step: 7/364, loss: 0.42469, accuracy: 0.83705\n",
            "Epoch: 15/30, step: 8/364, loss: 0.43062, accuracy: 0.82422\n",
            "Epoch: 15/30, step: 9/364, loss: 0.43338, accuracy: 0.82292\n",
            "Epoch: 15/30, step: 10/364, loss: 0.43376, accuracy: 0.82187\n",
            "Epoch: 15/30, step: 11/364, loss: 0.43297, accuracy: 0.82244\n",
            "Epoch: 15/30, step: 12/364, loss: 0.43296, accuracy: 0.81901\n",
            "Epoch: 15/30, step: 13/364, loss: 0.43259, accuracy: 0.81611\n",
            "Epoch: 15/30, step: 14/364, loss: 0.43304, accuracy: 0.81250\n",
            "Epoch: 15/30, step: 15/364, loss: 0.43544, accuracy: 0.80937\n",
            "Epoch: 15/30, step: 16/364, loss: 0.44119, accuracy: 0.80469\n",
            "Epoch: 15/30, step: 17/364, loss: 0.43981, accuracy: 0.80699\n",
            "Epoch: 15/30, step: 18/364, loss: 0.43345, accuracy: 0.81510\n",
            "Epoch: 15/30, step: 19/364, loss: 0.43517, accuracy: 0.81332\n",
            "Epoch: 15/30, step: 20/364, loss: 0.43832, accuracy: 0.81094\n",
            "Epoch: 15/30, step: 21/364, loss: 0.43774, accuracy: 0.81027\n",
            "Epoch: 15/30, step: 22/364, loss: 0.43686, accuracy: 0.81037\n",
            "Epoch: 15/30, step: 23/364, loss: 0.43512, accuracy: 0.81250\n",
            "Epoch: 15/30, step: 24/364, loss: 0.43498, accuracy: 0.81185\n",
            "Epoch: 15/30, step: 25/364, loss: 0.43563, accuracy: 0.81063\n",
            "Epoch: 15/30, step: 26/364, loss: 0.43220, accuracy: 0.81250\n",
            "Epoch: 15/30, step: 27/364, loss: 0.43228, accuracy: 0.81250\n",
            "Epoch: 15/30, step: 28/364, loss: 0.43177, accuracy: 0.81306\n",
            "Epoch: 15/30, step: 29/364, loss: 0.43176, accuracy: 0.81412\n",
            "Epoch: 15/30, step: 30/364, loss: 0.42994, accuracy: 0.81563\n",
            "Epoch: 15/30, step: 31/364, loss: 0.42923, accuracy: 0.81552\n",
            "Epoch: 15/30, step: 32/364, loss: 0.42950, accuracy: 0.81494\n",
            "Epoch: 15/30, step: 33/364, loss: 0.42975, accuracy: 0.81534\n",
            "Epoch: 15/30, step: 34/364, loss: 0.43189, accuracy: 0.81342\n",
            "Epoch: 15/30, step: 35/364, loss: 0.43187, accuracy: 0.81339\n",
            "Epoch: 15/30, step: 36/364, loss: 0.43330, accuracy: 0.81163\n",
            "Epoch: 15/30, step: 37/364, loss: 0.43415, accuracy: 0.81039\n",
            "Epoch: 15/30, step: 38/364, loss: 0.43675, accuracy: 0.80880\n",
            "Epoch: 15/30, step: 39/364, loss: 0.43653, accuracy: 0.80929\n",
            "Epoch: 15/30, step: 40/364, loss: 0.43534, accuracy: 0.81094\n",
            "Epoch: 15/30, step: 41/364, loss: 0.43757, accuracy: 0.81021\n",
            "Epoch: 15/30, step: 42/364, loss: 0.43530, accuracy: 0.81101\n",
            "Epoch: 15/30, step: 43/364, loss: 0.43514, accuracy: 0.81141\n",
            "Epoch: 15/30, step: 44/364, loss: 0.43386, accuracy: 0.81286\n",
            "Epoch: 15/30, step: 45/364, loss: 0.43339, accuracy: 0.81285\n",
            "Epoch: 15/30, step: 46/364, loss: 0.43131, accuracy: 0.81454\n",
            "Epoch: 15/30, step: 47/364, loss: 0.43172, accuracy: 0.81449\n",
            "Epoch: 15/30, step: 48/364, loss: 0.43192, accuracy: 0.81543\n",
            "Epoch: 15/30, step: 49/364, loss: 0.43083, accuracy: 0.81537\n",
            "Epoch: 15/30, step: 50/364, loss: 0.43174, accuracy: 0.81500\n",
            "Epoch: 15/30, step: 51/364, loss: 0.43224, accuracy: 0.81526\n",
            "Epoch: 15/30, step: 52/364, loss: 0.43298, accuracy: 0.81460\n",
            "Epoch: 15/30, step: 53/364, loss: 0.43274, accuracy: 0.81427\n",
            "Epoch: 15/30, step: 54/364, loss: 0.43281, accuracy: 0.81279\n",
            "Epoch: 15/30, step: 55/364, loss: 0.43199, accuracy: 0.81364\n",
            "Epoch: 15/30, step: 56/364, loss: 0.43182, accuracy: 0.81362\n",
            "Epoch: 15/30, step: 57/364, loss: 0.43136, accuracy: 0.81360\n",
            "Epoch: 15/30, step: 58/364, loss: 0.43118, accuracy: 0.81331\n",
            "Epoch: 15/30, step: 59/364, loss: 0.43176, accuracy: 0.81356\n",
            "Epoch: 15/30, step: 60/364, loss: 0.43275, accuracy: 0.81328\n",
            "Epoch: 15/30, step: 61/364, loss: 0.43236, accuracy: 0.81327\n",
            "Epoch: 15/30, step: 62/364, loss: 0.43268, accuracy: 0.81149\n",
            "Epoch: 15/30, step: 63/364, loss: 0.43180, accuracy: 0.81250\n",
            "Epoch: 15/30, step: 64/364, loss: 0.43246, accuracy: 0.81152\n",
            "Epoch: 15/30, step: 65/364, loss: 0.43176, accuracy: 0.81202\n",
            "Epoch: 15/30, step: 66/364, loss: 0.43195, accuracy: 0.81179\n",
            "Epoch: 15/30, step: 67/364, loss: 0.43148, accuracy: 0.81227\n",
            "Epoch: 15/30, step: 68/364, loss: 0.43091, accuracy: 0.81273\n",
            "Epoch: 15/30, step: 69/364, loss: 0.43081, accuracy: 0.81295\n",
            "Epoch: 15/30, step: 70/364, loss: 0.43079, accuracy: 0.81295\n",
            "Epoch: 15/30, step: 71/364, loss: 0.43039, accuracy: 0.81316\n",
            "Epoch: 15/30, step: 72/364, loss: 0.43005, accuracy: 0.81359\n",
            "Epoch: 15/30, step: 73/364, loss: 0.42967, accuracy: 0.81357\n",
            "Epoch: 15/30, step: 74/364, loss: 0.42911, accuracy: 0.81398\n",
            "Epoch: 15/30, step: 75/364, loss: 0.42870, accuracy: 0.81500\n",
            "Epoch: 15/30, step: 76/364, loss: 0.42818, accuracy: 0.81558\n",
            "Epoch: 15/30, step: 77/364, loss: 0.42895, accuracy: 0.81453\n",
            "Epoch: 15/30, step: 78/364, loss: 0.42873, accuracy: 0.81450\n",
            "Epoch: 15/30, step: 79/364, loss: 0.42915, accuracy: 0.81428\n",
            "Epoch: 15/30, step: 80/364, loss: 0.42827, accuracy: 0.81523\n",
            "Epoch: 15/30, step: 81/364, loss: 0.42888, accuracy: 0.81424\n",
            "Epoch: 15/30, step: 82/364, loss: 0.42926, accuracy: 0.81402\n",
            "Epoch: 15/30, step: 83/364, loss: 0.42932, accuracy: 0.81363\n",
            "Epoch: 15/30, step: 84/364, loss: 0.42980, accuracy: 0.81287\n",
            "Epoch: 15/30, step: 85/364, loss: 0.42940, accuracy: 0.81287\n",
            "Epoch: 15/30, step: 86/364, loss: 0.42988, accuracy: 0.81232\n",
            "Epoch: 15/30, step: 87/364, loss: 0.42987, accuracy: 0.81196\n",
            "Epoch: 15/30, step: 88/364, loss: 0.42923, accuracy: 0.81214\n",
            "Epoch: 15/30, step: 89/364, loss: 0.42873, accuracy: 0.81250\n",
            "Epoch: 15/30, step: 90/364, loss: 0.42895, accuracy: 0.81267\n",
            "Epoch: 15/30, step: 91/364, loss: 0.42888, accuracy: 0.81302\n",
            "Epoch: 15/30, step: 92/364, loss: 0.42888, accuracy: 0.81318\n",
            "Epoch: 15/30, step: 93/364, loss: 0.42911, accuracy: 0.81317\n",
            "Epoch: 15/30, step: 94/364, loss: 0.42907, accuracy: 0.81350\n",
            "Epoch: 15/30, step: 95/364, loss: 0.42884, accuracy: 0.81332\n",
            "Epoch: 15/30, step: 96/364, loss: 0.42947, accuracy: 0.81283\n",
            "Epoch: 15/30, step: 97/364, loss: 0.42918, accuracy: 0.81250\n",
            "Epoch: 15/30, step: 98/364, loss: 0.42930, accuracy: 0.81234\n",
            "Epoch: 15/30, step: 99/364, loss: 0.42904, accuracy: 0.81203\n",
            "Epoch: 15/30, step: 100/364, loss: 0.42800, accuracy: 0.81281\n",
            "Epoch: 15/30, step: 101/364, loss: 0.42749, accuracy: 0.81296\n",
            "Epoch: 15/30, step: 102/364, loss: 0.42689, accuracy: 0.81373\n",
            "Epoch: 15/30, step: 103/364, loss: 0.42663, accuracy: 0.81387\n",
            "Epoch: 15/30, step: 104/364, loss: 0.42833, accuracy: 0.81280\n",
            "Epoch: 15/30, step: 105/364, loss: 0.42810, accuracy: 0.81280\n",
            "Epoch: 15/30, step: 106/364, loss: 0.42771, accuracy: 0.81294\n",
            "Epoch: 15/30, step: 107/364, loss: 0.42767, accuracy: 0.81279\n",
            "Epoch: 15/30, step: 108/364, loss: 0.42796, accuracy: 0.81236\n",
            "Epoch: 15/30, step: 109/364, loss: 0.42761, accuracy: 0.81221\n",
            "Epoch: 15/30, step: 110/364, loss: 0.42780, accuracy: 0.81222\n",
            "Epoch: 15/30, step: 111/364, loss: 0.42779, accuracy: 0.81250\n",
            "Epoch: 15/30, step: 112/364, loss: 0.42854, accuracy: 0.81222\n",
            "Epoch: 15/30, step: 113/364, loss: 0.42850, accuracy: 0.81222\n",
            "Epoch: 15/30, step: 114/364, loss: 0.42753, accuracy: 0.81305\n",
            "Epoch: 15/30, step: 115/364, loss: 0.42729, accuracy: 0.81318\n",
            "Epoch: 15/30, step: 116/364, loss: 0.42651, accuracy: 0.81398\n",
            "Epoch: 15/30, step: 117/364, loss: 0.42680, accuracy: 0.81384\n",
            "Epoch: 15/30, step: 118/364, loss: 0.42754, accuracy: 0.81343\n",
            "Epoch: 15/30, step: 119/364, loss: 0.42759, accuracy: 0.81316\n",
            "Epoch: 15/30, step: 120/364, loss: 0.42776, accuracy: 0.81315\n",
            "Epoch: 15/30, step: 121/364, loss: 0.42813, accuracy: 0.81327\n",
            "Epoch: 15/30, step: 122/364, loss: 0.42831, accuracy: 0.81250\n",
            "Epoch: 15/30, step: 123/364, loss: 0.42820, accuracy: 0.81275\n",
            "Epoch: 15/30, step: 124/364, loss: 0.42802, accuracy: 0.81288\n",
            "Epoch: 15/30, step: 125/364, loss: 0.42722, accuracy: 0.81325\n",
            "Epoch: 15/30, step: 126/364, loss: 0.42645, accuracy: 0.81399\n",
            "Epoch: 15/30, step: 127/364, loss: 0.42627, accuracy: 0.81410\n",
            "Epoch: 15/30, step: 128/364, loss: 0.42616, accuracy: 0.81396\n",
            "Epoch: 15/30, step: 129/364, loss: 0.42600, accuracy: 0.81456\n",
            "Epoch: 15/30, step: 130/364, loss: 0.42569, accuracy: 0.81502\n",
            "Epoch: 15/30, step: 131/364, loss: 0.42564, accuracy: 0.81489\n",
            "Epoch: 15/30, step: 132/364, loss: 0.42598, accuracy: 0.81475\n",
            "Epoch: 15/30, step: 133/364, loss: 0.42640, accuracy: 0.81426\n",
            "Epoch: 15/30, step: 134/364, loss: 0.42657, accuracy: 0.81402\n",
            "Epoch: 15/30, step: 135/364, loss: 0.42685, accuracy: 0.81366\n",
            "Epoch: 15/30, step: 136/364, loss: 0.42722, accuracy: 0.81342\n",
            "Epoch: 15/30, step: 137/364, loss: 0.42672, accuracy: 0.81375\n",
            "Epoch: 15/30, step: 138/364, loss: 0.42728, accuracy: 0.81307\n",
            "Epoch: 15/30, step: 139/364, loss: 0.42670, accuracy: 0.81340\n",
            "Epoch: 15/30, step: 140/364, loss: 0.42616, accuracy: 0.81395\n",
            "Epoch: 15/30, step: 141/364, loss: 0.42625, accuracy: 0.81350\n",
            "Epoch: 15/30, step: 142/364, loss: 0.42672, accuracy: 0.81305\n",
            "Epoch: 15/30, step: 143/364, loss: 0.42642, accuracy: 0.81359\n",
            "Epoch: 15/30, step: 144/364, loss: 0.42697, accuracy: 0.81315\n",
            "Epoch: 15/30, step: 145/364, loss: 0.42692, accuracy: 0.81325\n",
            "Epoch: 15/30, step: 146/364, loss: 0.42757, accuracy: 0.81336\n",
            "Epoch: 15/30, step: 147/364, loss: 0.42745, accuracy: 0.81346\n",
            "Epoch: 15/30, step: 148/364, loss: 0.42756, accuracy: 0.81334\n",
            "Epoch: 15/30, step: 149/364, loss: 0.42725, accuracy: 0.81376\n",
            "Epoch: 15/30, step: 150/364, loss: 0.42730, accuracy: 0.81365\n",
            "Epoch: 15/30, step: 151/364, loss: 0.42730, accuracy: 0.81353\n",
            "Epoch: 15/30, step: 152/364, loss: 0.42743, accuracy: 0.81332\n",
            "Epoch: 15/30, step: 153/364, loss: 0.42719, accuracy: 0.81332\n",
            "Epoch: 15/30, step: 154/364, loss: 0.42696, accuracy: 0.81331\n",
            "Epoch: 15/30, step: 155/364, loss: 0.42684, accuracy: 0.81341\n",
            "Epoch: 15/30, step: 156/364, loss: 0.42615, accuracy: 0.81400\n",
            "Epoch: 15/30, step: 157/364, loss: 0.42606, accuracy: 0.81409\n",
            "Epoch: 15/30, step: 158/364, loss: 0.42596, accuracy: 0.81458\n",
            "Epoch: 15/30, step: 159/364, loss: 0.42603, accuracy: 0.81466\n",
            "Epoch: 15/30, step: 160/364, loss: 0.42622, accuracy: 0.81465\n",
            "Epoch: 15/30, step: 161/364, loss: 0.42605, accuracy: 0.81493\n",
            "Epoch: 15/30, step: 162/364, loss: 0.42610, accuracy: 0.81501\n",
            "Epoch: 15/30, step: 163/364, loss: 0.42634, accuracy: 0.81518\n",
            "Epoch: 15/30, step: 164/364, loss: 0.42623, accuracy: 0.81526\n",
            "Epoch: 15/30, step: 165/364, loss: 0.42560, accuracy: 0.81572\n",
            "Epoch: 15/30, step: 166/364, loss: 0.42580, accuracy: 0.81551\n",
            "Epoch: 15/30, step: 167/364, loss: 0.42540, accuracy: 0.81587\n",
            "Epoch: 15/30, step: 168/364, loss: 0.42513, accuracy: 0.81603\n",
            "Epoch: 15/30, step: 169/364, loss: 0.42539, accuracy: 0.81574\n",
            "Epoch: 15/30, step: 170/364, loss: 0.42514, accuracy: 0.81590\n",
            "Epoch: 15/30, step: 171/364, loss: 0.42543, accuracy: 0.81579\n",
            "Epoch: 15/30, step: 172/364, loss: 0.42541, accuracy: 0.81568\n",
            "Epoch: 15/30, step: 173/364, loss: 0.42533, accuracy: 0.81566\n",
            "Epoch: 15/30, step: 174/364, loss: 0.42542, accuracy: 0.81600\n",
            "Epoch: 15/30, step: 175/364, loss: 0.42594, accuracy: 0.81554\n",
            "Epoch: 15/30, step: 176/364, loss: 0.42619, accuracy: 0.81534\n",
            "Epoch: 15/30, step: 177/364, loss: 0.42667, accuracy: 0.81550\n",
            "Epoch: 15/30, step: 178/364, loss: 0.42649, accuracy: 0.81592\n",
            "Epoch: 15/30, step: 179/364, loss: 0.42656, accuracy: 0.81582\n",
            "Epoch: 15/30, step: 180/364, loss: 0.42657, accuracy: 0.81563\n",
            "Epoch: 15/30, step: 181/364, loss: 0.42668, accuracy: 0.81535\n",
            "Epoch: 15/30, step: 182/364, loss: 0.42628, accuracy: 0.81550\n",
            "Epoch: 15/30, step: 183/364, loss: 0.42650, accuracy: 0.81549\n",
            "Epoch: 15/30, step: 184/364, loss: 0.42637, accuracy: 0.81556\n",
            "Epoch: 15/30, step: 185/364, loss: 0.42662, accuracy: 0.81520\n",
            "Epoch: 15/30, step: 186/364, loss: 0.42674, accuracy: 0.81552\n",
            "Epoch: 15/30, step: 187/364, loss: 0.42662, accuracy: 0.81559\n",
            "Epoch: 15/30, step: 188/364, loss: 0.42623, accuracy: 0.81591\n",
            "Epoch: 15/30, step: 189/364, loss: 0.42593, accuracy: 0.81605\n",
            "Epoch: 15/30, step: 190/364, loss: 0.42561, accuracy: 0.81645\n",
            "Epoch: 15/30, step: 191/364, loss: 0.42544, accuracy: 0.81659\n",
            "Epoch: 15/30, step: 192/364, loss: 0.42512, accuracy: 0.81681\n",
            "Epoch: 15/30, step: 193/364, loss: 0.42551, accuracy: 0.81655\n",
            "Epoch: 15/30, step: 194/364, loss: 0.42519, accuracy: 0.81661\n",
            "Epoch: 15/30, step: 195/364, loss: 0.42533, accuracy: 0.81675\n",
            "Epoch: 15/30, step: 196/364, loss: 0.42489, accuracy: 0.81712\n",
            "Epoch: 15/30, step: 197/364, loss: 0.42503, accuracy: 0.81686\n",
            "Epoch: 15/30, step: 198/364, loss: 0.42482, accuracy: 0.81723\n",
            "Epoch: 15/30, step: 199/364, loss: 0.42434, accuracy: 0.81753\n",
            "Epoch: 15/30, step: 200/364, loss: 0.42431, accuracy: 0.81734\n",
            "Epoch: 15/30, step: 201/364, loss: 0.42408, accuracy: 0.81763\n",
            "Epoch: 15/30, step: 202/364, loss: 0.42404, accuracy: 0.81768\n",
            "Epoch: 15/30, step: 203/364, loss: 0.42415, accuracy: 0.81735\n",
            "Epoch: 15/30, step: 204/364, loss: 0.42376, accuracy: 0.81771\n",
            "Epoch: 15/30, step: 205/364, loss: 0.42410, accuracy: 0.81791\n",
            "Epoch: 15/30, step: 206/364, loss: 0.42425, accuracy: 0.81766\n",
            "Epoch: 15/30, step: 207/364, loss: 0.42414, accuracy: 0.81763\n",
            "Epoch: 15/30, step: 208/364, loss: 0.42442, accuracy: 0.81731\n",
            "Epoch: 15/30, step: 209/364, loss: 0.42416, accuracy: 0.81751\n",
            "Epoch: 15/30, step: 210/364, loss: 0.42413, accuracy: 0.81749\n",
            "Epoch: 15/30, step: 211/364, loss: 0.42386, accuracy: 0.81776\n",
            "Epoch: 15/30, step: 212/364, loss: 0.42386, accuracy: 0.81773\n",
            "Epoch: 15/30, step: 213/364, loss: 0.42372, accuracy: 0.81778\n",
            "Epoch: 15/30, step: 214/364, loss: 0.42401, accuracy: 0.81768\n",
            "Epoch: 15/30, step: 215/364, loss: 0.42401, accuracy: 0.81773\n",
            "Epoch: 15/30, step: 216/364, loss: 0.42401, accuracy: 0.81764\n",
            "Epoch: 15/30, step: 217/364, loss: 0.42387, accuracy: 0.81776\n",
            "Epoch: 15/30, step: 218/364, loss: 0.42421, accuracy: 0.81752\n",
            "Epoch: 15/30, step: 219/364, loss: 0.42464, accuracy: 0.81707\n",
            "Epoch: 15/30, step: 220/364, loss: 0.42457, accuracy: 0.81733\n",
            "Epoch: 15/30, step: 221/364, loss: 0.42442, accuracy: 0.81738\n",
            "Epoch: 15/30, step: 222/364, loss: 0.42451, accuracy: 0.81722\n",
            "Epoch: 15/30, step: 223/364, loss: 0.42426, accuracy: 0.81733\n",
            "Epoch: 15/30, step: 224/364, loss: 0.42413, accuracy: 0.81738\n",
            "Epoch: 15/30, step: 225/364, loss: 0.42391, accuracy: 0.81750\n",
            "Epoch: 15/30, step: 226/364, loss: 0.42435, accuracy: 0.81720\n",
            "Epoch: 15/30, step: 227/364, loss: 0.42443, accuracy: 0.81711\n",
            "Epoch: 15/30, step: 228/364, loss: 0.42424, accuracy: 0.81716\n",
            "Epoch: 15/30, step: 229/364, loss: 0.42405, accuracy: 0.81714\n",
            "Epoch: 15/30, step: 230/364, loss: 0.42435, accuracy: 0.81678\n",
            "Epoch: 15/30, step: 231/364, loss: 0.42447, accuracy: 0.81676\n",
            "Epoch: 15/30, step: 232/364, loss: 0.42467, accuracy: 0.81647\n",
            "Epoch: 15/30, step: 233/364, loss: 0.42508, accuracy: 0.81612\n",
            "Epoch: 15/30, step: 234/364, loss: 0.42493, accuracy: 0.81644\n",
            "Epoch: 15/30, step: 235/364, loss: 0.42462, accuracy: 0.81669\n",
            "Epoch: 15/30, step: 236/364, loss: 0.42463, accuracy: 0.81667\n",
            "Epoch: 15/30, step: 237/364, loss: 0.42472, accuracy: 0.81659\n",
            "Epoch: 15/30, step: 238/364, loss: 0.42439, accuracy: 0.81690\n",
            "Epoch: 15/30, step: 239/364, loss: 0.42451, accuracy: 0.81681\n",
            "Epoch: 15/30, step: 240/364, loss: 0.42448, accuracy: 0.81667\n",
            "Epoch: 15/30, step: 241/364, loss: 0.42459, accuracy: 0.81665\n",
            "Epoch: 15/30, step: 242/364, loss: 0.42439, accuracy: 0.81676\n",
            "Epoch: 15/30, step: 243/364, loss: 0.42453, accuracy: 0.81649\n",
            "Epoch: 15/30, step: 244/364, loss: 0.42521, accuracy: 0.81602\n",
            "Epoch: 15/30, step: 245/364, loss: 0.42540, accuracy: 0.81582\n",
            "Epoch: 15/30, step: 246/364, loss: 0.42515, accuracy: 0.81593\n",
            "Epoch: 15/30, step: 247/364, loss: 0.42538, accuracy: 0.81579\n",
            "Epoch: 15/30, step: 248/364, loss: 0.42556, accuracy: 0.81571\n",
            "Epoch: 15/30, step: 249/364, loss: 0.42573, accuracy: 0.81539\n",
            "Epoch: 15/30, step: 250/364, loss: 0.42553, accuracy: 0.81575\n",
            "Epoch: 15/30, step: 251/364, loss: 0.42553, accuracy: 0.81580\n",
            "Epoch: 15/30, step: 252/364, loss: 0.42563, accuracy: 0.81579\n",
            "Epoch: 15/30, step: 253/364, loss: 0.42566, accuracy: 0.81565\n",
            "Epoch: 15/30, step: 254/364, loss: 0.42555, accuracy: 0.81588\n",
            "Epoch: 15/30, step: 255/364, loss: 0.42577, accuracy: 0.81569\n",
            "Epoch: 15/30, step: 256/364, loss: 0.42581, accuracy: 0.81549\n",
            "Epoch: 15/30, step: 257/364, loss: 0.42585, accuracy: 0.81548\n",
            "Epoch: 15/30, step: 258/364, loss: 0.42600, accuracy: 0.81535\n",
            "Epoch: 15/30, step: 259/364, loss: 0.42574, accuracy: 0.81540\n",
            "Epoch: 15/30, step: 260/364, loss: 0.42557, accuracy: 0.81544\n",
            "Epoch: 15/30, step: 261/364, loss: 0.42558, accuracy: 0.81549\n",
            "Epoch: 15/30, step: 262/364, loss: 0.42528, accuracy: 0.81566\n",
            "Epoch: 15/30, step: 263/364, loss: 0.42508, accuracy: 0.81583\n",
            "Epoch: 15/30, step: 264/364, loss: 0.42489, accuracy: 0.81593\n",
            "Epoch: 15/30, step: 265/364, loss: 0.42480, accuracy: 0.81598\n",
            "Epoch: 15/30, step: 266/364, loss: 0.42459, accuracy: 0.81614\n",
            "Epoch: 15/30, step: 267/364, loss: 0.42442, accuracy: 0.81619\n",
            "Epoch: 15/30, step: 268/364, loss: 0.42467, accuracy: 0.81600\n",
            "Epoch: 15/30, step: 269/364, loss: 0.42521, accuracy: 0.81558\n",
            "Epoch: 15/30, step: 270/364, loss: 0.42527, accuracy: 0.81568\n",
            "Epoch: 15/30, step: 271/364, loss: 0.42569, accuracy: 0.81538\n",
            "Epoch: 15/30, step: 272/364, loss: 0.42588, accuracy: 0.81503\n",
            "Epoch: 15/30, step: 273/364, loss: 0.42551, accuracy: 0.81530\n",
            "Epoch: 15/30, step: 274/364, loss: 0.42565, accuracy: 0.81535\n",
            "Epoch: 15/30, step: 275/364, loss: 0.42562, accuracy: 0.81540\n",
            "Epoch: 15/30, step: 276/364, loss: 0.42577, accuracy: 0.81516\n",
            "Epoch: 15/30, step: 277/364, loss: 0.42600, accuracy: 0.81498\n",
            "Epoch: 15/30, step: 278/364, loss: 0.42618, accuracy: 0.81497\n",
            "Epoch: 15/30, step: 279/364, loss: 0.42635, accuracy: 0.81485\n",
            "Epoch: 15/30, step: 280/364, loss: 0.42681, accuracy: 0.81440\n",
            "Epoch: 15/30, step: 281/364, loss: 0.42664, accuracy: 0.81450\n",
            "Epoch: 15/30, step: 282/364, loss: 0.42636, accuracy: 0.81472\n",
            "Epoch: 15/30, step: 283/364, loss: 0.42633, accuracy: 0.81476\n",
            "Epoch: 15/30, step: 284/364, loss: 0.42653, accuracy: 0.81459\n",
            "Epoch: 15/30, step: 285/364, loss: 0.42651, accuracy: 0.81447\n",
            "Epoch: 15/30, step: 286/364, loss: 0.42626, accuracy: 0.81469\n",
            "Epoch: 15/30, step: 287/364, loss: 0.42614, accuracy: 0.81473\n",
            "Epoch: 15/30, step: 288/364, loss: 0.42639, accuracy: 0.81445\n",
            "Epoch: 15/30, step: 289/364, loss: 0.42635, accuracy: 0.81445\n",
            "Epoch: 15/30, step: 290/364, loss: 0.42612, accuracy: 0.81466\n",
            "Epoch: 15/30, step: 291/364, loss: 0.42602, accuracy: 0.81467\n",
            "Epoch: 15/30, train loss: 0.42602, train accuracy: 0.81467, valid loss: 0.61444, valid accuracy: 0.69153\n",
            "Epoch: 16/30, step: 1/364, loss: 0.39493, accuracy: 0.78125\n",
            "Epoch: 16/30, step: 2/364, loss: 0.37824, accuracy: 0.81250\n",
            "Epoch: 16/30, step: 3/364, loss: 0.40704, accuracy: 0.81250\n",
            "Epoch: 16/30, step: 4/364, loss: 0.40366, accuracy: 0.82812\n",
            "Epoch: 16/30, step: 5/364, loss: 0.41139, accuracy: 0.81563\n",
            "Epoch: 16/30, step: 6/364, loss: 0.40948, accuracy: 0.81771\n",
            "Epoch: 16/30, step: 7/364, loss: 0.40292, accuracy: 0.82143\n",
            "Epoch: 16/30, step: 8/364, loss: 0.42140, accuracy: 0.80469\n",
            "Epoch: 16/30, step: 9/364, loss: 0.42840, accuracy: 0.80382\n",
            "Epoch: 16/30, step: 10/364, loss: 0.43455, accuracy: 0.79844\n",
            "Epoch: 16/30, step: 11/364, loss: 0.44037, accuracy: 0.79545\n",
            "Epoch: 16/30, step: 12/364, loss: 0.43379, accuracy: 0.80339\n",
            "Epoch: 16/30, step: 13/364, loss: 0.43449, accuracy: 0.80048\n",
            "Epoch: 16/30, step: 14/364, loss: 0.42996, accuracy: 0.80246\n",
            "Epoch: 16/30, step: 15/364, loss: 0.42495, accuracy: 0.80625\n",
            "Epoch: 16/30, step: 16/364, loss: 0.41997, accuracy: 0.81152\n",
            "Epoch: 16/30, step: 17/364, loss: 0.42169, accuracy: 0.81066\n",
            "Epoch: 16/30, step: 18/364, loss: 0.41884, accuracy: 0.81510\n",
            "Epoch: 16/30, step: 19/364, loss: 0.42045, accuracy: 0.81168\n",
            "Epoch: 16/30, step: 20/364, loss: 0.41995, accuracy: 0.81094\n",
            "Epoch: 16/30, step: 21/364, loss: 0.41977, accuracy: 0.81027\n",
            "Epoch: 16/30, step: 22/364, loss: 0.42098, accuracy: 0.80824\n",
            "Epoch: 16/30, step: 23/364, loss: 0.41640, accuracy: 0.80910\n",
            "Epoch: 16/30, step: 24/364, loss: 0.41710, accuracy: 0.80924\n",
            "Epoch: 16/30, step: 25/364, loss: 0.41630, accuracy: 0.80813\n",
            "Epoch: 16/30, step: 26/364, loss: 0.42154, accuracy: 0.80529\n",
            "Epoch: 16/30, step: 27/364, loss: 0.41994, accuracy: 0.80556\n",
            "Epoch: 16/30, step: 28/364, loss: 0.41850, accuracy: 0.80692\n",
            "Epoch: 16/30, step: 29/364, loss: 0.41474, accuracy: 0.80981\n",
            "Epoch: 16/30, step: 30/364, loss: 0.41506, accuracy: 0.81042\n",
            "Epoch: 16/30, step: 31/364, loss: 0.41826, accuracy: 0.80948\n",
            "Epoch: 16/30, step: 32/364, loss: 0.41622, accuracy: 0.81006\n",
            "Epoch: 16/30, step: 33/364, loss: 0.41416, accuracy: 0.81297\n",
            "Epoch: 16/30, step: 34/364, loss: 0.41423, accuracy: 0.81158\n",
            "Epoch: 16/30, step: 35/364, loss: 0.41765, accuracy: 0.81071\n",
            "Epoch: 16/30, step: 36/364, loss: 0.41739, accuracy: 0.81163\n",
            "Epoch: 16/30, step: 37/364, loss: 0.41639, accuracy: 0.81166\n",
            "Epoch: 16/30, step: 38/364, loss: 0.41361, accuracy: 0.81332\n",
            "Epoch: 16/30, step: 39/364, loss: 0.41285, accuracy: 0.81410\n",
            "Epoch: 16/30, step: 40/364, loss: 0.41424, accuracy: 0.81367\n",
            "Epoch: 16/30, step: 41/364, loss: 0.41437, accuracy: 0.81326\n",
            "Epoch: 16/30, step: 42/364, loss: 0.41348, accuracy: 0.81510\n",
            "Epoch: 16/30, step: 43/364, loss: 0.41256, accuracy: 0.81541\n",
            "Epoch: 16/30, step: 44/364, loss: 0.41382, accuracy: 0.81499\n",
            "Epoch: 16/30, step: 45/364, loss: 0.41178, accuracy: 0.81736\n",
            "Epoch: 16/30, step: 46/364, loss: 0.41148, accuracy: 0.81760\n",
            "Epoch: 16/30, step: 47/364, loss: 0.41173, accuracy: 0.81848\n",
            "Epoch: 16/30, step: 48/364, loss: 0.41121, accuracy: 0.81803\n",
            "Epoch: 16/30, step: 49/364, loss: 0.41100, accuracy: 0.81824\n",
            "Epoch: 16/30, step: 50/364, loss: 0.41252, accuracy: 0.81656\n",
            "Epoch: 16/30, step: 51/364, loss: 0.41256, accuracy: 0.81710\n",
            "Epoch: 16/30, step: 52/364, loss: 0.41250, accuracy: 0.81701\n",
            "Epoch: 16/30, step: 53/364, loss: 0.41401, accuracy: 0.81604\n",
            "Epoch: 16/30, step: 54/364, loss: 0.41226, accuracy: 0.81742\n",
            "Epoch: 16/30, step: 55/364, loss: 0.41195, accuracy: 0.81705\n",
            "Epoch: 16/30, step: 56/364, loss: 0.41125, accuracy: 0.81724\n",
            "Epoch: 16/30, step: 57/364, loss: 0.41095, accuracy: 0.81798\n",
            "Epoch: 16/30, step: 58/364, loss: 0.41149, accuracy: 0.81789\n",
            "Epoch: 16/30, step: 59/364, loss: 0.41248, accuracy: 0.81621\n",
            "Epoch: 16/30, step: 60/364, loss: 0.41178, accuracy: 0.81693\n",
            "Epoch: 16/30, step: 61/364, loss: 0.41281, accuracy: 0.81660\n",
            "Epoch: 16/30, step: 62/364, loss: 0.41392, accuracy: 0.81502\n",
            "Epoch: 16/30, step: 63/364, loss: 0.41491, accuracy: 0.81448\n",
            "Epoch: 16/30, step: 64/364, loss: 0.41607, accuracy: 0.81348\n",
            "Epoch: 16/30, step: 65/364, loss: 0.41623, accuracy: 0.81298\n",
            "Epoch: 16/30, step: 66/364, loss: 0.41746, accuracy: 0.81274\n",
            "Epoch: 16/30, step: 67/364, loss: 0.41687, accuracy: 0.81297\n",
            "Epoch: 16/30, step: 68/364, loss: 0.41698, accuracy: 0.81181\n",
            "Epoch: 16/30, step: 69/364, loss: 0.41728, accuracy: 0.81205\n",
            "Epoch: 16/30, step: 70/364, loss: 0.41678, accuracy: 0.81228\n",
            "Epoch: 16/30, step: 71/364, loss: 0.41787, accuracy: 0.81118\n",
            "Epoch: 16/30, step: 72/364, loss: 0.41787, accuracy: 0.81120\n",
            "Epoch: 16/30, step: 73/364, loss: 0.41631, accuracy: 0.81207\n",
            "Epoch: 16/30, step: 74/364, loss: 0.41649, accuracy: 0.81208\n",
            "Epoch: 16/30, step: 75/364, loss: 0.41648, accuracy: 0.81250\n",
            "Epoch: 16/30, step: 76/364, loss: 0.41663, accuracy: 0.81271\n",
            "Epoch: 16/30, step: 77/364, loss: 0.41601, accuracy: 0.81311\n",
            "Epoch: 16/30, step: 78/364, loss: 0.41556, accuracy: 0.81370\n",
            "Epoch: 16/30, step: 79/364, loss: 0.41594, accuracy: 0.81329\n",
            "Epoch: 16/30, step: 80/364, loss: 0.41607, accuracy: 0.81348\n",
            "Epoch: 16/30, step: 81/364, loss: 0.41614, accuracy: 0.81366\n",
            "Epoch: 16/30, step: 82/364, loss: 0.41718, accuracy: 0.81250\n",
            "Epoch: 16/30, step: 83/364, loss: 0.41709, accuracy: 0.81269\n",
            "Epoch: 16/30, step: 84/364, loss: 0.41702, accuracy: 0.81362\n",
            "Epoch: 16/30, step: 85/364, loss: 0.41636, accuracy: 0.81434\n",
            "Epoch: 16/30, step: 86/364, loss: 0.41558, accuracy: 0.81468\n",
            "Epoch: 16/30, step: 87/364, loss: 0.41507, accuracy: 0.81501\n",
            "Epoch: 16/30, step: 88/364, loss: 0.41514, accuracy: 0.81516\n",
            "Epoch: 16/30, step: 89/364, loss: 0.41408, accuracy: 0.81636\n",
            "Epoch: 16/30, step: 90/364, loss: 0.41380, accuracy: 0.81684\n",
            "Epoch: 16/30, step: 91/364, loss: 0.41363, accuracy: 0.81714\n",
            "Epoch: 16/30, step: 92/364, loss: 0.41436, accuracy: 0.81641\n",
            "Epoch: 16/30, step: 93/364, loss: 0.41477, accuracy: 0.81636\n",
            "Epoch: 16/30, step: 94/364, loss: 0.41586, accuracy: 0.81582\n",
            "Epoch: 16/30, step: 95/364, loss: 0.41648, accuracy: 0.81530\n",
            "Epoch: 16/30, step: 96/364, loss: 0.41689, accuracy: 0.81462\n",
            "Epoch: 16/30, step: 97/364, loss: 0.41658, accuracy: 0.81492\n",
            "Epoch: 16/30, step: 98/364, loss: 0.41671, accuracy: 0.81473\n",
            "Epoch: 16/30, step: 99/364, loss: 0.41636, accuracy: 0.81487\n",
            "Epoch: 16/30, step: 100/364, loss: 0.41597, accuracy: 0.81500\n",
            "Epoch: 16/30, step: 101/364, loss: 0.41538, accuracy: 0.81559\n",
            "Epoch: 16/30, step: 102/364, loss: 0.41567, accuracy: 0.81556\n",
            "Epoch: 16/30, step: 103/364, loss: 0.41606, accuracy: 0.81478\n",
            "Epoch: 16/30, step: 104/364, loss: 0.41571, accuracy: 0.81505\n",
            "Epoch: 16/30, step: 105/364, loss: 0.41558, accuracy: 0.81592\n",
            "Epoch: 16/30, step: 106/364, loss: 0.41527, accuracy: 0.81619\n",
            "Epoch: 16/30, step: 107/364, loss: 0.41636, accuracy: 0.81600\n",
            "Epoch: 16/30, step: 108/364, loss: 0.41615, accuracy: 0.81641\n",
            "Epoch: 16/30, step: 109/364, loss: 0.41528, accuracy: 0.81680\n",
            "Epoch: 16/30, step: 110/364, loss: 0.41461, accuracy: 0.81733\n",
            "Epoch: 16/30, step: 111/364, loss: 0.41458, accuracy: 0.81743\n",
            "Epoch: 16/30, step: 112/364, loss: 0.41550, accuracy: 0.81696\n",
            "Epoch: 16/30, step: 113/364, loss: 0.41558, accuracy: 0.81651\n",
            "Epoch: 16/30, step: 114/364, loss: 0.41550, accuracy: 0.81661\n",
            "Epoch: 16/30, step: 115/364, loss: 0.41514, accuracy: 0.81726\n",
            "Epoch: 16/30, step: 116/364, loss: 0.41534, accuracy: 0.81735\n",
            "Epoch: 16/30, step: 117/364, loss: 0.41519, accuracy: 0.81757\n",
            "Epoch: 16/30, step: 118/364, loss: 0.41517, accuracy: 0.81766\n",
            "Epoch: 16/30, step: 119/364, loss: 0.41465, accuracy: 0.81828\n",
            "Epoch: 16/30, step: 120/364, loss: 0.41556, accuracy: 0.81732\n",
            "Epoch: 16/30, step: 121/364, loss: 0.41573, accuracy: 0.81741\n",
            "Epoch: 16/30, step: 122/364, loss: 0.41508, accuracy: 0.81801\n",
            "Epoch: 16/30, step: 123/364, loss: 0.41525, accuracy: 0.81809\n",
            "Epoch: 16/30, step: 124/364, loss: 0.41519, accuracy: 0.81792\n",
            "Epoch: 16/30, step: 125/364, loss: 0.41519, accuracy: 0.81800\n",
            "Epoch: 16/30, step: 126/364, loss: 0.41574, accuracy: 0.81758\n",
            "Epoch: 16/30, step: 127/364, loss: 0.41654, accuracy: 0.81754\n",
            "Epoch: 16/30, step: 128/364, loss: 0.41683, accuracy: 0.81702\n",
            "Epoch: 16/30, step: 129/364, loss: 0.41707, accuracy: 0.81674\n",
            "Epoch: 16/30, step: 130/364, loss: 0.41670, accuracy: 0.81719\n",
            "Epoch: 16/30, step: 131/364, loss: 0.41621, accuracy: 0.81727\n",
            "Epoch: 16/30, step: 132/364, loss: 0.41616, accuracy: 0.81735\n",
            "Epoch: 16/30, step: 133/364, loss: 0.41630, accuracy: 0.81708\n",
            "Epoch: 16/30, step: 134/364, loss: 0.41631, accuracy: 0.81670\n",
            "Epoch: 16/30, step: 135/364, loss: 0.41589, accuracy: 0.81713\n",
            "Epoch: 16/30, step: 136/364, loss: 0.41599, accuracy: 0.81698\n",
            "Epoch: 16/30, step: 137/364, loss: 0.41650, accuracy: 0.81626\n",
            "Epoch: 16/30, step: 138/364, loss: 0.41638, accuracy: 0.81658\n",
            "Epoch: 16/30, step: 139/364, loss: 0.41655, accuracy: 0.81643\n",
            "Epoch: 16/30, step: 140/364, loss: 0.41670, accuracy: 0.81618\n",
            "Epoch: 16/30, step: 141/364, loss: 0.41615, accuracy: 0.81649\n",
            "Epoch: 16/30, step: 142/364, loss: 0.41551, accuracy: 0.81723\n",
            "Epoch: 16/30, step: 143/364, loss: 0.41537, accuracy: 0.81731\n",
            "Epoch: 16/30, step: 144/364, loss: 0.41671, accuracy: 0.81586\n",
            "Epoch: 16/30, step: 145/364, loss: 0.41690, accuracy: 0.81563\n",
            "Epoch: 16/30, step: 146/364, loss: 0.41691, accuracy: 0.81560\n",
            "Epoch: 16/30, step: 147/364, loss: 0.41714, accuracy: 0.81558\n",
            "Epoch: 16/30, step: 148/364, loss: 0.41679, accuracy: 0.81588\n",
            "Epoch: 16/30, step: 149/364, loss: 0.41613, accuracy: 0.81617\n",
            "Epoch: 16/30, step: 150/364, loss: 0.41602, accuracy: 0.81625\n",
            "Epoch: 16/30, step: 151/364, loss: 0.41604, accuracy: 0.81623\n",
            "Epoch: 16/30, step: 152/364, loss: 0.41563, accuracy: 0.81641\n",
            "Epoch: 16/30, step: 153/364, loss: 0.41542, accuracy: 0.81689\n",
            "Epoch: 16/30, step: 154/364, loss: 0.41661, accuracy: 0.81615\n",
            "Epoch: 16/30, step: 155/364, loss: 0.41655, accuracy: 0.81633\n",
            "Epoch: 16/30, step: 156/364, loss: 0.41614, accuracy: 0.81641\n",
            "Epoch: 16/30, step: 157/364, loss: 0.41620, accuracy: 0.81628\n",
            "Epoch: 16/30, step: 158/364, loss: 0.41586, accuracy: 0.81655\n",
            "Epoch: 16/30, step: 159/364, loss: 0.41600, accuracy: 0.81653\n",
            "Epoch: 16/30, step: 160/364, loss: 0.41560, accuracy: 0.81660\n",
            "Epoch: 16/30, step: 161/364, loss: 0.41555, accuracy: 0.81667\n",
            "Epoch: 16/30, step: 162/364, loss: 0.41566, accuracy: 0.81674\n",
            "Epoch: 16/30, step: 163/364, loss: 0.41541, accuracy: 0.81691\n",
            "Epoch: 16/30, step: 164/364, loss: 0.41523, accuracy: 0.81698\n",
            "Epoch: 16/30, step: 165/364, loss: 0.41503, accuracy: 0.81705\n",
            "Epoch: 16/30, step: 166/364, loss: 0.41482, accuracy: 0.81739\n",
            "Epoch: 16/30, step: 167/364, loss: 0.41473, accuracy: 0.81718\n",
            "Epoch: 16/30, step: 168/364, loss: 0.41473, accuracy: 0.81715\n",
            "Epoch: 16/30, step: 169/364, loss: 0.41469, accuracy: 0.81749\n",
            "Epoch: 16/30, step: 170/364, loss: 0.41438, accuracy: 0.81765\n",
            "Epoch: 16/30, step: 171/364, loss: 0.41464, accuracy: 0.81743\n",
            "Epoch: 16/30, step: 172/364, loss: 0.41444, accuracy: 0.81777\n",
            "Epoch: 16/30, step: 173/364, loss: 0.41473, accuracy: 0.81729\n",
            "Epoch: 16/30, step: 174/364, loss: 0.41467, accuracy: 0.81744\n",
            "Epoch: 16/30, step: 175/364, loss: 0.41416, accuracy: 0.81813\n",
            "Epoch: 16/30, step: 176/364, loss: 0.41392, accuracy: 0.81800\n",
            "Epoch: 16/30, step: 177/364, loss: 0.41404, accuracy: 0.81797\n",
            "Epoch: 16/30, step: 178/364, loss: 0.41389, accuracy: 0.81812\n",
            "Epoch: 16/30, step: 179/364, loss: 0.41403, accuracy: 0.81791\n",
            "Epoch: 16/30, step: 180/364, loss: 0.41406, accuracy: 0.81762\n",
            "Epoch: 16/30, step: 181/364, loss: 0.41409, accuracy: 0.81751\n",
            "Epoch: 16/30, step: 182/364, loss: 0.41407, accuracy: 0.81757\n",
            "Epoch: 16/30, step: 183/364, loss: 0.41366, accuracy: 0.81796\n",
            "Epoch: 16/30, step: 184/364, loss: 0.41365, accuracy: 0.81768\n",
            "Epoch: 16/30, step: 185/364, loss: 0.41368, accuracy: 0.81774\n",
            "Epoch: 16/30, step: 186/364, loss: 0.41378, accuracy: 0.81754\n",
            "Epoch: 16/30, step: 187/364, loss: 0.41354, accuracy: 0.81776\n",
            "Epoch: 16/30, step: 188/364, loss: 0.41324, accuracy: 0.81807\n",
            "Epoch: 16/30, step: 189/364, loss: 0.41320, accuracy: 0.81779\n",
            "Epoch: 16/30, step: 190/364, loss: 0.41299, accuracy: 0.81793\n",
            "Epoch: 16/30, step: 191/364, loss: 0.41304, accuracy: 0.81814\n",
            "Epoch: 16/30, step: 192/364, loss: 0.41349, accuracy: 0.81795\n",
            "Epoch: 16/30, step: 193/364, loss: 0.41343, accuracy: 0.81744\n",
            "Epoch: 16/30, step: 194/364, loss: 0.41329, accuracy: 0.81757\n",
            "Epoch: 16/30, step: 195/364, loss: 0.41321, accuracy: 0.81755\n",
            "Epoch: 16/30, step: 196/364, loss: 0.41313, accuracy: 0.81768\n",
            "Epoch: 16/30, step: 197/364, loss: 0.41296, accuracy: 0.81781\n",
            "Epoch: 16/30, step: 198/364, loss: 0.41298, accuracy: 0.81771\n",
            "Epoch: 16/30, step: 199/364, loss: 0.41265, accuracy: 0.81800\n",
            "Epoch: 16/30, step: 200/364, loss: 0.41283, accuracy: 0.81773\n",
            "Epoch: 16/30, step: 201/364, loss: 0.41269, accuracy: 0.81786\n",
            "Epoch: 16/30, step: 202/364, loss: 0.41296, accuracy: 0.81791\n",
            "Epoch: 16/30, step: 203/364, loss: 0.41317, accuracy: 0.81758\n",
            "Epoch: 16/30, step: 204/364, loss: 0.41341, accuracy: 0.81748\n",
            "Epoch: 16/30, step: 205/364, loss: 0.41325, accuracy: 0.81761\n",
            "Epoch: 16/30, step: 206/364, loss: 0.41286, accuracy: 0.81796\n",
            "Epoch: 16/30, step: 207/364, loss: 0.41279, accuracy: 0.81786\n",
            "Epoch: 16/30, step: 208/364, loss: 0.41282, accuracy: 0.81791\n",
            "Epoch: 16/30, step: 209/364, loss: 0.41255, accuracy: 0.81803\n",
            "Epoch: 16/30, step: 210/364, loss: 0.41269, accuracy: 0.81793\n",
            "Epoch: 16/30, step: 211/364, loss: 0.41222, accuracy: 0.81828\n",
            "Epoch: 16/30, step: 212/364, loss: 0.41209, accuracy: 0.81818\n",
            "Epoch: 16/30, step: 213/364, loss: 0.41209, accuracy: 0.81822\n",
            "Epoch: 16/30, step: 214/364, loss: 0.41241, accuracy: 0.81805\n",
            "Epoch: 16/30, step: 215/364, loss: 0.41279, accuracy: 0.81795\n",
            "Epoch: 16/30, step: 216/364, loss: 0.41283, accuracy: 0.81785\n",
            "Epoch: 16/30, step: 217/364, loss: 0.41314, accuracy: 0.81776\n",
            "Epoch: 16/30, step: 218/364, loss: 0.41339, accuracy: 0.81730\n",
            "Epoch: 16/30, step: 219/364, loss: 0.41306, accuracy: 0.81757\n",
            "Epoch: 16/30, step: 220/364, loss: 0.41353, accuracy: 0.81712\n",
            "Epoch: 16/30, step: 221/364, loss: 0.41428, accuracy: 0.81667\n",
            "Epoch: 16/30, step: 222/364, loss: 0.41406, accuracy: 0.81679\n",
            "Epoch: 16/30, step: 223/364, loss: 0.41396, accuracy: 0.81684\n",
            "Epoch: 16/30, step: 224/364, loss: 0.41436, accuracy: 0.81648\n",
            "Epoch: 16/30, step: 225/364, loss: 0.41424, accuracy: 0.81660\n",
            "Epoch: 16/30, step: 226/364, loss: 0.41402, accuracy: 0.81679\n",
            "Epoch: 16/30, step: 227/364, loss: 0.41371, accuracy: 0.81704\n",
            "Epoch: 16/30, step: 228/364, loss: 0.41345, accuracy: 0.81716\n",
            "Epoch: 16/30, step: 229/364, loss: 0.41335, accuracy: 0.81714\n",
            "Epoch: 16/30, step: 230/364, loss: 0.41328, accuracy: 0.81692\n",
            "Epoch: 16/30, step: 231/364, loss: 0.41314, accuracy: 0.81703\n",
            "Epoch: 16/30, step: 232/364, loss: 0.41297, accuracy: 0.81708\n",
            "Epoch: 16/30, step: 233/364, loss: 0.41300, accuracy: 0.81713\n",
            "Epoch: 16/30, step: 234/364, loss: 0.41261, accuracy: 0.81731\n",
            "Epoch: 16/30, step: 235/364, loss: 0.41283, accuracy: 0.81715\n",
            "Epoch: 16/30, step: 236/364, loss: 0.41270, accuracy: 0.81700\n",
            "Epoch: 16/30, step: 237/364, loss: 0.41261, accuracy: 0.81698\n",
            "Epoch: 16/30, step: 238/364, loss: 0.41259, accuracy: 0.81690\n",
            "Epoch: 16/30, step: 239/364, loss: 0.41289, accuracy: 0.81655\n",
            "Epoch: 16/30, step: 240/364, loss: 0.41251, accuracy: 0.81693\n",
            "Epoch: 16/30, step: 241/364, loss: 0.41282, accuracy: 0.81658\n",
            "Epoch: 16/30, step: 242/364, loss: 0.41286, accuracy: 0.81650\n",
            "Epoch: 16/30, step: 243/364, loss: 0.41299, accuracy: 0.81636\n",
            "Epoch: 16/30, step: 244/364, loss: 0.41280, accuracy: 0.81653\n",
            "Epoch: 16/30, step: 245/364, loss: 0.41276, accuracy: 0.81652\n",
            "Epoch: 16/30, step: 246/364, loss: 0.41272, accuracy: 0.81650\n",
            "Epoch: 16/30, step: 247/364, loss: 0.41252, accuracy: 0.81668\n",
            "Epoch: 16/30, step: 248/364, loss: 0.41264, accuracy: 0.81647\n",
            "Epoch: 16/30, step: 249/364, loss: 0.41305, accuracy: 0.81614\n",
            "Epoch: 16/30, step: 250/364, loss: 0.41305, accuracy: 0.81619\n",
            "Epoch: 16/30, step: 251/364, loss: 0.41280, accuracy: 0.81636\n",
            "Epoch: 16/30, step: 252/364, loss: 0.41288, accuracy: 0.81634\n",
            "Epoch: 16/30, step: 253/364, loss: 0.41242, accuracy: 0.81664\n",
            "Epoch: 16/30, step: 254/364, loss: 0.41254, accuracy: 0.81656\n",
            "Epoch: 16/30, step: 255/364, loss: 0.41253, accuracy: 0.81648\n",
            "Epoch: 16/30, step: 256/364, loss: 0.41245, accuracy: 0.81671\n",
            "Epoch: 16/30, step: 257/364, loss: 0.41262, accuracy: 0.81670\n",
            "Epoch: 16/30, step: 258/364, loss: 0.41275, accuracy: 0.81674\n",
            "Epoch: 16/30, step: 259/364, loss: 0.41275, accuracy: 0.81672\n",
            "Epoch: 16/30, step: 260/364, loss: 0.41268, accuracy: 0.81677\n",
            "Epoch: 16/30, step: 261/364, loss: 0.41319, accuracy: 0.81657\n",
            "Epoch: 16/30, step: 262/364, loss: 0.41317, accuracy: 0.81673\n",
            "Epoch: 16/30, step: 263/364, loss: 0.41301, accuracy: 0.81702\n",
            "Epoch: 16/30, step: 264/364, loss: 0.41254, accuracy: 0.81712\n",
            "Epoch: 16/30, step: 265/364, loss: 0.41259, accuracy: 0.81710\n",
            "Epoch: 16/30, step: 266/364, loss: 0.41235, accuracy: 0.81720\n",
            "Epoch: 16/30, step: 267/364, loss: 0.41260, accuracy: 0.81689\n",
            "Epoch: 16/30, step: 268/364, loss: 0.41243, accuracy: 0.81711\n",
            "Epoch: 16/30, step: 269/364, loss: 0.41242, accuracy: 0.81715\n",
            "Epoch: 16/30, step: 270/364, loss: 0.41228, accuracy: 0.81742\n",
            "Epoch: 16/30, step: 271/364, loss: 0.41199, accuracy: 0.81757\n",
            "Epoch: 16/30, step: 272/364, loss: 0.41210, accuracy: 0.81733\n",
            "Epoch: 16/30, step: 273/364, loss: 0.41243, accuracy: 0.81702\n",
            "Epoch: 16/30, step: 274/364, loss: 0.41232, accuracy: 0.81706\n",
            "Epoch: 16/30, step: 275/364, loss: 0.41203, accuracy: 0.81733\n",
            "Epoch: 16/30, step: 276/364, loss: 0.41266, accuracy: 0.81697\n",
            "Epoch: 16/30, step: 277/364, loss: 0.41274, accuracy: 0.81696\n",
            "Epoch: 16/30, step: 278/364, loss: 0.41266, accuracy: 0.81694\n",
            "Epoch: 16/30, step: 279/364, loss: 0.41288, accuracy: 0.81692\n",
            "Epoch: 16/30, step: 280/364, loss: 0.41292, accuracy: 0.81685\n",
            "Epoch: 16/30, step: 281/364, loss: 0.41287, accuracy: 0.81700\n",
            "Epoch: 16/30, step: 282/364, loss: 0.41266, accuracy: 0.81727\n",
            "Epoch: 16/30, step: 283/364, loss: 0.41291, accuracy: 0.81714\n",
            "Epoch: 16/30, step: 284/364, loss: 0.41294, accuracy: 0.81712\n",
            "Epoch: 16/30, step: 285/364, loss: 0.41297, accuracy: 0.81694\n",
            "Epoch: 16/30, step: 286/364, loss: 0.41289, accuracy: 0.81693\n",
            "Epoch: 16/30, step: 287/364, loss: 0.41293, accuracy: 0.81707\n",
            "Epoch: 16/30, step: 288/364, loss: 0.41300, accuracy: 0.81700\n",
            "Epoch: 16/30, step: 289/364, loss: 0.41323, accuracy: 0.81672\n",
            "Epoch: 16/30, step: 290/364, loss: 0.41350, accuracy: 0.81654\n",
            "Epoch: 16/30, step: 291/364, loss: 0.41360, accuracy: 0.81639\n",
            "Epoch: 16/30, train loss: 0.41360, train accuracy: 0.81639, valid loss: 0.60544, valid accuracy: 0.69239\n",
            "Epoch: 17/30, step: 1/364, loss: 0.35526, accuracy: 0.89062\n",
            "Epoch: 17/30, step: 2/364, loss: 0.37359, accuracy: 0.85938\n",
            "Epoch: 17/30, step: 3/364, loss: 0.37066, accuracy: 0.84896\n",
            "Epoch: 17/30, step: 4/364, loss: 0.37423, accuracy: 0.83984\n",
            "Epoch: 17/30, step: 5/364, loss: 0.37032, accuracy: 0.84688\n",
            "Epoch: 17/30, step: 6/364, loss: 0.37734, accuracy: 0.84115\n",
            "Epoch: 17/30, step: 7/364, loss: 0.39018, accuracy: 0.82589\n",
            "Epoch: 17/30, step: 8/364, loss: 0.39209, accuracy: 0.82422\n",
            "Epoch: 17/30, step: 9/364, loss: 0.38483, accuracy: 0.82986\n",
            "Epoch: 17/30, step: 10/364, loss: 0.38884, accuracy: 0.83125\n",
            "Epoch: 17/30, step: 11/364, loss: 0.40006, accuracy: 0.82670\n",
            "Epoch: 17/30, step: 12/364, loss: 0.40023, accuracy: 0.83203\n",
            "Epoch: 17/30, step: 13/364, loss: 0.40450, accuracy: 0.83053\n",
            "Epoch: 17/30, step: 14/364, loss: 0.40229, accuracy: 0.83259\n",
            "Epoch: 17/30, step: 15/364, loss: 0.39608, accuracy: 0.83646\n",
            "Epoch: 17/30, step: 16/364, loss: 0.39630, accuracy: 0.83594\n",
            "Epoch: 17/30, step: 17/364, loss: 0.39724, accuracy: 0.83364\n",
            "Epoch: 17/30, step: 18/364, loss: 0.39898, accuracy: 0.83507\n",
            "Epoch: 17/30, step: 19/364, loss: 0.39736, accuracy: 0.84046\n",
            "Epoch: 17/30, step: 20/364, loss: 0.39811, accuracy: 0.83750\n",
            "Epoch: 17/30, step: 21/364, loss: 0.39772, accuracy: 0.83705\n",
            "Epoch: 17/30, step: 22/364, loss: 0.40043, accuracy: 0.83452\n",
            "Epoch: 17/30, step: 23/364, loss: 0.40323, accuracy: 0.83220\n",
            "Epoch: 17/30, step: 24/364, loss: 0.40353, accuracy: 0.83203\n",
            "Epoch: 17/30, step: 25/364, loss: 0.40307, accuracy: 0.83125\n",
            "Epoch: 17/30, step: 26/364, loss: 0.40255, accuracy: 0.82993\n",
            "Epoch: 17/30, step: 27/364, loss: 0.40451, accuracy: 0.82870\n",
            "Epoch: 17/30, step: 28/364, loss: 0.41005, accuracy: 0.82366\n",
            "Epoch: 17/30, step: 29/364, loss: 0.40641, accuracy: 0.82597\n",
            "Epoch: 17/30, step: 30/364, loss: 0.40517, accuracy: 0.82760\n",
            "Epoch: 17/30, step: 31/364, loss: 0.40398, accuracy: 0.82964\n",
            "Epoch: 17/30, step: 32/364, loss: 0.40379, accuracy: 0.82959\n",
            "Epoch: 17/30, step: 33/364, loss: 0.40411, accuracy: 0.82907\n",
            "Epoch: 17/30, step: 34/364, loss: 0.40472, accuracy: 0.82767\n",
            "Epoch: 17/30, step: 35/364, loss: 0.40411, accuracy: 0.82768\n",
            "Epoch: 17/30, step: 36/364, loss: 0.40547, accuracy: 0.82639\n",
            "Epoch: 17/30, step: 37/364, loss: 0.40476, accuracy: 0.82728\n",
            "Epoch: 17/30, step: 38/364, loss: 0.40425, accuracy: 0.82730\n",
            "Epoch: 17/30, step: 39/364, loss: 0.40371, accuracy: 0.82772\n",
            "Epoch: 17/30, step: 40/364, loss: 0.40509, accuracy: 0.82617\n",
            "Epoch: 17/30, step: 41/364, loss: 0.40244, accuracy: 0.82774\n",
            "Epoch: 17/30, step: 42/364, loss: 0.40128, accuracy: 0.82924\n",
            "Epoch: 17/30, step: 43/364, loss: 0.40198, accuracy: 0.82922\n",
            "Epoch: 17/30, step: 44/364, loss: 0.40052, accuracy: 0.83061\n",
            "Epoch: 17/30, step: 45/364, loss: 0.40050, accuracy: 0.83125\n",
            "Epoch: 17/30, step: 46/364, loss: 0.40013, accuracy: 0.83084\n",
            "Epoch: 17/30, step: 47/364, loss: 0.40086, accuracy: 0.82979\n",
            "Epoch: 17/30, step: 48/364, loss: 0.40005, accuracy: 0.83008\n",
            "Epoch: 17/30, step: 49/364, loss: 0.40095, accuracy: 0.82940\n",
            "Epoch: 17/30, step: 50/364, loss: 0.40202, accuracy: 0.82812\n",
            "Epoch: 17/30, step: 51/364, loss: 0.40395, accuracy: 0.82629\n",
            "Epoch: 17/30, step: 52/364, loss: 0.40316, accuracy: 0.82722\n",
            "Epoch: 17/30, step: 53/364, loss: 0.40282, accuracy: 0.82754\n",
            "Epoch: 17/30, step: 54/364, loss: 0.40129, accuracy: 0.82928\n",
            "Epoch: 17/30, step: 55/364, loss: 0.40180, accuracy: 0.82841\n",
            "Epoch: 17/30, step: 56/364, loss: 0.40347, accuracy: 0.82729\n",
            "Epoch: 17/30, step: 57/364, loss: 0.40428, accuracy: 0.82730\n",
            "Epoch: 17/30, step: 58/364, loss: 0.40374, accuracy: 0.82812\n",
            "Epoch: 17/30, step: 59/364, loss: 0.40345, accuracy: 0.82839\n",
            "Epoch: 17/30, step: 60/364, loss: 0.40268, accuracy: 0.82917\n",
            "Epoch: 17/30, step: 61/364, loss: 0.40147, accuracy: 0.82966\n",
            "Epoch: 17/30, step: 62/364, loss: 0.40230, accuracy: 0.82863\n",
            "Epoch: 17/30, step: 63/364, loss: 0.40323, accuracy: 0.82788\n",
            "Epoch: 17/30, step: 64/364, loss: 0.40117, accuracy: 0.82886\n",
            "Epoch: 17/30, step: 65/364, loss: 0.40032, accuracy: 0.82909\n",
            "Epoch: 17/30, step: 66/364, loss: 0.39872, accuracy: 0.83002\n",
            "Epoch: 17/30, step: 67/364, loss: 0.39810, accuracy: 0.82952\n",
            "Epoch: 17/30, step: 68/364, loss: 0.39979, accuracy: 0.82835\n",
            "Epoch: 17/30, step: 69/364, loss: 0.39981, accuracy: 0.82880\n",
            "Epoch: 17/30, step: 70/364, loss: 0.39958, accuracy: 0.82924\n",
            "Epoch: 17/30, step: 71/364, loss: 0.40085, accuracy: 0.82812\n",
            "Epoch: 17/30, step: 72/364, loss: 0.40091, accuracy: 0.82726\n",
            "Epoch: 17/30, step: 73/364, loss: 0.39998, accuracy: 0.82770\n",
            "Epoch: 17/30, step: 74/364, loss: 0.39993, accuracy: 0.82728\n",
            "Epoch: 17/30, step: 75/364, loss: 0.39872, accuracy: 0.82812\n",
            "Epoch: 17/30, step: 76/364, loss: 0.39817, accuracy: 0.82833\n",
            "Epoch: 17/30, step: 77/364, loss: 0.39803, accuracy: 0.82812\n",
            "Epoch: 17/30, step: 78/364, loss: 0.39716, accuracy: 0.82893\n",
            "Epoch: 17/30, step: 79/364, loss: 0.39703, accuracy: 0.82872\n",
            "Epoch: 17/30, step: 80/364, loss: 0.39728, accuracy: 0.82852\n",
            "Epoch: 17/30, step: 81/364, loss: 0.39732, accuracy: 0.82832\n",
            "Epoch: 17/30, step: 82/364, loss: 0.39832, accuracy: 0.82736\n",
            "Epoch: 17/30, step: 83/364, loss: 0.39730, accuracy: 0.82850\n",
            "Epoch: 17/30, step: 84/364, loss: 0.39743, accuracy: 0.82831\n",
            "Epoch: 17/30, step: 85/364, loss: 0.39843, accuracy: 0.82721\n",
            "Epoch: 17/30, step: 86/364, loss: 0.39899, accuracy: 0.82667\n",
            "Epoch: 17/30, step: 87/364, loss: 0.39891, accuracy: 0.82633\n",
            "Epoch: 17/30, step: 88/364, loss: 0.39816, accuracy: 0.82688\n",
            "Epoch: 17/30, step: 89/364, loss: 0.39759, accuracy: 0.82725\n",
            "Epoch: 17/30, step: 90/364, loss: 0.39701, accuracy: 0.82795\n",
            "Epoch: 17/30, step: 91/364, loss: 0.39747, accuracy: 0.82795\n",
            "Epoch: 17/30, step: 92/364, loss: 0.39754, accuracy: 0.82779\n",
            "Epoch: 17/30, step: 93/364, loss: 0.39734, accuracy: 0.82812\n",
            "Epoch: 17/30, step: 94/364, loss: 0.39714, accuracy: 0.82862\n",
            "Epoch: 17/30, step: 95/364, loss: 0.39679, accuracy: 0.82895\n",
            "Epoch: 17/30, step: 96/364, loss: 0.39668, accuracy: 0.82894\n",
            "Epoch: 17/30, step: 97/364, loss: 0.39677, accuracy: 0.82829\n",
            "Epoch: 17/30, step: 98/364, loss: 0.39705, accuracy: 0.82781\n",
            "Epoch: 17/30, step: 99/364, loss: 0.39685, accuracy: 0.82765\n",
            "Epoch: 17/30, step: 100/364, loss: 0.39757, accuracy: 0.82703\n",
            "Epoch: 17/30, step: 101/364, loss: 0.39774, accuracy: 0.82704\n",
            "Epoch: 17/30, step: 102/364, loss: 0.39790, accuracy: 0.82721\n",
            "Epoch: 17/30, step: 103/364, loss: 0.39738, accuracy: 0.82737\n",
            "Epoch: 17/30, step: 104/364, loss: 0.39671, accuracy: 0.82752\n",
            "Epoch: 17/30, step: 105/364, loss: 0.39707, accuracy: 0.82723\n",
            "Epoch: 17/30, step: 106/364, loss: 0.39655, accuracy: 0.82768\n",
            "Epoch: 17/30, step: 107/364, loss: 0.39651, accuracy: 0.82769\n",
            "Epoch: 17/30, step: 108/364, loss: 0.39704, accuracy: 0.82784\n",
            "Epoch: 17/30, step: 109/364, loss: 0.39650, accuracy: 0.82856\n",
            "Epoch: 17/30, step: 110/364, loss: 0.39778, accuracy: 0.82756\n",
            "Epoch: 17/30, step: 111/364, loss: 0.39795, accuracy: 0.82700\n",
            "Epoch: 17/30, step: 112/364, loss: 0.39804, accuracy: 0.82715\n",
            "Epoch: 17/30, step: 113/364, loss: 0.39737, accuracy: 0.82757\n",
            "Epoch: 17/30, step: 114/364, loss: 0.39851, accuracy: 0.82675\n",
            "Epoch: 17/30, step: 115/364, loss: 0.39746, accuracy: 0.82745\n",
            "Epoch: 17/30, step: 116/364, loss: 0.39756, accuracy: 0.82732\n",
            "Epoch: 17/30, step: 117/364, loss: 0.39713, accuracy: 0.82746\n",
            "Epoch: 17/30, step: 118/364, loss: 0.39756, accuracy: 0.82680\n",
            "Epoch: 17/30, step: 119/364, loss: 0.39708, accuracy: 0.82721\n",
            "Epoch: 17/30, step: 120/364, loss: 0.39713, accuracy: 0.82682\n",
            "Epoch: 17/30, step: 121/364, loss: 0.39712, accuracy: 0.82658\n",
            "Epoch: 17/30, step: 122/364, loss: 0.39750, accuracy: 0.82620\n",
            "Epoch: 17/30, step: 123/364, loss: 0.39722, accuracy: 0.82673\n",
            "Epoch: 17/30, step: 124/364, loss: 0.39729, accuracy: 0.82686\n",
            "Epoch: 17/30, step: 125/364, loss: 0.39690, accuracy: 0.82700\n",
            "Epoch: 17/30, step: 126/364, loss: 0.39684, accuracy: 0.82713\n",
            "Epoch: 17/30, step: 127/364, loss: 0.39706, accuracy: 0.82714\n",
            "Epoch: 17/30, step: 128/364, loss: 0.39668, accuracy: 0.82727\n",
            "Epoch: 17/30, step: 129/364, loss: 0.39650, accuracy: 0.82752\n",
            "Epoch: 17/30, step: 130/364, loss: 0.39661, accuracy: 0.82728\n",
            "Epoch: 17/30, step: 131/364, loss: 0.39648, accuracy: 0.82705\n",
            "Epoch: 17/30, step: 132/364, loss: 0.39646, accuracy: 0.82718\n",
            "Epoch: 17/30, step: 133/364, loss: 0.39637, accuracy: 0.82730\n",
            "Epoch: 17/30, step: 134/364, loss: 0.39616, accuracy: 0.82743\n",
            "Epoch: 17/30, step: 135/364, loss: 0.39603, accuracy: 0.82731\n",
            "Epoch: 17/30, step: 136/364, loss: 0.39669, accuracy: 0.82663\n",
            "Epoch: 17/30, step: 137/364, loss: 0.39682, accuracy: 0.82653\n",
            "Epoch: 17/30, step: 138/364, loss: 0.39683, accuracy: 0.82631\n",
            "Epoch: 17/30, step: 139/364, loss: 0.39658, accuracy: 0.82633\n",
            "Epoch: 17/30, step: 140/364, loss: 0.39624, accuracy: 0.82634\n",
            "Epoch: 17/30, step: 141/364, loss: 0.39641, accuracy: 0.82591\n",
            "Epoch: 17/30, step: 142/364, loss: 0.39632, accuracy: 0.82603\n",
            "Epoch: 17/30, step: 143/364, loss: 0.39605, accuracy: 0.82627\n",
            "Epoch: 17/30, step: 144/364, loss: 0.39556, accuracy: 0.82650\n",
            "Epoch: 17/30, step: 145/364, loss: 0.39496, accuracy: 0.82694\n",
            "Epoch: 17/30, step: 146/364, loss: 0.39473, accuracy: 0.82705\n",
            "Epoch: 17/30, step: 147/364, loss: 0.39485, accuracy: 0.82685\n",
            "Epoch: 17/30, step: 148/364, loss: 0.39491, accuracy: 0.82644\n",
            "Epoch: 17/30, step: 149/364, loss: 0.39458, accuracy: 0.82666\n",
            "Epoch: 17/30, step: 150/364, loss: 0.39527, accuracy: 0.82625\n",
            "Epoch: 17/30, step: 151/364, loss: 0.39509, accuracy: 0.82626\n",
            "Epoch: 17/30, step: 152/364, loss: 0.39421, accuracy: 0.82679\n",
            "Epoch: 17/30, step: 153/364, loss: 0.39379, accuracy: 0.82721\n",
            "Epoch: 17/30, step: 154/364, loss: 0.39378, accuracy: 0.82701\n",
            "Epoch: 17/30, step: 155/364, loss: 0.39382, accuracy: 0.82681\n",
            "Epoch: 17/30, step: 156/364, loss: 0.39419, accuracy: 0.82642\n",
            "Epoch: 17/30, step: 157/364, loss: 0.39504, accuracy: 0.82594\n",
            "Epoch: 17/30, step: 158/364, loss: 0.39544, accuracy: 0.82575\n",
            "Epoch: 17/30, step: 159/364, loss: 0.39599, accuracy: 0.82537\n",
            "Epoch: 17/30, step: 160/364, loss: 0.39575, accuracy: 0.82568\n",
            "Epoch: 17/30, step: 161/364, loss: 0.39557, accuracy: 0.82560\n",
            "Epoch: 17/30, step: 162/364, loss: 0.39558, accuracy: 0.82533\n",
            "Epoch: 17/30, step: 163/364, loss: 0.39575, accuracy: 0.82496\n",
            "Epoch: 17/30, step: 164/364, loss: 0.39548, accuracy: 0.82489\n",
            "Epoch: 17/30, step: 165/364, loss: 0.39499, accuracy: 0.82509\n",
            "Epoch: 17/30, step: 166/364, loss: 0.39447, accuracy: 0.82549\n",
            "Epoch: 17/30, step: 167/364, loss: 0.39481, accuracy: 0.82522\n",
            "Epoch: 17/30, step: 168/364, loss: 0.39504, accuracy: 0.82506\n",
            "Epoch: 17/30, step: 169/364, loss: 0.39519, accuracy: 0.82480\n",
            "Epoch: 17/30, step: 170/364, loss: 0.39548, accuracy: 0.82454\n",
            "Epoch: 17/30, step: 171/364, loss: 0.39587, accuracy: 0.82447\n",
            "Epoch: 17/30, step: 172/364, loss: 0.39546, accuracy: 0.82467\n",
            "Epoch: 17/30, step: 173/364, loss: 0.39540, accuracy: 0.82460\n",
            "Epoch: 17/30, step: 174/364, loss: 0.39567, accuracy: 0.82435\n",
            "Epoch: 17/30, step: 175/364, loss: 0.39502, accuracy: 0.82464\n",
            "Epoch: 17/30, step: 176/364, loss: 0.39485, accuracy: 0.82457\n",
            "Epoch: 17/30, step: 177/364, loss: 0.39501, accuracy: 0.82415\n",
            "Epoch: 17/30, step: 178/364, loss: 0.39509, accuracy: 0.82426\n",
            "Epoch: 17/30, step: 179/364, loss: 0.39547, accuracy: 0.82402\n",
            "Epoch: 17/30, step: 180/364, loss: 0.39507, accuracy: 0.82439\n",
            "Epoch: 17/30, step: 181/364, loss: 0.39559, accuracy: 0.82441\n",
            "Epoch: 17/30, step: 182/364, loss: 0.39578, accuracy: 0.82452\n",
            "Epoch: 17/30, step: 183/364, loss: 0.39619, accuracy: 0.82411\n",
            "Epoch: 17/30, step: 184/364, loss: 0.39671, accuracy: 0.82354\n",
            "Epoch: 17/30, step: 185/364, loss: 0.39710, accuracy: 0.82340\n",
            "Epoch: 17/30, step: 186/364, loss: 0.39701, accuracy: 0.82342\n",
            "Epoch: 17/30, step: 187/364, loss: 0.39656, accuracy: 0.82386\n",
            "Epoch: 17/30, step: 188/364, loss: 0.39626, accuracy: 0.82405\n",
            "Epoch: 17/30, step: 189/364, loss: 0.39587, accuracy: 0.82449\n",
            "Epoch: 17/30, step: 190/364, loss: 0.39575, accuracy: 0.82442\n",
            "Epoch: 17/30, step: 191/364, loss: 0.39543, accuracy: 0.82485\n",
            "Epoch: 17/30, step: 192/364, loss: 0.39530, accuracy: 0.82487\n",
            "Epoch: 17/30, step: 193/364, loss: 0.39523, accuracy: 0.82513\n",
            "Epoch: 17/30, step: 194/364, loss: 0.39535, accuracy: 0.82498\n",
            "Epoch: 17/30, step: 195/364, loss: 0.39514, accuracy: 0.82484\n",
            "Epoch: 17/30, step: 196/364, loss: 0.39565, accuracy: 0.82446\n",
            "Epoch: 17/30, step: 197/364, loss: 0.39566, accuracy: 0.82448\n",
            "Epoch: 17/30, step: 198/364, loss: 0.39552, accuracy: 0.82473\n",
            "Epoch: 17/30, step: 199/364, loss: 0.39561, accuracy: 0.82475\n",
            "Epoch: 17/30, step: 200/364, loss: 0.39564, accuracy: 0.82477\n",
            "Epoch: 17/30, step: 201/364, loss: 0.39597, accuracy: 0.82463\n",
            "Epoch: 17/30, step: 202/364, loss: 0.39643, accuracy: 0.82441\n",
            "Epoch: 17/30, step: 203/364, loss: 0.39613, accuracy: 0.82489\n",
            "Epoch: 17/30, step: 204/364, loss: 0.39584, accuracy: 0.82521\n",
            "Epoch: 17/30, step: 205/364, loss: 0.39554, accuracy: 0.82569\n",
            "Epoch: 17/30, step: 206/364, loss: 0.39539, accuracy: 0.82585\n",
            "Epoch: 17/30, step: 207/364, loss: 0.39521, accuracy: 0.82601\n",
            "Epoch: 17/30, step: 208/364, loss: 0.39485, accuracy: 0.82640\n",
            "Epoch: 17/30, step: 209/364, loss: 0.39498, accuracy: 0.82641\n",
            "Epoch: 17/30, step: 210/364, loss: 0.39457, accuracy: 0.82671\n",
            "Epoch: 17/30, step: 211/364, loss: 0.39428, accuracy: 0.82679\n",
            "Epoch: 17/30, step: 212/364, loss: 0.39411, accuracy: 0.82695\n",
            "Epoch: 17/30, step: 213/364, loss: 0.39374, accuracy: 0.82732\n",
            "Epoch: 17/30, step: 214/364, loss: 0.39414, accuracy: 0.82696\n",
            "Epoch: 17/30, step: 215/364, loss: 0.39404, accuracy: 0.82682\n",
            "Epoch: 17/30, step: 216/364, loss: 0.39418, accuracy: 0.82646\n",
            "Epoch: 17/30, step: 217/364, loss: 0.39406, accuracy: 0.82668\n",
            "Epoch: 17/30, step: 218/364, loss: 0.39400, accuracy: 0.82669\n",
            "Epoch: 17/30, step: 219/364, loss: 0.39400, accuracy: 0.82648\n",
            "Epoch: 17/30, step: 220/364, loss: 0.39367, accuracy: 0.82663\n",
            "Epoch: 17/30, step: 221/364, loss: 0.39371, accuracy: 0.82657\n",
            "Epoch: 17/30, step: 222/364, loss: 0.39344, accuracy: 0.82693\n",
            "Epoch: 17/30, step: 223/364, loss: 0.39348, accuracy: 0.82700\n",
            "Epoch: 17/30, step: 224/364, loss: 0.39318, accuracy: 0.82743\n",
            "Epoch: 17/30, step: 225/364, loss: 0.39323, accuracy: 0.82743\n",
            "Epoch: 17/30, step: 226/364, loss: 0.39316, accuracy: 0.82730\n",
            "Epoch: 17/30, step: 227/364, loss: 0.39304, accuracy: 0.82744\n",
            "Epoch: 17/30, step: 228/364, loss: 0.39318, accuracy: 0.82744\n",
            "Epoch: 17/30, step: 229/364, loss: 0.39395, accuracy: 0.82703\n",
            "Epoch: 17/30, step: 230/364, loss: 0.39427, accuracy: 0.82690\n",
            "Epoch: 17/30, step: 231/364, loss: 0.39406, accuracy: 0.82725\n",
            "Epoch: 17/30, step: 232/364, loss: 0.39406, accuracy: 0.82718\n",
            "Epoch: 17/30, step: 233/364, loss: 0.39378, accuracy: 0.82745\n",
            "Epoch: 17/30, step: 234/364, loss: 0.39385, accuracy: 0.82732\n",
            "Epoch: 17/30, step: 235/364, loss: 0.39413, accuracy: 0.82733\n",
            "Epoch: 17/30, step: 236/364, loss: 0.39421, accuracy: 0.82713\n",
            "Epoch: 17/30, step: 237/364, loss: 0.39444, accuracy: 0.82667\n",
            "Epoch: 17/30, step: 238/364, loss: 0.39475, accuracy: 0.82668\n",
            "Epoch: 17/30, step: 239/364, loss: 0.39477, accuracy: 0.82669\n",
            "Epoch: 17/30, step: 240/364, loss: 0.39502, accuracy: 0.82637\n",
            "Epoch: 17/30, step: 241/364, loss: 0.39512, accuracy: 0.82618\n",
            "Epoch: 17/30, step: 242/364, loss: 0.39493, accuracy: 0.82638\n",
            "Epoch: 17/30, step: 243/364, loss: 0.39542, accuracy: 0.82607\n",
            "Epoch: 17/30, step: 244/364, loss: 0.39529, accuracy: 0.82620\n",
            "Epoch: 17/30, step: 245/364, loss: 0.39520, accuracy: 0.82647\n",
            "Epoch: 17/30, step: 246/364, loss: 0.39501, accuracy: 0.82654\n",
            "Epoch: 17/30, step: 247/364, loss: 0.39447, accuracy: 0.82699\n",
            "Epoch: 17/30, step: 248/364, loss: 0.39425, accuracy: 0.82718\n",
            "Epoch: 17/30, step: 249/364, loss: 0.39454, accuracy: 0.82693\n",
            "Epoch: 17/30, step: 250/364, loss: 0.39421, accuracy: 0.82713\n",
            "Epoch: 17/30, step: 251/364, loss: 0.39410, accuracy: 0.82719\n",
            "Epoch: 17/30, step: 252/364, loss: 0.39377, accuracy: 0.82744\n",
            "Epoch: 17/30, step: 253/364, loss: 0.39392, accuracy: 0.82720\n",
            "Epoch: 17/30, step: 254/364, loss: 0.39359, accuracy: 0.82751\n",
            "Epoch: 17/30, step: 255/364, loss: 0.39330, accuracy: 0.82751\n",
            "Epoch: 17/30, step: 256/364, loss: 0.39292, accuracy: 0.82770\n",
            "Epoch: 17/30, step: 257/364, loss: 0.39286, accuracy: 0.82764\n",
            "Epoch: 17/30, step: 258/364, loss: 0.39297, accuracy: 0.82764\n",
            "Epoch: 17/30, step: 259/364, loss: 0.39309, accuracy: 0.82746\n",
            "Epoch: 17/30, step: 260/364, loss: 0.39294, accuracy: 0.82764\n",
            "Epoch: 17/30, step: 261/364, loss: 0.39345, accuracy: 0.82747\n",
            "Epoch: 17/30, step: 262/364, loss: 0.39349, accuracy: 0.82753\n",
            "Epoch: 17/30, step: 263/364, loss: 0.39319, accuracy: 0.82771\n",
            "Epoch: 17/30, step: 264/364, loss: 0.39337, accuracy: 0.82753\n",
            "Epoch: 17/30, step: 265/364, loss: 0.39331, accuracy: 0.82771\n",
            "Epoch: 17/30, step: 266/364, loss: 0.39342, accuracy: 0.82760\n",
            "Epoch: 17/30, step: 267/364, loss: 0.39339, accuracy: 0.82748\n",
            "Epoch: 17/30, step: 268/364, loss: 0.39352, accuracy: 0.82725\n",
            "Epoch: 17/30, step: 269/364, loss: 0.39329, accuracy: 0.82749\n",
            "Epoch: 17/30, step: 270/364, loss: 0.39288, accuracy: 0.82766\n",
            "Epoch: 17/30, step: 271/364, loss: 0.39275, accuracy: 0.82766\n",
            "Epoch: 17/30, step: 272/364, loss: 0.39295, accuracy: 0.82772\n",
            "Epoch: 17/30, step: 273/364, loss: 0.39284, accuracy: 0.82778\n",
            "Epoch: 17/30, step: 274/364, loss: 0.39297, accuracy: 0.82750\n",
            "Epoch: 17/30, step: 275/364, loss: 0.39265, accuracy: 0.82778\n",
            "Epoch: 17/30, step: 276/364, loss: 0.39307, accuracy: 0.82739\n",
            "Epoch: 17/30, step: 277/364, loss: 0.39301, accuracy: 0.82750\n",
            "Epoch: 17/30, step: 278/364, loss: 0.39284, accuracy: 0.82756\n",
            "Epoch: 17/30, step: 279/364, loss: 0.39304, accuracy: 0.82734\n",
            "Epoch: 17/30, step: 280/364, loss: 0.39281, accuracy: 0.82751\n",
            "Epoch: 17/30, step: 281/364, loss: 0.39294, accuracy: 0.82735\n",
            "Epoch: 17/30, step: 282/364, loss: 0.39271, accuracy: 0.82757\n",
            "Epoch: 17/30, step: 283/364, loss: 0.39260, accuracy: 0.82752\n",
            "Epoch: 17/30, step: 284/364, loss: 0.39253, accuracy: 0.82763\n",
            "Epoch: 17/30, step: 285/364, loss: 0.39252, accuracy: 0.82774\n",
            "Epoch: 17/30, step: 286/364, loss: 0.39263, accuracy: 0.82752\n",
            "Epoch: 17/30, step: 287/364, loss: 0.39280, accuracy: 0.82747\n",
            "Epoch: 17/30, step: 288/364, loss: 0.39261, accuracy: 0.82764\n",
            "Epoch: 17/30, step: 289/364, loss: 0.39255, accuracy: 0.82769\n",
            "Epoch: 17/30, step: 290/364, loss: 0.39250, accuracy: 0.82780\n",
            "Epoch: 17/30, step: 291/364, loss: 0.39297, accuracy: 0.82757\n",
            "Epoch: 17/30, train loss: 0.39297, train accuracy: 0.82757, valid loss: 0.60798, valid accuracy: 0.69755\n",
            "Epoch: 18/30, step: 1/364, loss: 0.36728, accuracy: 0.84375\n",
            "Epoch: 18/30, step: 2/364, loss: 0.38091, accuracy: 0.84375\n",
            "Epoch: 18/30, step: 3/364, loss: 0.38878, accuracy: 0.83854\n",
            "Epoch: 18/30, step: 4/364, loss: 0.39100, accuracy: 0.83203\n",
            "Epoch: 18/30, step: 5/364, loss: 0.38433, accuracy: 0.83750\n",
            "Epoch: 18/30, step: 6/364, loss: 0.39368, accuracy: 0.82552\n",
            "Epoch: 18/30, step: 7/364, loss: 0.38419, accuracy: 0.83259\n",
            "Epoch: 18/30, step: 8/364, loss: 0.39331, accuracy: 0.83008\n",
            "Epoch: 18/30, step: 9/364, loss: 0.39656, accuracy: 0.82639\n",
            "Epoch: 18/30, step: 10/364, loss: 0.38910, accuracy: 0.83281\n",
            "Epoch: 18/30, step: 11/364, loss: 0.39229, accuracy: 0.83239\n",
            "Epoch: 18/30, step: 12/364, loss: 0.39409, accuracy: 0.83073\n",
            "Epoch: 18/30, step: 13/364, loss: 0.38907, accuracy: 0.83534\n",
            "Epoch: 18/30, step: 14/364, loss: 0.38969, accuracy: 0.83594\n",
            "Epoch: 18/30, step: 15/364, loss: 0.38716, accuracy: 0.83750\n",
            "Epoch: 18/30, step: 16/364, loss: 0.38958, accuracy: 0.83691\n",
            "Epoch: 18/30, step: 17/364, loss: 0.39008, accuracy: 0.83548\n",
            "Epoch: 18/30, step: 18/364, loss: 0.39011, accuracy: 0.83420\n",
            "Epoch: 18/30, step: 19/364, loss: 0.39232, accuracy: 0.83306\n",
            "Epoch: 18/30, step: 20/364, loss: 0.38582, accuracy: 0.84062\n",
            "Epoch: 18/30, step: 21/364, loss: 0.38478, accuracy: 0.84003\n",
            "Epoch: 18/30, step: 22/364, loss: 0.38861, accuracy: 0.83594\n",
            "Epoch: 18/30, step: 23/364, loss: 0.38519, accuracy: 0.83832\n",
            "Epoch: 18/30, step: 24/364, loss: 0.38484, accuracy: 0.83854\n",
            "Epoch: 18/30, step: 25/364, loss: 0.38295, accuracy: 0.83938\n",
            "Epoch: 18/30, step: 26/364, loss: 0.38484, accuracy: 0.84014\n",
            "Epoch: 18/30, step: 27/364, loss: 0.38445, accuracy: 0.83970\n",
            "Epoch: 18/30, step: 28/364, loss: 0.38414, accuracy: 0.83929\n",
            "Epoch: 18/30, step: 29/364, loss: 0.38406, accuracy: 0.83890\n",
            "Epoch: 18/30, step: 30/364, loss: 0.38189, accuracy: 0.83958\n",
            "Epoch: 18/30, step: 31/364, loss: 0.38069, accuracy: 0.83972\n",
            "Epoch: 18/30, step: 32/364, loss: 0.37972, accuracy: 0.84082\n",
            "Epoch: 18/30, step: 33/364, loss: 0.37944, accuracy: 0.83996\n",
            "Epoch: 18/30, step: 34/364, loss: 0.37776, accuracy: 0.84007\n",
            "Epoch: 18/30, step: 35/364, loss: 0.37634, accuracy: 0.84018\n",
            "Epoch: 18/30, step: 36/364, loss: 0.37654, accuracy: 0.83898\n",
            "Epoch: 18/30, step: 37/364, loss: 0.37760, accuracy: 0.83742\n",
            "Epoch: 18/30, step: 38/364, loss: 0.37842, accuracy: 0.83717\n",
            "Epoch: 18/30, step: 39/364, loss: 0.37816, accuracy: 0.83854\n",
            "Epoch: 18/30, step: 40/364, loss: 0.37868, accuracy: 0.83750\n",
            "Epoch: 18/30, step: 41/364, loss: 0.38133, accuracy: 0.83613\n",
            "Epoch: 18/30, step: 42/364, loss: 0.38221, accuracy: 0.83519\n",
            "Epoch: 18/30, step: 43/364, loss: 0.38559, accuracy: 0.83212\n",
            "Epoch: 18/30, step: 44/364, loss: 0.38664, accuracy: 0.83132\n",
            "Epoch: 18/30, step: 45/364, loss: 0.38733, accuracy: 0.83056\n",
            "Epoch: 18/30, step: 46/364, loss: 0.38665, accuracy: 0.83084\n",
            "Epoch: 18/30, step: 47/364, loss: 0.38535, accuracy: 0.83178\n",
            "Epoch: 18/30, step: 48/364, loss: 0.38559, accuracy: 0.83105\n",
            "Epoch: 18/30, step: 49/364, loss: 0.38569, accuracy: 0.83036\n",
            "Epoch: 18/30, step: 50/364, loss: 0.38648, accuracy: 0.83000\n",
            "Epoch: 18/30, step: 51/364, loss: 0.38815, accuracy: 0.82812\n",
            "Epoch: 18/30, step: 52/364, loss: 0.38544, accuracy: 0.82933\n",
            "Epoch: 18/30, step: 53/364, loss: 0.38534, accuracy: 0.82960\n",
            "Epoch: 18/30, step: 54/364, loss: 0.38641, accuracy: 0.82957\n",
            "Epoch: 18/30, step: 55/364, loss: 0.38691, accuracy: 0.82841\n",
            "Epoch: 18/30, step: 56/364, loss: 0.38563, accuracy: 0.82952\n",
            "Epoch: 18/30, step: 57/364, loss: 0.38400, accuracy: 0.83087\n",
            "Epoch: 18/30, step: 58/364, loss: 0.38567, accuracy: 0.82893\n",
            "Epoch: 18/30, step: 59/364, loss: 0.38522, accuracy: 0.82892\n",
            "Epoch: 18/30, step: 60/364, loss: 0.38496, accuracy: 0.82891\n",
            "Epoch: 18/30, step: 61/364, loss: 0.38521, accuracy: 0.82966\n",
            "Epoch: 18/30, step: 62/364, loss: 0.38574, accuracy: 0.82939\n",
            "Epoch: 18/30, step: 63/364, loss: 0.38467, accuracy: 0.83011\n",
            "Epoch: 18/30, step: 64/364, loss: 0.38532, accuracy: 0.82935\n",
            "Epoch: 18/30, step: 65/364, loss: 0.38524, accuracy: 0.82861\n",
            "Epoch: 18/30, step: 66/364, loss: 0.38665, accuracy: 0.82812\n",
            "Epoch: 18/30, step: 67/364, loss: 0.38923, accuracy: 0.82649\n",
            "Epoch: 18/30, step: 68/364, loss: 0.38999, accuracy: 0.82606\n",
            "Epoch: 18/30, step: 69/364, loss: 0.38919, accuracy: 0.82677\n",
            "Epoch: 18/30, step: 70/364, loss: 0.38822, accuracy: 0.82790\n",
            "Epoch: 18/30, step: 71/364, loss: 0.38883, accuracy: 0.82790\n",
            "Epoch: 18/30, step: 72/364, loss: 0.38742, accuracy: 0.82943\n",
            "Epoch: 18/30, step: 73/364, loss: 0.38619, accuracy: 0.83048\n",
            "Epoch: 18/30, step: 74/364, loss: 0.38578, accuracy: 0.83066\n",
            "Epoch: 18/30, step: 75/364, loss: 0.38590, accuracy: 0.83042\n",
            "Epoch: 18/30, step: 76/364, loss: 0.38524, accuracy: 0.83100\n",
            "Epoch: 18/30, step: 77/364, loss: 0.38447, accuracy: 0.83097\n",
            "Epoch: 18/30, step: 78/364, loss: 0.38413, accuracy: 0.83093\n",
            "Epoch: 18/30, step: 79/364, loss: 0.38390, accuracy: 0.83149\n",
            "Epoch: 18/30, step: 80/364, loss: 0.38432, accuracy: 0.83086\n",
            "Epoch: 18/30, step: 81/364, loss: 0.38394, accuracy: 0.83063\n",
            "Epoch: 18/30, step: 82/364, loss: 0.38436, accuracy: 0.83079\n",
            "Epoch: 18/30, step: 83/364, loss: 0.38295, accuracy: 0.83133\n",
            "Epoch: 18/30, step: 84/364, loss: 0.38284, accuracy: 0.83185\n",
            "Epoch: 18/30, step: 85/364, loss: 0.38383, accuracy: 0.83125\n",
            "Epoch: 18/30, step: 86/364, loss: 0.38443, accuracy: 0.83121\n",
            "Epoch: 18/30, step: 87/364, loss: 0.38397, accuracy: 0.83172\n",
            "Epoch: 18/30, step: 88/364, loss: 0.38367, accuracy: 0.83185\n",
            "Epoch: 18/30, step: 89/364, loss: 0.38319, accuracy: 0.83251\n",
            "Epoch: 18/30, step: 90/364, loss: 0.38310, accuracy: 0.83212\n",
            "Epoch: 18/30, step: 91/364, loss: 0.38344, accuracy: 0.83122\n",
            "Epoch: 18/30, step: 92/364, loss: 0.38385, accuracy: 0.83067\n",
            "Epoch: 18/30, step: 93/364, loss: 0.38309, accuracy: 0.83149\n",
            "Epoch: 18/30, step: 94/364, loss: 0.38238, accuracy: 0.83211\n",
            "Epoch: 18/30, step: 95/364, loss: 0.38211, accuracy: 0.83240\n",
            "Epoch: 18/30, step: 96/364, loss: 0.38216, accuracy: 0.83252\n",
            "Epoch: 18/30, step: 97/364, loss: 0.38177, accuracy: 0.83264\n",
            "Epoch: 18/30, step: 98/364, loss: 0.38220, accuracy: 0.83211\n",
            "Epoch: 18/30, step: 99/364, loss: 0.38234, accuracy: 0.83254\n",
            "Epoch: 18/30, step: 100/364, loss: 0.38311, accuracy: 0.83234\n",
            "Epoch: 18/30, step: 101/364, loss: 0.38292, accuracy: 0.83261\n",
            "Epoch: 18/30, step: 102/364, loss: 0.38356, accuracy: 0.83241\n",
            "Epoch: 18/30, step: 103/364, loss: 0.38381, accuracy: 0.83222\n",
            "Epoch: 18/30, step: 104/364, loss: 0.38369, accuracy: 0.83248\n",
            "Epoch: 18/30, step: 105/364, loss: 0.38331, accuracy: 0.83304\n",
            "Epoch: 18/30, step: 106/364, loss: 0.38249, accuracy: 0.83402\n",
            "Epoch: 18/30, step: 107/364, loss: 0.38301, accuracy: 0.83382\n",
            "Epoch: 18/30, step: 108/364, loss: 0.38289, accuracy: 0.83377\n",
            "Epoch: 18/30, step: 109/364, loss: 0.38266, accuracy: 0.83386\n",
            "Epoch: 18/30, step: 110/364, loss: 0.38269, accuracy: 0.83395\n",
            "Epoch: 18/30, step: 111/364, loss: 0.38286, accuracy: 0.83404\n",
            "Epoch: 18/30, step: 112/364, loss: 0.38215, accuracy: 0.83440\n",
            "Epoch: 18/30, step: 113/364, loss: 0.38180, accuracy: 0.83449\n",
            "Epoch: 18/30, step: 114/364, loss: 0.38142, accuracy: 0.83457\n",
            "Epoch: 18/30, step: 115/364, loss: 0.38121, accuracy: 0.83492\n",
            "Epoch: 18/30, step: 116/364, loss: 0.38106, accuracy: 0.83486\n",
            "Epoch: 18/30, step: 117/364, loss: 0.38100, accuracy: 0.83494\n",
            "Epoch: 18/30, step: 118/364, loss: 0.38056, accuracy: 0.83528\n",
            "Epoch: 18/30, step: 119/364, loss: 0.38059, accuracy: 0.83495\n",
            "Epoch: 18/30, step: 120/364, loss: 0.38103, accuracy: 0.83516\n",
            "Epoch: 18/30, step: 121/364, loss: 0.38054, accuracy: 0.83561\n",
            "Epoch: 18/30, step: 122/364, loss: 0.38002, accuracy: 0.83607\n",
            "Epoch: 18/30, step: 123/364, loss: 0.37966, accuracy: 0.83626\n",
            "Epoch: 18/30, step: 124/364, loss: 0.37916, accuracy: 0.83657\n",
            "Epoch: 18/30, step: 125/364, loss: 0.37978, accuracy: 0.83562\n",
            "Epoch: 18/30, step: 126/364, loss: 0.37941, accuracy: 0.83519\n",
            "Epoch: 18/30, step: 127/364, loss: 0.37954, accuracy: 0.83526\n",
            "Epoch: 18/30, step: 128/364, loss: 0.38009, accuracy: 0.83459\n",
            "Epoch: 18/30, step: 129/364, loss: 0.38065, accuracy: 0.83394\n",
            "Epoch: 18/30, step: 130/364, loss: 0.38082, accuracy: 0.83365\n",
            "Epoch: 18/30, step: 131/364, loss: 0.38124, accuracy: 0.83337\n",
            "Epoch: 18/30, step: 132/364, loss: 0.38108, accuracy: 0.83369\n",
            "Epoch: 18/30, step: 133/364, loss: 0.38066, accuracy: 0.83412\n",
            "Epoch: 18/30, step: 134/364, loss: 0.38055, accuracy: 0.83384\n",
            "Epoch: 18/30, step: 135/364, loss: 0.38084, accuracy: 0.83356\n",
            "Epoch: 18/30, step: 136/364, loss: 0.38135, accuracy: 0.83318\n",
            "Epoch: 18/30, step: 137/364, loss: 0.38182, accuracy: 0.83303\n",
            "Epoch: 18/30, step: 138/364, loss: 0.38156, accuracy: 0.83299\n",
            "Epoch: 18/30, step: 139/364, loss: 0.38098, accuracy: 0.83341\n",
            "Epoch: 18/30, step: 140/364, loss: 0.38082, accuracy: 0.83326\n",
            "Epoch: 18/30, step: 141/364, loss: 0.38062, accuracy: 0.83344\n",
            "Epoch: 18/30, step: 142/364, loss: 0.38085, accuracy: 0.83352\n",
            "Epoch: 18/30, step: 143/364, loss: 0.38072, accuracy: 0.83348\n",
            "Epoch: 18/30, step: 144/364, loss: 0.38087, accuracy: 0.83333\n",
            "Epoch: 18/30, step: 145/364, loss: 0.38076, accuracy: 0.83351\n",
            "Epoch: 18/30, step: 146/364, loss: 0.38021, accuracy: 0.83390\n",
            "Epoch: 18/30, step: 147/364, loss: 0.38058, accuracy: 0.83397\n",
            "Epoch: 18/30, step: 148/364, loss: 0.38070, accuracy: 0.83383\n",
            "Epoch: 18/30, step: 149/364, loss: 0.38035, accuracy: 0.83421\n",
            "Epoch: 18/30, step: 150/364, loss: 0.38048, accuracy: 0.83438\n",
            "Epoch: 18/30, step: 151/364, loss: 0.38037, accuracy: 0.83454\n",
            "Epoch: 18/30, step: 152/364, loss: 0.38092, accuracy: 0.83419\n",
            "Epoch: 18/30, step: 153/364, loss: 0.38109, accuracy: 0.83415\n",
            "Epoch: 18/30, step: 154/364, loss: 0.38126, accuracy: 0.83401\n",
            "Epoch: 18/30, step: 155/364, loss: 0.38110, accuracy: 0.83417\n",
            "Epoch: 18/30, step: 156/364, loss: 0.38063, accuracy: 0.83433\n",
            "Epoch: 18/30, step: 157/364, loss: 0.38031, accuracy: 0.83469\n",
            "Epoch: 18/30, step: 158/364, loss: 0.38001, accuracy: 0.83515\n",
            "Epoch: 18/30, step: 159/364, loss: 0.37969, accuracy: 0.83550\n",
            "Epoch: 18/30, step: 160/364, loss: 0.37935, accuracy: 0.83574\n",
            "Epoch: 18/30, step: 161/364, loss: 0.37932, accuracy: 0.83540\n",
            "Epoch: 18/30, step: 162/364, loss: 0.38063, accuracy: 0.83459\n",
            "Epoch: 18/30, step: 163/364, loss: 0.38087, accuracy: 0.83436\n",
            "Epoch: 18/30, step: 164/364, loss: 0.38080, accuracy: 0.83432\n",
            "Epoch: 18/30, step: 165/364, loss: 0.38059, accuracy: 0.83419\n",
            "Epoch: 18/30, step: 166/364, loss: 0.38008, accuracy: 0.83443\n",
            "Epoch: 18/30, step: 167/364, loss: 0.38031, accuracy: 0.83430\n",
            "Epoch: 18/30, step: 168/364, loss: 0.38038, accuracy: 0.83426\n",
            "Epoch: 18/30, step: 169/364, loss: 0.38030, accuracy: 0.83450\n",
            "Epoch: 18/30, step: 170/364, loss: 0.38106, accuracy: 0.83382\n",
            "Epoch: 18/30, step: 171/364, loss: 0.38140, accuracy: 0.83324\n",
            "Epoch: 18/30, step: 172/364, loss: 0.38158, accuracy: 0.83312\n",
            "Epoch: 18/30, step: 173/364, loss: 0.38095, accuracy: 0.83363\n",
            "Epoch: 18/30, step: 174/364, loss: 0.38146, accuracy: 0.83333\n",
            "Epoch: 18/30, step: 175/364, loss: 0.38132, accuracy: 0.83366\n",
            "Epoch: 18/30, step: 176/364, loss: 0.38106, accuracy: 0.83381\n",
            "Epoch: 18/30, step: 177/364, loss: 0.38083, accuracy: 0.83395\n",
            "Epoch: 18/30, step: 178/364, loss: 0.38038, accuracy: 0.83436\n",
            "Epoch: 18/30, step: 179/364, loss: 0.38014, accuracy: 0.83467\n",
            "Epoch: 18/30, step: 180/364, loss: 0.38016, accuracy: 0.83481\n",
            "Epoch: 18/30, step: 181/364, loss: 0.37982, accuracy: 0.83503\n",
            "Epoch: 18/30, step: 182/364, loss: 0.37944, accuracy: 0.83534\n",
            "Epoch: 18/30, step: 183/364, loss: 0.37957, accuracy: 0.83504\n",
            "Epoch: 18/30, step: 184/364, loss: 0.37987, accuracy: 0.83483\n",
            "Epoch: 18/30, step: 185/364, loss: 0.37954, accuracy: 0.83522\n",
            "Epoch: 18/30, step: 186/364, loss: 0.37931, accuracy: 0.83510\n",
            "Epoch: 18/30, step: 187/364, loss: 0.37927, accuracy: 0.83498\n",
            "Epoch: 18/30, step: 188/364, loss: 0.37910, accuracy: 0.83511\n",
            "Epoch: 18/30, step: 189/364, loss: 0.37899, accuracy: 0.83515\n",
            "Epoch: 18/30, step: 190/364, loss: 0.37924, accuracy: 0.83503\n",
            "Epoch: 18/30, step: 191/364, loss: 0.37935, accuracy: 0.83483\n",
            "Epoch: 18/30, step: 192/364, loss: 0.37934, accuracy: 0.83488\n",
            "Epoch: 18/30, step: 193/364, loss: 0.37900, accuracy: 0.83509\n",
            "Epoch: 18/30, step: 194/364, loss: 0.37855, accuracy: 0.83537\n",
            "Epoch: 18/30, step: 195/364, loss: 0.37864, accuracy: 0.83510\n",
            "Epoch: 18/30, step: 196/364, loss: 0.37890, accuracy: 0.83498\n",
            "Epoch: 18/30, step: 197/364, loss: 0.37943, accuracy: 0.83479\n",
            "Epoch: 18/30, step: 198/364, loss: 0.37909, accuracy: 0.83483\n",
            "Epoch: 18/30, step: 199/364, loss: 0.37901, accuracy: 0.83488\n",
            "Epoch: 18/30, step: 200/364, loss: 0.37832, accuracy: 0.83539\n",
            "Epoch: 18/30, step: 201/364, loss: 0.37760, accuracy: 0.83605\n",
            "Epoch: 18/30, step: 202/364, loss: 0.37756, accuracy: 0.83617\n",
            "Epoch: 18/30, step: 203/364, loss: 0.37743, accuracy: 0.83613\n",
            "Epoch: 18/30, step: 204/364, loss: 0.37757, accuracy: 0.83601\n",
            "Epoch: 18/30, step: 205/364, loss: 0.37740, accuracy: 0.83613\n",
            "Epoch: 18/30, step: 206/364, loss: 0.37765, accuracy: 0.83579\n",
            "Epoch: 18/30, step: 207/364, loss: 0.37727, accuracy: 0.83605\n",
            "Epoch: 18/30, step: 208/364, loss: 0.37691, accuracy: 0.83631\n",
            "Epoch: 18/30, step: 209/364, loss: 0.37708, accuracy: 0.83620\n",
            "Epoch: 18/30, step: 210/364, loss: 0.37742, accuracy: 0.83586\n",
            "Epoch: 18/30, step: 211/364, loss: 0.37732, accuracy: 0.83575\n",
            "Epoch: 18/30, step: 212/364, loss: 0.37705, accuracy: 0.83586\n",
            "Epoch: 18/30, step: 213/364, loss: 0.37747, accuracy: 0.83575\n",
            "Epoch: 18/30, step: 214/364, loss: 0.37728, accuracy: 0.83601\n",
            "Epoch: 18/30, step: 215/364, loss: 0.37749, accuracy: 0.83597\n",
            "Epoch: 18/30, step: 216/364, loss: 0.37695, accuracy: 0.83623\n",
            "Epoch: 18/30, step: 217/364, loss: 0.37641, accuracy: 0.83648\n",
            "Epoch: 18/30, step: 218/364, loss: 0.37656, accuracy: 0.83630\n",
            "Epoch: 18/30, step: 219/364, loss: 0.37656, accuracy: 0.83633\n",
            "Epoch: 18/30, step: 220/364, loss: 0.37619, accuracy: 0.83658\n",
            "Epoch: 18/30, step: 221/364, loss: 0.37619, accuracy: 0.83668\n",
            "Epoch: 18/30, step: 222/364, loss: 0.37656, accuracy: 0.83636\n",
            "Epoch: 18/30, step: 223/364, loss: 0.37627, accuracy: 0.83646\n",
            "Epoch: 18/30, step: 224/364, loss: 0.37617, accuracy: 0.83650\n",
            "Epoch: 18/30, step: 225/364, loss: 0.37591, accuracy: 0.83688\n",
            "Epoch: 18/30, step: 226/364, loss: 0.37580, accuracy: 0.83711\n",
            "Epoch: 18/30, step: 227/364, loss: 0.37537, accuracy: 0.83742\n",
            "Epoch: 18/30, step: 228/364, loss: 0.37552, accuracy: 0.83738\n",
            "Epoch: 18/30, step: 229/364, loss: 0.37540, accuracy: 0.83754\n",
            "Epoch: 18/30, step: 230/364, loss: 0.37540, accuracy: 0.83764\n",
            "Epoch: 18/30, step: 231/364, loss: 0.37495, accuracy: 0.83807\n",
            "Epoch: 18/30, step: 232/364, loss: 0.37503, accuracy: 0.83823\n",
            "Epoch: 18/30, step: 233/364, loss: 0.37502, accuracy: 0.83805\n",
            "Epoch: 18/30, step: 234/364, loss: 0.37500, accuracy: 0.83807\n",
            "Epoch: 18/30, step: 235/364, loss: 0.37492, accuracy: 0.83816\n",
            "Epoch: 18/30, step: 236/364, loss: 0.37483, accuracy: 0.83825\n",
            "Epoch: 18/30, step: 237/364, loss: 0.37501, accuracy: 0.83795\n",
            "Epoch: 18/30, step: 238/364, loss: 0.37489, accuracy: 0.83817\n",
            "Epoch: 18/30, step: 239/364, loss: 0.37491, accuracy: 0.83832\n",
            "Epoch: 18/30, step: 240/364, loss: 0.37453, accuracy: 0.83861\n",
            "Epoch: 18/30, step: 241/364, loss: 0.37478, accuracy: 0.83856\n",
            "Epoch: 18/30, step: 242/364, loss: 0.37481, accuracy: 0.83858\n",
            "Epoch: 18/30, step: 243/364, loss: 0.37485, accuracy: 0.83854\n",
            "Epoch: 18/30, step: 244/364, loss: 0.37445, accuracy: 0.83882\n",
            "Epoch: 18/30, step: 245/364, loss: 0.37401, accuracy: 0.83935\n",
            "Epoch: 18/30, step: 246/364, loss: 0.37345, accuracy: 0.83975\n",
            "Epoch: 18/30, step: 247/364, loss: 0.37389, accuracy: 0.83939\n",
            "Epoch: 18/30, step: 248/364, loss: 0.37391, accuracy: 0.83934\n",
            "Epoch: 18/30, step: 249/364, loss: 0.37410, accuracy: 0.83929\n",
            "Epoch: 18/30, step: 250/364, loss: 0.37458, accuracy: 0.83894\n",
            "Epoch: 18/30, step: 251/364, loss: 0.37476, accuracy: 0.83883\n",
            "Epoch: 18/30, step: 252/364, loss: 0.37439, accuracy: 0.83910\n",
            "Epoch: 18/30, step: 253/364, loss: 0.37442, accuracy: 0.83912\n",
            "Epoch: 18/30, step: 254/364, loss: 0.37443, accuracy: 0.83907\n",
            "Epoch: 18/30, step: 255/364, loss: 0.37390, accuracy: 0.83952\n",
            "Epoch: 18/30, step: 256/364, loss: 0.37386, accuracy: 0.83942\n",
            "Epoch: 18/30, step: 257/364, loss: 0.37388, accuracy: 0.83937\n",
            "Epoch: 18/30, step: 258/364, loss: 0.37434, accuracy: 0.83915\n",
            "Epoch: 18/30, step: 259/364, loss: 0.37419, accuracy: 0.83923\n",
            "Epoch: 18/30, step: 260/364, loss: 0.37432, accuracy: 0.83906\n",
            "Epoch: 18/30, step: 261/364, loss: 0.37423, accuracy: 0.83896\n",
            "Epoch: 18/30, step: 262/364, loss: 0.37421, accuracy: 0.83910\n",
            "Epoch: 18/30, step: 263/364, loss: 0.37408, accuracy: 0.83918\n",
            "Epoch: 18/30, step: 264/364, loss: 0.37406, accuracy: 0.83913\n",
            "Epoch: 18/30, step: 265/364, loss: 0.37424, accuracy: 0.83903\n",
            "Epoch: 18/30, step: 266/364, loss: 0.37450, accuracy: 0.83887\n",
            "Epoch: 18/30, step: 267/364, loss: 0.37441, accuracy: 0.83889\n",
            "Epoch: 18/30, step: 268/364, loss: 0.37408, accuracy: 0.83914\n",
            "Epoch: 18/30, step: 269/364, loss: 0.37395, accuracy: 0.83928\n",
            "Epoch: 18/30, step: 270/364, loss: 0.37377, accuracy: 0.83935\n",
            "Epoch: 18/30, step: 271/364, loss: 0.37414, accuracy: 0.83908\n",
            "Epoch: 18/30, step: 272/364, loss: 0.37399, accuracy: 0.83921\n",
            "Epoch: 18/30, step: 273/364, loss: 0.37370, accuracy: 0.83929\n",
            "Epoch: 18/30, step: 274/364, loss: 0.37360, accuracy: 0.83942\n",
            "Epoch: 18/30, step: 275/364, loss: 0.37349, accuracy: 0.83943\n",
            "Epoch: 18/30, step: 276/364, loss: 0.37376, accuracy: 0.83916\n",
            "Epoch: 18/30, step: 277/364, loss: 0.37376, accuracy: 0.83918\n",
            "Epoch: 18/30, step: 278/364, loss: 0.37367, accuracy: 0.83942\n",
            "Epoch: 18/30, step: 279/364, loss: 0.37355, accuracy: 0.83944\n",
            "Epoch: 18/30, step: 280/364, loss: 0.37347, accuracy: 0.83951\n",
            "Epoch: 18/30, step: 281/364, loss: 0.37349, accuracy: 0.83958\n",
            "Epoch: 18/30, step: 282/364, loss: 0.37394, accuracy: 0.83921\n",
            "Epoch: 18/30, step: 283/364, loss: 0.37375, accuracy: 0.83928\n",
            "Epoch: 18/30, step: 284/364, loss: 0.37351, accuracy: 0.83940\n",
            "Epoch: 18/30, step: 285/364, loss: 0.37359, accuracy: 0.83931\n",
            "Epoch: 18/30, step: 286/364, loss: 0.37322, accuracy: 0.83965\n",
            "Epoch: 18/30, step: 287/364, loss: 0.37305, accuracy: 0.83972\n",
            "Epoch: 18/30, step: 288/364, loss: 0.37290, accuracy: 0.83979\n",
            "Epoch: 18/30, step: 289/364, loss: 0.37301, accuracy: 0.83975\n",
            "Epoch: 18/30, step: 290/364, loss: 0.37267, accuracy: 0.83998\n",
            "Epoch: 18/30, step: 291/364, loss: 0.37243, accuracy: 0.84003\n",
            "Epoch: 18/30, train loss: 0.37243, train accuracy: 0.84003, valid loss: 0.61745, valid accuracy: 0.69970\n",
            "Epoch: 19/30, step: 1/364, loss: 0.30137, accuracy: 0.87500\n",
            "Epoch: 19/30, step: 2/364, loss: 0.31031, accuracy: 0.86719\n",
            "Epoch: 19/30, step: 3/364, loss: 0.34396, accuracy: 0.84896\n",
            "Epoch: 19/30, step: 4/364, loss: 0.35441, accuracy: 0.84375\n",
            "Epoch: 19/30, step: 5/364, loss: 0.37816, accuracy: 0.82812\n",
            "Epoch: 19/30, step: 6/364, loss: 0.36336, accuracy: 0.84375\n",
            "Epoch: 19/30, step: 7/364, loss: 0.37389, accuracy: 0.83482\n",
            "Epoch: 19/30, step: 8/364, loss: 0.37948, accuracy: 0.82617\n",
            "Epoch: 19/30, step: 9/364, loss: 0.38724, accuracy: 0.81771\n",
            "Epoch: 19/30, step: 10/364, loss: 0.38297, accuracy: 0.82344\n",
            "Epoch: 19/30, step: 11/364, loss: 0.37784, accuracy: 0.82670\n",
            "Epoch: 19/30, step: 12/364, loss: 0.37806, accuracy: 0.82943\n",
            "Epoch: 19/30, step: 13/364, loss: 0.37428, accuracy: 0.83534\n",
            "Epoch: 19/30, step: 14/364, loss: 0.37277, accuracy: 0.83594\n",
            "Epoch: 19/30, step: 15/364, loss: 0.36345, accuracy: 0.84271\n",
            "Epoch: 19/30, step: 16/364, loss: 0.36548, accuracy: 0.84277\n",
            "Epoch: 19/30, step: 17/364, loss: 0.35838, accuracy: 0.84835\n",
            "Epoch: 19/30, step: 18/364, loss: 0.35906, accuracy: 0.84809\n",
            "Epoch: 19/30, step: 19/364, loss: 0.35591, accuracy: 0.84786\n",
            "Epoch: 19/30, step: 20/364, loss: 0.35909, accuracy: 0.84609\n",
            "Epoch: 19/30, step: 21/364, loss: 0.35803, accuracy: 0.84524\n",
            "Epoch: 19/30, step: 22/364, loss: 0.35831, accuracy: 0.84375\n",
            "Epoch: 19/30, step: 23/364, loss: 0.36122, accuracy: 0.84307\n",
            "Epoch: 19/30, step: 24/364, loss: 0.36173, accuracy: 0.84310\n",
            "Epoch: 19/30, step: 25/364, loss: 0.36465, accuracy: 0.84062\n",
            "Epoch: 19/30, step: 26/364, loss: 0.36249, accuracy: 0.84195\n",
            "Epoch: 19/30, step: 27/364, loss: 0.36157, accuracy: 0.84433\n",
            "Epoch: 19/30, step: 28/364, loss: 0.36064, accuracy: 0.84542\n",
            "Epoch: 19/30, step: 29/364, loss: 0.35912, accuracy: 0.84644\n",
            "Epoch: 19/30, step: 30/364, loss: 0.35918, accuracy: 0.84583\n",
            "Epoch: 19/30, step: 31/364, loss: 0.35805, accuracy: 0.84728\n",
            "Epoch: 19/30, step: 32/364, loss: 0.35821, accuracy: 0.84766\n",
            "Epoch: 19/30, step: 33/364, loss: 0.35614, accuracy: 0.84848\n",
            "Epoch: 19/30, step: 34/364, loss: 0.36008, accuracy: 0.84559\n",
            "Epoch: 19/30, step: 35/364, loss: 0.36181, accuracy: 0.84554\n",
            "Epoch: 19/30, step: 36/364, loss: 0.36149, accuracy: 0.84462\n",
            "Epoch: 19/30, step: 37/364, loss: 0.36159, accuracy: 0.84375\n",
            "Epoch: 19/30, step: 38/364, loss: 0.36155, accuracy: 0.84334\n",
            "Epoch: 19/30, step: 39/364, loss: 0.36181, accuracy: 0.84375\n",
            "Epoch: 19/30, step: 40/364, loss: 0.36126, accuracy: 0.84375\n",
            "Epoch: 19/30, step: 41/364, loss: 0.36203, accuracy: 0.84223\n",
            "Epoch: 19/30, step: 42/364, loss: 0.36220, accuracy: 0.84226\n",
            "Epoch: 19/30, step: 43/364, loss: 0.36397, accuracy: 0.84121\n",
            "Epoch: 19/30, step: 44/364, loss: 0.36365, accuracy: 0.84126\n",
            "Epoch: 19/30, step: 45/364, loss: 0.36189, accuracy: 0.84306\n",
            "Epoch: 19/30, step: 46/364, loss: 0.36341, accuracy: 0.84171\n",
            "Epoch: 19/30, step: 47/364, loss: 0.36216, accuracy: 0.84309\n",
            "Epoch: 19/30, step: 48/364, loss: 0.36261, accuracy: 0.84245\n",
            "Epoch: 19/30, step: 49/364, loss: 0.36035, accuracy: 0.84471\n",
            "Epoch: 19/30, step: 50/364, loss: 0.36031, accuracy: 0.84406\n",
            "Epoch: 19/30, step: 51/364, loss: 0.36251, accuracy: 0.84314\n",
            "Epoch: 19/30, step: 52/364, loss: 0.36108, accuracy: 0.84465\n",
            "Epoch: 19/30, step: 53/364, loss: 0.36050, accuracy: 0.84552\n",
            "Epoch: 19/30, step: 54/364, loss: 0.36172, accuracy: 0.84433\n",
            "Epoch: 19/30, step: 55/364, loss: 0.36125, accuracy: 0.84432\n",
            "Epoch: 19/30, step: 56/364, loss: 0.36139, accuracy: 0.84375\n",
            "Epoch: 19/30, step: 57/364, loss: 0.36157, accuracy: 0.84348\n",
            "Epoch: 19/30, step: 58/364, loss: 0.36124, accuracy: 0.84456\n",
            "Epoch: 19/30, step: 59/364, loss: 0.36163, accuracy: 0.84428\n",
            "Epoch: 19/30, step: 60/364, loss: 0.36213, accuracy: 0.84297\n",
            "Epoch: 19/30, step: 61/364, loss: 0.36382, accuracy: 0.84144\n",
            "Epoch: 19/30, step: 62/364, loss: 0.36309, accuracy: 0.84199\n",
            "Epoch: 19/30, step: 63/364, loss: 0.36286, accuracy: 0.84226\n",
            "Epoch: 19/30, step: 64/364, loss: 0.36176, accuracy: 0.84326\n",
            "Epoch: 19/30, step: 65/364, loss: 0.36163, accuracy: 0.84327\n",
            "Epoch: 19/30, step: 66/364, loss: 0.36118, accuracy: 0.84328\n",
            "Epoch: 19/30, step: 67/364, loss: 0.36045, accuracy: 0.84422\n",
            "Epoch: 19/30, step: 68/364, loss: 0.36025, accuracy: 0.84398\n",
            "Epoch: 19/30, step: 69/364, loss: 0.36078, accuracy: 0.84375\n",
            "Epoch: 19/30, step: 70/364, loss: 0.36009, accuracy: 0.84397\n",
            "Epoch: 19/30, step: 71/364, loss: 0.35901, accuracy: 0.84485\n",
            "Epoch: 19/30, step: 72/364, loss: 0.35894, accuracy: 0.84505\n",
            "Epoch: 19/30, step: 73/364, loss: 0.35908, accuracy: 0.84503\n",
            "Epoch: 19/30, step: 74/364, loss: 0.35840, accuracy: 0.84565\n",
            "Epoch: 19/30, step: 75/364, loss: 0.35832, accuracy: 0.84583\n",
            "Epoch: 19/30, step: 76/364, loss: 0.35804, accuracy: 0.84581\n",
            "Epoch: 19/30, step: 77/364, loss: 0.35809, accuracy: 0.84558\n",
            "Epoch: 19/30, step: 78/364, loss: 0.35705, accuracy: 0.84615\n",
            "Epoch: 19/30, step: 79/364, loss: 0.35637, accuracy: 0.84632\n",
            "Epoch: 19/30, step: 80/364, loss: 0.35594, accuracy: 0.84648\n",
            "Epoch: 19/30, step: 81/364, loss: 0.35544, accuracy: 0.84684\n",
            "Epoch: 19/30, step: 82/364, loss: 0.35432, accuracy: 0.84794\n",
            "Epoch: 19/30, step: 83/364, loss: 0.35388, accuracy: 0.84846\n",
            "Epoch: 19/30, step: 84/364, loss: 0.35380, accuracy: 0.84859\n",
            "Epoch: 19/30, step: 85/364, loss: 0.35407, accuracy: 0.84835\n",
            "Epoch: 19/30, step: 86/364, loss: 0.35486, accuracy: 0.84775\n",
            "Epoch: 19/30, step: 87/364, loss: 0.35473, accuracy: 0.84770\n",
            "Epoch: 19/30, step: 88/364, loss: 0.35403, accuracy: 0.84837\n",
            "Epoch: 19/30, step: 89/364, loss: 0.35449, accuracy: 0.84744\n",
            "Epoch: 19/30, step: 90/364, loss: 0.35470, accuracy: 0.84722\n",
            "Epoch: 19/30, step: 91/364, loss: 0.35406, accuracy: 0.84736\n",
            "Epoch: 19/30, step: 92/364, loss: 0.35450, accuracy: 0.84715\n",
            "Epoch: 19/30, step: 93/364, loss: 0.35447, accuracy: 0.84694\n",
            "Epoch: 19/30, step: 94/364, loss: 0.35408, accuracy: 0.84741\n",
            "Epoch: 19/30, step: 95/364, loss: 0.35500, accuracy: 0.84638\n",
            "Epoch: 19/30, step: 96/364, loss: 0.35574, accuracy: 0.84554\n",
            "Epoch: 19/30, step: 97/364, loss: 0.35497, accuracy: 0.84617\n",
            "Epoch: 19/30, step: 98/364, loss: 0.35461, accuracy: 0.84678\n",
            "Epoch: 19/30, step: 99/364, loss: 0.35396, accuracy: 0.84754\n",
            "Epoch: 19/30, step: 100/364, loss: 0.35422, accuracy: 0.84703\n",
            "Epoch: 19/30, step: 101/364, loss: 0.35403, accuracy: 0.84715\n",
            "Epoch: 19/30, step: 102/364, loss: 0.35453, accuracy: 0.84712\n",
            "Epoch: 19/30, step: 103/364, loss: 0.35489, accuracy: 0.84739\n",
            "Epoch: 19/30, step: 104/364, loss: 0.35470, accuracy: 0.84781\n",
            "Epoch: 19/30, step: 105/364, loss: 0.35480, accuracy: 0.84762\n",
            "Epoch: 19/30, step: 106/364, loss: 0.35477, accuracy: 0.84773\n",
            "Epoch: 19/30, step: 107/364, loss: 0.35526, accuracy: 0.84740\n",
            "Epoch: 19/30, step: 108/364, loss: 0.35549, accuracy: 0.84693\n",
            "Epoch: 19/30, step: 109/364, loss: 0.35524, accuracy: 0.84719\n",
            "Epoch: 19/30, step: 110/364, loss: 0.35612, accuracy: 0.84602\n",
            "Epoch: 19/30, step: 111/364, loss: 0.35575, accuracy: 0.84671\n",
            "Epoch: 19/30, step: 112/364, loss: 0.35572, accuracy: 0.84696\n",
            "Epoch: 19/30, step: 113/364, loss: 0.35503, accuracy: 0.84748\n",
            "Epoch: 19/30, step: 114/364, loss: 0.35486, accuracy: 0.84759\n",
            "Epoch: 19/30, step: 115/364, loss: 0.35502, accuracy: 0.84755\n",
            "Epoch: 19/30, step: 116/364, loss: 0.35506, accuracy: 0.84752\n",
            "Epoch: 19/30, step: 117/364, loss: 0.35475, accuracy: 0.84789\n",
            "Epoch: 19/30, step: 118/364, loss: 0.35473, accuracy: 0.84772\n",
            "Epoch: 19/30, step: 119/364, loss: 0.35477, accuracy: 0.84769\n",
            "Epoch: 19/30, step: 120/364, loss: 0.35394, accuracy: 0.84818\n",
            "Epoch: 19/30, step: 121/364, loss: 0.35373, accuracy: 0.84827\n",
            "Epoch: 19/30, step: 122/364, loss: 0.35333, accuracy: 0.84849\n",
            "Epoch: 19/30, step: 123/364, loss: 0.35306, accuracy: 0.84870\n",
            "Epoch: 19/30, step: 124/364, loss: 0.35308, accuracy: 0.84829\n",
            "Epoch: 19/30, step: 125/364, loss: 0.35327, accuracy: 0.84787\n",
            "Epoch: 19/30, step: 126/364, loss: 0.35305, accuracy: 0.84797\n",
            "Epoch: 19/30, step: 127/364, loss: 0.35325, accuracy: 0.84769\n",
            "Epoch: 19/30, step: 128/364, loss: 0.35303, accuracy: 0.84802\n",
            "Epoch: 19/30, step: 129/364, loss: 0.35341, accuracy: 0.84775\n",
            "Epoch: 19/30, step: 130/364, loss: 0.35302, accuracy: 0.84820\n",
            "Epoch: 19/30, step: 131/364, loss: 0.35231, accuracy: 0.84888\n",
            "Epoch: 19/30, step: 132/364, loss: 0.35250, accuracy: 0.84896\n",
            "Epoch: 19/30, step: 133/364, loss: 0.35298, accuracy: 0.84892\n",
            "Epoch: 19/30, step: 134/364, loss: 0.35293, accuracy: 0.84876\n",
            "Epoch: 19/30, step: 135/364, loss: 0.35344, accuracy: 0.84850\n",
            "Epoch: 19/30, step: 136/364, loss: 0.35329, accuracy: 0.84858\n",
            "Epoch: 19/30, step: 137/364, loss: 0.35308, accuracy: 0.84865\n",
            "Epoch: 19/30, step: 138/364, loss: 0.35266, accuracy: 0.84896\n",
            "Epoch: 19/30, step: 139/364, loss: 0.35245, accuracy: 0.84903\n",
            "Epoch: 19/30, step: 140/364, loss: 0.35301, accuracy: 0.84855\n",
            "Epoch: 19/30, step: 141/364, loss: 0.35369, accuracy: 0.84807\n",
            "Epoch: 19/30, step: 142/364, loss: 0.35328, accuracy: 0.84848\n",
            "Epoch: 19/30, step: 143/364, loss: 0.35318, accuracy: 0.84834\n",
            "Epoch: 19/30, step: 144/364, loss: 0.35365, accuracy: 0.84809\n",
            "Epoch: 19/30, step: 145/364, loss: 0.35361, accuracy: 0.84806\n",
            "Epoch: 19/30, step: 146/364, loss: 0.35321, accuracy: 0.84857\n",
            "Epoch: 19/30, step: 147/364, loss: 0.35317, accuracy: 0.84875\n",
            "Epoch: 19/30, step: 148/364, loss: 0.35365, accuracy: 0.84840\n",
            "Epoch: 19/30, step: 149/364, loss: 0.35398, accuracy: 0.84805\n",
            "Epoch: 19/30, step: 150/364, loss: 0.35368, accuracy: 0.84812\n",
            "Epoch: 19/30, step: 151/364, loss: 0.35458, accuracy: 0.84768\n",
            "Epoch: 19/30, step: 152/364, loss: 0.35437, accuracy: 0.84796\n",
            "Epoch: 19/30, step: 153/364, loss: 0.35394, accuracy: 0.84845\n",
            "Epoch: 19/30, step: 154/364, loss: 0.35395, accuracy: 0.84872\n",
            "Epoch: 19/30, step: 155/364, loss: 0.35391, accuracy: 0.84879\n",
            "Epoch: 19/30, step: 156/364, loss: 0.35435, accuracy: 0.84856\n",
            "Epoch: 19/30, step: 157/364, loss: 0.35403, accuracy: 0.84883\n",
            "Epoch: 19/30, step: 158/364, loss: 0.35457, accuracy: 0.84850\n",
            "Epoch: 19/30, step: 159/364, loss: 0.35479, accuracy: 0.84837\n",
            "Epoch: 19/30, step: 160/364, loss: 0.35471, accuracy: 0.84854\n",
            "Epoch: 19/30, step: 161/364, loss: 0.35500, accuracy: 0.84860\n",
            "Epoch: 19/30, step: 162/364, loss: 0.35446, accuracy: 0.84867\n",
            "Epoch: 19/30, step: 163/364, loss: 0.35517, accuracy: 0.84826\n",
            "Epoch: 19/30, step: 164/364, loss: 0.35558, accuracy: 0.84775\n",
            "Epoch: 19/30, step: 165/364, loss: 0.35576, accuracy: 0.84773\n",
            "Epoch: 19/30, step: 166/364, loss: 0.35564, accuracy: 0.84780\n",
            "Epoch: 19/30, step: 167/364, loss: 0.35569, accuracy: 0.84777\n",
            "Epoch: 19/30, step: 168/364, loss: 0.35575, accuracy: 0.84747\n",
            "Epoch: 19/30, step: 169/364, loss: 0.35562, accuracy: 0.84763\n",
            "Epoch: 19/30, step: 170/364, loss: 0.35544, accuracy: 0.84798\n",
            "Epoch: 19/30, step: 171/364, loss: 0.35594, accuracy: 0.84768\n",
            "Epoch: 19/30, step: 172/364, loss: 0.35648, accuracy: 0.84747\n",
            "Epoch: 19/30, step: 173/364, loss: 0.35657, accuracy: 0.84736\n",
            "Epoch: 19/30, step: 174/364, loss: 0.35641, accuracy: 0.84761\n",
            "Epoch: 19/30, step: 175/364, loss: 0.35628, accuracy: 0.84786\n",
            "Epoch: 19/30, step: 176/364, loss: 0.35587, accuracy: 0.84810\n",
            "Epoch: 19/30, step: 177/364, loss: 0.35606, accuracy: 0.84808\n",
            "Epoch: 19/30, step: 178/364, loss: 0.35600, accuracy: 0.84796\n",
            "Epoch: 19/30, step: 179/364, loss: 0.35509, accuracy: 0.84838\n",
            "Epoch: 19/30, step: 180/364, loss: 0.35575, accuracy: 0.84748\n",
            "Epoch: 19/30, step: 181/364, loss: 0.35586, accuracy: 0.84755\n",
            "Epoch: 19/30, step: 182/364, loss: 0.35547, accuracy: 0.84761\n",
            "Epoch: 19/30, step: 183/364, loss: 0.35549, accuracy: 0.84776\n",
            "Epoch: 19/30, step: 184/364, loss: 0.35540, accuracy: 0.84774\n",
            "Epoch: 19/30, step: 185/364, loss: 0.35625, accuracy: 0.84721\n",
            "Epoch: 19/30, step: 186/364, loss: 0.35626, accuracy: 0.84711\n",
            "Epoch: 19/30, step: 187/364, loss: 0.35626, accuracy: 0.84709\n",
            "Epoch: 19/30, step: 188/364, loss: 0.35602, accuracy: 0.84741\n",
            "Epoch: 19/30, step: 189/364, loss: 0.35544, accuracy: 0.84780\n",
            "Epoch: 19/30, step: 190/364, loss: 0.35552, accuracy: 0.84770\n",
            "Epoch: 19/30, step: 191/364, loss: 0.35570, accuracy: 0.84784\n",
            "Epoch: 19/30, step: 192/364, loss: 0.35562, accuracy: 0.84806\n",
            "Epoch: 19/30, step: 193/364, loss: 0.35507, accuracy: 0.84845\n",
            "Epoch: 19/30, step: 194/364, loss: 0.35520, accuracy: 0.84826\n",
            "Epoch: 19/30, step: 195/364, loss: 0.35529, accuracy: 0.84808\n",
            "Epoch: 19/30, step: 196/364, loss: 0.35490, accuracy: 0.84845\n",
            "Epoch: 19/30, step: 197/364, loss: 0.35549, accuracy: 0.84819\n",
            "Epoch: 19/30, step: 198/364, loss: 0.35528, accuracy: 0.84833\n",
            "Epoch: 19/30, step: 199/364, loss: 0.35571, accuracy: 0.84791\n",
            "Epoch: 19/30, step: 200/364, loss: 0.35561, accuracy: 0.84805\n",
            "Epoch: 19/30, step: 201/364, loss: 0.35548, accuracy: 0.84818\n",
            "Epoch: 19/30, step: 202/364, loss: 0.35554, accuracy: 0.84831\n",
            "Epoch: 19/30, step: 203/364, loss: 0.35520, accuracy: 0.84845\n",
            "Epoch: 19/30, step: 204/364, loss: 0.35509, accuracy: 0.84842\n",
            "Epoch: 19/30, step: 205/364, loss: 0.35444, accuracy: 0.84893\n",
            "Epoch: 19/30, step: 206/364, loss: 0.35428, accuracy: 0.84906\n",
            "Epoch: 19/30, step: 207/364, loss: 0.35376, accuracy: 0.84934\n",
            "Epoch: 19/30, step: 208/364, loss: 0.35333, accuracy: 0.84961\n",
            "Epoch: 19/30, step: 209/364, loss: 0.35344, accuracy: 0.84936\n",
            "Epoch: 19/30, step: 210/364, loss: 0.35331, accuracy: 0.84940\n",
            "Epoch: 19/30, step: 211/364, loss: 0.35364, accuracy: 0.84886\n",
            "Epoch: 19/30, step: 212/364, loss: 0.35338, accuracy: 0.84898\n",
            "Epoch: 19/30, step: 213/364, loss: 0.35341, accuracy: 0.84918\n",
            "Epoch: 19/30, step: 214/364, loss: 0.35359, accuracy: 0.84908\n",
            "Epoch: 19/30, step: 215/364, loss: 0.35322, accuracy: 0.84949\n",
            "Epoch: 19/30, step: 216/364, loss: 0.35301, accuracy: 0.84968\n",
            "Epoch: 19/30, step: 217/364, loss: 0.35264, accuracy: 0.85001\n",
            "Epoch: 19/30, step: 218/364, loss: 0.35261, accuracy: 0.85020\n",
            "Epoch: 19/30, step: 219/364, loss: 0.35221, accuracy: 0.85053\n",
            "Epoch: 19/30, step: 220/364, loss: 0.35195, accuracy: 0.85078\n",
            "Epoch: 19/30, step: 221/364, loss: 0.35176, accuracy: 0.85096\n",
            "Epoch: 19/30, step: 222/364, loss: 0.35140, accuracy: 0.85128\n",
            "Epoch: 19/30, step: 223/364, loss: 0.35139, accuracy: 0.85104\n",
            "Epoch: 19/30, step: 224/364, loss: 0.35141, accuracy: 0.85114\n",
            "Epoch: 19/30, step: 225/364, loss: 0.35119, accuracy: 0.85132\n",
            "Epoch: 19/30, step: 226/364, loss: 0.35096, accuracy: 0.85163\n",
            "Epoch: 19/30, step: 227/364, loss: 0.35131, accuracy: 0.85146\n",
            "Epoch: 19/30, step: 228/364, loss: 0.35136, accuracy: 0.85143\n",
            "Epoch: 19/30, step: 229/364, loss: 0.35132, accuracy: 0.85146\n",
            "Epoch: 19/30, step: 230/364, loss: 0.35161, accuracy: 0.85129\n",
            "Epoch: 19/30, step: 231/364, loss: 0.35141, accuracy: 0.85153\n",
            "Epoch: 19/30, step: 232/364, loss: 0.35118, accuracy: 0.85170\n",
            "Epoch: 19/30, step: 233/364, loss: 0.35096, accuracy: 0.85186\n",
            "Epoch: 19/30, step: 234/364, loss: 0.35075, accuracy: 0.85183\n",
            "Epoch: 19/30, step: 235/364, loss: 0.35094, accuracy: 0.85153\n",
            "Epoch: 19/30, step: 236/364, loss: 0.35101, accuracy: 0.85143\n",
            "Epoch: 19/30, step: 237/364, loss: 0.35071, accuracy: 0.85153\n",
            "Epoch: 19/30, step: 238/364, loss: 0.35071, accuracy: 0.85169\n",
            "Epoch: 19/30, step: 239/364, loss: 0.35065, accuracy: 0.85166\n",
            "Epoch: 19/30, step: 240/364, loss: 0.35053, accuracy: 0.85150\n",
            "Epoch: 19/30, step: 241/364, loss: 0.35045, accuracy: 0.85166\n",
            "Epoch: 19/30, step: 242/364, loss: 0.35118, accuracy: 0.85111\n",
            "Epoch: 19/30, step: 243/364, loss: 0.35117, accuracy: 0.85108\n",
            "Epoch: 19/30, step: 244/364, loss: 0.35165, accuracy: 0.85092\n",
            "Epoch: 19/30, step: 245/364, loss: 0.35134, accuracy: 0.85096\n",
            "Epoch: 19/30, step: 246/364, loss: 0.35124, accuracy: 0.85099\n",
            "Epoch: 19/30, step: 247/364, loss: 0.35131, accuracy: 0.85096\n",
            "Epoch: 19/30, step: 248/364, loss: 0.35154, accuracy: 0.85093\n",
            "Epoch: 19/30, step: 249/364, loss: 0.35148, accuracy: 0.85084\n",
            "Epoch: 19/30, step: 250/364, loss: 0.35138, accuracy: 0.85094\n",
            "Epoch: 19/30, step: 251/364, loss: 0.35147, accuracy: 0.85097\n",
            "Epoch: 19/30, step: 252/364, loss: 0.35098, accuracy: 0.85138\n",
            "Epoch: 19/30, step: 253/364, loss: 0.35100, accuracy: 0.85128\n",
            "Epoch: 19/30, step: 254/364, loss: 0.35087, accuracy: 0.85156\n",
            "Epoch: 19/30, step: 255/364, loss: 0.35110, accuracy: 0.85141\n",
            "Epoch: 19/30, step: 256/364, loss: 0.35121, accuracy: 0.85107\n",
            "Epoch: 19/30, step: 257/364, loss: 0.35073, accuracy: 0.85129\n",
            "Epoch: 19/30, step: 258/364, loss: 0.35056, accuracy: 0.85132\n",
            "Epoch: 19/30, step: 259/364, loss: 0.35047, accuracy: 0.85147\n",
            "Epoch: 19/30, step: 260/364, loss: 0.35026, accuracy: 0.85156\n",
            "Epoch: 19/30, step: 261/364, loss: 0.34993, accuracy: 0.85177\n",
            "Epoch: 19/30, step: 262/364, loss: 0.35019, accuracy: 0.85150\n",
            "Epoch: 19/30, step: 263/364, loss: 0.35051, accuracy: 0.85124\n",
            "Epoch: 19/30, step: 264/364, loss: 0.35070, accuracy: 0.85115\n",
            "Epoch: 19/30, step: 265/364, loss: 0.35111, accuracy: 0.85088\n",
            "Epoch: 19/30, step: 266/364, loss: 0.35113, accuracy: 0.85086\n",
            "Epoch: 19/30, step: 267/364, loss: 0.35109, accuracy: 0.85095\n",
            "Epoch: 19/30, step: 268/364, loss: 0.35093, accuracy: 0.85115\n",
            "Epoch: 19/30, step: 269/364, loss: 0.35096, accuracy: 0.85113\n",
            "Epoch: 19/30, step: 270/364, loss: 0.35102, accuracy: 0.85093\n",
            "Epoch: 19/30, step: 271/364, loss: 0.35092, accuracy: 0.85101\n",
            "Epoch: 19/30, step: 272/364, loss: 0.35162, accuracy: 0.85076\n",
            "Epoch: 19/30, step: 273/364, loss: 0.35146, accuracy: 0.85085\n",
            "Epoch: 19/30, step: 274/364, loss: 0.35119, accuracy: 0.85111\n",
            "Epoch: 19/30, step: 275/364, loss: 0.35124, accuracy: 0.85108\n",
            "Epoch: 19/30, step: 276/364, loss: 0.35118, accuracy: 0.85128\n",
            "Epoch: 19/30, step: 277/364, loss: 0.35107, accuracy: 0.85131\n",
            "Epoch: 19/30, step: 278/364, loss: 0.35087, accuracy: 0.85139\n",
            "Epoch: 19/30, step: 279/364, loss: 0.35092, accuracy: 0.85148\n",
            "Epoch: 19/30, step: 280/364, loss: 0.35083, accuracy: 0.85145\n",
            "Epoch: 19/30, step: 281/364, loss: 0.35046, accuracy: 0.85176\n",
            "Epoch: 19/30, step: 282/364, loss: 0.35051, accuracy: 0.85173\n",
            "Epoch: 19/30, step: 283/364, loss: 0.35067, accuracy: 0.85148\n",
            "Epoch: 19/30, step: 284/364, loss: 0.35054, accuracy: 0.85167\n",
            "Epoch: 19/30, step: 285/364, loss: 0.35070, accuracy: 0.85164\n",
            "Epoch: 19/30, step: 286/364, loss: 0.35073, accuracy: 0.85178\n",
            "Epoch: 19/30, step: 287/364, loss: 0.35067, accuracy: 0.85186\n",
            "Epoch: 19/30, step: 288/364, loss: 0.35042, accuracy: 0.85200\n",
            "Epoch: 19/30, step: 289/364, loss: 0.35029, accuracy: 0.85208\n",
            "Epoch: 19/30, step: 290/364, loss: 0.35014, accuracy: 0.85221\n",
            "Epoch: 19/30, step: 291/364, loss: 0.35024, accuracy: 0.85212\n",
            "Epoch: 19/30, train loss: 0.35024, train accuracy: 0.85212, valid loss: 0.63424, valid accuracy: 0.69626\n",
            "Epoch: 20/30, step: 1/364, loss: 0.31814, accuracy: 0.84375\n",
            "Epoch: 20/30, step: 2/364, loss: 0.38775, accuracy: 0.82031\n",
            "Epoch: 20/30, step: 3/364, loss: 0.39983, accuracy: 0.80208\n",
            "Epoch: 20/30, step: 4/364, loss: 0.38158, accuracy: 0.82031\n",
            "Epoch: 20/30, step: 5/364, loss: 0.37201, accuracy: 0.82812\n",
            "Epoch: 20/30, step: 6/364, loss: 0.35737, accuracy: 0.83594\n",
            "Epoch: 20/30, step: 7/364, loss: 0.35529, accuracy: 0.83482\n",
            "Epoch: 20/30, step: 8/364, loss: 0.35614, accuracy: 0.82227\n",
            "Epoch: 20/30, step: 9/364, loss: 0.35768, accuracy: 0.82465\n",
            "Epoch: 20/30, step: 10/364, loss: 0.36647, accuracy: 0.82344\n",
            "Epoch: 20/30, step: 11/364, loss: 0.35943, accuracy: 0.82670\n",
            "Epoch: 20/30, step: 12/364, loss: 0.35201, accuracy: 0.83203\n",
            "Epoch: 20/30, step: 13/364, loss: 0.34237, accuracy: 0.84014\n",
            "Epoch: 20/30, step: 14/364, loss: 0.34692, accuracy: 0.83705\n",
            "Epoch: 20/30, step: 15/364, loss: 0.34652, accuracy: 0.84062\n",
            "Epoch: 20/30, step: 16/364, loss: 0.34451, accuracy: 0.84180\n",
            "Epoch: 20/30, step: 17/364, loss: 0.34476, accuracy: 0.84283\n",
            "Epoch: 20/30, step: 18/364, loss: 0.33769, accuracy: 0.84809\n",
            "Epoch: 20/30, step: 19/364, loss: 0.33844, accuracy: 0.84786\n",
            "Epoch: 20/30, step: 20/364, loss: 0.33668, accuracy: 0.85000\n",
            "Epoch: 20/30, step: 21/364, loss: 0.33279, accuracy: 0.85268\n",
            "Epoch: 20/30, step: 22/364, loss: 0.33285, accuracy: 0.85227\n",
            "Epoch: 20/30, step: 23/364, loss: 0.32926, accuracy: 0.85530\n",
            "Epoch: 20/30, step: 24/364, loss: 0.32796, accuracy: 0.85677\n",
            "Epoch: 20/30, step: 25/364, loss: 0.32795, accuracy: 0.85688\n",
            "Epoch: 20/30, step: 26/364, loss: 0.32841, accuracy: 0.85697\n",
            "Epoch: 20/30, step: 27/364, loss: 0.32936, accuracy: 0.85706\n",
            "Epoch: 20/30, step: 28/364, loss: 0.32939, accuracy: 0.85826\n",
            "Epoch: 20/30, step: 29/364, loss: 0.32895, accuracy: 0.85991\n",
            "Epoch: 20/30, step: 30/364, loss: 0.32995, accuracy: 0.85938\n",
            "Epoch: 20/30, step: 31/364, loss: 0.33267, accuracy: 0.85887\n",
            "Epoch: 20/30, step: 32/364, loss: 0.33434, accuracy: 0.85840\n",
            "Epoch: 20/30, step: 33/364, loss: 0.33416, accuracy: 0.85748\n",
            "Epoch: 20/30, step: 34/364, loss: 0.33530, accuracy: 0.85662\n",
            "Epoch: 20/30, step: 35/364, loss: 0.33147, accuracy: 0.85938\n",
            "Epoch: 20/30, step: 36/364, loss: 0.32964, accuracy: 0.86111\n",
            "Epoch: 20/30, step: 37/364, loss: 0.32807, accuracy: 0.86318\n",
            "Epoch: 20/30, step: 38/364, loss: 0.32969, accuracy: 0.86184\n",
            "Epoch: 20/30, step: 39/364, loss: 0.33236, accuracy: 0.86058\n",
            "Epoch: 20/30, step: 40/364, loss: 0.33258, accuracy: 0.85938\n",
            "Epoch: 20/30, step: 41/364, loss: 0.33177, accuracy: 0.86014\n",
            "Epoch: 20/30, step: 42/364, loss: 0.33535, accuracy: 0.85677\n",
            "Epoch: 20/30, step: 43/364, loss: 0.33499, accuracy: 0.85683\n",
            "Epoch: 20/30, step: 44/364, loss: 0.33585, accuracy: 0.85724\n",
            "Epoch: 20/30, step: 45/364, loss: 0.33579, accuracy: 0.85764\n",
            "Epoch: 20/30, step: 46/364, loss: 0.33374, accuracy: 0.85938\n",
            "Epoch: 20/30, step: 47/364, loss: 0.33401, accuracy: 0.85871\n",
            "Epoch: 20/30, step: 48/364, loss: 0.33387, accuracy: 0.85840\n",
            "Epoch: 20/30, step: 49/364, loss: 0.33616, accuracy: 0.85587\n",
            "Epoch: 20/30, step: 50/364, loss: 0.33680, accuracy: 0.85500\n",
            "Epoch: 20/30, step: 51/364, loss: 0.33646, accuracy: 0.85509\n",
            "Epoch: 20/30, step: 52/364, loss: 0.33613, accuracy: 0.85517\n",
            "Epoch: 20/30, step: 53/364, loss: 0.33740, accuracy: 0.85495\n",
            "Epoch: 20/30, step: 54/364, loss: 0.33745, accuracy: 0.85561\n",
            "Epoch: 20/30, step: 55/364, loss: 0.33835, accuracy: 0.85511\n",
            "Epoch: 20/30, step: 56/364, loss: 0.33831, accuracy: 0.85491\n",
            "Epoch: 20/30, step: 57/364, loss: 0.33664, accuracy: 0.85609\n",
            "Epoch: 20/30, step: 58/364, loss: 0.33587, accuracy: 0.85641\n",
            "Epoch: 20/30, step: 59/364, loss: 0.33620, accuracy: 0.85620\n",
            "Epoch: 20/30, step: 60/364, loss: 0.33624, accuracy: 0.85651\n",
            "Epoch: 20/30, step: 61/364, loss: 0.33564, accuracy: 0.85656\n",
            "Epoch: 20/30, step: 62/364, loss: 0.33472, accuracy: 0.85761\n",
            "Epoch: 20/30, step: 63/364, loss: 0.33414, accuracy: 0.85838\n",
            "Epoch: 20/30, step: 64/364, loss: 0.33426, accuracy: 0.85864\n",
            "Epoch: 20/30, step: 65/364, loss: 0.33280, accuracy: 0.85913\n",
            "Epoch: 20/30, step: 66/364, loss: 0.33132, accuracy: 0.86032\n",
            "Epoch: 20/30, step: 67/364, loss: 0.33044, accuracy: 0.86054\n",
            "Epoch: 20/30, step: 68/364, loss: 0.33161, accuracy: 0.85938\n",
            "Epoch: 20/30, step: 69/364, loss: 0.33200, accuracy: 0.85870\n",
            "Epoch: 20/30, step: 70/364, loss: 0.33172, accuracy: 0.85915\n",
            "Epoch: 20/30, step: 71/364, loss: 0.33125, accuracy: 0.85938\n",
            "Epoch: 20/30, step: 72/364, loss: 0.33040, accuracy: 0.85981\n",
            "Epoch: 20/30, step: 73/364, loss: 0.33041, accuracy: 0.85916\n",
            "Epoch: 20/30, step: 74/364, loss: 0.33065, accuracy: 0.85895\n",
            "Epoch: 20/30, step: 75/364, loss: 0.33020, accuracy: 0.85875\n",
            "Epoch: 20/30, step: 76/364, loss: 0.32953, accuracy: 0.85938\n",
            "Epoch: 20/30, step: 77/364, loss: 0.33012, accuracy: 0.85856\n",
            "Epoch: 20/30, step: 78/364, loss: 0.33024, accuracy: 0.85837\n",
            "Epoch: 20/30, step: 79/364, loss: 0.32993, accuracy: 0.85878\n",
            "Epoch: 20/30, step: 80/364, loss: 0.33109, accuracy: 0.85820\n",
            "Epoch: 20/30, step: 81/364, loss: 0.33146, accuracy: 0.85841\n",
            "Epoch: 20/30, step: 82/364, loss: 0.33141, accuracy: 0.85804\n",
            "Epoch: 20/30, step: 83/364, loss: 0.33084, accuracy: 0.85862\n",
            "Epoch: 20/30, step: 84/364, loss: 0.33080, accuracy: 0.85882\n",
            "Epoch: 20/30, step: 85/364, loss: 0.33167, accuracy: 0.85827\n",
            "Epoch: 20/30, step: 86/364, loss: 0.33147, accuracy: 0.85810\n",
            "Epoch: 20/30, step: 87/364, loss: 0.33112, accuracy: 0.85884\n",
            "Epoch: 20/30, step: 88/364, loss: 0.33101, accuracy: 0.85920\n",
            "Epoch: 20/30, step: 89/364, loss: 0.33088, accuracy: 0.85955\n",
            "Epoch: 20/30, step: 90/364, loss: 0.32987, accuracy: 0.86059\n",
            "Epoch: 20/30, step: 91/364, loss: 0.32975, accuracy: 0.86041\n",
            "Epoch: 20/30, step: 92/364, loss: 0.32995, accuracy: 0.86022\n",
            "Epoch: 20/30, step: 93/364, loss: 0.32974, accuracy: 0.86022\n",
            "Epoch: 20/30, step: 94/364, loss: 0.32951, accuracy: 0.86070\n",
            "Epoch: 20/30, step: 95/364, loss: 0.33004, accuracy: 0.86069\n",
            "Epoch: 20/30, step: 96/364, loss: 0.33079, accuracy: 0.86035\n",
            "Epoch: 20/30, step: 97/364, loss: 0.33076, accuracy: 0.86002\n",
            "Epoch: 20/30, step: 98/364, loss: 0.33120, accuracy: 0.85953\n",
            "Epoch: 20/30, step: 99/364, loss: 0.33220, accuracy: 0.85953\n",
            "Epoch: 20/30, step: 100/364, loss: 0.33263, accuracy: 0.85906\n",
            "Epoch: 20/30, step: 101/364, loss: 0.33336, accuracy: 0.85845\n",
            "Epoch: 20/30, step: 102/364, loss: 0.33328, accuracy: 0.85861\n",
            "Epoch: 20/30, step: 103/364, loss: 0.33307, accuracy: 0.85892\n",
            "Epoch: 20/30, step: 104/364, loss: 0.33254, accuracy: 0.85922\n",
            "Epoch: 20/30, step: 105/364, loss: 0.33174, accuracy: 0.85982\n",
            "Epoch: 20/30, step: 106/364, loss: 0.33250, accuracy: 0.85908\n",
            "Epoch: 20/30, step: 107/364, loss: 0.33262, accuracy: 0.85923\n",
            "Epoch: 20/30, step: 108/364, loss: 0.33231, accuracy: 0.85981\n",
            "Epoch: 20/30, step: 109/364, loss: 0.33235, accuracy: 0.85995\n",
            "Epoch: 20/30, step: 110/364, loss: 0.33295, accuracy: 0.85952\n",
            "Epoch: 20/30, step: 111/364, loss: 0.33329, accuracy: 0.85909\n",
            "Epoch: 20/30, step: 112/364, loss: 0.33275, accuracy: 0.85951\n",
            "Epoch: 20/30, step: 113/364, loss: 0.33223, accuracy: 0.85993\n",
            "Epoch: 20/30, step: 114/364, loss: 0.33130, accuracy: 0.86061\n",
            "Epoch: 20/30, step: 115/364, loss: 0.33078, accuracy: 0.86060\n",
            "Epoch: 20/30, step: 116/364, loss: 0.33129, accuracy: 0.86032\n",
            "Epoch: 20/30, step: 117/364, loss: 0.33055, accuracy: 0.86098\n",
            "Epoch: 20/30, step: 118/364, loss: 0.33114, accuracy: 0.86083\n",
            "Epoch: 20/30, step: 119/364, loss: 0.33224, accuracy: 0.86029\n",
            "Epoch: 20/30, step: 120/364, loss: 0.33165, accuracy: 0.86055\n",
            "Epoch: 20/30, step: 121/364, loss: 0.33199, accuracy: 0.86054\n",
            "Epoch: 20/30, step: 122/364, loss: 0.33197, accuracy: 0.86078\n",
            "Epoch: 20/30, step: 123/364, loss: 0.33229, accuracy: 0.86077\n",
            "Epoch: 20/30, step: 124/364, loss: 0.33229, accuracy: 0.86101\n",
            "Epoch: 20/30, step: 125/364, loss: 0.33166, accuracy: 0.86150\n",
            "Epoch: 20/30, step: 126/364, loss: 0.33328, accuracy: 0.86012\n",
            "Epoch: 20/30, step: 127/364, loss: 0.33331, accuracy: 0.86048\n",
            "Epoch: 20/30, step: 128/364, loss: 0.33361, accuracy: 0.86023\n",
            "Epoch: 20/30, step: 129/364, loss: 0.33367, accuracy: 0.86071\n",
            "Epoch: 20/30, step: 130/364, loss: 0.33401, accuracy: 0.86058\n",
            "Epoch: 20/30, step: 131/364, loss: 0.33341, accuracy: 0.86104\n",
            "Epoch: 20/30, step: 132/364, loss: 0.33333, accuracy: 0.86080\n",
            "Epoch: 20/30, step: 133/364, loss: 0.33375, accuracy: 0.86055\n",
            "Epoch: 20/30, step: 134/364, loss: 0.33353, accuracy: 0.86077\n",
            "Epoch: 20/30, step: 135/364, loss: 0.33318, accuracy: 0.86088\n",
            "Epoch: 20/30, step: 136/364, loss: 0.33364, accuracy: 0.86052\n",
            "Epoch: 20/30, step: 137/364, loss: 0.33413, accuracy: 0.86006\n",
            "Epoch: 20/30, step: 138/364, loss: 0.33381, accuracy: 0.86005\n",
            "Epoch: 20/30, step: 139/364, loss: 0.33401, accuracy: 0.86016\n",
            "Epoch: 20/30, step: 140/364, loss: 0.33354, accuracy: 0.86049\n",
            "Epoch: 20/30, step: 141/364, loss: 0.33359, accuracy: 0.86048\n",
            "Epoch: 20/30, step: 142/364, loss: 0.33333, accuracy: 0.86026\n",
            "Epoch: 20/30, step: 143/364, loss: 0.33369, accuracy: 0.85992\n",
            "Epoch: 20/30, step: 144/364, loss: 0.33328, accuracy: 0.85992\n",
            "Epoch: 20/30, step: 145/364, loss: 0.33300, accuracy: 0.86013\n",
            "Epoch: 20/30, step: 146/364, loss: 0.33314, accuracy: 0.86012\n",
            "Epoch: 20/30, step: 147/364, loss: 0.33298, accuracy: 0.85991\n",
            "Epoch: 20/30, step: 148/364, loss: 0.33257, accuracy: 0.86022\n",
            "Epoch: 20/30, step: 149/364, loss: 0.33241, accuracy: 0.86042\n",
            "Epoch: 20/30, step: 150/364, loss: 0.33222, accuracy: 0.86052\n",
            "Epoch: 20/30, step: 151/364, loss: 0.33168, accuracy: 0.86093\n",
            "Epoch: 20/30, step: 152/364, loss: 0.33173, accuracy: 0.86112\n",
            "Epoch: 20/30, step: 153/364, loss: 0.33221, accuracy: 0.86101\n",
            "Epoch: 20/30, step: 154/364, loss: 0.33241, accuracy: 0.86090\n",
            "Epoch: 20/30, step: 155/364, loss: 0.33245, accuracy: 0.86069\n",
            "Epoch: 20/30, step: 156/364, loss: 0.33229, accuracy: 0.86098\n",
            "Epoch: 20/30, step: 157/364, loss: 0.33198, accuracy: 0.86137\n",
            "Epoch: 20/30, step: 158/364, loss: 0.33206, accuracy: 0.86106\n",
            "Epoch: 20/30, step: 159/364, loss: 0.33193, accuracy: 0.86124\n",
            "Epoch: 20/30, step: 160/364, loss: 0.33198, accuracy: 0.86123\n",
            "Epoch: 20/30, step: 161/364, loss: 0.33200, accuracy: 0.86112\n",
            "Epoch: 20/30, step: 162/364, loss: 0.33235, accuracy: 0.86092\n",
            "Epoch: 20/30, step: 163/364, loss: 0.33264, accuracy: 0.86091\n",
            "Epoch: 20/30, step: 164/364, loss: 0.33269, accuracy: 0.86109\n",
            "Epoch: 20/30, step: 165/364, loss: 0.33257, accuracy: 0.86117\n",
            "Epoch: 20/30, step: 166/364, loss: 0.33311, accuracy: 0.86069\n",
            "Epoch: 20/30, step: 167/364, loss: 0.33292, accuracy: 0.86068\n",
            "Epoch: 20/30, step: 168/364, loss: 0.33239, accuracy: 0.86105\n",
            "Epoch: 20/30, step: 169/364, loss: 0.33296, accuracy: 0.86058\n",
            "Epoch: 20/30, step: 170/364, loss: 0.33351, accuracy: 0.86039\n",
            "Epoch: 20/30, step: 171/364, loss: 0.33362, accuracy: 0.86029\n",
            "Epoch: 20/30, step: 172/364, loss: 0.33401, accuracy: 0.86010\n",
            "Epoch: 20/30, step: 173/364, loss: 0.33416, accuracy: 0.86019\n",
            "Epoch: 20/30, step: 174/364, loss: 0.33426, accuracy: 0.85991\n",
            "Epoch: 20/30, step: 175/364, loss: 0.33440, accuracy: 0.85955\n",
            "Epoch: 20/30, step: 176/364, loss: 0.33413, accuracy: 0.85955\n",
            "Epoch: 20/30, step: 177/364, loss: 0.33501, accuracy: 0.85876\n",
            "Epoch: 20/30, step: 178/364, loss: 0.33502, accuracy: 0.85859\n",
            "Epoch: 20/30, step: 179/364, loss: 0.33529, accuracy: 0.85850\n",
            "Epoch: 20/30, step: 180/364, loss: 0.33505, accuracy: 0.85851\n",
            "Epoch: 20/30, step: 181/364, loss: 0.33470, accuracy: 0.85877\n",
            "Epoch: 20/30, step: 182/364, loss: 0.33475, accuracy: 0.85877\n",
            "Epoch: 20/30, step: 183/364, loss: 0.33472, accuracy: 0.85869\n",
            "Epoch: 20/30, step: 184/364, loss: 0.33511, accuracy: 0.85836\n",
            "Epoch: 20/30, step: 185/364, loss: 0.33494, accuracy: 0.85853\n",
            "Epoch: 20/30, step: 186/364, loss: 0.33437, accuracy: 0.85887\n",
            "Epoch: 20/30, step: 187/364, loss: 0.33484, accuracy: 0.85854\n",
            "Epoch: 20/30, step: 188/364, loss: 0.33426, accuracy: 0.85913\n",
            "Epoch: 20/30, step: 189/364, loss: 0.33454, accuracy: 0.85880\n",
            "Epoch: 20/30, step: 190/364, loss: 0.33479, accuracy: 0.85855\n",
            "Epoch: 20/30, step: 191/364, loss: 0.33446, accuracy: 0.85864\n",
            "Epoch: 20/30, step: 192/364, loss: 0.33472, accuracy: 0.85848\n",
            "Epoch: 20/30, step: 193/364, loss: 0.33517, accuracy: 0.85816\n",
            "Epoch: 20/30, step: 194/364, loss: 0.33537, accuracy: 0.85809\n",
            "Epoch: 20/30, step: 195/364, loss: 0.33539, accuracy: 0.85801\n",
            "Epoch: 20/30, step: 196/364, loss: 0.33549, accuracy: 0.85770\n",
            "Epoch: 20/30, step: 197/364, loss: 0.33564, accuracy: 0.85771\n",
            "Epoch: 20/30, step: 198/364, loss: 0.33561, accuracy: 0.85772\n",
            "Epoch: 20/30, step: 199/364, loss: 0.33585, accuracy: 0.85773\n",
            "Epoch: 20/30, step: 200/364, loss: 0.33604, accuracy: 0.85750\n",
            "Epoch: 20/30, step: 201/364, loss: 0.33588, accuracy: 0.85735\n",
            "Epoch: 20/30, step: 202/364, loss: 0.33643, accuracy: 0.85690\n",
            "Epoch: 20/30, step: 203/364, loss: 0.33638, accuracy: 0.85699\n",
            "Epoch: 20/30, step: 204/364, loss: 0.33673, accuracy: 0.85708\n",
            "Epoch: 20/30, step: 205/364, loss: 0.33712, accuracy: 0.85694\n",
            "Epoch: 20/30, step: 206/364, loss: 0.33765, accuracy: 0.85672\n",
            "Epoch: 20/30, step: 207/364, loss: 0.33741, accuracy: 0.85704\n",
            "Epoch: 20/30, step: 208/364, loss: 0.33705, accuracy: 0.85712\n",
            "Epoch: 20/30, step: 209/364, loss: 0.33691, accuracy: 0.85721\n",
            "Epoch: 20/30, step: 210/364, loss: 0.33660, accuracy: 0.85751\n",
            "Epoch: 20/30, step: 211/364, loss: 0.33650, accuracy: 0.85760\n",
            "Epoch: 20/30, step: 212/364, loss: 0.33580, accuracy: 0.85805\n",
            "Epoch: 20/30, step: 213/364, loss: 0.33543, accuracy: 0.85827\n",
            "Epoch: 20/30, step: 214/364, loss: 0.33557, accuracy: 0.85813\n",
            "Epoch: 20/30, step: 215/364, loss: 0.33573, accuracy: 0.85814\n",
            "Epoch: 20/30, step: 216/364, loss: 0.33581, accuracy: 0.85822\n",
            "Epoch: 20/30, step: 217/364, loss: 0.33544, accuracy: 0.85837\n",
            "Epoch: 20/30, step: 218/364, loss: 0.33539, accuracy: 0.85837\n",
            "Epoch: 20/30, step: 219/364, loss: 0.33502, accuracy: 0.85866\n",
            "Epoch: 20/30, step: 220/364, loss: 0.33560, accuracy: 0.85838\n",
            "Epoch: 20/30, step: 221/364, loss: 0.33638, accuracy: 0.85789\n",
            "Epoch: 20/30, step: 222/364, loss: 0.33615, accuracy: 0.85811\n",
            "Epoch: 20/30, step: 223/364, loss: 0.33617, accuracy: 0.85804\n",
            "Epoch: 20/30, step: 224/364, loss: 0.33659, accuracy: 0.85805\n",
            "Epoch: 20/30, step: 225/364, loss: 0.33652, accuracy: 0.85806\n",
            "Epoch: 20/30, step: 226/364, loss: 0.33673, accuracy: 0.85792\n",
            "Epoch: 20/30, step: 227/364, loss: 0.33646, accuracy: 0.85820\n",
            "Epoch: 20/30, step: 228/364, loss: 0.33668, accuracy: 0.85807\n",
            "Epoch: 20/30, step: 229/364, loss: 0.33634, accuracy: 0.85828\n",
            "Epoch: 20/30, step: 230/364, loss: 0.33637, accuracy: 0.85829\n",
            "Epoch: 20/30, step: 231/364, loss: 0.33624, accuracy: 0.85836\n",
            "Epoch: 20/30, step: 232/364, loss: 0.33610, accuracy: 0.85843\n",
            "Epoch: 20/30, step: 233/364, loss: 0.33640, accuracy: 0.85830\n",
            "Epoch: 20/30, step: 234/364, loss: 0.33648, accuracy: 0.85831\n",
            "Epoch: 20/30, step: 235/364, loss: 0.33655, accuracy: 0.85844\n",
            "Epoch: 20/30, step: 236/364, loss: 0.33616, accuracy: 0.85865\n",
            "Epoch: 20/30, step: 237/364, loss: 0.33656, accuracy: 0.85839\n",
            "Epoch: 20/30, step: 238/364, loss: 0.33687, accuracy: 0.85813\n",
            "Epoch: 20/30, step: 239/364, loss: 0.33683, accuracy: 0.85813\n",
            "Epoch: 20/30, step: 240/364, loss: 0.33672, accuracy: 0.85833\n",
            "Epoch: 20/30, step: 241/364, loss: 0.33644, accuracy: 0.85873\n",
            "Epoch: 20/30, step: 242/364, loss: 0.33690, accuracy: 0.85834\n",
            "Epoch: 20/30, step: 243/364, loss: 0.33673, accuracy: 0.85847\n",
            "Epoch: 20/30, step: 244/364, loss: 0.33673, accuracy: 0.85854\n",
            "Epoch: 20/30, step: 245/364, loss: 0.33698, accuracy: 0.85835\n",
            "Epoch: 20/30, step: 246/364, loss: 0.33710, accuracy: 0.85804\n",
            "Epoch: 20/30, step: 247/364, loss: 0.33690, accuracy: 0.85811\n",
            "Epoch: 20/30, step: 248/364, loss: 0.33687, accuracy: 0.85818\n",
            "Epoch: 20/30, step: 249/364, loss: 0.33664, accuracy: 0.85812\n",
            "Epoch: 20/30, step: 250/364, loss: 0.33676, accuracy: 0.85800\n",
            "Epoch: 20/30, step: 251/364, loss: 0.33699, accuracy: 0.85788\n",
            "Epoch: 20/30, step: 252/364, loss: 0.33700, accuracy: 0.85795\n",
            "Epoch: 20/30, step: 253/364, loss: 0.33680, accuracy: 0.85808\n",
            "Epoch: 20/30, step: 254/364, loss: 0.33658, accuracy: 0.85827\n",
            "Epoch: 20/30, step: 255/364, loss: 0.33671, accuracy: 0.85827\n",
            "Epoch: 20/30, step: 256/364, loss: 0.33662, accuracy: 0.85828\n",
            "Epoch: 20/30, step: 257/364, loss: 0.33665, accuracy: 0.85834\n",
            "Epoch: 20/30, step: 258/364, loss: 0.33644, accuracy: 0.85841\n",
            "Epoch: 20/30, step: 259/364, loss: 0.33613, accuracy: 0.85877\n",
            "Epoch: 20/30, step: 260/364, loss: 0.33618, accuracy: 0.85865\n",
            "Epoch: 20/30, step: 261/364, loss: 0.33612, accuracy: 0.85884\n",
            "Epoch: 20/30, step: 262/364, loss: 0.33622, accuracy: 0.85884\n",
            "Epoch: 20/30, step: 263/364, loss: 0.33597, accuracy: 0.85908\n",
            "Epoch: 20/30, step: 264/364, loss: 0.33625, accuracy: 0.85884\n",
            "Epoch: 20/30, step: 265/364, loss: 0.33597, accuracy: 0.85914\n",
            "Epoch: 20/30, step: 266/364, loss: 0.33646, accuracy: 0.85896\n",
            "Epoch: 20/30, step: 267/364, loss: 0.33600, accuracy: 0.85926\n",
            "Epoch: 20/30, step: 268/364, loss: 0.33593, accuracy: 0.85932\n",
            "Epoch: 20/30, step: 269/364, loss: 0.33609, accuracy: 0.85938\n",
            "Epoch: 20/30, step: 270/364, loss: 0.33602, accuracy: 0.85938\n",
            "Epoch: 20/30, step: 271/364, loss: 0.33591, accuracy: 0.85938\n",
            "Epoch: 20/30, step: 272/364, loss: 0.33587, accuracy: 0.85943\n",
            "Epoch: 20/30, step: 273/364, loss: 0.33587, accuracy: 0.85932\n",
            "Epoch: 20/30, step: 274/364, loss: 0.33567, accuracy: 0.85955\n",
            "Epoch: 20/30, step: 275/364, loss: 0.33552, accuracy: 0.85960\n",
            "Epoch: 20/30, step: 276/364, loss: 0.33561, accuracy: 0.85926\n",
            "Epoch: 20/30, step: 277/364, loss: 0.33551, accuracy: 0.85921\n",
            "Epoch: 20/30, step: 278/364, loss: 0.33546, accuracy: 0.85926\n",
            "Epoch: 20/30, step: 279/364, loss: 0.33495, accuracy: 0.85960\n",
            "Epoch: 20/30, step: 280/364, loss: 0.33477, accuracy: 0.85982\n",
            "Epoch: 20/30, step: 281/364, loss: 0.33442, accuracy: 0.86010\n",
            "Epoch: 20/30, step: 282/364, loss: 0.33428, accuracy: 0.86015\n",
            "Epoch: 20/30, step: 283/364, loss: 0.33416, accuracy: 0.86026\n",
            "Epoch: 20/30, step: 284/364, loss: 0.33404, accuracy: 0.86026\n",
            "Epoch: 20/30, step: 285/364, loss: 0.33412, accuracy: 0.86031\n",
            "Epoch: 20/30, step: 286/364, loss: 0.33418, accuracy: 0.86025\n",
            "Epoch: 20/30, step: 287/364, loss: 0.33459, accuracy: 0.85970\n",
            "Epoch: 20/30, step: 288/364, loss: 0.33446, accuracy: 0.85975\n",
            "Epoch: 20/30, step: 289/364, loss: 0.33422, accuracy: 0.85986\n",
            "Epoch: 20/30, step: 290/364, loss: 0.33405, accuracy: 0.86002\n",
            "Epoch: 20/30, step: 291/364, loss: 0.33416, accuracy: 0.86008\n",
            "Epoch: 20/30, train loss: 0.33416, train accuracy: 0.86008, valid loss: 0.66015, valid accuracy: 0.68250\n",
            "Epoch: 21/30, step: 1/364, loss: 0.28338, accuracy: 0.90625\n",
            "Epoch: 21/30, step: 2/364, loss: 0.34558, accuracy: 0.87500\n",
            "Epoch: 21/30, step: 3/364, loss: 0.36727, accuracy: 0.84375\n",
            "Epoch: 21/30, step: 4/364, loss: 0.38304, accuracy: 0.82422\n",
            "Epoch: 21/30, step: 5/364, loss: 0.37859, accuracy: 0.82187\n",
            "Epoch: 21/30, step: 6/364, loss: 0.35549, accuracy: 0.84115\n",
            "Epoch: 21/30, step: 7/364, loss: 0.33163, accuracy: 0.86161\n",
            "Epoch: 21/30, step: 8/364, loss: 0.31827, accuracy: 0.87500\n",
            "Epoch: 21/30, step: 9/364, loss: 0.32025, accuracy: 0.87500\n",
            "Epoch: 21/30, step: 10/364, loss: 0.32282, accuracy: 0.87187\n",
            "Epoch: 21/30, step: 11/364, loss: 0.32058, accuracy: 0.87500\n",
            "Epoch: 21/30, step: 12/364, loss: 0.31476, accuracy: 0.87630\n",
            "Epoch: 21/30, step: 13/364, loss: 0.30977, accuracy: 0.87861\n",
            "Epoch: 21/30, step: 14/364, loss: 0.30917, accuracy: 0.88058\n",
            "Epoch: 21/30, step: 15/364, loss: 0.31321, accuracy: 0.87500\n",
            "Epoch: 21/30, step: 16/364, loss: 0.30825, accuracy: 0.87988\n",
            "Epoch: 21/30, step: 17/364, loss: 0.30918, accuracy: 0.87684\n",
            "Epoch: 21/30, step: 18/364, loss: 0.30885, accuracy: 0.87587\n",
            "Epoch: 21/30, step: 19/364, loss: 0.31421, accuracy: 0.87171\n",
            "Epoch: 21/30, step: 20/364, loss: 0.31118, accuracy: 0.87344\n",
            "Epoch: 21/30, step: 21/364, loss: 0.31006, accuracy: 0.87500\n",
            "Epoch: 21/30, step: 22/364, loss: 0.31479, accuracy: 0.87358\n",
            "Epoch: 21/30, step: 23/364, loss: 0.31330, accuracy: 0.87500\n",
            "Epoch: 21/30, step: 24/364, loss: 0.31269, accuracy: 0.87565\n",
            "Epoch: 21/30, step: 25/364, loss: 0.31817, accuracy: 0.87313\n",
            "Epoch: 21/30, step: 26/364, loss: 0.31508, accuracy: 0.87620\n",
            "Epoch: 21/30, step: 27/364, loss: 0.31659, accuracy: 0.87616\n",
            "Epoch: 21/30, step: 28/364, loss: 0.31601, accuracy: 0.87612\n",
            "Epoch: 21/30, step: 29/364, loss: 0.31516, accuracy: 0.87662\n",
            "Epoch: 21/30, step: 30/364, loss: 0.31566, accuracy: 0.87708\n",
            "Epoch: 21/30, step: 31/364, loss: 0.31479, accuracy: 0.87802\n",
            "Epoch: 21/30, step: 32/364, loss: 0.31301, accuracy: 0.87939\n",
            "Epoch: 21/30, step: 33/364, loss: 0.31148, accuracy: 0.87973\n",
            "Epoch: 21/30, step: 34/364, loss: 0.31205, accuracy: 0.87960\n",
            "Epoch: 21/30, step: 35/364, loss: 0.31388, accuracy: 0.87679\n",
            "Epoch: 21/30, step: 36/364, loss: 0.31375, accuracy: 0.87543\n",
            "Epoch: 21/30, step: 37/364, loss: 0.31192, accuracy: 0.87627\n",
            "Epoch: 21/30, step: 38/364, loss: 0.31266, accuracy: 0.87500\n",
            "Epoch: 21/30, step: 39/364, loss: 0.31211, accuracy: 0.87540\n",
            "Epoch: 21/30, step: 40/364, loss: 0.31008, accuracy: 0.87656\n",
            "Epoch: 21/30, step: 41/364, loss: 0.30917, accuracy: 0.87691\n",
            "Epoch: 21/30, step: 42/364, loss: 0.30861, accuracy: 0.87798\n",
            "Epoch: 21/30, step: 43/364, loss: 0.30805, accuracy: 0.87827\n",
            "Epoch: 21/30, step: 44/364, loss: 0.30947, accuracy: 0.87678\n",
            "Epoch: 21/30, step: 45/364, loss: 0.31069, accuracy: 0.87535\n",
            "Epoch: 21/30, step: 46/364, loss: 0.31295, accuracy: 0.87330\n",
            "Epoch: 21/30, step: 47/364, loss: 0.31166, accuracy: 0.87434\n",
            "Epoch: 21/30, step: 48/364, loss: 0.31372, accuracy: 0.87207\n",
            "Epoch: 21/30, step: 49/364, loss: 0.31467, accuracy: 0.87245\n",
            "Epoch: 21/30, step: 50/364, loss: 0.31509, accuracy: 0.87219\n",
            "Epoch: 21/30, step: 51/364, loss: 0.31440, accuracy: 0.87286\n",
            "Epoch: 21/30, step: 52/364, loss: 0.31469, accuracy: 0.87260\n",
            "Epoch: 21/30, step: 53/364, loss: 0.31475, accuracy: 0.87294\n",
            "Epoch: 21/30, step: 54/364, loss: 0.31485, accuracy: 0.87240\n",
            "Epoch: 21/30, step: 55/364, loss: 0.31387, accuracy: 0.87273\n",
            "Epoch: 21/30, step: 56/364, loss: 0.31629, accuracy: 0.87081\n",
            "Epoch: 21/30, step: 57/364, loss: 0.31683, accuracy: 0.87089\n",
            "Epoch: 21/30, step: 58/364, loss: 0.31663, accuracy: 0.87123\n",
            "Epoch: 21/30, step: 59/364, loss: 0.31681, accuracy: 0.87050\n",
            "Epoch: 21/30, step: 60/364, loss: 0.31686, accuracy: 0.87083\n",
            "Epoch: 21/30, step: 61/364, loss: 0.31555, accuracy: 0.87218\n",
            "Epoch: 21/30, step: 62/364, loss: 0.31583, accuracy: 0.87198\n",
            "Epoch: 21/30, step: 63/364, loss: 0.31652, accuracy: 0.87202\n",
            "Epoch: 21/30, step: 64/364, loss: 0.31857, accuracy: 0.87085\n",
            "Epoch: 21/30, step: 65/364, loss: 0.31728, accuracy: 0.87163\n",
            "Epoch: 21/30, step: 66/364, loss: 0.31619, accuracy: 0.87216\n",
            "Epoch: 21/30, step: 67/364, loss: 0.31645, accuracy: 0.87150\n",
            "Epoch: 21/30, step: 68/364, loss: 0.31547, accuracy: 0.87224\n",
            "Epoch: 21/30, step: 69/364, loss: 0.31658, accuracy: 0.87092\n",
            "Epoch: 21/30, step: 70/364, loss: 0.31589, accuracy: 0.87121\n",
            "Epoch: 21/30, step: 71/364, loss: 0.31590, accuracy: 0.87082\n",
            "Epoch: 21/30, step: 72/364, loss: 0.31633, accuracy: 0.87088\n",
            "Epoch: 21/30, step: 73/364, loss: 0.31832, accuracy: 0.86965\n",
            "Epoch: 21/30, step: 74/364, loss: 0.31839, accuracy: 0.86930\n",
            "Epoch: 21/30, step: 75/364, loss: 0.31822, accuracy: 0.86896\n",
            "Epoch: 21/30, step: 76/364, loss: 0.31821, accuracy: 0.86863\n",
            "Epoch: 21/30, step: 77/364, loss: 0.31810, accuracy: 0.86912\n",
            "Epoch: 21/30, step: 78/364, loss: 0.31812, accuracy: 0.86899\n",
            "Epoch: 21/30, step: 79/364, loss: 0.31728, accuracy: 0.86946\n",
            "Epoch: 21/30, step: 80/364, loss: 0.31750, accuracy: 0.86934\n",
            "Epoch: 21/30, step: 81/364, loss: 0.31716, accuracy: 0.86979\n",
            "Epoch: 21/30, step: 82/364, loss: 0.31769, accuracy: 0.86890\n",
            "Epoch: 21/30, step: 83/364, loss: 0.31975, accuracy: 0.86785\n",
            "Epoch: 21/30, step: 84/364, loss: 0.31993, accuracy: 0.86663\n",
            "Epoch: 21/30, step: 85/364, loss: 0.31973, accuracy: 0.86673\n",
            "Epoch: 21/30, step: 86/364, loss: 0.31940, accuracy: 0.86664\n",
            "Epoch: 21/30, step: 87/364, loss: 0.31931, accuracy: 0.86674\n",
            "Epoch: 21/30, step: 88/364, loss: 0.31842, accuracy: 0.86737\n",
            "Epoch: 21/30, step: 89/364, loss: 0.31927, accuracy: 0.86692\n",
            "Epoch: 21/30, step: 90/364, loss: 0.31937, accuracy: 0.86667\n",
            "Epoch: 21/30, step: 91/364, loss: 0.31847, accuracy: 0.86762\n",
            "Epoch: 21/30, step: 92/364, loss: 0.31921, accuracy: 0.86719\n",
            "Epoch: 21/30, step: 93/364, loss: 0.31888, accuracy: 0.86761\n",
            "Epoch: 21/30, step: 94/364, loss: 0.31859, accuracy: 0.86802\n",
            "Epoch: 21/30, step: 95/364, loss: 0.31741, accuracy: 0.86908\n",
            "Epoch: 21/30, step: 96/364, loss: 0.31739, accuracy: 0.86882\n",
            "Epoch: 21/30, step: 97/364, loss: 0.31706, accuracy: 0.86920\n",
            "Epoch: 21/30, step: 98/364, loss: 0.31659, accuracy: 0.86958\n",
            "Epoch: 21/30, step: 99/364, loss: 0.31603, accuracy: 0.87027\n",
            "Epoch: 21/30, step: 100/364, loss: 0.31542, accuracy: 0.87063\n",
            "Epoch: 21/30, step: 101/364, loss: 0.31444, accuracy: 0.87144\n",
            "Epoch: 21/30, step: 102/364, loss: 0.31459, accuracy: 0.87132\n",
            "Epoch: 21/30, step: 103/364, loss: 0.31437, accuracy: 0.87166\n",
            "Epoch: 21/30, step: 104/364, loss: 0.31433, accuracy: 0.87154\n",
            "Epoch: 21/30, step: 105/364, loss: 0.31423, accuracy: 0.87187\n",
            "Epoch: 21/30, step: 106/364, loss: 0.31389, accuracy: 0.87235\n",
            "Epoch: 21/30, step: 107/364, loss: 0.31422, accuracy: 0.87223\n",
            "Epoch: 21/30, step: 108/364, loss: 0.31525, accuracy: 0.87153\n",
            "Epoch: 21/30, step: 109/364, loss: 0.31502, accuracy: 0.87170\n",
            "Epoch: 21/30, step: 110/364, loss: 0.31492, accuracy: 0.87159\n",
            "Epoch: 21/30, step: 111/364, loss: 0.31481, accuracy: 0.87190\n",
            "Epoch: 21/30, step: 112/364, loss: 0.31415, accuracy: 0.87263\n",
            "Epoch: 21/30, step: 113/364, loss: 0.31379, accuracy: 0.87293\n",
            "Epoch: 21/30, step: 114/364, loss: 0.31475, accuracy: 0.87212\n",
            "Epoch: 21/30, step: 115/364, loss: 0.31563, accuracy: 0.87147\n",
            "Epoch: 21/30, step: 116/364, loss: 0.31531, accuracy: 0.87217\n",
            "Epoch: 21/30, step: 117/364, loss: 0.31667, accuracy: 0.87126\n",
            "Epoch: 21/30, step: 118/364, loss: 0.31609, accuracy: 0.87169\n",
            "Epoch: 21/30, step: 119/364, loss: 0.31573, accuracy: 0.87198\n",
            "Epoch: 21/30, step: 120/364, loss: 0.31644, accuracy: 0.87109\n",
            "Epoch: 21/30, step: 121/364, loss: 0.31680, accuracy: 0.87087\n",
            "Epoch: 21/30, step: 122/364, loss: 0.31681, accuracy: 0.87077\n",
            "Epoch: 21/30, step: 123/364, loss: 0.31711, accuracy: 0.87043\n",
            "Epoch: 21/30, step: 124/364, loss: 0.31695, accuracy: 0.87059\n",
            "Epoch: 21/30, step: 125/364, loss: 0.31622, accuracy: 0.87125\n",
            "Epoch: 21/30, step: 126/364, loss: 0.31620, accuracy: 0.87103\n",
            "Epoch: 21/30, step: 127/364, loss: 0.31586, accuracy: 0.87119\n",
            "Epoch: 21/30, step: 128/364, loss: 0.31677, accuracy: 0.87048\n",
            "Epoch: 21/30, step: 129/364, loss: 0.31666, accuracy: 0.87064\n",
            "Epoch: 21/30, step: 130/364, loss: 0.31680, accuracy: 0.87043\n",
            "Epoch: 21/30, step: 131/364, loss: 0.31645, accuracy: 0.87083\n",
            "Epoch: 21/30, step: 132/364, loss: 0.31597, accuracy: 0.87121\n",
            "Epoch: 21/30, step: 133/364, loss: 0.31496, accuracy: 0.87159\n",
            "Epoch: 21/30, step: 134/364, loss: 0.31463, accuracy: 0.87162\n",
            "Epoch: 21/30, step: 135/364, loss: 0.31603, accuracy: 0.87049\n",
            "Epoch: 21/30, step: 136/364, loss: 0.31610, accuracy: 0.87063\n",
            "Epoch: 21/30, step: 137/364, loss: 0.31590, accuracy: 0.87078\n",
            "Epoch: 21/30, step: 138/364, loss: 0.31493, accuracy: 0.87126\n",
            "Epoch: 21/30, step: 139/364, loss: 0.31482, accuracy: 0.87140\n",
            "Epoch: 21/30, step: 140/364, loss: 0.31415, accuracy: 0.87176\n",
            "Epoch: 21/30, step: 141/364, loss: 0.31406, accuracy: 0.87179\n",
            "Epoch: 21/30, step: 142/364, loss: 0.31404, accuracy: 0.87170\n",
            "Epoch: 21/30, step: 143/364, loss: 0.31395, accuracy: 0.87172\n",
            "Epoch: 21/30, step: 144/364, loss: 0.31465, accuracy: 0.87109\n",
            "Epoch: 21/30, step: 145/364, loss: 0.31430, accuracy: 0.87134\n",
            "Epoch: 21/30, step: 146/364, loss: 0.31446, accuracy: 0.87147\n",
            "Epoch: 21/30, step: 147/364, loss: 0.31473, accuracy: 0.87160\n",
            "Epoch: 21/30, step: 148/364, loss: 0.31455, accuracy: 0.87162\n",
            "Epoch: 21/30, step: 149/364, loss: 0.31502, accuracy: 0.87091\n",
            "Epoch: 21/30, step: 150/364, loss: 0.31503, accuracy: 0.87125\n",
            "Epoch: 21/30, step: 151/364, loss: 0.31440, accuracy: 0.87190\n",
            "Epoch: 21/30, step: 152/364, loss: 0.31531, accuracy: 0.87192\n",
            "Epoch: 21/30, step: 153/364, loss: 0.31511, accuracy: 0.87214\n",
            "Epoch: 21/30, step: 154/364, loss: 0.31450, accuracy: 0.87246\n",
            "Epoch: 21/30, step: 155/364, loss: 0.31459, accuracy: 0.87238\n",
            "Epoch: 21/30, step: 156/364, loss: 0.31429, accuracy: 0.87250\n",
            "Epoch: 21/30, step: 157/364, loss: 0.31403, accuracy: 0.87281\n",
            "Epoch: 21/30, step: 158/364, loss: 0.31441, accuracy: 0.87233\n",
            "Epoch: 21/30, step: 159/364, loss: 0.31495, accuracy: 0.87215\n",
            "Epoch: 21/30, step: 160/364, loss: 0.31560, accuracy: 0.87158\n",
            "Epoch: 21/30, step: 161/364, loss: 0.31526, accuracy: 0.87180\n",
            "Epoch: 21/30, step: 162/364, loss: 0.31536, accuracy: 0.87172\n",
            "Epoch: 21/30, step: 163/364, loss: 0.31538, accuracy: 0.87174\n",
            "Epoch: 21/30, step: 164/364, loss: 0.31498, accuracy: 0.87214\n",
            "Epoch: 21/30, step: 165/364, loss: 0.31466, accuracy: 0.87216\n",
            "Epoch: 21/30, step: 166/364, loss: 0.31427, accuracy: 0.87236\n",
            "Epoch: 21/30, step: 167/364, loss: 0.31372, accuracy: 0.87275\n",
            "Epoch: 21/30, step: 168/364, loss: 0.31401, accuracy: 0.87240\n",
            "Epoch: 21/30, step: 169/364, loss: 0.31426, accuracy: 0.87232\n",
            "Epoch: 21/30, step: 170/364, loss: 0.31413, accuracy: 0.87224\n",
            "Epoch: 21/30, step: 171/364, loss: 0.31459, accuracy: 0.87198\n",
            "Epoch: 21/30, step: 172/364, loss: 0.31434, accuracy: 0.87227\n",
            "Epoch: 21/30, step: 173/364, loss: 0.31489, accuracy: 0.87184\n",
            "Epoch: 21/30, step: 174/364, loss: 0.31505, accuracy: 0.87150\n",
            "Epoch: 21/30, step: 175/364, loss: 0.31497, accuracy: 0.87125\n",
            "Epoch: 21/30, step: 176/364, loss: 0.31462, accuracy: 0.87154\n",
            "Epoch: 21/30, step: 177/364, loss: 0.31486, accuracy: 0.87129\n",
            "Epoch: 21/30, step: 178/364, loss: 0.31483, accuracy: 0.87131\n",
            "Epoch: 21/30, step: 179/364, loss: 0.31537, accuracy: 0.87107\n",
            "Epoch: 21/30, step: 180/364, loss: 0.31506, accuracy: 0.87135\n",
            "Epoch: 21/30, step: 181/364, loss: 0.31503, accuracy: 0.87137\n",
            "Epoch: 21/30, step: 182/364, loss: 0.31497, accuracy: 0.87174\n",
            "Epoch: 21/30, step: 183/364, loss: 0.31473, accuracy: 0.87184\n",
            "Epoch: 21/30, step: 184/364, loss: 0.31444, accuracy: 0.87194\n",
            "Epoch: 21/30, step: 185/364, loss: 0.31431, accuracy: 0.87238\n",
            "Epoch: 21/30, step: 186/364, loss: 0.31418, accuracy: 0.87240\n",
            "Epoch: 21/30, step: 187/364, loss: 0.31395, accuracy: 0.87233\n",
            "Epoch: 21/30, step: 188/364, loss: 0.31434, accuracy: 0.87217\n",
            "Epoch: 21/30, step: 189/364, loss: 0.31409, accuracy: 0.87227\n",
            "Epoch: 21/30, step: 190/364, loss: 0.31402, accuracy: 0.87237\n",
            "Epoch: 21/30, step: 191/364, loss: 0.31365, accuracy: 0.87246\n",
            "Epoch: 21/30, step: 192/364, loss: 0.31381, accuracy: 0.87231\n",
            "Epoch: 21/30, step: 193/364, loss: 0.31395, accuracy: 0.87200\n",
            "Epoch: 21/30, step: 194/364, loss: 0.31383, accuracy: 0.87226\n",
            "Epoch: 21/30, step: 195/364, loss: 0.31348, accuracy: 0.87252\n",
            "Epoch: 21/30, step: 196/364, loss: 0.31385, accuracy: 0.87237\n",
            "Epoch: 21/30, step: 197/364, loss: 0.31344, accuracy: 0.87278\n",
            "Epoch: 21/30, step: 198/364, loss: 0.31289, accuracy: 0.87318\n",
            "Epoch: 21/30, step: 199/364, loss: 0.31271, accuracy: 0.87327\n",
            "Epoch: 21/30, step: 200/364, loss: 0.31269, accuracy: 0.87328\n",
            "Epoch: 21/30, step: 201/364, loss: 0.31252, accuracy: 0.87345\n",
            "Epoch: 21/30, step: 202/364, loss: 0.31254, accuracy: 0.87338\n",
            "Epoch: 21/30, step: 203/364, loss: 0.31205, accuracy: 0.87377\n",
            "Epoch: 21/30, step: 204/364, loss: 0.31187, accuracy: 0.87377\n",
            "Epoch: 21/30, step: 205/364, loss: 0.31168, accuracy: 0.87386\n",
            "Epoch: 21/30, step: 206/364, loss: 0.31194, accuracy: 0.87379\n",
            "Epoch: 21/30, step: 207/364, loss: 0.31200, accuracy: 0.87357\n",
            "Epoch: 21/30, step: 208/364, loss: 0.31167, accuracy: 0.87372\n",
            "Epoch: 21/30, step: 209/364, loss: 0.31171, accuracy: 0.87336\n",
            "Epoch: 21/30, step: 210/364, loss: 0.31149, accuracy: 0.87351\n",
            "Epoch: 21/30, step: 211/364, loss: 0.31183, accuracy: 0.87337\n",
            "Epoch: 21/30, step: 212/364, loss: 0.31250, accuracy: 0.87301\n",
            "Epoch: 21/30, step: 213/364, loss: 0.31251, accuracy: 0.87295\n",
            "Epoch: 21/30, step: 214/364, loss: 0.31288, accuracy: 0.87266\n",
            "Epoch: 21/30, step: 215/364, loss: 0.31328, accuracy: 0.87231\n",
            "Epoch: 21/30, step: 216/364, loss: 0.31367, accuracy: 0.87218\n",
            "Epoch: 21/30, step: 217/364, loss: 0.31382, accuracy: 0.87212\n",
            "Epoch: 21/30, step: 218/364, loss: 0.31408, accuracy: 0.87206\n",
            "Epoch: 21/30, step: 219/364, loss: 0.31383, accuracy: 0.87229\n",
            "Epoch: 21/30, step: 220/364, loss: 0.31392, accuracy: 0.87223\n",
            "Epoch: 21/30, step: 221/364, loss: 0.31336, accuracy: 0.87260\n",
            "Epoch: 21/30, step: 222/364, loss: 0.31328, accuracy: 0.87261\n",
            "Epoch: 21/30, step: 223/364, loss: 0.31349, accuracy: 0.87241\n",
            "Epoch: 21/30, step: 224/364, loss: 0.31296, accuracy: 0.87284\n",
            "Epoch: 21/30, step: 225/364, loss: 0.31320, accuracy: 0.87278\n",
            "Epoch: 21/30, step: 226/364, loss: 0.31342, accuracy: 0.87258\n",
            "Epoch: 21/30, step: 227/364, loss: 0.31364, accuracy: 0.87232\n",
            "Epoch: 21/30, step: 228/364, loss: 0.31350, accuracy: 0.87233\n",
            "Epoch: 21/30, step: 229/364, loss: 0.31368, accuracy: 0.87227\n",
            "Epoch: 21/30, step: 230/364, loss: 0.31347, accuracy: 0.87242\n",
            "Epoch: 21/30, step: 231/364, loss: 0.31330, accuracy: 0.87263\n",
            "Epoch: 21/30, step: 232/364, loss: 0.31328, accuracy: 0.87258\n",
            "Epoch: 21/30, step: 233/364, loss: 0.31303, accuracy: 0.87272\n",
            "Epoch: 21/30, step: 234/364, loss: 0.31311, accuracy: 0.87280\n",
            "Epoch: 21/30, step: 235/364, loss: 0.31283, accuracy: 0.87320\n",
            "Epoch: 21/30, step: 236/364, loss: 0.31254, accuracy: 0.87348\n",
            "Epoch: 21/30, step: 237/364, loss: 0.31212, accuracy: 0.87368\n",
            "Epoch: 21/30, step: 238/364, loss: 0.31205, accuracy: 0.87382\n",
            "Epoch: 21/30, step: 239/364, loss: 0.31170, accuracy: 0.87408\n",
            "Epoch: 21/30, step: 240/364, loss: 0.31178, accuracy: 0.87396\n",
            "Epoch: 21/30, step: 241/364, loss: 0.31167, accuracy: 0.87390\n",
            "Epoch: 21/30, step: 242/364, loss: 0.31144, accuracy: 0.87397\n",
            "Epoch: 21/30, step: 243/364, loss: 0.31127, accuracy: 0.87404\n",
            "Epoch: 21/30, step: 244/364, loss: 0.31144, accuracy: 0.87410\n",
            "Epoch: 21/30, step: 245/364, loss: 0.31113, accuracy: 0.87430\n",
            "Epoch: 21/30, step: 246/364, loss: 0.31140, accuracy: 0.87411\n",
            "Epoch: 21/30, step: 247/364, loss: 0.31116, accuracy: 0.87424\n",
            "Epoch: 21/30, step: 248/364, loss: 0.31147, accuracy: 0.87424\n",
            "Epoch: 21/30, step: 249/364, loss: 0.31146, accuracy: 0.87412\n",
            "Epoch: 21/30, step: 250/364, loss: 0.31129, accuracy: 0.87425\n",
            "Epoch: 21/30, step: 251/364, loss: 0.31133, accuracy: 0.87419\n",
            "Epoch: 21/30, step: 252/364, loss: 0.31173, accuracy: 0.87395\n",
            "Epoch: 21/30, step: 253/364, loss: 0.31230, accuracy: 0.87370\n",
            "Epoch: 21/30, step: 254/364, loss: 0.31228, accuracy: 0.87377\n",
            "Epoch: 21/30, step: 255/364, loss: 0.31212, accuracy: 0.87384\n",
            "Epoch: 21/30, step: 256/364, loss: 0.31200, accuracy: 0.87396\n",
            "Epoch: 21/30, step: 257/364, loss: 0.31184, accuracy: 0.87421\n",
            "Epoch: 21/30, step: 258/364, loss: 0.31162, accuracy: 0.87427\n",
            "Epoch: 21/30, step: 259/364, loss: 0.31169, accuracy: 0.87422\n",
            "Epoch: 21/30, step: 260/364, loss: 0.31151, accuracy: 0.87440\n",
            "Epoch: 21/30, step: 261/364, loss: 0.31143, accuracy: 0.87440\n",
            "Epoch: 21/30, step: 262/364, loss: 0.31147, accuracy: 0.87434\n",
            "Epoch: 21/30, step: 263/364, loss: 0.31157, accuracy: 0.87423\n",
            "Epoch: 21/30, step: 264/364, loss: 0.31192, accuracy: 0.87405\n",
            "Epoch: 21/30, step: 265/364, loss: 0.31209, accuracy: 0.87400\n",
            "Epoch: 21/30, step: 266/364, loss: 0.31168, accuracy: 0.87424\n",
            "Epoch: 21/30, step: 267/364, loss: 0.31152, accuracy: 0.87436\n",
            "Epoch: 21/30, step: 268/364, loss: 0.31154, accuracy: 0.87436\n",
            "Epoch: 21/30, step: 269/364, loss: 0.31150, accuracy: 0.87442\n",
            "Epoch: 21/30, step: 270/364, loss: 0.31135, accuracy: 0.87454\n",
            "Epoch: 21/30, step: 271/364, loss: 0.31134, accuracy: 0.87448\n",
            "Epoch: 21/30, step: 272/364, loss: 0.31133, accuracy: 0.87454\n",
            "Epoch: 21/30, step: 273/364, loss: 0.31131, accuracy: 0.87448\n",
            "Epoch: 21/30, step: 274/364, loss: 0.31184, accuracy: 0.87432\n",
            "Epoch: 21/30, step: 275/364, loss: 0.31167, accuracy: 0.87432\n",
            "Epoch: 21/30, step: 276/364, loss: 0.31149, accuracy: 0.87438\n",
            "Epoch: 21/30, step: 277/364, loss: 0.31148, accuracy: 0.87432\n",
            "Epoch: 21/30, step: 278/364, loss: 0.31146, accuracy: 0.87433\n",
            "Epoch: 21/30, step: 279/364, loss: 0.31170, accuracy: 0.87410\n",
            "Epoch: 21/30, step: 280/364, loss: 0.31174, accuracy: 0.87400\n",
            "Epoch: 21/30, step: 281/364, loss: 0.31183, accuracy: 0.87405\n",
            "Epoch: 21/30, step: 282/364, loss: 0.31155, accuracy: 0.87422\n",
            "Epoch: 21/30, step: 283/364, loss: 0.31161, accuracy: 0.87412\n",
            "Epoch: 21/30, step: 284/364, loss: 0.31141, accuracy: 0.87434\n",
            "Epoch: 21/30, step: 285/364, loss: 0.31171, accuracy: 0.87412\n",
            "Epoch: 21/30, step: 286/364, loss: 0.31206, accuracy: 0.87396\n",
            "Epoch: 21/30, step: 287/364, loss: 0.31164, accuracy: 0.87429\n",
            "Epoch: 21/30, step: 288/364, loss: 0.31187, accuracy: 0.87397\n",
            "Epoch: 21/30, step: 289/364, loss: 0.31178, accuracy: 0.87397\n",
            "Epoch: 21/30, step: 290/364, loss: 0.31217, accuracy: 0.87355\n",
            "Epoch: 21/30, step: 291/364, loss: 0.31190, accuracy: 0.87372\n",
            "Epoch: 21/30, train loss: 0.31190, train accuracy: 0.87372, valid loss: 0.67219, valid accuracy: 0.69089\n",
            "Epoch: 22/30, step: 1/364, loss: 0.29925, accuracy: 0.85938\n",
            "Epoch: 22/30, step: 2/364, loss: 0.27238, accuracy: 0.89062\n",
            "Epoch: 22/30, step: 3/364, loss: 0.27969, accuracy: 0.89583\n",
            "Epoch: 22/30, step: 4/364, loss: 0.27172, accuracy: 0.90234\n",
            "Epoch: 22/30, step: 5/364, loss: 0.29197, accuracy: 0.90000\n",
            "Epoch: 22/30, step: 6/364, loss: 0.30663, accuracy: 0.89062\n",
            "Epoch: 22/30, step: 7/364, loss: 0.29748, accuracy: 0.89509\n",
            "Epoch: 22/30, step: 8/364, loss: 0.29957, accuracy: 0.89453\n",
            "Epoch: 22/30, step: 9/364, loss: 0.30796, accuracy: 0.88021\n",
            "Epoch: 22/30, step: 10/364, loss: 0.30007, accuracy: 0.88594\n",
            "Epoch: 22/30, step: 11/364, loss: 0.30024, accuracy: 0.88494\n",
            "Epoch: 22/30, step: 12/364, loss: 0.29804, accuracy: 0.89062\n",
            "Epoch: 22/30, step: 13/364, loss: 0.30037, accuracy: 0.88702\n",
            "Epoch: 22/30, step: 14/364, loss: 0.30099, accuracy: 0.88839\n",
            "Epoch: 22/30, step: 15/364, loss: 0.29629, accuracy: 0.88958\n",
            "Epoch: 22/30, step: 16/364, loss: 0.29439, accuracy: 0.89160\n",
            "Epoch: 22/30, step: 17/364, loss: 0.29483, accuracy: 0.88971\n",
            "Epoch: 22/30, step: 18/364, loss: 0.30004, accuracy: 0.88715\n",
            "Epoch: 22/30, step: 19/364, loss: 0.29934, accuracy: 0.88487\n",
            "Epoch: 22/30, step: 20/364, loss: 0.30322, accuracy: 0.88203\n",
            "Epoch: 22/30, step: 21/364, loss: 0.30249, accuracy: 0.88170\n",
            "Epoch: 22/30, step: 22/364, loss: 0.30030, accuracy: 0.88281\n",
            "Epoch: 22/30, step: 23/364, loss: 0.29959, accuracy: 0.88383\n",
            "Epoch: 22/30, step: 24/364, loss: 0.29769, accuracy: 0.88607\n",
            "Epoch: 22/30, step: 25/364, loss: 0.29685, accuracy: 0.88563\n",
            "Epoch: 22/30, step: 26/364, loss: 0.29729, accuracy: 0.88582\n",
            "Epoch: 22/30, step: 27/364, loss: 0.29502, accuracy: 0.88773\n",
            "Epoch: 22/30, step: 28/364, loss: 0.29624, accuracy: 0.88616\n",
            "Epoch: 22/30, step: 29/364, loss: 0.29741, accuracy: 0.88416\n",
            "Epoch: 22/30, step: 30/364, loss: 0.30108, accuracy: 0.88125\n",
            "Epoch: 22/30, step: 31/364, loss: 0.30190, accuracy: 0.88004\n",
            "Epoch: 22/30, step: 32/364, loss: 0.30008, accuracy: 0.88184\n",
            "Epoch: 22/30, step: 33/364, loss: 0.30129, accuracy: 0.88021\n",
            "Epoch: 22/30, step: 34/364, loss: 0.30255, accuracy: 0.87868\n",
            "Epoch: 22/30, step: 35/364, loss: 0.30229, accuracy: 0.87857\n",
            "Epoch: 22/30, step: 36/364, loss: 0.29941, accuracy: 0.88064\n",
            "Epoch: 22/30, step: 37/364, loss: 0.29840, accuracy: 0.88176\n",
            "Epoch: 22/30, step: 38/364, loss: 0.29865, accuracy: 0.88240\n",
            "Epoch: 22/30, step: 39/364, loss: 0.29976, accuracy: 0.88261\n",
            "Epoch: 22/30, step: 40/364, loss: 0.29870, accuracy: 0.88320\n",
            "Epoch: 22/30, step: 41/364, loss: 0.29756, accuracy: 0.88377\n",
            "Epoch: 22/30, step: 42/364, loss: 0.29795, accuracy: 0.88356\n",
            "Epoch: 22/30, step: 43/364, loss: 0.30036, accuracy: 0.88154\n",
            "Epoch: 22/30, step: 44/364, loss: 0.30107, accuracy: 0.88104\n",
            "Epoch: 22/30, step: 45/364, loss: 0.30296, accuracy: 0.87951\n",
            "Epoch: 22/30, step: 46/364, loss: 0.30295, accuracy: 0.88010\n",
            "Epoch: 22/30, step: 47/364, loss: 0.30267, accuracy: 0.87932\n",
            "Epoch: 22/30, step: 48/364, loss: 0.30398, accuracy: 0.87891\n",
            "Epoch: 22/30, step: 49/364, loss: 0.30295, accuracy: 0.87946\n",
            "Epoch: 22/30, step: 50/364, loss: 0.30214, accuracy: 0.88031\n",
            "Epoch: 22/30, step: 51/364, loss: 0.30068, accuracy: 0.88082\n",
            "Epoch: 22/30, step: 52/364, loss: 0.30084, accuracy: 0.88041\n",
            "Epoch: 22/30, step: 53/364, loss: 0.30045, accuracy: 0.88001\n",
            "Epoch: 22/30, step: 54/364, loss: 0.30134, accuracy: 0.87934\n",
            "Epoch: 22/30, step: 55/364, loss: 0.29993, accuracy: 0.87983\n",
            "Epoch: 22/30, step: 56/364, loss: 0.29931, accuracy: 0.88002\n",
            "Epoch: 22/30, step: 57/364, loss: 0.30006, accuracy: 0.87966\n",
            "Epoch: 22/30, step: 58/364, loss: 0.29965, accuracy: 0.88039\n",
            "Epoch: 22/30, step: 59/364, loss: 0.30121, accuracy: 0.87897\n",
            "Epoch: 22/30, step: 60/364, loss: 0.30052, accuracy: 0.87891\n",
            "Epoch: 22/30, step: 61/364, loss: 0.30025, accuracy: 0.87859\n",
            "Epoch: 22/30, step: 62/364, loss: 0.30041, accuracy: 0.87903\n",
            "Epoch: 22/30, step: 63/364, loss: 0.30022, accuracy: 0.87946\n",
            "Epoch: 22/30, step: 64/364, loss: 0.30061, accuracy: 0.87915\n",
            "Epoch: 22/30, step: 65/364, loss: 0.30072, accuracy: 0.87837\n",
            "Epoch: 22/30, step: 66/364, loss: 0.30215, accuracy: 0.87689\n",
            "Epoch: 22/30, step: 67/364, loss: 0.30088, accuracy: 0.87803\n",
            "Epoch: 22/30, step: 68/364, loss: 0.29952, accuracy: 0.87914\n",
            "Epoch: 22/30, step: 69/364, loss: 0.29899, accuracy: 0.87976\n",
            "Epoch: 22/30, step: 70/364, loss: 0.29906, accuracy: 0.87969\n",
            "Epoch: 22/30, step: 71/364, loss: 0.29904, accuracy: 0.88006\n",
            "Epoch: 22/30, step: 72/364, loss: 0.29822, accuracy: 0.88064\n",
            "Epoch: 22/30, step: 73/364, loss: 0.29763, accuracy: 0.88057\n",
            "Epoch: 22/30, step: 74/364, loss: 0.29675, accuracy: 0.88070\n",
            "Epoch: 22/30, step: 75/364, loss: 0.29623, accuracy: 0.88083\n",
            "Epoch: 22/30, step: 76/364, loss: 0.29571, accuracy: 0.88158\n",
            "Epoch: 22/30, step: 77/364, loss: 0.29460, accuracy: 0.88251\n",
            "Epoch: 22/30, step: 78/364, loss: 0.29468, accuracy: 0.88221\n",
            "Epoch: 22/30, step: 79/364, loss: 0.29893, accuracy: 0.87975\n",
            "Epoch: 22/30, step: 80/364, loss: 0.29915, accuracy: 0.87949\n",
            "Epoch: 22/30, step: 81/364, loss: 0.29916, accuracy: 0.87924\n",
            "Epoch: 22/30, step: 82/364, loss: 0.29876, accuracy: 0.87919\n",
            "Epoch: 22/30, step: 83/364, loss: 0.29837, accuracy: 0.87914\n",
            "Epoch: 22/30, step: 84/364, loss: 0.29739, accuracy: 0.88002\n",
            "Epoch: 22/30, step: 85/364, loss: 0.29592, accuracy: 0.88107\n",
            "Epoch: 22/30, step: 86/364, loss: 0.29492, accuracy: 0.88118\n",
            "Epoch: 22/30, step: 87/364, loss: 0.29425, accuracy: 0.88111\n",
            "Epoch: 22/30, step: 88/364, loss: 0.29494, accuracy: 0.88086\n",
            "Epoch: 22/30, step: 89/364, loss: 0.29731, accuracy: 0.87956\n",
            "Epoch: 22/30, step: 90/364, loss: 0.29619, accuracy: 0.88073\n",
            "Epoch: 22/30, step: 91/364, loss: 0.29545, accuracy: 0.88135\n",
            "Epoch: 22/30, step: 92/364, loss: 0.29565, accuracy: 0.88111\n",
            "Epoch: 22/30, step: 93/364, loss: 0.29471, accuracy: 0.88172\n",
            "Epoch: 22/30, step: 94/364, loss: 0.29439, accuracy: 0.88165\n",
            "Epoch: 22/30, step: 95/364, loss: 0.29478, accuracy: 0.88125\n",
            "Epoch: 22/30, step: 96/364, loss: 0.29562, accuracy: 0.88053\n",
            "Epoch: 22/30, step: 97/364, loss: 0.29660, accuracy: 0.88015\n",
            "Epoch: 22/30, step: 98/364, loss: 0.29533, accuracy: 0.88122\n",
            "Epoch: 22/30, step: 99/364, loss: 0.29480, accuracy: 0.88163\n",
            "Epoch: 22/30, step: 100/364, loss: 0.29499, accuracy: 0.88156\n",
            "Epoch: 22/30, step: 101/364, loss: 0.29585, accuracy: 0.88103\n",
            "Epoch: 22/30, step: 102/364, loss: 0.29656, accuracy: 0.88097\n",
            "Epoch: 22/30, step: 103/364, loss: 0.29651, accuracy: 0.88137\n",
            "Epoch: 22/30, step: 104/364, loss: 0.29553, accuracy: 0.88221\n",
            "Epoch: 22/30, step: 105/364, loss: 0.29523, accuracy: 0.88244\n",
            "Epoch: 22/30, step: 106/364, loss: 0.29577, accuracy: 0.88208\n",
            "Epoch: 22/30, step: 107/364, loss: 0.29742, accuracy: 0.88113\n",
            "Epoch: 22/30, step: 108/364, loss: 0.29733, accuracy: 0.88122\n",
            "Epoch: 22/30, step: 109/364, loss: 0.29668, accuracy: 0.88188\n",
            "Epoch: 22/30, step: 110/364, loss: 0.29577, accuracy: 0.88239\n",
            "Epoch: 22/30, step: 111/364, loss: 0.29615, accuracy: 0.88218\n",
            "Epoch: 22/30, step: 112/364, loss: 0.29665, accuracy: 0.88114\n",
            "Epoch: 22/30, step: 113/364, loss: 0.29672, accuracy: 0.88108\n",
            "Epoch: 22/30, step: 114/364, loss: 0.29675, accuracy: 0.88103\n",
            "Epoch: 22/30, step: 115/364, loss: 0.29701, accuracy: 0.88084\n",
            "Epoch: 22/30, step: 116/364, loss: 0.29653, accuracy: 0.88133\n",
            "Epoch: 22/30, step: 117/364, loss: 0.29697, accuracy: 0.88101\n",
            "Epoch: 22/30, step: 118/364, loss: 0.29619, accuracy: 0.88122\n",
            "Epoch: 22/30, step: 119/364, loss: 0.29616, accuracy: 0.88130\n",
            "Epoch: 22/30, step: 120/364, loss: 0.29635, accuracy: 0.88151\n",
            "Epoch: 22/30, step: 121/364, loss: 0.29742, accuracy: 0.88107\n",
            "Epoch: 22/30, step: 122/364, loss: 0.29782, accuracy: 0.88051\n",
            "Epoch: 22/30, step: 123/364, loss: 0.29703, accuracy: 0.88084\n",
            "Epoch: 22/30, step: 124/364, loss: 0.29773, accuracy: 0.88067\n",
            "Epoch: 22/30, step: 125/364, loss: 0.29771, accuracy: 0.88087\n",
            "Epoch: 22/30, step: 126/364, loss: 0.29726, accuracy: 0.88120\n",
            "Epoch: 22/30, step: 127/364, loss: 0.29730, accuracy: 0.88115\n",
            "Epoch: 22/30, step: 128/364, loss: 0.29742, accuracy: 0.88123\n",
            "Epoch: 22/30, step: 129/364, loss: 0.29776, accuracy: 0.88094\n",
            "Epoch: 22/30, step: 130/364, loss: 0.29768, accuracy: 0.88053\n",
            "Epoch: 22/30, step: 131/364, loss: 0.29755, accuracy: 0.88049\n",
            "Epoch: 22/30, step: 132/364, loss: 0.29722, accuracy: 0.88080\n",
            "Epoch: 22/30, step: 133/364, loss: 0.29667, accuracy: 0.88146\n",
            "Epoch: 22/30, step: 134/364, loss: 0.29706, accuracy: 0.88118\n",
            "Epoch: 22/30, step: 135/364, loss: 0.29669, accuracy: 0.88148\n",
            "Epoch: 22/30, step: 136/364, loss: 0.29741, accuracy: 0.88120\n",
            "Epoch: 22/30, step: 137/364, loss: 0.29719, accuracy: 0.88150\n",
            "Epoch: 22/30, step: 138/364, loss: 0.29741, accuracy: 0.88134\n",
            "Epoch: 22/30, step: 139/364, loss: 0.29701, accuracy: 0.88163\n",
            "Epoch: 22/30, step: 140/364, loss: 0.29731, accuracy: 0.88147\n",
            "Epoch: 22/30, step: 141/364, loss: 0.29674, accuracy: 0.88198\n",
            "Epoch: 22/30, step: 142/364, loss: 0.29683, accuracy: 0.88193\n",
            "Epoch: 22/30, step: 143/364, loss: 0.29677, accuracy: 0.88210\n",
            "Epoch: 22/30, step: 144/364, loss: 0.29637, accuracy: 0.88238\n",
            "Epoch: 22/30, step: 145/364, loss: 0.29621, accuracy: 0.88244\n",
            "Epoch: 22/30, step: 146/364, loss: 0.29647, accuracy: 0.88228\n",
            "Epoch: 22/30, step: 147/364, loss: 0.29657, accuracy: 0.88212\n",
            "Epoch: 22/30, step: 148/364, loss: 0.29655, accuracy: 0.88218\n",
            "Epoch: 22/30, step: 149/364, loss: 0.29622, accuracy: 0.88245\n",
            "Epoch: 22/30, step: 150/364, loss: 0.29568, accuracy: 0.88302\n",
            "Epoch: 22/30, step: 151/364, loss: 0.29574, accuracy: 0.88286\n",
            "Epoch: 22/30, step: 152/364, loss: 0.29584, accuracy: 0.88302\n",
            "Epoch: 22/30, step: 153/364, loss: 0.29580, accuracy: 0.88286\n",
            "Epoch: 22/30, step: 154/364, loss: 0.29584, accuracy: 0.88281\n",
            "Epoch: 22/30, step: 155/364, loss: 0.29684, accuracy: 0.88206\n",
            "Epoch: 22/30, step: 156/364, loss: 0.29693, accuracy: 0.88221\n",
            "Epoch: 22/30, step: 157/364, loss: 0.29719, accuracy: 0.88187\n",
            "Epoch: 22/30, step: 158/364, loss: 0.29682, accuracy: 0.88232\n",
            "Epoch: 22/30, step: 159/364, loss: 0.29669, accuracy: 0.88237\n",
            "Epoch: 22/30, step: 160/364, loss: 0.29662, accuracy: 0.88232\n",
            "Epoch: 22/30, step: 161/364, loss: 0.29649, accuracy: 0.88218\n",
            "Epoch: 22/30, step: 162/364, loss: 0.29637, accuracy: 0.88233\n",
            "Epoch: 22/30, step: 163/364, loss: 0.29618, accuracy: 0.88248\n",
            "Epoch: 22/30, step: 164/364, loss: 0.29543, accuracy: 0.88281\n",
            "Epoch: 22/30, step: 165/364, loss: 0.29553, accuracy: 0.88239\n",
            "Epoch: 22/30, step: 166/364, loss: 0.29542, accuracy: 0.88244\n",
            "Epoch: 22/30, step: 167/364, loss: 0.29499, accuracy: 0.88286\n",
            "Epoch: 22/30, step: 168/364, loss: 0.29478, accuracy: 0.88281\n",
            "Epoch: 22/30, step: 169/364, loss: 0.29489, accuracy: 0.88258\n",
            "Epoch: 22/30, step: 170/364, loss: 0.29555, accuracy: 0.88226\n",
            "Epoch: 22/30, step: 171/364, loss: 0.29511, accuracy: 0.88240\n",
            "Epoch: 22/30, step: 172/364, loss: 0.29494, accuracy: 0.88227\n",
            "Epoch: 22/30, step: 173/364, loss: 0.29471, accuracy: 0.88259\n",
            "Epoch: 22/30, step: 174/364, loss: 0.29454, accuracy: 0.88245\n",
            "Epoch: 22/30, step: 175/364, loss: 0.29427, accuracy: 0.88250\n",
            "Epoch: 22/30, step: 176/364, loss: 0.29430, accuracy: 0.88263\n",
            "Epoch: 22/30, step: 177/364, loss: 0.29388, accuracy: 0.88286\n",
            "Epoch: 22/30, step: 178/364, loss: 0.29368, accuracy: 0.88308\n",
            "Epoch: 22/30, step: 179/364, loss: 0.29304, accuracy: 0.88355\n",
            "Epoch: 22/30, step: 180/364, loss: 0.29254, accuracy: 0.88394\n",
            "Epoch: 22/30, step: 181/364, loss: 0.29199, accuracy: 0.88432\n",
            "Epoch: 22/30, step: 182/364, loss: 0.29203, accuracy: 0.88427\n",
            "Epoch: 22/30, step: 183/364, loss: 0.29234, accuracy: 0.88405\n",
            "Epoch: 22/30, step: 184/364, loss: 0.29212, accuracy: 0.88400\n",
            "Epoch: 22/30, step: 185/364, loss: 0.29251, accuracy: 0.88395\n",
            "Epoch: 22/30, step: 186/364, loss: 0.29246, accuracy: 0.88374\n",
            "Epoch: 22/30, step: 187/364, loss: 0.29259, accuracy: 0.88344\n",
            "Epoch: 22/30, step: 188/364, loss: 0.29270, accuracy: 0.88314\n",
            "Epoch: 22/30, step: 189/364, loss: 0.29295, accuracy: 0.88277\n",
            "Epoch: 22/30, step: 190/364, loss: 0.29314, accuracy: 0.88265\n",
            "Epoch: 22/30, step: 191/364, loss: 0.29287, accuracy: 0.88277\n",
            "Epoch: 22/30, step: 192/364, loss: 0.29302, accuracy: 0.88281\n",
            "Epoch: 22/30, step: 193/364, loss: 0.29252, accuracy: 0.88310\n",
            "Epoch: 22/30, step: 194/364, loss: 0.29302, accuracy: 0.88273\n",
            "Epoch: 22/30, step: 195/364, loss: 0.29275, accuracy: 0.88269\n",
            "Epoch: 22/30, step: 196/364, loss: 0.29247, accuracy: 0.88265\n",
            "Epoch: 22/30, step: 197/364, loss: 0.29206, accuracy: 0.88301\n",
            "Epoch: 22/30, step: 198/364, loss: 0.29275, accuracy: 0.88265\n",
            "Epoch: 22/30, step: 199/364, loss: 0.29272, accuracy: 0.88277\n",
            "Epoch: 22/30, step: 200/364, loss: 0.29327, accuracy: 0.88258\n",
            "Epoch: 22/30, step: 201/364, loss: 0.29342, accuracy: 0.88270\n",
            "Epoch: 22/30, step: 202/364, loss: 0.29378, accuracy: 0.88235\n",
            "Epoch: 22/30, step: 203/364, loss: 0.29350, accuracy: 0.88254\n",
            "Epoch: 22/30, step: 204/364, loss: 0.29347, accuracy: 0.88258\n",
            "Epoch: 22/30, step: 205/364, loss: 0.29302, accuracy: 0.88285\n",
            "Epoch: 22/30, step: 206/364, loss: 0.29280, accuracy: 0.88289\n",
            "Epoch: 22/30, step: 207/364, loss: 0.29324, accuracy: 0.88270\n",
            "Epoch: 22/30, step: 208/364, loss: 0.29310, accuracy: 0.88274\n",
            "Epoch: 22/30, step: 209/364, loss: 0.29265, accuracy: 0.88322\n",
            "Epoch: 22/30, step: 210/364, loss: 0.29241, accuracy: 0.88356\n",
            "Epoch: 22/30, step: 211/364, loss: 0.29215, accuracy: 0.88374\n",
            "Epoch: 22/30, step: 212/364, loss: 0.29228, accuracy: 0.88370\n",
            "Epoch: 22/30, step: 213/364, loss: 0.29224, accuracy: 0.88366\n",
            "Epoch: 22/30, step: 214/364, loss: 0.29220, accuracy: 0.88362\n",
            "Epoch: 22/30, step: 215/364, loss: 0.29232, accuracy: 0.88372\n",
            "Epoch: 22/30, step: 216/364, loss: 0.29250, accuracy: 0.88375\n",
            "Epoch: 22/30, step: 217/364, loss: 0.29297, accuracy: 0.88364\n",
            "Epoch: 22/30, step: 218/364, loss: 0.29308, accuracy: 0.88353\n",
            "Epoch: 22/30, step: 219/364, loss: 0.29291, accuracy: 0.88356\n",
            "Epoch: 22/30, step: 220/364, loss: 0.29323, accuracy: 0.88317\n",
            "Epoch: 22/30, step: 221/364, loss: 0.29339, accuracy: 0.88313\n",
            "Epoch: 22/30, step: 222/364, loss: 0.29378, accuracy: 0.88267\n",
            "Epoch: 22/30, step: 223/364, loss: 0.29351, accuracy: 0.88278\n",
            "Epoch: 22/30, step: 224/364, loss: 0.29351, accuracy: 0.88281\n",
            "Epoch: 22/30, step: 225/364, loss: 0.29421, accuracy: 0.88264\n",
            "Epoch: 22/30, step: 226/364, loss: 0.29452, accuracy: 0.88240\n",
            "Epoch: 22/30, step: 227/364, loss: 0.29457, accuracy: 0.88237\n",
            "Epoch: 22/30, step: 228/364, loss: 0.29503, accuracy: 0.88199\n",
            "Epoch: 22/30, step: 229/364, loss: 0.29499, accuracy: 0.88196\n",
            "Epoch: 22/30, step: 230/364, loss: 0.29526, accuracy: 0.88179\n",
            "Epoch: 22/30, step: 231/364, loss: 0.29594, accuracy: 0.88122\n",
            "Epoch: 22/30, step: 232/364, loss: 0.29634, accuracy: 0.88093\n",
            "Epoch: 22/30, step: 233/364, loss: 0.29609, accuracy: 0.88097\n",
            "Epoch: 22/30, step: 234/364, loss: 0.29586, accuracy: 0.88114\n",
            "Epoch: 22/30, step: 235/364, loss: 0.29551, accuracy: 0.88132\n",
            "Epoch: 22/30, step: 236/364, loss: 0.29548, accuracy: 0.88136\n",
            "Epoch: 22/30, step: 237/364, loss: 0.29602, accuracy: 0.88093\n",
            "Epoch: 22/30, step: 238/364, loss: 0.29583, accuracy: 0.88111\n",
            "Epoch: 22/30, step: 239/364, loss: 0.29582, accuracy: 0.88115\n",
            "Epoch: 22/30, step: 240/364, loss: 0.29543, accuracy: 0.88138\n",
            "Epoch: 22/30, step: 241/364, loss: 0.29554, accuracy: 0.88122\n",
            "Epoch: 22/30, step: 242/364, loss: 0.29580, accuracy: 0.88100\n",
            "Epoch: 22/30, step: 243/364, loss: 0.29551, accuracy: 0.88111\n",
            "Epoch: 22/30, step: 244/364, loss: 0.29536, accuracy: 0.88128\n",
            "Epoch: 22/30, step: 245/364, loss: 0.29522, accuracy: 0.88125\n",
            "Epoch: 22/30, step: 246/364, loss: 0.29581, accuracy: 0.88078\n",
            "Epoch: 22/30, step: 247/364, loss: 0.29566, accuracy: 0.88082\n",
            "Epoch: 22/30, step: 248/364, loss: 0.29547, accuracy: 0.88080\n",
            "Epoch: 22/30, step: 249/364, loss: 0.29551, accuracy: 0.88084\n",
            "Epoch: 22/30, step: 250/364, loss: 0.29564, accuracy: 0.88087\n",
            "Epoch: 22/30, step: 251/364, loss: 0.29519, accuracy: 0.88104\n",
            "Epoch: 22/30, step: 252/364, loss: 0.29518, accuracy: 0.88095\n",
            "Epoch: 22/30, step: 253/364, loss: 0.29511, accuracy: 0.88099\n",
            "Epoch: 22/30, step: 254/364, loss: 0.29502, accuracy: 0.88097\n",
            "Epoch: 22/30, step: 255/364, loss: 0.29618, accuracy: 0.88027\n",
            "Epoch: 22/30, step: 256/364, loss: 0.29592, accuracy: 0.88049\n",
            "Epoch: 22/30, step: 257/364, loss: 0.29603, accuracy: 0.88041\n",
            "Epoch: 22/30, step: 258/364, loss: 0.29634, accuracy: 0.87997\n",
            "Epoch: 22/30, step: 259/364, loss: 0.29593, accuracy: 0.88025\n",
            "Epoch: 22/30, step: 260/364, loss: 0.29566, accuracy: 0.88047\n",
            "Epoch: 22/30, step: 261/364, loss: 0.29558, accuracy: 0.88051\n",
            "Epoch: 22/30, step: 262/364, loss: 0.29546, accuracy: 0.88073\n",
            "Epoch: 22/30, step: 263/364, loss: 0.29536, accuracy: 0.88082\n",
            "Epoch: 22/30, step: 264/364, loss: 0.29525, accuracy: 0.88074\n",
            "Epoch: 22/30, step: 265/364, loss: 0.29496, accuracy: 0.88101\n",
            "Epoch: 22/30, step: 266/364, loss: 0.29461, accuracy: 0.88129\n",
            "Epoch: 22/30, step: 267/364, loss: 0.29462, accuracy: 0.88132\n",
            "Epoch: 22/30, step: 268/364, loss: 0.29458, accuracy: 0.88130\n",
            "Epoch: 22/30, step: 269/364, loss: 0.29456, accuracy: 0.88122\n",
            "Epoch: 22/30, step: 270/364, loss: 0.29465, accuracy: 0.88125\n",
            "Epoch: 22/30, step: 271/364, loss: 0.29475, accuracy: 0.88128\n",
            "Epoch: 22/30, step: 272/364, loss: 0.29485, accuracy: 0.88126\n",
            "Epoch: 22/30, step: 273/364, loss: 0.29454, accuracy: 0.88141\n",
            "Epoch: 22/30, step: 274/364, loss: 0.29430, accuracy: 0.88161\n",
            "Epoch: 22/30, step: 275/364, loss: 0.29408, accuracy: 0.88165\n",
            "Epoch: 22/30, step: 276/364, loss: 0.29453, accuracy: 0.88140\n",
            "Epoch: 22/30, step: 277/364, loss: 0.29433, accuracy: 0.88154\n",
            "Epoch: 22/30, step: 278/364, loss: 0.29419, accuracy: 0.88158\n",
            "Epoch: 22/30, step: 279/364, loss: 0.29398, accuracy: 0.88178\n",
            "Epoch: 22/30, step: 280/364, loss: 0.29391, accuracy: 0.88192\n",
            "Epoch: 22/30, step: 281/364, loss: 0.29382, accuracy: 0.88190\n",
            "Epoch: 22/30, step: 282/364, loss: 0.29356, accuracy: 0.88193\n",
            "Epoch: 22/30, step: 283/364, loss: 0.29324, accuracy: 0.88207\n",
            "Epoch: 22/30, step: 284/364, loss: 0.29319, accuracy: 0.88210\n",
            "Epoch: 22/30, step: 285/364, loss: 0.29337, accuracy: 0.88180\n",
            "Epoch: 22/30, step: 286/364, loss: 0.29333, accuracy: 0.88172\n",
            "Epoch: 22/30, step: 287/364, loss: 0.29308, accuracy: 0.88191\n",
            "Epoch: 22/30, step: 288/364, loss: 0.29274, accuracy: 0.88216\n",
            "Epoch: 22/30, step: 289/364, loss: 0.29269, accuracy: 0.88214\n",
            "Epoch: 22/30, step: 290/364, loss: 0.29271, accuracy: 0.88200\n",
            "Epoch: 22/30, step: 291/364, loss: 0.29275, accuracy: 0.88189\n",
            "Epoch: 22/30, train loss: 0.29275, train accuracy: 0.88189, valid loss: 0.67837, valid accuracy: 0.69046\n",
            "Epoch: 23/30, step: 1/364, loss: 0.26409, accuracy: 0.92188\n",
            "Epoch: 23/30, step: 2/364, loss: 0.26870, accuracy: 0.88281\n",
            "Epoch: 23/30, step: 3/364, loss: 0.26088, accuracy: 0.89062\n",
            "Epoch: 23/30, step: 4/364, loss: 0.30193, accuracy: 0.87109\n",
            "Epoch: 23/30, step: 5/364, loss: 0.29448, accuracy: 0.88125\n",
            "Epoch: 23/30, step: 6/364, loss: 0.28999, accuracy: 0.87760\n",
            "Epoch: 23/30, step: 7/364, loss: 0.27771, accuracy: 0.88393\n",
            "Epoch: 23/30, step: 8/364, loss: 0.27661, accuracy: 0.88477\n",
            "Epoch: 23/30, step: 9/364, loss: 0.27213, accuracy: 0.88542\n",
            "Epoch: 23/30, step: 10/364, loss: 0.29421, accuracy: 0.87500\n",
            "Epoch: 23/30, step: 11/364, loss: 0.30153, accuracy: 0.87074\n",
            "Epoch: 23/30, step: 12/364, loss: 0.29878, accuracy: 0.87630\n",
            "Epoch: 23/30, step: 13/364, loss: 0.29219, accuracy: 0.87861\n",
            "Epoch: 23/30, step: 14/364, loss: 0.28841, accuracy: 0.88170\n",
            "Epoch: 23/30, step: 15/364, loss: 0.29331, accuracy: 0.87813\n",
            "Epoch: 23/30, step: 16/364, loss: 0.30734, accuracy: 0.87598\n",
            "Epoch: 23/30, step: 17/364, loss: 0.30283, accuracy: 0.87960\n",
            "Epoch: 23/30, step: 18/364, loss: 0.30116, accuracy: 0.87934\n",
            "Epoch: 23/30, step: 19/364, loss: 0.29940, accuracy: 0.88240\n",
            "Epoch: 23/30, step: 20/364, loss: 0.29931, accuracy: 0.88125\n",
            "Epoch: 23/30, step: 21/364, loss: 0.29996, accuracy: 0.88021\n",
            "Epoch: 23/30, step: 22/364, loss: 0.30095, accuracy: 0.87784\n",
            "Epoch: 23/30, step: 23/364, loss: 0.30069, accuracy: 0.87500\n",
            "Epoch: 23/30, step: 24/364, loss: 0.29683, accuracy: 0.87695\n",
            "Epoch: 23/30, step: 25/364, loss: 0.29292, accuracy: 0.87937\n",
            "Epoch: 23/30, step: 26/364, loss: 0.29169, accuracy: 0.88041\n",
            "Epoch: 23/30, step: 27/364, loss: 0.29708, accuracy: 0.87558\n",
            "Epoch: 23/30, step: 28/364, loss: 0.29404, accuracy: 0.87723\n",
            "Epoch: 23/30, step: 29/364, loss: 0.29210, accuracy: 0.87931\n",
            "Epoch: 23/30, step: 30/364, loss: 0.28974, accuracy: 0.88021\n",
            "Epoch: 23/30, step: 31/364, loss: 0.28912, accuracy: 0.88004\n",
            "Epoch: 23/30, step: 32/364, loss: 0.28868, accuracy: 0.87891\n",
            "Epoch: 23/30, step: 33/364, loss: 0.29132, accuracy: 0.87784\n",
            "Epoch: 23/30, step: 34/364, loss: 0.28968, accuracy: 0.87822\n",
            "Epoch: 23/30, step: 35/364, loss: 0.28705, accuracy: 0.87946\n",
            "Epoch: 23/30, step: 36/364, loss: 0.28701, accuracy: 0.87934\n",
            "Epoch: 23/30, step: 37/364, loss: 0.28588, accuracy: 0.88091\n",
            "Epoch: 23/30, step: 38/364, loss: 0.28448, accuracy: 0.88240\n",
            "Epoch: 23/30, step: 39/364, loss: 0.28367, accuracy: 0.88301\n",
            "Epoch: 23/30, step: 40/364, loss: 0.28490, accuracy: 0.88242\n",
            "Epoch: 23/30, step: 41/364, loss: 0.28802, accuracy: 0.88186\n",
            "Epoch: 23/30, step: 42/364, loss: 0.28619, accuracy: 0.88356\n",
            "Epoch: 23/30, step: 43/364, loss: 0.28545, accuracy: 0.88408\n",
            "Epoch: 23/30, step: 44/364, loss: 0.28556, accuracy: 0.88388\n",
            "Epoch: 23/30, step: 45/364, loss: 0.28859, accuracy: 0.88299\n",
            "Epoch: 23/30, step: 46/364, loss: 0.28873, accuracy: 0.88247\n",
            "Epoch: 23/30, step: 47/364, loss: 0.28903, accuracy: 0.88265\n",
            "Epoch: 23/30, step: 48/364, loss: 0.28947, accuracy: 0.88249\n",
            "Epoch: 23/30, step: 49/364, loss: 0.29036, accuracy: 0.88202\n",
            "Epoch: 23/30, step: 50/364, loss: 0.28880, accuracy: 0.88219\n",
            "Epoch: 23/30, step: 51/364, loss: 0.28770, accuracy: 0.88358\n",
            "Epoch: 23/30, step: 52/364, loss: 0.28737, accuracy: 0.88401\n",
            "Epoch: 23/30, step: 53/364, loss: 0.28719, accuracy: 0.88443\n",
            "Epoch: 23/30, step: 54/364, loss: 0.28703, accuracy: 0.88426\n",
            "Epoch: 23/30, step: 55/364, loss: 0.28609, accuracy: 0.88494\n",
            "Epoch: 23/30, step: 56/364, loss: 0.28653, accuracy: 0.88421\n",
            "Epoch: 23/30, step: 57/364, loss: 0.28563, accuracy: 0.88459\n",
            "Epoch: 23/30, step: 58/364, loss: 0.28460, accuracy: 0.88551\n",
            "Epoch: 23/30, step: 59/364, loss: 0.28363, accuracy: 0.88559\n",
            "Epoch: 23/30, step: 60/364, loss: 0.28429, accuracy: 0.88516\n",
            "Epoch: 23/30, step: 61/364, loss: 0.28440, accuracy: 0.88499\n",
            "Epoch: 23/30, step: 62/364, loss: 0.28791, accuracy: 0.88306\n",
            "Epoch: 23/30, step: 63/364, loss: 0.28685, accuracy: 0.88442\n",
            "Epoch: 23/30, step: 64/364, loss: 0.28653, accuracy: 0.88477\n",
            "Epoch: 23/30, step: 65/364, loss: 0.28538, accuracy: 0.88558\n",
            "Epoch: 23/30, step: 66/364, loss: 0.28559, accuracy: 0.88494\n",
            "Epoch: 23/30, step: 67/364, loss: 0.28451, accuracy: 0.88573\n",
            "Epoch: 23/30, step: 68/364, loss: 0.28493, accuracy: 0.88557\n",
            "Epoch: 23/30, step: 69/364, loss: 0.28446, accuracy: 0.88542\n",
            "Epoch: 23/30, step: 70/364, loss: 0.28397, accuracy: 0.88571\n",
            "Epoch: 23/30, step: 71/364, loss: 0.28418, accuracy: 0.88556\n",
            "Epoch: 23/30, step: 72/364, loss: 0.28397, accuracy: 0.88585\n",
            "Epoch: 23/30, step: 73/364, loss: 0.28520, accuracy: 0.88506\n",
            "Epoch: 23/30, step: 74/364, loss: 0.28549, accuracy: 0.88492\n",
            "Epoch: 23/30, step: 75/364, loss: 0.28545, accuracy: 0.88479\n",
            "Epoch: 23/30, step: 76/364, loss: 0.28485, accuracy: 0.88487\n",
            "Epoch: 23/30, step: 77/364, loss: 0.28391, accuracy: 0.88575\n",
            "Epoch: 23/30, step: 78/364, loss: 0.28428, accuracy: 0.88502\n",
            "Epoch: 23/30, step: 79/364, loss: 0.28490, accuracy: 0.88489\n",
            "Epoch: 23/30, step: 80/364, loss: 0.28486, accuracy: 0.88496\n",
            "Epoch: 23/30, step: 81/364, loss: 0.28451, accuracy: 0.88542\n",
            "Epoch: 23/30, step: 82/364, loss: 0.28442, accuracy: 0.88510\n",
            "Epoch: 23/30, step: 83/364, loss: 0.28424, accuracy: 0.88460\n",
            "Epoch: 23/30, step: 84/364, loss: 0.28409, accuracy: 0.88467\n",
            "Epoch: 23/30, step: 85/364, loss: 0.28428, accuracy: 0.88456\n",
            "Epoch: 23/30, step: 86/364, loss: 0.28287, accuracy: 0.88554\n",
            "Epoch: 23/30, step: 87/364, loss: 0.28290, accuracy: 0.88578\n",
            "Epoch: 23/30, step: 88/364, loss: 0.28326, accuracy: 0.88512\n",
            "Epoch: 23/30, step: 89/364, loss: 0.28443, accuracy: 0.88448\n",
            "Epoch: 23/30, step: 90/364, loss: 0.28351, accuracy: 0.88576\n",
            "Epoch: 23/30, step: 91/364, loss: 0.28306, accuracy: 0.88616\n",
            "Epoch: 23/30, step: 92/364, loss: 0.28316, accuracy: 0.88621\n",
            "Epoch: 23/30, step: 93/364, loss: 0.28378, accuracy: 0.88558\n",
            "Epoch: 23/30, step: 94/364, loss: 0.28385, accuracy: 0.88531\n",
            "Epoch: 23/30, step: 95/364, loss: 0.28439, accuracy: 0.88487\n",
            "Epoch: 23/30, step: 96/364, loss: 0.28530, accuracy: 0.88444\n",
            "Epoch: 23/30, step: 97/364, loss: 0.28422, accuracy: 0.88531\n",
            "Epoch: 23/30, step: 98/364, loss: 0.28405, accuracy: 0.88536\n",
            "Epoch: 23/30, step: 99/364, loss: 0.28557, accuracy: 0.88479\n",
            "Epoch: 23/30, step: 100/364, loss: 0.28520, accuracy: 0.88500\n",
            "Epoch: 23/30, step: 101/364, loss: 0.28440, accuracy: 0.88567\n",
            "Epoch: 23/30, step: 102/364, loss: 0.28468, accuracy: 0.88511\n",
            "Epoch: 23/30, step: 103/364, loss: 0.28520, accuracy: 0.88516\n",
            "Epoch: 23/30, step: 104/364, loss: 0.28453, accuracy: 0.88567\n",
            "Epoch: 23/30, step: 105/364, loss: 0.28561, accuracy: 0.88527\n",
            "Epoch: 23/30, step: 106/364, loss: 0.28508, accuracy: 0.88576\n",
            "Epoch: 23/30, step: 107/364, loss: 0.28492, accuracy: 0.88566\n",
            "Epoch: 23/30, step: 108/364, loss: 0.28463, accuracy: 0.88585\n",
            "Epoch: 23/30, step: 109/364, loss: 0.28402, accuracy: 0.88632\n",
            "Epoch: 23/30, step: 110/364, loss: 0.28351, accuracy: 0.88665\n",
            "Epoch: 23/30, step: 111/364, loss: 0.28393, accuracy: 0.88654\n",
            "Epoch: 23/30, step: 112/364, loss: 0.28356, accuracy: 0.88686\n",
            "Epoch: 23/30, step: 113/364, loss: 0.28480, accuracy: 0.88620\n",
            "Epoch: 23/30, step: 114/364, loss: 0.28511, accuracy: 0.88624\n",
            "Epoch: 23/30, step: 115/364, loss: 0.28466, accuracy: 0.88682\n",
            "Epoch: 23/30, step: 116/364, loss: 0.28471, accuracy: 0.88699\n",
            "Epoch: 23/30, step: 117/364, loss: 0.28439, accuracy: 0.88729\n",
            "Epoch: 23/30, step: 118/364, loss: 0.28421, accuracy: 0.88731\n",
            "Epoch: 23/30, step: 119/364, loss: 0.28536, accuracy: 0.88629\n",
            "Epoch: 23/30, step: 120/364, loss: 0.28556, accuracy: 0.88568\n",
            "Epoch: 23/30, step: 121/364, loss: 0.28500, accuracy: 0.88611\n",
            "Epoch: 23/30, step: 122/364, loss: 0.28510, accuracy: 0.88601\n",
            "Epoch: 23/30, step: 123/364, loss: 0.28457, accuracy: 0.88631\n",
            "Epoch: 23/30, step: 124/364, loss: 0.28422, accuracy: 0.88647\n",
            "Epoch: 23/30, step: 125/364, loss: 0.28451, accuracy: 0.88613\n",
            "Epoch: 23/30, step: 126/364, loss: 0.28382, accuracy: 0.88653\n",
            "Epoch: 23/30, step: 127/364, loss: 0.28340, accuracy: 0.88656\n",
            "Epoch: 23/30, step: 128/364, loss: 0.28260, accuracy: 0.88708\n",
            "Epoch: 23/30, step: 129/364, loss: 0.28212, accuracy: 0.88748\n",
            "Epoch: 23/30, step: 130/364, loss: 0.28191, accuracy: 0.88774\n",
            "Epoch: 23/30, step: 131/364, loss: 0.28169, accuracy: 0.88800\n",
            "Epoch: 23/30, step: 132/364, loss: 0.28257, accuracy: 0.88719\n",
            "Epoch: 23/30, step: 133/364, loss: 0.28181, accuracy: 0.88769\n",
            "Epoch: 23/30, step: 134/364, loss: 0.28124, accuracy: 0.88829\n",
            "Epoch: 23/30, step: 135/364, loss: 0.28086, accuracy: 0.88843\n",
            "Epoch: 23/30, step: 136/364, loss: 0.28147, accuracy: 0.88821\n",
            "Epoch: 23/30, step: 137/364, loss: 0.28129, accuracy: 0.88812\n",
            "Epoch: 23/30, step: 138/364, loss: 0.28095, accuracy: 0.88847\n",
            "Epoch: 23/30, step: 139/364, loss: 0.28083, accuracy: 0.88838\n",
            "Epoch: 23/30, step: 140/364, loss: 0.28050, accuracy: 0.88839\n",
            "Epoch: 23/30, step: 141/364, loss: 0.28048, accuracy: 0.88830\n",
            "Epoch: 23/30, step: 142/364, loss: 0.28047, accuracy: 0.88809\n",
            "Epoch: 23/30, step: 143/364, loss: 0.28048, accuracy: 0.88822\n",
            "Epoch: 23/30, step: 144/364, loss: 0.28059, accuracy: 0.88813\n",
            "Epoch: 23/30, step: 145/364, loss: 0.28049, accuracy: 0.88804\n",
            "Epoch: 23/30, step: 146/364, loss: 0.27952, accuracy: 0.88848\n",
            "Epoch: 23/30, step: 147/364, loss: 0.27938, accuracy: 0.88861\n",
            "Epoch: 23/30, step: 148/364, loss: 0.27928, accuracy: 0.88862\n",
            "Epoch: 23/30, step: 149/364, loss: 0.28049, accuracy: 0.88769\n",
            "Epoch: 23/30, step: 150/364, loss: 0.27982, accuracy: 0.88823\n",
            "Epoch: 23/30, step: 151/364, loss: 0.27998, accuracy: 0.88825\n",
            "Epoch: 23/30, step: 152/364, loss: 0.28036, accuracy: 0.88806\n",
            "Epoch: 23/30, step: 153/364, loss: 0.28007, accuracy: 0.88828\n",
            "Epoch: 23/30, step: 154/364, loss: 0.28031, accuracy: 0.88829\n",
            "Epoch: 23/30, step: 155/364, loss: 0.28056, accuracy: 0.88821\n",
            "Epoch: 23/30, step: 156/364, loss: 0.28015, accuracy: 0.88842\n",
            "Epoch: 23/30, step: 157/364, loss: 0.28020, accuracy: 0.88854\n",
            "Epoch: 23/30, step: 158/364, loss: 0.28053, accuracy: 0.88825\n",
            "Epoch: 23/30, step: 159/364, loss: 0.28010, accuracy: 0.88836\n",
            "Epoch: 23/30, step: 160/364, loss: 0.28045, accuracy: 0.88799\n",
            "Epoch: 23/30, step: 161/364, loss: 0.28023, accuracy: 0.88800\n",
            "Epoch: 23/30, step: 162/364, loss: 0.28072, accuracy: 0.88764\n",
            "Epoch: 23/30, step: 163/364, loss: 0.28062, accuracy: 0.88775\n",
            "Epoch: 23/30, step: 164/364, loss: 0.28009, accuracy: 0.88796\n",
            "Epoch: 23/30, step: 165/364, loss: 0.28002, accuracy: 0.88807\n",
            "Epoch: 23/30, step: 166/364, loss: 0.28118, accuracy: 0.88752\n",
            "Epoch: 23/30, step: 167/364, loss: 0.28063, accuracy: 0.88782\n",
            "Epoch: 23/30, step: 168/364, loss: 0.28100, accuracy: 0.88774\n",
            "Epoch: 23/30, step: 169/364, loss: 0.28060, accuracy: 0.88804\n",
            "Epoch: 23/30, step: 170/364, loss: 0.28080, accuracy: 0.88768\n",
            "Epoch: 23/30, step: 171/364, loss: 0.28041, accuracy: 0.88798\n",
            "Epoch: 23/30, step: 172/364, loss: 0.28112, accuracy: 0.88735\n",
            "Epoch: 23/30, step: 173/364, loss: 0.28056, accuracy: 0.88783\n",
            "Epoch: 23/30, step: 174/364, loss: 0.28072, accuracy: 0.88748\n",
            "Epoch: 23/30, step: 175/364, loss: 0.28204, accuracy: 0.88661\n",
            "Epoch: 23/30, step: 176/364, loss: 0.28171, accuracy: 0.88681\n",
            "Epoch: 23/30, step: 177/364, loss: 0.28167, accuracy: 0.88674\n",
            "Epoch: 23/30, step: 178/364, loss: 0.28154, accuracy: 0.88685\n",
            "Epoch: 23/30, step: 179/364, loss: 0.28121, accuracy: 0.88705\n",
            "Epoch: 23/30, step: 180/364, loss: 0.28172, accuracy: 0.88681\n",
            "Epoch: 23/30, step: 181/364, loss: 0.28144, accuracy: 0.88700\n",
            "Epoch: 23/30, step: 182/364, loss: 0.28194, accuracy: 0.88659\n",
            "Epoch: 23/30, step: 183/364, loss: 0.28156, accuracy: 0.88687\n",
            "Epoch: 23/30, step: 184/364, loss: 0.28132, accuracy: 0.88697\n",
            "Epoch: 23/30, step: 185/364, loss: 0.28133, accuracy: 0.88691\n",
            "Epoch: 23/30, step: 186/364, loss: 0.28170, accuracy: 0.88659\n",
            "Epoch: 23/30, step: 187/364, loss: 0.28172, accuracy: 0.88661\n",
            "Epoch: 23/30, step: 188/364, loss: 0.28150, accuracy: 0.88688\n",
            "Epoch: 23/30, step: 189/364, loss: 0.28119, accuracy: 0.88715\n",
            "Epoch: 23/30, step: 190/364, loss: 0.28116, accuracy: 0.88717\n",
            "Epoch: 23/30, step: 191/364, loss: 0.28094, accuracy: 0.88719\n",
            "Epoch: 23/30, step: 192/364, loss: 0.28093, accuracy: 0.88713\n",
            "Epoch: 23/30, step: 193/364, loss: 0.28085, accuracy: 0.88722\n",
            "Epoch: 23/30, step: 194/364, loss: 0.28051, accuracy: 0.88732\n",
            "Epoch: 23/30, step: 195/364, loss: 0.28019, accuracy: 0.88742\n",
            "Epoch: 23/30, step: 196/364, loss: 0.27989, accuracy: 0.88768\n",
            "Epoch: 23/30, step: 197/364, loss: 0.27954, accuracy: 0.88785\n",
            "Epoch: 23/30, step: 198/364, loss: 0.27984, accuracy: 0.88755\n",
            "Epoch: 23/30, step: 199/364, loss: 0.28013, accuracy: 0.88733\n",
            "Epoch: 23/30, step: 200/364, loss: 0.27982, accuracy: 0.88750\n",
            "Epoch: 23/30, step: 201/364, loss: 0.28011, accuracy: 0.88720\n",
            "Epoch: 23/30, step: 202/364, loss: 0.27993, accuracy: 0.88714\n",
            "Epoch: 23/30, step: 203/364, loss: 0.28031, accuracy: 0.88708\n",
            "Epoch: 23/30, step: 204/364, loss: 0.27986, accuracy: 0.88733\n",
            "Epoch: 23/30, step: 205/364, loss: 0.27976, accuracy: 0.88735\n",
            "Epoch: 23/30, step: 206/364, loss: 0.27915, accuracy: 0.88774\n",
            "Epoch: 23/30, step: 207/364, loss: 0.27904, accuracy: 0.88783\n",
            "Epoch: 23/30, step: 208/364, loss: 0.27863, accuracy: 0.88815\n",
            "Epoch: 23/30, step: 209/364, loss: 0.27905, accuracy: 0.88801\n",
            "Epoch: 23/30, step: 210/364, loss: 0.27908, accuracy: 0.88795\n",
            "Epoch: 23/30, step: 211/364, loss: 0.27906, accuracy: 0.88789\n",
            "Epoch: 23/30, step: 212/364, loss: 0.27884, accuracy: 0.88805\n",
            "Epoch: 23/30, step: 213/364, loss: 0.27859, accuracy: 0.88820\n",
            "Epoch: 23/30, step: 214/364, loss: 0.27829, accuracy: 0.88843\n",
            "Epoch: 23/30, step: 215/364, loss: 0.27815, accuracy: 0.88852\n",
            "Epoch: 23/30, step: 216/364, loss: 0.27790, accuracy: 0.88874\n",
            "Epoch: 23/30, step: 217/364, loss: 0.27733, accuracy: 0.88918\n",
            "Epoch: 23/30, step: 218/364, loss: 0.27751, accuracy: 0.88912\n",
            "Epoch: 23/30, step: 219/364, loss: 0.27787, accuracy: 0.88877\n",
            "Epoch: 23/30, step: 220/364, loss: 0.27745, accuracy: 0.88920\n",
            "Epoch: 23/30, step: 221/364, loss: 0.27744, accuracy: 0.88921\n",
            "Epoch: 23/30, step: 222/364, loss: 0.27733, accuracy: 0.88915\n",
            "Epoch: 23/30, step: 223/364, loss: 0.27737, accuracy: 0.88915\n",
            "Epoch: 23/30, step: 224/364, loss: 0.27752, accuracy: 0.88895\n",
            "Epoch: 23/30, step: 225/364, loss: 0.27763, accuracy: 0.88882\n",
            "Epoch: 23/30, step: 226/364, loss: 0.27765, accuracy: 0.88876\n",
            "Epoch: 23/30, step: 227/364, loss: 0.27754, accuracy: 0.88890\n",
            "Epoch: 23/30, step: 228/364, loss: 0.27785, accuracy: 0.88877\n",
            "Epoch: 23/30, step: 229/364, loss: 0.27774, accuracy: 0.88899\n",
            "Epoch: 23/30, step: 230/364, loss: 0.27778, accuracy: 0.88906\n",
            "Epoch: 23/30, step: 231/364, loss: 0.27776, accuracy: 0.88907\n",
            "Epoch: 23/30, step: 232/364, loss: 0.27779, accuracy: 0.88901\n",
            "Epoch: 23/30, step: 233/364, loss: 0.27771, accuracy: 0.88902\n",
            "Epoch: 23/30, step: 234/364, loss: 0.27820, accuracy: 0.88882\n",
            "Epoch: 23/30, step: 235/364, loss: 0.27799, accuracy: 0.88883\n",
            "Epoch: 23/30, step: 236/364, loss: 0.27799, accuracy: 0.88884\n",
            "Epoch: 23/30, step: 237/364, loss: 0.27756, accuracy: 0.88911\n",
            "Epoch: 23/30, step: 238/364, loss: 0.27734, accuracy: 0.88918\n",
            "Epoch: 23/30, step: 239/364, loss: 0.27725, accuracy: 0.88925\n",
            "Epoch: 23/30, step: 240/364, loss: 0.27685, accuracy: 0.88952\n",
            "Epoch: 23/30, step: 241/364, loss: 0.27663, accuracy: 0.88972\n",
            "Epoch: 23/30, step: 242/364, loss: 0.27641, accuracy: 0.88979\n",
            "Epoch: 23/30, step: 243/364, loss: 0.27601, accuracy: 0.89005\n",
            "Epoch: 23/30, step: 244/364, loss: 0.27615, accuracy: 0.88992\n",
            "Epoch: 23/30, step: 245/364, loss: 0.27593, accuracy: 0.89005\n",
            "Epoch: 23/30, step: 246/364, loss: 0.27551, accuracy: 0.89031\n",
            "Epoch: 23/30, step: 247/364, loss: 0.27542, accuracy: 0.89044\n",
            "Epoch: 23/30, step: 248/364, loss: 0.27562, accuracy: 0.89031\n",
            "Epoch: 23/30, step: 249/364, loss: 0.27582, accuracy: 0.89012\n",
            "Epoch: 23/30, step: 250/364, loss: 0.27577, accuracy: 0.89019\n",
            "Epoch: 23/30, step: 251/364, loss: 0.27588, accuracy: 0.89006\n",
            "Epoch: 23/30, step: 252/364, loss: 0.27556, accuracy: 0.89025\n",
            "Epoch: 23/30, step: 253/364, loss: 0.27566, accuracy: 0.89001\n",
            "Epoch: 23/30, step: 254/364, loss: 0.27559, accuracy: 0.89001\n",
            "Epoch: 23/30, step: 255/364, loss: 0.27540, accuracy: 0.89013\n",
            "Epoch: 23/30, step: 256/364, loss: 0.27567, accuracy: 0.89001\n",
            "Epoch: 23/30, step: 257/364, loss: 0.27540, accuracy: 0.89026\n",
            "Epoch: 23/30, step: 258/364, loss: 0.27625, accuracy: 0.88978\n",
            "Epoch: 23/30, step: 259/364, loss: 0.27616, accuracy: 0.88984\n",
            "Epoch: 23/30, step: 260/364, loss: 0.27599, accuracy: 0.88996\n",
            "Epoch: 23/30, step: 261/364, loss: 0.27611, accuracy: 0.88985\n",
            "Epoch: 23/30, step: 262/364, loss: 0.27628, accuracy: 0.88979\n",
            "Epoch: 23/30, step: 263/364, loss: 0.27639, accuracy: 0.88973\n",
            "Epoch: 23/30, step: 264/364, loss: 0.27633, accuracy: 0.88986\n",
            "Epoch: 23/30, step: 265/364, loss: 0.27639, accuracy: 0.88986\n",
            "Epoch: 23/30, step: 266/364, loss: 0.27639, accuracy: 0.89004\n",
            "Epoch: 23/30, step: 267/364, loss: 0.27646, accuracy: 0.88981\n",
            "Epoch: 23/30, step: 268/364, loss: 0.27647, accuracy: 0.88969\n",
            "Epoch: 23/30, step: 269/364, loss: 0.27694, accuracy: 0.88941\n",
            "Epoch: 23/30, step: 270/364, loss: 0.27690, accuracy: 0.88947\n",
            "Epoch: 23/30, step: 271/364, loss: 0.27684, accuracy: 0.88953\n",
            "Epoch: 23/30, step: 272/364, loss: 0.27697, accuracy: 0.88930\n",
            "Epoch: 23/30, step: 273/364, loss: 0.27690, accuracy: 0.88937\n",
            "Epoch: 23/30, step: 274/364, loss: 0.27724, accuracy: 0.88903\n",
            "Epoch: 23/30, step: 275/364, loss: 0.27706, accuracy: 0.88920\n",
            "Epoch: 23/30, step: 276/364, loss: 0.27676, accuracy: 0.88949\n",
            "Epoch: 23/30, step: 277/364, loss: 0.27634, accuracy: 0.88978\n",
            "Epoch: 23/30, step: 278/364, loss: 0.27654, accuracy: 0.88961\n",
            "Epoch: 23/30, step: 279/364, loss: 0.27641, accuracy: 0.88967\n",
            "Epoch: 23/30, step: 280/364, loss: 0.27639, accuracy: 0.88962\n",
            "Epoch: 23/30, step: 281/364, loss: 0.27606, accuracy: 0.88990\n",
            "Epoch: 23/30, step: 282/364, loss: 0.27595, accuracy: 0.88990\n",
            "Epoch: 23/30, step: 283/364, loss: 0.27559, accuracy: 0.89013\n",
            "Epoch: 23/30, step: 284/364, loss: 0.27540, accuracy: 0.89024\n",
            "Epoch: 23/30, step: 285/364, loss: 0.27516, accuracy: 0.89030\n",
            "Epoch: 23/30, step: 286/364, loss: 0.27502, accuracy: 0.89041\n",
            "Epoch: 23/30, step: 287/364, loss: 0.27503, accuracy: 0.89046\n",
            "Epoch: 23/30, step: 288/364, loss: 0.27522, accuracy: 0.89025\n",
            "Epoch: 23/30, step: 289/364, loss: 0.27486, accuracy: 0.89052\n",
            "Epoch: 23/30, step: 290/364, loss: 0.27480, accuracy: 0.89068\n",
            "Epoch: 23/30, step: 291/364, loss: 0.27523, accuracy: 0.89044\n",
            "Epoch: 23/30, train loss: 0.27523, train accuracy: 0.89044, valid loss: 0.67955, valid accuracy: 0.69690\n",
            "Epoch: 24/30, step: 1/364, loss: 0.15187, accuracy: 0.96875\n",
            "Epoch: 24/30, step: 2/364, loss: 0.19672, accuracy: 0.92969\n",
            "Epoch: 24/30, step: 3/364, loss: 0.18355, accuracy: 0.93750\n",
            "Epoch: 24/30, step: 4/364, loss: 0.19790, accuracy: 0.93359\n",
            "Epoch: 24/30, step: 5/364, loss: 0.20681, accuracy: 0.92813\n",
            "Epoch: 24/30, step: 6/364, loss: 0.20603, accuracy: 0.92969\n",
            "Epoch: 24/30, step: 7/364, loss: 0.21400, accuracy: 0.92411\n",
            "Epoch: 24/30, step: 8/364, loss: 0.21817, accuracy: 0.91992\n",
            "Epoch: 24/30, step: 9/364, loss: 0.22882, accuracy: 0.91319\n",
            "Epoch: 24/30, step: 10/364, loss: 0.24143, accuracy: 0.90625\n",
            "Epoch: 24/30, step: 11/364, loss: 0.23476, accuracy: 0.91051\n",
            "Epoch: 24/30, step: 12/364, loss: 0.23386, accuracy: 0.91146\n",
            "Epoch: 24/30, step: 13/364, loss: 0.25903, accuracy: 0.90024\n",
            "Epoch: 24/30, step: 14/364, loss: 0.27071, accuracy: 0.89286\n",
            "Epoch: 24/30, step: 15/364, loss: 0.26924, accuracy: 0.89479\n",
            "Epoch: 24/30, step: 16/364, loss: 0.26495, accuracy: 0.89746\n",
            "Epoch: 24/30, step: 17/364, loss: 0.26122, accuracy: 0.90165\n",
            "Epoch: 24/30, step: 18/364, loss: 0.26030, accuracy: 0.90104\n",
            "Epoch: 24/30, step: 19/364, loss: 0.25975, accuracy: 0.89885\n",
            "Epoch: 24/30, step: 20/364, loss: 0.26015, accuracy: 0.90000\n",
            "Epoch: 24/30, step: 21/364, loss: 0.25631, accuracy: 0.90253\n",
            "Epoch: 24/30, step: 22/364, loss: 0.25651, accuracy: 0.90128\n",
            "Epoch: 24/30, step: 23/364, loss: 0.25575, accuracy: 0.90149\n",
            "Epoch: 24/30, step: 24/364, loss: 0.25818, accuracy: 0.89779\n",
            "Epoch: 24/30, step: 25/364, loss: 0.25984, accuracy: 0.89625\n",
            "Epoch: 24/30, step: 26/364, loss: 0.25658, accuracy: 0.89904\n",
            "Epoch: 24/30, step: 27/364, loss: 0.25463, accuracy: 0.89988\n",
            "Epoch: 24/30, step: 28/364, loss: 0.25516, accuracy: 0.90067\n",
            "Epoch: 24/30, step: 29/364, loss: 0.25373, accuracy: 0.90140\n",
            "Epoch: 24/30, step: 30/364, loss: 0.25320, accuracy: 0.90104\n",
            "Epoch: 24/30, step: 31/364, loss: 0.25395, accuracy: 0.90121\n",
            "Epoch: 24/30, step: 32/364, loss: 0.25512, accuracy: 0.90039\n",
            "Epoch: 24/30, step: 33/364, loss: 0.25610, accuracy: 0.89915\n",
            "Epoch: 24/30, step: 34/364, loss: 0.25714, accuracy: 0.89890\n",
            "Epoch: 24/30, step: 35/364, loss: 0.25630, accuracy: 0.89955\n",
            "Epoch: 24/30, step: 36/364, loss: 0.25883, accuracy: 0.89670\n",
            "Epoch: 24/30, step: 37/364, loss: 0.25867, accuracy: 0.89611\n",
            "Epoch: 24/30, step: 38/364, loss: 0.25662, accuracy: 0.89720\n",
            "Epoch: 24/30, step: 39/364, loss: 0.25896, accuracy: 0.89583\n",
            "Epoch: 24/30, step: 40/364, loss: 0.25776, accuracy: 0.89688\n",
            "Epoch: 24/30, step: 41/364, loss: 0.25768, accuracy: 0.89787\n",
            "Epoch: 24/30, step: 42/364, loss: 0.25775, accuracy: 0.89732\n",
            "Epoch: 24/30, step: 43/364, loss: 0.25693, accuracy: 0.89789\n",
            "Epoch: 24/30, step: 44/364, loss: 0.25472, accuracy: 0.89915\n",
            "Epoch: 24/30, step: 45/364, loss: 0.25277, accuracy: 0.90000\n",
            "Epoch: 24/30, step: 46/364, loss: 0.25229, accuracy: 0.89980\n",
            "Epoch: 24/30, step: 47/364, loss: 0.25099, accuracy: 0.90027\n",
            "Epoch: 24/30, step: 48/364, loss: 0.25341, accuracy: 0.89909\n",
            "Epoch: 24/30, step: 49/364, loss: 0.25327, accuracy: 0.89923\n",
            "Epoch: 24/30, step: 50/364, loss: 0.25485, accuracy: 0.89812\n",
            "Epoch: 24/30, step: 51/364, loss: 0.25746, accuracy: 0.89706\n",
            "Epoch: 24/30, step: 52/364, loss: 0.25747, accuracy: 0.89754\n",
            "Epoch: 24/30, step: 53/364, loss: 0.25674, accuracy: 0.89800\n",
            "Epoch: 24/30, step: 54/364, loss: 0.25525, accuracy: 0.89902\n",
            "Epoch: 24/30, step: 55/364, loss: 0.25475, accuracy: 0.89886\n",
            "Epoch: 24/30, step: 56/364, loss: 0.25392, accuracy: 0.89872\n",
            "Epoch: 24/30, step: 57/364, loss: 0.25439, accuracy: 0.89857\n",
            "Epoch: 24/30, step: 58/364, loss: 0.25342, accuracy: 0.89898\n",
            "Epoch: 24/30, step: 59/364, loss: 0.25336, accuracy: 0.89910\n",
            "Epoch: 24/30, step: 60/364, loss: 0.25222, accuracy: 0.89974\n",
            "Epoch: 24/30, step: 61/364, loss: 0.25145, accuracy: 0.90036\n",
            "Epoch: 24/30, step: 62/364, loss: 0.25112, accuracy: 0.89970\n",
            "Epoch: 24/30, step: 63/364, loss: 0.25323, accuracy: 0.89906\n",
            "Epoch: 24/30, step: 64/364, loss: 0.25412, accuracy: 0.89868\n",
            "Epoch: 24/30, step: 65/364, loss: 0.25368, accuracy: 0.89832\n",
            "Epoch: 24/30, step: 66/364, loss: 0.25243, accuracy: 0.89915\n",
            "Epoch: 24/30, step: 67/364, loss: 0.25121, accuracy: 0.89972\n",
            "Epoch: 24/30, step: 68/364, loss: 0.25195, accuracy: 0.89982\n",
            "Epoch: 24/30, step: 69/364, loss: 0.25064, accuracy: 0.90082\n",
            "Epoch: 24/30, step: 70/364, loss: 0.25022, accuracy: 0.90089\n",
            "Epoch: 24/30, step: 71/364, loss: 0.25059, accuracy: 0.90075\n",
            "Epoch: 24/30, step: 72/364, loss: 0.25405, accuracy: 0.89779\n",
            "Epoch: 24/30, step: 73/364, loss: 0.25461, accuracy: 0.89790\n",
            "Epoch: 24/30, step: 74/364, loss: 0.25555, accuracy: 0.89738\n",
            "Epoch: 24/30, step: 75/364, loss: 0.25458, accuracy: 0.89792\n",
            "Epoch: 24/30, step: 76/364, loss: 0.25349, accuracy: 0.89885\n",
            "Epoch: 24/30, step: 77/364, loss: 0.25285, accuracy: 0.89894\n",
            "Epoch: 24/30, step: 78/364, loss: 0.25451, accuracy: 0.89824\n",
            "Epoch: 24/30, step: 79/364, loss: 0.25670, accuracy: 0.89656\n",
            "Epoch: 24/30, step: 80/364, loss: 0.25541, accuracy: 0.89746\n",
            "Epoch: 24/30, step: 81/364, loss: 0.25608, accuracy: 0.89757\n",
            "Epoch: 24/30, step: 82/364, loss: 0.25555, accuracy: 0.89787\n",
            "Epoch: 24/30, step: 83/364, loss: 0.25505, accuracy: 0.89834\n",
            "Epoch: 24/30, step: 84/364, loss: 0.25434, accuracy: 0.89900\n",
            "Epoch: 24/30, step: 85/364, loss: 0.25432, accuracy: 0.89908\n",
            "Epoch: 24/30, step: 86/364, loss: 0.25436, accuracy: 0.89916\n",
            "Epoch: 24/30, step: 87/364, loss: 0.25385, accuracy: 0.89943\n",
            "Epoch: 24/30, step: 88/364, loss: 0.25322, accuracy: 0.89968\n",
            "Epoch: 24/30, step: 89/364, loss: 0.25455, accuracy: 0.89888\n",
            "Epoch: 24/30, step: 90/364, loss: 0.25436, accuracy: 0.89896\n",
            "Epoch: 24/30, step: 91/364, loss: 0.25472, accuracy: 0.89921\n",
            "Epoch: 24/30, step: 92/364, loss: 0.25553, accuracy: 0.89878\n",
            "Epoch: 24/30, step: 93/364, loss: 0.25491, accuracy: 0.89953\n",
            "Epoch: 24/30, step: 94/364, loss: 0.25586, accuracy: 0.89894\n",
            "Epoch: 24/30, step: 95/364, loss: 0.25474, accuracy: 0.89951\n",
            "Epoch: 24/30, step: 96/364, loss: 0.25424, accuracy: 0.90023\n",
            "Epoch: 24/30, step: 97/364, loss: 0.25409, accuracy: 0.90045\n",
            "Epoch: 24/30, step: 98/364, loss: 0.25385, accuracy: 0.90051\n",
            "Epoch: 24/30, step: 99/364, loss: 0.25329, accuracy: 0.90057\n",
            "Epoch: 24/30, step: 100/364, loss: 0.25321, accuracy: 0.90062\n",
            "Epoch: 24/30, step: 101/364, loss: 0.25340, accuracy: 0.90068\n",
            "Epoch: 24/30, step: 102/364, loss: 0.25383, accuracy: 0.90012\n",
            "Epoch: 24/30, step: 103/364, loss: 0.25412, accuracy: 0.90018\n",
            "Epoch: 24/30, step: 104/364, loss: 0.25526, accuracy: 0.89979\n",
            "Epoch: 24/30, step: 105/364, loss: 0.25582, accuracy: 0.89955\n",
            "Epoch: 24/30, step: 106/364, loss: 0.25592, accuracy: 0.89903\n",
            "Epoch: 24/30, step: 107/364, loss: 0.25601, accuracy: 0.89895\n",
            "Epoch: 24/30, step: 108/364, loss: 0.25640, accuracy: 0.89829\n",
            "Epoch: 24/30, step: 109/364, loss: 0.25732, accuracy: 0.89765\n",
            "Epoch: 24/30, step: 110/364, loss: 0.25776, accuracy: 0.89688\n",
            "Epoch: 24/30, step: 111/364, loss: 0.25765, accuracy: 0.89738\n",
            "Epoch: 24/30, step: 112/364, loss: 0.25821, accuracy: 0.89746\n",
            "Epoch: 24/30, step: 113/364, loss: 0.25974, accuracy: 0.89671\n",
            "Epoch: 24/30, step: 114/364, loss: 0.25922, accuracy: 0.89679\n",
            "Epoch: 24/30, step: 115/364, loss: 0.25898, accuracy: 0.89728\n",
            "Epoch: 24/30, step: 116/364, loss: 0.25931, accuracy: 0.89696\n",
            "Epoch: 24/30, step: 117/364, loss: 0.25848, accuracy: 0.89757\n",
            "Epoch: 24/30, step: 118/364, loss: 0.25839, accuracy: 0.89791\n",
            "Epoch: 24/30, step: 119/364, loss: 0.25890, accuracy: 0.89772\n",
            "Epoch: 24/30, step: 120/364, loss: 0.25978, accuracy: 0.89727\n",
            "Epoch: 24/30, step: 121/364, loss: 0.25937, accuracy: 0.89773\n",
            "Epoch: 24/30, step: 122/364, loss: 0.25927, accuracy: 0.89780\n",
            "Epoch: 24/30, step: 123/364, loss: 0.25883, accuracy: 0.89812\n",
            "Epoch: 24/30, step: 124/364, loss: 0.25869, accuracy: 0.89844\n",
            "Epoch: 24/30, step: 125/364, loss: 0.25868, accuracy: 0.89812\n",
            "Epoch: 24/30, step: 126/364, loss: 0.25906, accuracy: 0.89745\n",
            "Epoch: 24/30, step: 127/364, loss: 0.25846, accuracy: 0.89801\n",
            "Epoch: 24/30, step: 128/364, loss: 0.25778, accuracy: 0.89868\n",
            "Epoch: 24/30, step: 129/364, loss: 0.25742, accuracy: 0.89886\n",
            "Epoch: 24/30, step: 130/364, loss: 0.25726, accuracy: 0.89880\n",
            "Epoch: 24/30, step: 131/364, loss: 0.25736, accuracy: 0.89874\n",
            "Epoch: 24/30, step: 132/364, loss: 0.25729, accuracy: 0.89867\n",
            "Epoch: 24/30, step: 133/364, loss: 0.25746, accuracy: 0.89838\n",
            "Epoch: 24/30, step: 134/364, loss: 0.25711, accuracy: 0.89855\n",
            "Epoch: 24/30, step: 135/364, loss: 0.25792, accuracy: 0.89780\n",
            "Epoch: 24/30, step: 136/364, loss: 0.25800, accuracy: 0.89798\n",
            "Epoch: 24/30, step: 137/364, loss: 0.25759, accuracy: 0.89827\n",
            "Epoch: 24/30, step: 138/364, loss: 0.25844, accuracy: 0.89776\n",
            "Epoch: 24/30, step: 139/364, loss: 0.25839, accuracy: 0.89793\n",
            "Epoch: 24/30, step: 140/364, loss: 0.25937, accuracy: 0.89754\n",
            "Epoch: 24/30, step: 141/364, loss: 0.25949, accuracy: 0.89738\n",
            "Epoch: 24/30, step: 142/364, loss: 0.25913, accuracy: 0.89778\n",
            "Epoch: 24/30, step: 143/364, loss: 0.25902, accuracy: 0.89795\n",
            "Epoch: 24/30, step: 144/364, loss: 0.25985, accuracy: 0.89757\n",
            "Epoch: 24/30, step: 145/364, loss: 0.25947, accuracy: 0.89763\n",
            "Epoch: 24/30, step: 146/364, loss: 0.25938, accuracy: 0.89769\n",
            "Epoch: 24/30, step: 147/364, loss: 0.25918, accuracy: 0.89796\n",
            "Epoch: 24/30, step: 148/364, loss: 0.25925, accuracy: 0.89791\n",
            "Epoch: 24/30, step: 149/364, loss: 0.25959, accuracy: 0.89797\n",
            "Epoch: 24/30, step: 150/364, loss: 0.25954, accuracy: 0.89781\n",
            "Epoch: 24/30, step: 151/364, loss: 0.25972, accuracy: 0.89756\n",
            "Epoch: 24/30, step: 152/364, loss: 0.26025, accuracy: 0.89731\n",
            "Epoch: 24/30, step: 153/364, loss: 0.26065, accuracy: 0.89737\n",
            "Epoch: 24/30, step: 154/364, loss: 0.26033, accuracy: 0.89773\n",
            "Epoch: 24/30, step: 155/364, loss: 0.26088, accuracy: 0.89738\n",
            "Epoch: 24/30, step: 156/364, loss: 0.26087, accuracy: 0.89744\n",
            "Epoch: 24/30, step: 157/364, loss: 0.26136, accuracy: 0.89699\n",
            "Epoch: 24/30, step: 158/364, loss: 0.26151, accuracy: 0.89686\n",
            "Epoch: 24/30, step: 159/364, loss: 0.26165, accuracy: 0.89691\n",
            "Epoch: 24/30, step: 160/364, loss: 0.26167, accuracy: 0.89678\n",
            "Epoch: 24/30, step: 161/364, loss: 0.26151, accuracy: 0.89693\n",
            "Epoch: 24/30, step: 162/364, loss: 0.26204, accuracy: 0.89670\n",
            "Epoch: 24/30, step: 163/364, loss: 0.26190, accuracy: 0.89686\n",
            "Epoch: 24/30, step: 164/364, loss: 0.26153, accuracy: 0.89710\n",
            "Epoch: 24/30, step: 165/364, loss: 0.26143, accuracy: 0.89688\n",
            "Epoch: 24/30, step: 166/364, loss: 0.26082, accuracy: 0.89712\n",
            "Epoch: 24/30, step: 167/364, loss: 0.26081, accuracy: 0.89708\n",
            "Epoch: 24/30, step: 168/364, loss: 0.26067, accuracy: 0.89723\n",
            "Epoch: 24/30, step: 169/364, loss: 0.26118, accuracy: 0.89682\n",
            "Epoch: 24/30, step: 170/364, loss: 0.26115, accuracy: 0.89678\n",
            "Epoch: 24/30, step: 171/364, loss: 0.26124, accuracy: 0.89656\n",
            "Epoch: 24/30, step: 172/364, loss: 0.26124, accuracy: 0.89671\n",
            "Epoch: 24/30, step: 173/364, loss: 0.26103, accuracy: 0.89668\n",
            "Epoch: 24/30, step: 174/364, loss: 0.26126, accuracy: 0.89646\n",
            "Epoch: 24/30, step: 175/364, loss: 0.26103, accuracy: 0.89661\n",
            "Epoch: 24/30, step: 176/364, loss: 0.26076, accuracy: 0.89675\n",
            "Epoch: 24/30, step: 177/364, loss: 0.26119, accuracy: 0.89636\n",
            "Epoch: 24/30, step: 178/364, loss: 0.26091, accuracy: 0.89659\n",
            "Epoch: 24/30, step: 179/364, loss: 0.26115, accuracy: 0.89656\n",
            "Epoch: 24/30, step: 180/364, loss: 0.26064, accuracy: 0.89688\n",
            "Epoch: 24/30, step: 181/364, loss: 0.26013, accuracy: 0.89710\n",
            "Epoch: 24/30, step: 182/364, loss: 0.26014, accuracy: 0.89715\n",
            "Epoch: 24/30, step: 183/364, loss: 0.26028, accuracy: 0.89728\n",
            "Epoch: 24/30, step: 184/364, loss: 0.26000, accuracy: 0.89750\n",
            "Epoch: 24/30, step: 185/364, loss: 0.25999, accuracy: 0.89780\n",
            "Epoch: 24/30, step: 186/364, loss: 0.25939, accuracy: 0.89819\n",
            "Epoch: 24/30, step: 187/364, loss: 0.25983, accuracy: 0.89798\n",
            "Epoch: 24/30, step: 188/364, loss: 0.25971, accuracy: 0.89786\n",
            "Epoch: 24/30, step: 189/364, loss: 0.25936, accuracy: 0.89807\n",
            "Epoch: 24/30, step: 190/364, loss: 0.25960, accuracy: 0.89786\n",
            "Epoch: 24/30, step: 191/364, loss: 0.25985, accuracy: 0.89766\n",
            "Epoch: 24/30, step: 192/364, loss: 0.25952, accuracy: 0.89795\n",
            "Epoch: 24/30, step: 193/364, loss: 0.25950, accuracy: 0.89783\n",
            "Epoch: 24/30, step: 194/364, loss: 0.25905, accuracy: 0.89812\n",
            "Epoch: 24/30, step: 195/364, loss: 0.25892, accuracy: 0.89816\n",
            "Epoch: 24/30, step: 196/364, loss: 0.25864, accuracy: 0.89820\n",
            "Epoch: 24/30, step: 197/364, loss: 0.25836, accuracy: 0.89824\n",
            "Epoch: 24/30, step: 198/364, loss: 0.25866, accuracy: 0.89796\n",
            "Epoch: 24/30, step: 199/364, loss: 0.25843, accuracy: 0.89816\n",
            "Epoch: 24/30, step: 200/364, loss: 0.25875, accuracy: 0.89789\n",
            "Epoch: 24/30, step: 201/364, loss: 0.25860, accuracy: 0.89801\n",
            "Epoch: 24/30, step: 202/364, loss: 0.25867, accuracy: 0.89790\n",
            "Epoch: 24/30, step: 203/364, loss: 0.25852, accuracy: 0.89801\n",
            "Epoch: 24/30, step: 204/364, loss: 0.25836, accuracy: 0.89836\n",
            "Epoch: 24/30, step: 205/364, loss: 0.25800, accuracy: 0.89848\n",
            "Epoch: 24/30, step: 206/364, loss: 0.25758, accuracy: 0.89867\n",
            "Epoch: 24/30, step: 207/364, loss: 0.25761, accuracy: 0.89870\n",
            "Epoch: 24/30, step: 208/364, loss: 0.25753, accuracy: 0.89866\n",
            "Epoch: 24/30, step: 209/364, loss: 0.25844, accuracy: 0.89795\n",
            "Epoch: 24/30, step: 210/364, loss: 0.25827, accuracy: 0.89799\n",
            "Epoch: 24/30, step: 211/364, loss: 0.25825, accuracy: 0.89796\n",
            "Epoch: 24/30, step: 212/364, loss: 0.25810, accuracy: 0.89800\n",
            "Epoch: 24/30, step: 213/364, loss: 0.25803, accuracy: 0.89825\n",
            "Epoch: 24/30, step: 214/364, loss: 0.25787, accuracy: 0.89844\n",
            "Epoch: 24/30, step: 215/364, loss: 0.25768, accuracy: 0.89869\n",
            "Epoch: 24/30, step: 216/364, loss: 0.25762, accuracy: 0.89880\n",
            "Epoch: 24/30, step: 217/364, loss: 0.25767, accuracy: 0.89876\n",
            "Epoch: 24/30, step: 218/364, loss: 0.25754, accuracy: 0.89887\n",
            "Epoch: 24/30, step: 219/364, loss: 0.25788, accuracy: 0.89869\n",
            "Epoch: 24/30, step: 220/364, loss: 0.25763, accuracy: 0.89893\n",
            "Epoch: 24/30, step: 221/364, loss: 0.25765, accuracy: 0.89890\n",
            "Epoch: 24/30, step: 222/364, loss: 0.25747, accuracy: 0.89900\n",
            "Epoch: 24/30, step: 223/364, loss: 0.25698, accuracy: 0.89938\n",
            "Epoch: 24/30, step: 224/364, loss: 0.25705, accuracy: 0.89927\n",
            "Epoch: 24/30, step: 225/364, loss: 0.25738, accuracy: 0.89931\n",
            "Epoch: 24/30, step: 226/364, loss: 0.25689, accuracy: 0.89954\n",
            "Epoch: 24/30, step: 227/364, loss: 0.25644, accuracy: 0.89985\n",
            "Epoch: 24/30, step: 228/364, loss: 0.25614, accuracy: 0.89995\n",
            "Epoch: 24/30, step: 229/364, loss: 0.25620, accuracy: 0.89990\n",
            "Epoch: 24/30, step: 230/364, loss: 0.25614, accuracy: 0.90007\n",
            "Epoch: 24/30, step: 231/364, loss: 0.25624, accuracy: 0.89989\n",
            "Epoch: 24/30, step: 232/364, loss: 0.25605, accuracy: 0.89999\n",
            "Epoch: 24/30, step: 233/364, loss: 0.25614, accuracy: 0.89988\n",
            "Epoch: 24/30, step: 234/364, loss: 0.25646, accuracy: 0.89957\n",
            "Epoch: 24/30, step: 235/364, loss: 0.25630, accuracy: 0.89947\n",
            "Epoch: 24/30, step: 236/364, loss: 0.25626, accuracy: 0.89963\n",
            "Epoch: 24/30, step: 237/364, loss: 0.25651, accuracy: 0.89953\n",
            "Epoch: 24/30, step: 238/364, loss: 0.25629, accuracy: 0.89955\n",
            "Epoch: 24/30, step: 239/364, loss: 0.25617, accuracy: 0.89958\n",
            "Epoch: 24/30, step: 240/364, loss: 0.25583, accuracy: 0.89974\n",
            "Epoch: 24/30, step: 241/364, loss: 0.25570, accuracy: 0.89990\n",
            "Epoch: 24/30, step: 242/364, loss: 0.25548, accuracy: 0.90005\n",
            "Epoch: 24/30, step: 243/364, loss: 0.25544, accuracy: 0.90014\n",
            "Epoch: 24/30, step: 244/364, loss: 0.25565, accuracy: 0.90004\n",
            "Epoch: 24/30, step: 245/364, loss: 0.25532, accuracy: 0.90026\n",
            "Epoch: 24/30, step: 246/364, loss: 0.25576, accuracy: 0.90003\n",
            "Epoch: 24/30, step: 247/364, loss: 0.25662, accuracy: 0.89973\n",
            "Epoch: 24/30, step: 248/364, loss: 0.25655, accuracy: 0.89989\n",
            "Epoch: 24/30, step: 249/364, loss: 0.25654, accuracy: 0.89997\n",
            "Epoch: 24/30, step: 250/364, loss: 0.25645, accuracy: 0.90000\n",
            "Epoch: 24/30, step: 251/364, loss: 0.25617, accuracy: 0.90015\n",
            "Epoch: 24/30, step: 252/364, loss: 0.25598, accuracy: 0.90030\n",
            "Epoch: 24/30, step: 253/364, loss: 0.25601, accuracy: 0.90020\n",
            "Epoch: 24/30, step: 254/364, loss: 0.25603, accuracy: 0.90010\n",
            "Epoch: 24/30, step: 255/364, loss: 0.25610, accuracy: 0.90006\n",
            "Epoch: 24/30, step: 256/364, loss: 0.25658, accuracy: 0.89972\n",
            "Epoch: 24/30, step: 257/364, loss: 0.25643, accuracy: 0.89981\n",
            "Epoch: 24/30, step: 258/364, loss: 0.25600, accuracy: 0.90007\n",
            "Epoch: 24/30, step: 259/364, loss: 0.25594, accuracy: 0.90010\n",
            "Epoch: 24/30, step: 260/364, loss: 0.25587, accuracy: 0.90012\n",
            "Epoch: 24/30, step: 261/364, loss: 0.25601, accuracy: 0.90002\n",
            "Epoch: 24/30, step: 262/364, loss: 0.25579, accuracy: 0.90017\n",
            "Epoch: 24/30, step: 263/364, loss: 0.25616, accuracy: 0.89989\n",
            "Epoch: 24/30, step: 264/364, loss: 0.25625, accuracy: 0.89980\n",
            "Epoch: 24/30, step: 265/364, loss: 0.25617, accuracy: 0.89982\n",
            "Epoch: 24/30, step: 266/364, loss: 0.25607, accuracy: 0.89985\n",
            "Epoch: 24/30, step: 267/364, loss: 0.25597, accuracy: 0.89987\n",
            "Epoch: 24/30, step: 268/364, loss: 0.25581, accuracy: 0.89995\n",
            "Epoch: 24/30, step: 269/364, loss: 0.25577, accuracy: 0.90003\n",
            "Epoch: 24/30, step: 270/364, loss: 0.25571, accuracy: 0.90006\n",
            "Epoch: 24/30, step: 271/364, loss: 0.25551, accuracy: 0.90014\n",
            "Epoch: 24/30, step: 272/364, loss: 0.25531, accuracy: 0.90022\n",
            "Epoch: 24/30, step: 273/364, loss: 0.25489, accuracy: 0.90041\n",
            "Epoch: 24/30, step: 274/364, loss: 0.25485, accuracy: 0.90049\n",
            "Epoch: 24/30, step: 275/364, loss: 0.25446, accuracy: 0.90062\n",
            "Epoch: 24/30, step: 276/364, loss: 0.25476, accuracy: 0.90036\n",
            "Epoch: 24/30, step: 277/364, loss: 0.25471, accuracy: 0.90033\n",
            "Epoch: 24/30, step: 278/364, loss: 0.25517, accuracy: 0.89996\n",
            "Epoch: 24/30, step: 279/364, loss: 0.25490, accuracy: 0.90020\n",
            "Epoch: 24/30, step: 280/364, loss: 0.25486, accuracy: 0.90011\n",
            "Epoch: 24/30, step: 281/364, loss: 0.25471, accuracy: 0.90024\n",
            "Epoch: 24/30, step: 282/364, loss: 0.25455, accuracy: 0.90038\n",
            "Epoch: 24/30, step: 283/364, loss: 0.25440, accuracy: 0.90051\n",
            "Epoch: 24/30, step: 284/364, loss: 0.25452, accuracy: 0.90042\n",
            "Epoch: 24/30, step: 285/364, loss: 0.25439, accuracy: 0.90066\n",
            "Epoch: 24/30, step: 286/364, loss: 0.25430, accuracy: 0.90073\n",
            "Epoch: 24/30, step: 287/364, loss: 0.25420, accuracy: 0.90075\n",
            "Epoch: 24/30, step: 288/364, loss: 0.25477, accuracy: 0.90034\n",
            "Epoch: 24/30, step: 289/364, loss: 0.25526, accuracy: 0.90009\n",
            "Epoch: 24/30, step: 290/364, loss: 0.25516, accuracy: 0.90011\n",
            "Epoch: 24/30, step: 291/364, loss: 0.25539, accuracy: 0.89989\n",
            "Epoch: 24/30, train loss: 0.25539, train accuracy: 0.89989, valid loss: 0.71764, valid accuracy: 0.68680\n",
            "Epoch: 25/30, step: 1/364, loss: 0.33246, accuracy: 0.85938\n",
            "Epoch: 25/30, step: 2/364, loss: 0.33921, accuracy: 0.85156\n",
            "Epoch: 25/30, step: 3/364, loss: 0.28710, accuracy: 0.88021\n",
            "Epoch: 25/30, step: 4/364, loss: 0.28271, accuracy: 0.88281\n",
            "Epoch: 25/30, step: 5/364, loss: 0.26085, accuracy: 0.89688\n",
            "Epoch: 25/30, step: 6/364, loss: 0.25241, accuracy: 0.90104\n",
            "Epoch: 25/30, step: 7/364, loss: 0.25473, accuracy: 0.89732\n",
            "Epoch: 25/30, step: 8/364, loss: 0.26668, accuracy: 0.89453\n",
            "Epoch: 25/30, step: 9/364, loss: 0.26882, accuracy: 0.88715\n",
            "Epoch: 25/30, step: 10/364, loss: 0.27074, accuracy: 0.88437\n",
            "Epoch: 25/30, step: 11/364, loss: 0.26617, accuracy: 0.88920\n",
            "Epoch: 25/30, step: 12/364, loss: 0.27936, accuracy: 0.87760\n",
            "Epoch: 25/30, step: 13/364, loss: 0.27654, accuracy: 0.87861\n",
            "Epoch: 25/30, step: 14/364, loss: 0.26821, accuracy: 0.88504\n",
            "Epoch: 25/30, step: 15/364, loss: 0.26073, accuracy: 0.88958\n",
            "Epoch: 25/30, step: 16/364, loss: 0.25917, accuracy: 0.88965\n",
            "Epoch: 25/30, step: 17/364, loss: 0.25447, accuracy: 0.89062\n",
            "Epoch: 25/30, step: 18/364, loss: 0.25833, accuracy: 0.88889\n",
            "Epoch: 25/30, step: 19/364, loss: 0.25967, accuracy: 0.88651\n",
            "Epoch: 25/30, step: 20/364, loss: 0.25602, accuracy: 0.88906\n",
            "Epoch: 25/30, step: 21/364, loss: 0.25925, accuracy: 0.88839\n",
            "Epoch: 25/30, step: 22/364, loss: 0.25611, accuracy: 0.88991\n",
            "Epoch: 25/30, step: 23/364, loss: 0.25698, accuracy: 0.88791\n",
            "Epoch: 25/30, step: 24/364, loss: 0.25645, accuracy: 0.88802\n",
            "Epoch: 25/30, step: 25/364, loss: 0.25298, accuracy: 0.89125\n",
            "Epoch: 25/30, step: 26/364, loss: 0.24970, accuracy: 0.89363\n",
            "Epoch: 25/30, step: 27/364, loss: 0.24893, accuracy: 0.89352\n",
            "Epoch: 25/30, step: 28/364, loss: 0.24997, accuracy: 0.89174\n",
            "Epoch: 25/30, step: 29/364, loss: 0.24937, accuracy: 0.89224\n",
            "Epoch: 25/30, step: 30/364, loss: 0.24846, accuracy: 0.89375\n",
            "Epoch: 25/30, step: 31/364, loss: 0.24778, accuracy: 0.89365\n",
            "Epoch: 25/30, step: 32/364, loss: 0.24919, accuracy: 0.89307\n",
            "Epoch: 25/30, step: 33/364, loss: 0.24945, accuracy: 0.89252\n",
            "Epoch: 25/30, step: 34/364, loss: 0.24945, accuracy: 0.89338\n",
            "Epoch: 25/30, step: 35/364, loss: 0.25079, accuracy: 0.89286\n",
            "Epoch: 25/30, step: 36/364, loss: 0.25387, accuracy: 0.89062\n",
            "Epoch: 25/30, step: 37/364, loss: 0.25288, accuracy: 0.89147\n",
            "Epoch: 25/30, step: 38/364, loss: 0.25242, accuracy: 0.89186\n",
            "Epoch: 25/30, step: 39/364, loss: 0.25365, accuracy: 0.89223\n",
            "Epoch: 25/30, step: 40/364, loss: 0.25369, accuracy: 0.89219\n",
            "Epoch: 25/30, step: 41/364, loss: 0.25211, accuracy: 0.89367\n",
            "Epoch: 25/30, step: 42/364, loss: 0.24963, accuracy: 0.89546\n",
            "Epoch: 25/30, step: 43/364, loss: 0.24794, accuracy: 0.89680\n",
            "Epoch: 25/30, step: 44/364, loss: 0.24620, accuracy: 0.89773\n",
            "Epoch: 25/30, step: 45/364, loss: 0.24549, accuracy: 0.89861\n",
            "Epoch: 25/30, step: 46/364, loss: 0.24562, accuracy: 0.89878\n",
            "Epoch: 25/30, step: 47/364, loss: 0.24657, accuracy: 0.89827\n",
            "Epoch: 25/30, step: 48/364, loss: 0.24469, accuracy: 0.89941\n",
            "Epoch: 25/30, step: 49/364, loss: 0.24370, accuracy: 0.90019\n",
            "Epoch: 25/30, step: 50/364, loss: 0.24340, accuracy: 0.90062\n",
            "Epoch: 25/30, step: 51/364, loss: 0.24405, accuracy: 0.89982\n",
            "Epoch: 25/30, step: 52/364, loss: 0.24418, accuracy: 0.89934\n",
            "Epoch: 25/30, step: 53/364, loss: 0.24344, accuracy: 0.90006\n",
            "Epoch: 25/30, step: 54/364, loss: 0.24289, accuracy: 0.90017\n",
            "Epoch: 25/30, step: 55/364, loss: 0.24111, accuracy: 0.90142\n",
            "Epoch: 25/30, step: 56/364, loss: 0.24108, accuracy: 0.90067\n",
            "Epoch: 25/30, step: 57/364, loss: 0.24037, accuracy: 0.90104\n",
            "Epoch: 25/30, step: 58/364, loss: 0.23856, accuracy: 0.90221\n",
            "Epoch: 25/30, step: 59/364, loss: 0.23865, accuracy: 0.90228\n",
            "Epoch: 25/30, step: 60/364, loss: 0.23953, accuracy: 0.90156\n",
            "Epoch: 25/30, step: 61/364, loss: 0.23993, accuracy: 0.90190\n",
            "Epoch: 25/30, step: 62/364, loss: 0.23990, accuracy: 0.90197\n",
            "Epoch: 25/30, step: 63/364, loss: 0.23919, accuracy: 0.90253\n",
            "Epoch: 25/30, step: 64/364, loss: 0.23903, accuracy: 0.90283\n",
            "Epoch: 25/30, step: 65/364, loss: 0.24023, accuracy: 0.90192\n",
            "Epoch: 25/30, step: 66/364, loss: 0.24310, accuracy: 0.89962\n",
            "Epoch: 25/30, step: 67/364, loss: 0.24294, accuracy: 0.90042\n",
            "Epoch: 25/30, step: 68/364, loss: 0.24354, accuracy: 0.90074\n",
            "Epoch: 25/30, step: 69/364, loss: 0.24337, accuracy: 0.90104\n",
            "Epoch: 25/30, step: 70/364, loss: 0.24253, accuracy: 0.90089\n",
            "Epoch: 25/30, step: 71/364, loss: 0.24266, accuracy: 0.90097\n",
            "Epoch: 25/30, step: 72/364, loss: 0.24239, accuracy: 0.90126\n",
            "Epoch: 25/30, step: 73/364, loss: 0.24322, accuracy: 0.90111\n",
            "Epoch: 25/30, step: 74/364, loss: 0.24526, accuracy: 0.90013\n",
            "Epoch: 25/30, step: 75/364, loss: 0.24587, accuracy: 0.89958\n",
            "Epoch: 25/30, step: 76/364, loss: 0.24501, accuracy: 0.89988\n",
            "Epoch: 25/30, step: 77/364, loss: 0.24421, accuracy: 0.90016\n",
            "Epoch: 25/30, step: 78/364, loss: 0.24450, accuracy: 0.90004\n",
            "Epoch: 25/30, step: 79/364, loss: 0.24403, accuracy: 0.89992\n",
            "Epoch: 25/30, step: 80/364, loss: 0.24382, accuracy: 0.90039\n",
            "Epoch: 25/30, step: 81/364, loss: 0.24379, accuracy: 0.90046\n",
            "Epoch: 25/30, step: 82/364, loss: 0.24411, accuracy: 0.89958\n",
            "Epoch: 25/30, step: 83/364, loss: 0.24440, accuracy: 0.89928\n",
            "Epoch: 25/30, step: 84/364, loss: 0.24390, accuracy: 0.89955\n",
            "Epoch: 25/30, step: 85/364, loss: 0.24355, accuracy: 0.89963\n",
            "Epoch: 25/30, step: 86/364, loss: 0.24294, accuracy: 0.89989\n",
            "Epoch: 25/30, step: 87/364, loss: 0.24201, accuracy: 0.90050\n",
            "Epoch: 25/30, step: 88/364, loss: 0.24327, accuracy: 0.89986\n",
            "Epoch: 25/30, step: 89/364, loss: 0.24353, accuracy: 0.89923\n",
            "Epoch: 25/30, step: 90/364, loss: 0.24370, accuracy: 0.89878\n",
            "Epoch: 25/30, step: 91/364, loss: 0.24491, accuracy: 0.89818\n",
            "Epoch: 25/30, step: 92/364, loss: 0.24584, accuracy: 0.89776\n",
            "Epoch: 25/30, step: 93/364, loss: 0.24755, accuracy: 0.89617\n",
            "Epoch: 25/30, step: 94/364, loss: 0.24895, accuracy: 0.89561\n",
            "Epoch: 25/30, step: 95/364, loss: 0.24867, accuracy: 0.89572\n",
            "Epoch: 25/30, step: 96/364, loss: 0.24877, accuracy: 0.89600\n",
            "Epoch: 25/30, step: 97/364, loss: 0.24771, accuracy: 0.89691\n",
            "Epoch: 25/30, step: 98/364, loss: 0.24727, accuracy: 0.89716\n",
            "Epoch: 25/30, step: 99/364, loss: 0.24673, accuracy: 0.89757\n",
            "Epoch: 25/30, step: 100/364, loss: 0.24808, accuracy: 0.89688\n",
            "Epoch: 25/30, step: 101/364, loss: 0.24809, accuracy: 0.89681\n",
            "Epoch: 25/30, step: 102/364, loss: 0.24850, accuracy: 0.89629\n",
            "Epoch: 25/30, step: 103/364, loss: 0.24861, accuracy: 0.89639\n",
            "Epoch: 25/30, step: 104/364, loss: 0.24842, accuracy: 0.89678\n",
            "Epoch: 25/30, step: 105/364, loss: 0.24805, accuracy: 0.89717\n",
            "Epoch: 25/30, step: 106/364, loss: 0.24864, accuracy: 0.89652\n",
            "Epoch: 25/30, step: 107/364, loss: 0.24923, accuracy: 0.89603\n",
            "Epoch: 25/30, step: 108/364, loss: 0.25156, accuracy: 0.89511\n",
            "Epoch: 25/30, step: 109/364, loss: 0.25189, accuracy: 0.89507\n",
            "Epoch: 25/30, step: 110/364, loss: 0.25199, accuracy: 0.89503\n",
            "Epoch: 25/30, step: 111/364, loss: 0.25156, accuracy: 0.89555\n",
            "Epoch: 25/30, step: 112/364, loss: 0.25188, accuracy: 0.89537\n",
            "Epoch: 25/30, step: 113/364, loss: 0.25199, accuracy: 0.89546\n",
            "Epoch: 25/30, step: 114/364, loss: 0.25155, accuracy: 0.89570\n",
            "Epoch: 25/30, step: 115/364, loss: 0.25084, accuracy: 0.89620\n",
            "Epoch: 25/30, step: 116/364, loss: 0.25140, accuracy: 0.89601\n",
            "Epoch: 25/30, step: 117/364, loss: 0.25204, accuracy: 0.89610\n",
            "Epoch: 25/30, step: 118/364, loss: 0.25167, accuracy: 0.89645\n",
            "Epoch: 25/30, step: 119/364, loss: 0.25146, accuracy: 0.89653\n",
            "Epoch: 25/30, step: 120/364, loss: 0.25174, accuracy: 0.89661\n",
            "Epoch: 25/30, step: 121/364, loss: 0.25243, accuracy: 0.89618\n",
            "Epoch: 25/30, step: 122/364, loss: 0.25367, accuracy: 0.89536\n",
            "Epoch: 25/30, step: 123/364, loss: 0.25376, accuracy: 0.89533\n",
            "Epoch: 25/30, step: 124/364, loss: 0.25352, accuracy: 0.89541\n",
            "Epoch: 25/30, step: 125/364, loss: 0.25282, accuracy: 0.89587\n",
            "Epoch: 25/30, step: 126/364, loss: 0.25309, accuracy: 0.89559\n",
            "Epoch: 25/30, step: 127/364, loss: 0.25239, accuracy: 0.89604\n",
            "Epoch: 25/30, step: 128/364, loss: 0.25204, accuracy: 0.89600\n",
            "Epoch: 25/30, step: 129/364, loss: 0.25242, accuracy: 0.89547\n",
            "Epoch: 25/30, step: 130/364, loss: 0.25275, accuracy: 0.89543\n",
            "Epoch: 25/30, step: 131/364, loss: 0.25245, accuracy: 0.89563\n",
            "Epoch: 25/30, step: 132/364, loss: 0.25242, accuracy: 0.89560\n",
            "Epoch: 25/30, step: 133/364, loss: 0.25284, accuracy: 0.89521\n",
            "Epoch: 25/30, step: 134/364, loss: 0.25300, accuracy: 0.89541\n",
            "Epoch: 25/30, step: 135/364, loss: 0.25228, accuracy: 0.89572\n",
            "Epoch: 25/30, step: 136/364, loss: 0.25228, accuracy: 0.89557\n",
            "Epoch: 25/30, step: 137/364, loss: 0.25225, accuracy: 0.89576\n",
            "Epoch: 25/30, step: 138/364, loss: 0.25214, accuracy: 0.89572\n",
            "Epoch: 25/30, step: 139/364, loss: 0.25253, accuracy: 0.89535\n",
            "Epoch: 25/30, step: 140/364, loss: 0.25234, accuracy: 0.89554\n",
            "Epoch: 25/30, step: 141/364, loss: 0.25243, accuracy: 0.89572\n",
            "Epoch: 25/30, step: 142/364, loss: 0.25283, accuracy: 0.89514\n",
            "Epoch: 25/30, step: 143/364, loss: 0.25240, accuracy: 0.89532\n",
            "Epoch: 25/30, step: 144/364, loss: 0.25182, accuracy: 0.89572\n",
            "Epoch: 25/30, step: 145/364, loss: 0.25225, accuracy: 0.89547\n",
            "Epoch: 25/30, step: 146/364, loss: 0.25234, accuracy: 0.89565\n",
            "Epoch: 25/30, step: 147/364, loss: 0.25230, accuracy: 0.89562\n",
            "Epoch: 25/30, step: 148/364, loss: 0.25227, accuracy: 0.89548\n",
            "Epoch: 25/30, step: 149/364, loss: 0.25203, accuracy: 0.89555\n",
            "Epoch: 25/30, step: 150/364, loss: 0.25273, accuracy: 0.89531\n",
            "Epoch: 25/30, step: 151/364, loss: 0.25330, accuracy: 0.89507\n",
            "Epoch: 25/30, step: 152/364, loss: 0.25286, accuracy: 0.89556\n",
            "Epoch: 25/30, step: 153/364, loss: 0.25289, accuracy: 0.89563\n",
            "Epoch: 25/30, step: 154/364, loss: 0.25245, accuracy: 0.89590\n",
            "Epoch: 25/30, step: 155/364, loss: 0.25282, accuracy: 0.89577\n",
            "Epoch: 25/30, step: 156/364, loss: 0.25306, accuracy: 0.89553\n",
            "Epoch: 25/30, step: 157/364, loss: 0.25352, accuracy: 0.89510\n",
            "Epoch: 25/30, step: 158/364, loss: 0.25319, accuracy: 0.89527\n",
            "Epoch: 25/30, step: 159/364, loss: 0.25267, accuracy: 0.89564\n",
            "Epoch: 25/30, step: 160/364, loss: 0.25224, accuracy: 0.89590\n",
            "Epoch: 25/30, step: 161/364, loss: 0.25323, accuracy: 0.89528\n",
            "Epoch: 25/30, step: 162/364, loss: 0.25256, accuracy: 0.89554\n",
            "Epoch: 25/30, step: 163/364, loss: 0.25242, accuracy: 0.89561\n",
            "Epoch: 25/30, step: 164/364, loss: 0.25229, accuracy: 0.89567\n",
            "Epoch: 25/30, step: 165/364, loss: 0.25250, accuracy: 0.89564\n",
            "Epoch: 25/30, step: 166/364, loss: 0.25185, accuracy: 0.89618\n",
            "Epoch: 25/30, step: 167/364, loss: 0.25157, accuracy: 0.89624\n",
            "Epoch: 25/30, step: 168/364, loss: 0.25151, accuracy: 0.89630\n",
            "Epoch: 25/30, step: 169/364, loss: 0.25071, accuracy: 0.89682\n",
            "Epoch: 25/30, step: 170/364, loss: 0.25012, accuracy: 0.89715\n",
            "Epoch: 25/30, step: 171/364, loss: 0.25025, accuracy: 0.89702\n",
            "Epoch: 25/30, step: 172/364, loss: 0.24982, accuracy: 0.89726\n",
            "Epoch: 25/30, step: 173/364, loss: 0.24963, accuracy: 0.89740\n",
            "Epoch: 25/30, step: 174/364, loss: 0.25002, accuracy: 0.89700\n",
            "Epoch: 25/30, step: 175/364, loss: 0.24997, accuracy: 0.89705\n",
            "Epoch: 25/30, step: 176/364, loss: 0.25067, accuracy: 0.89693\n",
            "Epoch: 25/30, step: 177/364, loss: 0.25052, accuracy: 0.89707\n",
            "Epoch: 25/30, step: 178/364, loss: 0.25225, accuracy: 0.89607\n",
            "Epoch: 25/30, step: 179/364, loss: 0.25189, accuracy: 0.89639\n",
            "Epoch: 25/30, step: 180/364, loss: 0.25203, accuracy: 0.89627\n",
            "Epoch: 25/30, step: 181/364, loss: 0.25187, accuracy: 0.89658\n",
            "Epoch: 25/30, step: 182/364, loss: 0.25165, accuracy: 0.89663\n",
            "Epoch: 25/30, step: 183/364, loss: 0.25138, accuracy: 0.89686\n",
            "Epoch: 25/30, step: 184/364, loss: 0.25188, accuracy: 0.89665\n",
            "Epoch: 25/30, step: 185/364, loss: 0.25156, accuracy: 0.89671\n",
            "Epoch: 25/30, step: 186/364, loss: 0.25150, accuracy: 0.89676\n",
            "Epoch: 25/30, step: 187/364, loss: 0.25159, accuracy: 0.89656\n",
            "Epoch: 25/30, step: 188/364, loss: 0.25135, accuracy: 0.89686\n",
            "Epoch: 25/30, step: 189/364, loss: 0.25096, accuracy: 0.89707\n",
            "Epoch: 25/30, step: 190/364, loss: 0.25093, accuracy: 0.89737\n",
            "Epoch: 25/30, step: 191/364, loss: 0.25042, accuracy: 0.89766\n",
            "Epoch: 25/30, step: 192/364, loss: 0.25068, accuracy: 0.89746\n",
            "Epoch: 25/30, step: 193/364, loss: 0.25076, accuracy: 0.89743\n",
            "Epoch: 25/30, step: 194/364, loss: 0.25037, accuracy: 0.89763\n",
            "Epoch: 25/30, step: 195/364, loss: 0.25017, accuracy: 0.89776\n",
            "Epoch: 25/30, step: 196/364, loss: 0.24990, accuracy: 0.89804\n",
            "Epoch: 25/30, step: 197/364, loss: 0.24948, accuracy: 0.89824\n",
            "Epoch: 25/30, step: 198/364, loss: 0.24918, accuracy: 0.89844\n",
            "Epoch: 25/30, step: 199/364, loss: 0.24935, accuracy: 0.89848\n",
            "Epoch: 25/30, step: 200/364, loss: 0.24880, accuracy: 0.89875\n",
            "Epoch: 25/30, step: 201/364, loss: 0.24921, accuracy: 0.89863\n",
            "Epoch: 25/30, step: 202/364, loss: 0.24869, accuracy: 0.89906\n",
            "Epoch: 25/30, step: 203/364, loss: 0.24846, accuracy: 0.89940\n",
            "Epoch: 25/30, step: 204/364, loss: 0.24831, accuracy: 0.89951\n",
            "Epoch: 25/30, step: 205/364, loss: 0.24820, accuracy: 0.89962\n",
            "Epoch: 25/30, step: 206/364, loss: 0.24781, accuracy: 0.89980\n",
            "Epoch: 25/30, step: 207/364, loss: 0.24760, accuracy: 0.89998\n",
            "Epoch: 25/30, step: 208/364, loss: 0.24716, accuracy: 0.90024\n",
            "Epoch: 25/30, step: 209/364, loss: 0.24680, accuracy: 0.90049\n",
            "Epoch: 25/30, step: 210/364, loss: 0.24673, accuracy: 0.90045\n",
            "Epoch: 25/30, step: 211/364, loss: 0.24697, accuracy: 0.90055\n",
            "Epoch: 25/30, step: 212/364, loss: 0.24683, accuracy: 0.90072\n",
            "Epoch: 25/30, step: 213/364, loss: 0.24661, accuracy: 0.90089\n",
            "Epoch: 25/30, step: 214/364, loss: 0.24628, accuracy: 0.90107\n",
            "Epoch: 25/30, step: 215/364, loss: 0.24601, accuracy: 0.90116\n",
            "Epoch: 25/30, step: 216/364, loss: 0.24624, accuracy: 0.90104\n",
            "Epoch: 25/30, step: 217/364, loss: 0.24637, accuracy: 0.90099\n",
            "Epoch: 25/30, step: 218/364, loss: 0.24689, accuracy: 0.90087\n",
            "Epoch: 25/30, step: 219/364, loss: 0.24721, accuracy: 0.90054\n",
            "Epoch: 25/30, step: 220/364, loss: 0.24685, accuracy: 0.90078\n",
            "Epoch: 25/30, step: 221/364, loss: 0.24650, accuracy: 0.90088\n",
            "Epoch: 25/30, step: 222/364, loss: 0.24641, accuracy: 0.90090\n",
            "Epoch: 25/30, step: 223/364, loss: 0.24656, accuracy: 0.90078\n",
            "Epoch: 25/30, step: 224/364, loss: 0.24641, accuracy: 0.90102\n",
            "Epoch: 25/30, step: 225/364, loss: 0.24593, accuracy: 0.90125\n",
            "Epoch: 25/30, step: 226/364, loss: 0.24581, accuracy: 0.90148\n",
            "Epoch: 25/30, step: 227/364, loss: 0.24555, accuracy: 0.90171\n",
            "Epoch: 25/30, step: 228/364, loss: 0.24517, accuracy: 0.90193\n",
            "Epoch: 25/30, step: 229/364, loss: 0.24480, accuracy: 0.90229\n",
            "Epoch: 25/30, step: 230/364, loss: 0.24519, accuracy: 0.90217\n",
            "Epoch: 25/30, step: 231/364, loss: 0.24566, accuracy: 0.90185\n",
            "Epoch: 25/30, step: 232/364, loss: 0.24596, accuracy: 0.90154\n",
            "Epoch: 25/30, step: 233/364, loss: 0.24583, accuracy: 0.90162\n",
            "Epoch: 25/30, step: 234/364, loss: 0.24567, accuracy: 0.90178\n",
            "Epoch: 25/30, step: 235/364, loss: 0.24560, accuracy: 0.90166\n",
            "Epoch: 25/30, step: 236/364, loss: 0.24579, accuracy: 0.90175\n",
            "Epoch: 25/30, step: 237/364, loss: 0.24640, accuracy: 0.90150\n",
            "Epoch: 25/30, step: 238/364, loss: 0.24630, accuracy: 0.90146\n",
            "Epoch: 25/30, step: 239/364, loss: 0.24674, accuracy: 0.90115\n",
            "Epoch: 25/30, step: 240/364, loss: 0.24635, accuracy: 0.90143\n",
            "Epoch: 25/30, step: 241/364, loss: 0.24670, accuracy: 0.90126\n",
            "Epoch: 25/30, step: 242/364, loss: 0.24675, accuracy: 0.90121\n",
            "Epoch: 25/30, step: 243/364, loss: 0.24634, accuracy: 0.90149\n",
            "Epoch: 25/30, step: 244/364, loss: 0.24635, accuracy: 0.90145\n",
            "Epoch: 25/30, step: 245/364, loss: 0.24662, accuracy: 0.90140\n",
            "Epoch: 25/30, step: 246/364, loss: 0.24625, accuracy: 0.90174\n",
            "Epoch: 25/30, step: 247/364, loss: 0.24613, accuracy: 0.90182\n",
            "Epoch: 25/30, step: 248/364, loss: 0.24558, accuracy: 0.90215\n",
            "Epoch: 25/30, step: 249/364, loss: 0.24536, accuracy: 0.90230\n",
            "Epoch: 25/30, step: 250/364, loss: 0.24594, accuracy: 0.90194\n",
            "Epoch: 25/30, step: 251/364, loss: 0.24654, accuracy: 0.90189\n",
            "Epoch: 25/30, step: 252/364, loss: 0.24637, accuracy: 0.90197\n",
            "Epoch: 25/30, step: 253/364, loss: 0.24616, accuracy: 0.90205\n",
            "Epoch: 25/30, step: 254/364, loss: 0.24621, accuracy: 0.90194\n",
            "Epoch: 25/30, step: 255/364, loss: 0.24591, accuracy: 0.90208\n",
            "Epoch: 25/30, step: 256/364, loss: 0.24559, accuracy: 0.90234\n",
            "Epoch: 25/30, step: 257/364, loss: 0.24577, accuracy: 0.90224\n",
            "Epoch: 25/30, step: 258/364, loss: 0.24572, accuracy: 0.90231\n",
            "Epoch: 25/30, step: 259/364, loss: 0.24566, accuracy: 0.90239\n",
            "Epoch: 25/30, step: 260/364, loss: 0.24610, accuracy: 0.90216\n",
            "Epoch: 25/30, step: 261/364, loss: 0.24592, accuracy: 0.90230\n",
            "Epoch: 25/30, step: 262/364, loss: 0.24584, accuracy: 0.90237\n",
            "Epoch: 25/30, step: 263/364, loss: 0.24585, accuracy: 0.90221\n",
            "Epoch: 25/30, step: 264/364, loss: 0.24585, accuracy: 0.90223\n",
            "Epoch: 25/30, step: 265/364, loss: 0.24559, accuracy: 0.90242\n",
            "Epoch: 25/30, step: 266/364, loss: 0.24539, accuracy: 0.90249\n",
            "Epoch: 25/30, step: 267/364, loss: 0.24533, accuracy: 0.90262\n",
            "Epoch: 25/30, step: 268/364, loss: 0.24545, accuracy: 0.90246\n",
            "Epoch: 25/30, step: 269/364, loss: 0.24551, accuracy: 0.90247\n",
            "Epoch: 25/30, step: 270/364, loss: 0.24559, accuracy: 0.90237\n",
            "Epoch: 25/30, step: 271/364, loss: 0.24554, accuracy: 0.90239\n",
            "Epoch: 25/30, step: 272/364, loss: 0.24560, accuracy: 0.90240\n",
            "Epoch: 25/30, step: 273/364, loss: 0.24602, accuracy: 0.90219\n",
            "Epoch: 25/30, step: 274/364, loss: 0.24611, accuracy: 0.90209\n",
            "Epoch: 25/30, step: 275/364, loss: 0.24586, accuracy: 0.90222\n",
            "Epoch: 25/30, step: 276/364, loss: 0.24584, accuracy: 0.90212\n",
            "Epoch: 25/30, step: 277/364, loss: 0.24563, accuracy: 0.90225\n",
            "Epoch: 25/30, step: 278/364, loss: 0.24577, accuracy: 0.90203\n",
            "Epoch: 25/30, step: 279/364, loss: 0.24553, accuracy: 0.90222\n",
            "Epoch: 25/30, step: 280/364, loss: 0.24528, accuracy: 0.90229\n",
            "Epoch: 25/30, step: 281/364, loss: 0.24541, accuracy: 0.90225\n",
            "Epoch: 25/30, step: 282/364, loss: 0.24566, accuracy: 0.90209\n",
            "Epoch: 25/30, step: 283/364, loss: 0.24548, accuracy: 0.90216\n",
            "Epoch: 25/30, step: 284/364, loss: 0.24553, accuracy: 0.90207\n",
            "Epoch: 25/30, step: 285/364, loss: 0.24568, accuracy: 0.90203\n",
            "Epoch: 25/30, step: 286/364, loss: 0.24589, accuracy: 0.90204\n",
            "Epoch: 25/30, step: 287/364, loss: 0.24593, accuracy: 0.90206\n",
            "Epoch: 25/30, step: 288/364, loss: 0.24587, accuracy: 0.90224\n",
            "Epoch: 25/30, step: 289/364, loss: 0.24574, accuracy: 0.90241\n",
            "Epoch: 25/30, step: 290/364, loss: 0.24586, accuracy: 0.90226\n",
            "Epoch: 25/30, step: 291/364, loss: 0.24577, accuracy: 0.90226\n",
            "Epoch: 25/30, train loss: 0.24577, train accuracy: 0.90226, valid loss: 0.79124, valid accuracy: 0.66939\n",
            "Epoch: 26/30, step: 1/364, loss: 0.34351, accuracy: 0.84375\n",
            "Epoch: 26/30, step: 2/364, loss: 0.27382, accuracy: 0.87500\n",
            "Epoch: 26/30, step: 3/364, loss: 0.25748, accuracy: 0.90104\n",
            "Epoch: 26/30, step: 4/364, loss: 0.23511, accuracy: 0.91406\n",
            "Epoch: 26/30, step: 5/364, loss: 0.26832, accuracy: 0.89062\n",
            "Epoch: 26/30, step: 6/364, loss: 0.25419, accuracy: 0.89844\n",
            "Epoch: 26/30, step: 7/364, loss: 0.24460, accuracy: 0.90625\n",
            "Epoch: 26/30, step: 8/364, loss: 0.23373, accuracy: 0.91016\n",
            "Epoch: 26/30, step: 9/364, loss: 0.23113, accuracy: 0.91493\n",
            "Epoch: 26/30, step: 10/364, loss: 0.22732, accuracy: 0.92031\n",
            "Epoch: 26/30, step: 11/364, loss: 0.21954, accuracy: 0.92330\n",
            "Epoch: 26/30, step: 12/364, loss: 0.21734, accuracy: 0.92448\n",
            "Epoch: 26/30, step: 13/364, loss: 0.21551, accuracy: 0.92188\n",
            "Epoch: 26/30, step: 14/364, loss: 0.22604, accuracy: 0.91741\n",
            "Epoch: 26/30, step: 15/364, loss: 0.22583, accuracy: 0.91562\n",
            "Epoch: 26/30, step: 16/364, loss: 0.23012, accuracy: 0.91211\n",
            "Epoch: 26/30, step: 17/364, loss: 0.23416, accuracy: 0.90901\n",
            "Epoch: 26/30, step: 18/364, loss: 0.24063, accuracy: 0.90451\n",
            "Epoch: 26/30, step: 19/364, loss: 0.24691, accuracy: 0.90214\n",
            "Epoch: 26/30, step: 20/364, loss: 0.24824, accuracy: 0.89844\n",
            "Epoch: 26/30, step: 21/364, loss: 0.25001, accuracy: 0.89658\n",
            "Epoch: 26/30, step: 22/364, loss: 0.24916, accuracy: 0.89560\n",
            "Epoch: 26/30, step: 23/364, loss: 0.24350, accuracy: 0.89946\n",
            "Epoch: 26/30, step: 24/364, loss: 0.23898, accuracy: 0.90234\n",
            "Epoch: 26/30, step: 25/364, loss: 0.23616, accuracy: 0.90500\n",
            "Epoch: 26/30, step: 26/364, loss: 0.23588, accuracy: 0.90445\n",
            "Epoch: 26/30, step: 27/364, loss: 0.23440, accuracy: 0.90625\n",
            "Epoch: 26/30, step: 28/364, loss: 0.23590, accuracy: 0.90625\n",
            "Epoch: 26/30, step: 29/364, loss: 0.23746, accuracy: 0.90517\n",
            "Epoch: 26/30, step: 30/364, loss: 0.23727, accuracy: 0.90469\n",
            "Epoch: 26/30, step: 31/364, loss: 0.23810, accuracy: 0.90423\n",
            "Epoch: 26/30, step: 32/364, loss: 0.23427, accuracy: 0.90674\n",
            "Epoch: 26/30, step: 33/364, loss: 0.23247, accuracy: 0.90862\n",
            "Epoch: 26/30, step: 34/364, loss: 0.23203, accuracy: 0.90855\n",
            "Epoch: 26/30, step: 35/364, loss: 0.23181, accuracy: 0.90848\n",
            "Epoch: 26/30, step: 36/364, loss: 0.23096, accuracy: 0.91016\n",
            "Epoch: 26/30, step: 37/364, loss: 0.22966, accuracy: 0.91047\n",
            "Epoch: 26/30, step: 38/364, loss: 0.22825, accuracy: 0.91201\n",
            "Epoch: 26/30, step: 39/364, loss: 0.22782, accuracy: 0.91226\n",
            "Epoch: 26/30, step: 40/364, loss: 0.22997, accuracy: 0.91094\n",
            "Epoch: 26/30, step: 41/364, loss: 0.22827, accuracy: 0.91197\n",
            "Epoch: 26/30, step: 42/364, loss: 0.22964, accuracy: 0.91109\n",
            "Epoch: 26/30, step: 43/364, loss: 0.23166, accuracy: 0.90879\n",
            "Epoch: 26/30, step: 44/364, loss: 0.22948, accuracy: 0.91016\n",
            "Epoch: 26/30, step: 45/364, loss: 0.22908, accuracy: 0.91042\n",
            "Epoch: 26/30, step: 46/364, loss: 0.22729, accuracy: 0.91168\n",
            "Epoch: 26/30, step: 47/364, loss: 0.22637, accuracy: 0.91223\n",
            "Epoch: 26/30, step: 48/364, loss: 0.22578, accuracy: 0.91211\n",
            "Epoch: 26/30, step: 49/364, loss: 0.22670, accuracy: 0.91167\n",
            "Epoch: 26/30, step: 50/364, loss: 0.22768, accuracy: 0.91031\n",
            "Epoch: 26/30, step: 51/364, loss: 0.22632, accuracy: 0.91085\n",
            "Epoch: 26/30, step: 52/364, loss: 0.22439, accuracy: 0.91226\n",
            "Epoch: 26/30, step: 53/364, loss: 0.22459, accuracy: 0.91215\n",
            "Epoch: 26/30, step: 54/364, loss: 0.22480, accuracy: 0.91204\n",
            "Epoch: 26/30, step: 55/364, loss: 0.22439, accuracy: 0.91222\n",
            "Epoch: 26/30, step: 56/364, loss: 0.22531, accuracy: 0.91211\n",
            "Epoch: 26/30, step: 57/364, loss: 0.22426, accuracy: 0.91310\n",
            "Epoch: 26/30, step: 58/364, loss: 0.22485, accuracy: 0.91272\n",
            "Epoch: 26/30, step: 59/364, loss: 0.22472, accuracy: 0.91234\n",
            "Epoch: 26/30, step: 60/364, loss: 0.22393, accuracy: 0.91276\n",
            "Epoch: 26/30, step: 61/364, loss: 0.22861, accuracy: 0.91009\n",
            "Epoch: 26/30, step: 62/364, loss: 0.22705, accuracy: 0.91079\n",
            "Epoch: 26/30, step: 63/364, loss: 0.22597, accuracy: 0.91121\n",
            "Epoch: 26/30, step: 64/364, loss: 0.22742, accuracy: 0.90991\n",
            "Epoch: 26/30, step: 65/364, loss: 0.22925, accuracy: 0.90817\n",
            "Epoch: 26/30, step: 66/364, loss: 0.22956, accuracy: 0.90767\n",
            "Epoch: 26/30, step: 67/364, loss: 0.22927, accuracy: 0.90765\n",
            "Epoch: 26/30, step: 68/364, loss: 0.23007, accuracy: 0.90763\n",
            "Epoch: 26/30, step: 69/364, loss: 0.23095, accuracy: 0.90784\n",
            "Epoch: 26/30, step: 70/364, loss: 0.23046, accuracy: 0.90826\n",
            "Epoch: 26/30, step: 71/364, loss: 0.23200, accuracy: 0.90735\n",
            "Epoch: 26/30, step: 72/364, loss: 0.23158, accuracy: 0.90777\n",
            "Epoch: 26/30, step: 73/364, loss: 0.23073, accuracy: 0.90860\n",
            "Epoch: 26/30, step: 74/364, loss: 0.23085, accuracy: 0.90836\n",
            "Epoch: 26/30, step: 75/364, loss: 0.23062, accuracy: 0.90875\n",
            "Epoch: 26/30, step: 76/364, loss: 0.22958, accuracy: 0.90933\n",
            "Epoch: 26/30, step: 77/364, loss: 0.23001, accuracy: 0.90950\n",
            "Epoch: 26/30, step: 78/364, loss: 0.22906, accuracy: 0.91066\n",
            "Epoch: 26/30, step: 79/364, loss: 0.22960, accuracy: 0.91021\n",
            "Epoch: 26/30, step: 80/364, loss: 0.22983, accuracy: 0.90996\n",
            "Epoch: 26/30, step: 81/364, loss: 0.22894, accuracy: 0.91049\n",
            "Epoch: 26/30, step: 82/364, loss: 0.22795, accuracy: 0.91120\n",
            "Epoch: 26/30, step: 83/364, loss: 0.22754, accuracy: 0.91209\n",
            "Epoch: 26/30, step: 84/364, loss: 0.22853, accuracy: 0.91127\n",
            "Epoch: 26/30, step: 85/364, loss: 0.22773, accuracy: 0.91176\n",
            "Epoch: 26/30, step: 86/364, loss: 0.22709, accuracy: 0.91261\n",
            "Epoch: 26/30, step: 87/364, loss: 0.22751, accuracy: 0.91236\n",
            "Epoch: 26/30, step: 88/364, loss: 0.22815, accuracy: 0.91158\n",
            "Epoch: 26/30, step: 89/364, loss: 0.22753, accuracy: 0.91204\n",
            "Epoch: 26/30, step: 90/364, loss: 0.22661, accuracy: 0.91250\n",
            "Epoch: 26/30, step: 91/364, loss: 0.22754, accuracy: 0.91157\n",
            "Epoch: 26/30, step: 92/364, loss: 0.22746, accuracy: 0.91151\n",
            "Epoch: 26/30, step: 93/364, loss: 0.22719, accuracy: 0.91129\n",
            "Epoch: 26/30, step: 94/364, loss: 0.22749, accuracy: 0.91107\n",
            "Epoch: 26/30, step: 95/364, loss: 0.22785, accuracy: 0.91069\n",
            "Epoch: 26/30, step: 96/364, loss: 0.22727, accuracy: 0.91113\n",
            "Epoch: 26/30, step: 97/364, loss: 0.22745, accuracy: 0.91124\n",
            "Epoch: 26/30, step: 98/364, loss: 0.22696, accuracy: 0.91183\n",
            "Epoch: 26/30, step: 99/364, loss: 0.22679, accuracy: 0.91177\n",
            "Epoch: 26/30, step: 100/364, loss: 0.22640, accuracy: 0.91188\n",
            "Epoch: 26/30, step: 101/364, loss: 0.22603, accuracy: 0.91244\n",
            "Epoch: 26/30, step: 102/364, loss: 0.22682, accuracy: 0.91176\n",
            "Epoch: 26/30, step: 103/364, loss: 0.22666, accuracy: 0.91171\n",
            "Epoch: 26/30, step: 104/364, loss: 0.22634, accuracy: 0.91226\n",
            "Epoch: 26/30, step: 105/364, loss: 0.22650, accuracy: 0.91235\n",
            "Epoch: 26/30, step: 106/364, loss: 0.22662, accuracy: 0.91229\n",
            "Epoch: 26/30, step: 107/364, loss: 0.22653, accuracy: 0.91209\n",
            "Epoch: 26/30, step: 108/364, loss: 0.22744, accuracy: 0.91175\n",
            "Epoch: 26/30, step: 109/364, loss: 0.22801, accuracy: 0.91184\n",
            "Epoch: 26/30, step: 110/364, loss: 0.22938, accuracy: 0.91136\n",
            "Epoch: 26/30, step: 111/364, loss: 0.23042, accuracy: 0.91075\n",
            "Epoch: 26/30, step: 112/364, loss: 0.23001, accuracy: 0.91127\n",
            "Epoch: 26/30, step: 113/364, loss: 0.22993, accuracy: 0.91123\n",
            "Epoch: 26/30, step: 114/364, loss: 0.22908, accuracy: 0.91173\n",
            "Epoch: 26/30, step: 115/364, loss: 0.22884, accuracy: 0.91209\n",
            "Epoch: 26/30, step: 116/364, loss: 0.22792, accuracy: 0.91272\n",
            "Epoch: 26/30, step: 117/364, loss: 0.22840, accuracy: 0.91213\n",
            "Epoch: 26/30, step: 118/364, loss: 0.22896, accuracy: 0.91155\n",
            "Epoch: 26/30, step: 119/364, loss: 0.22895, accuracy: 0.91150\n",
            "Epoch: 26/30, step: 120/364, loss: 0.22869, accuracy: 0.91146\n",
            "Epoch: 26/30, step: 121/364, loss: 0.22820, accuracy: 0.91167\n",
            "Epoch: 26/30, step: 122/364, loss: 0.22801, accuracy: 0.91201\n",
            "Epoch: 26/30, step: 123/364, loss: 0.22777, accuracy: 0.91222\n",
            "Epoch: 26/30, step: 124/364, loss: 0.22764, accuracy: 0.91230\n",
            "Epoch: 26/30, step: 125/364, loss: 0.22679, accuracy: 0.91275\n",
            "Epoch: 26/30, step: 126/364, loss: 0.22645, accuracy: 0.91282\n",
            "Epoch: 26/30, step: 127/364, loss: 0.22710, accuracy: 0.91265\n",
            "Epoch: 26/30, step: 128/364, loss: 0.22686, accuracy: 0.91260\n",
            "Epoch: 26/30, step: 129/364, loss: 0.22643, accuracy: 0.91267\n",
            "Epoch: 26/30, step: 130/364, loss: 0.22744, accuracy: 0.91202\n",
            "Epoch: 26/30, step: 131/364, loss: 0.22714, accuracy: 0.91198\n",
            "Epoch: 26/30, step: 132/364, loss: 0.22661, accuracy: 0.91229\n",
            "Epoch: 26/30, step: 133/364, loss: 0.22692, accuracy: 0.91212\n",
            "Epoch: 26/30, step: 134/364, loss: 0.22671, accuracy: 0.91231\n",
            "Epoch: 26/30, step: 135/364, loss: 0.22647, accuracy: 0.91262\n",
            "Epoch: 26/30, step: 136/364, loss: 0.22716, accuracy: 0.91188\n",
            "Epoch: 26/30, step: 137/364, loss: 0.22691, accuracy: 0.91207\n",
            "Epoch: 26/30, step: 138/364, loss: 0.22664, accuracy: 0.91236\n",
            "Epoch: 26/30, step: 139/364, loss: 0.22619, accuracy: 0.91266\n",
            "Epoch: 26/30, step: 140/364, loss: 0.22557, accuracy: 0.91317\n",
            "Epoch: 26/30, step: 141/364, loss: 0.22600, accuracy: 0.91290\n",
            "Epoch: 26/30, step: 142/364, loss: 0.22547, accuracy: 0.91296\n",
            "Epoch: 26/30, step: 143/364, loss: 0.22530, accuracy: 0.91313\n",
            "Epoch: 26/30, step: 144/364, loss: 0.22530, accuracy: 0.91309\n",
            "Epoch: 26/30, step: 145/364, loss: 0.22537, accuracy: 0.91315\n",
            "Epoch: 26/30, step: 146/364, loss: 0.22519, accuracy: 0.91321\n",
            "Epoch: 26/30, step: 147/364, loss: 0.22596, accuracy: 0.91273\n",
            "Epoch: 26/30, step: 148/364, loss: 0.22679, accuracy: 0.91237\n",
            "Epoch: 26/30, step: 149/364, loss: 0.22650, accuracy: 0.91254\n",
            "Epoch: 26/30, step: 150/364, loss: 0.22644, accuracy: 0.91281\n",
            "Epoch: 26/30, step: 151/364, loss: 0.22704, accuracy: 0.91225\n",
            "Epoch: 26/30, step: 152/364, loss: 0.22671, accuracy: 0.91242\n",
            "Epoch: 26/30, step: 153/364, loss: 0.22609, accuracy: 0.91268\n",
            "Epoch: 26/30, step: 154/364, loss: 0.22579, accuracy: 0.91284\n",
            "Epoch: 26/30, step: 155/364, loss: 0.22577, accuracy: 0.91280\n",
            "Epoch: 26/30, step: 156/364, loss: 0.22561, accuracy: 0.91256\n",
            "Epoch: 26/30, step: 157/364, loss: 0.22533, accuracy: 0.91282\n",
            "Epoch: 26/30, step: 158/364, loss: 0.22528, accuracy: 0.91317\n",
            "Epoch: 26/30, step: 159/364, loss: 0.22514, accuracy: 0.91333\n",
            "Epoch: 26/30, step: 160/364, loss: 0.22589, accuracy: 0.91270\n",
            "Epoch: 26/30, step: 161/364, loss: 0.22578, accuracy: 0.91285\n",
            "Epoch: 26/30, step: 162/364, loss: 0.22573, accuracy: 0.91291\n",
            "Epoch: 26/30, step: 163/364, loss: 0.22658, accuracy: 0.91238\n",
            "Epoch: 26/30, step: 164/364, loss: 0.22641, accuracy: 0.91244\n",
            "Epoch: 26/30, step: 165/364, loss: 0.22672, accuracy: 0.91231\n",
            "Epoch: 26/30, step: 166/364, loss: 0.22666, accuracy: 0.91246\n",
            "Epoch: 26/30, step: 167/364, loss: 0.22667, accuracy: 0.91233\n",
            "Epoch: 26/30, step: 168/364, loss: 0.22638, accuracy: 0.91257\n",
            "Epoch: 26/30, step: 169/364, loss: 0.22645, accuracy: 0.91272\n",
            "Epoch: 26/30, step: 170/364, loss: 0.22593, accuracy: 0.91296\n",
            "Epoch: 26/30, step: 171/364, loss: 0.22614, accuracy: 0.91301\n",
            "Epoch: 26/30, step: 172/364, loss: 0.22616, accuracy: 0.91306\n",
            "Epoch: 26/30, step: 173/364, loss: 0.22671, accuracy: 0.91266\n",
            "Epoch: 26/30, step: 174/364, loss: 0.22728, accuracy: 0.91236\n",
            "Epoch: 26/30, step: 175/364, loss: 0.22730, accuracy: 0.91223\n",
            "Epoch: 26/30, step: 176/364, loss: 0.22754, accuracy: 0.91220\n",
            "Epoch: 26/30, step: 177/364, loss: 0.22844, accuracy: 0.91163\n",
            "Epoch: 26/30, step: 178/364, loss: 0.22860, accuracy: 0.91160\n",
            "Epoch: 26/30, step: 179/364, loss: 0.22852, accuracy: 0.91157\n",
            "Epoch: 26/30, step: 180/364, loss: 0.22849, accuracy: 0.91146\n",
            "Epoch: 26/30, step: 181/364, loss: 0.22802, accuracy: 0.91177\n",
            "Epoch: 26/30, step: 182/364, loss: 0.22839, accuracy: 0.91149\n",
            "Epoch: 26/30, step: 183/364, loss: 0.22802, accuracy: 0.91180\n",
            "Epoch: 26/30, step: 184/364, loss: 0.22777, accuracy: 0.91185\n",
            "Epoch: 26/30, step: 185/364, loss: 0.22734, accuracy: 0.91216\n",
            "Epoch: 26/30, step: 186/364, loss: 0.22730, accuracy: 0.91205\n",
            "Epoch: 26/30, step: 187/364, loss: 0.22818, accuracy: 0.91160\n",
            "Epoch: 26/30, step: 188/364, loss: 0.22770, accuracy: 0.91190\n",
            "Epoch: 26/30, step: 189/364, loss: 0.22819, accuracy: 0.91154\n",
            "Epoch: 26/30, step: 190/364, loss: 0.22776, accuracy: 0.91201\n",
            "Epoch: 26/30, step: 191/364, loss: 0.22728, accuracy: 0.91230\n",
            "Epoch: 26/30, step: 192/364, loss: 0.22785, accuracy: 0.91227\n",
            "Epoch: 26/30, step: 193/364, loss: 0.22793, accuracy: 0.91224\n",
            "Epoch: 26/30, step: 194/364, loss: 0.22779, accuracy: 0.91213\n",
            "Epoch: 26/30, step: 195/364, loss: 0.22782, accuracy: 0.91210\n",
            "Epoch: 26/30, step: 196/364, loss: 0.22760, accuracy: 0.91215\n",
            "Epoch: 26/30, step: 197/364, loss: 0.22709, accuracy: 0.91260\n",
            "Epoch: 26/30, step: 198/364, loss: 0.22722, accuracy: 0.91233\n",
            "Epoch: 26/30, step: 199/364, loss: 0.22708, accuracy: 0.91245\n",
            "Epoch: 26/30, step: 200/364, loss: 0.22660, accuracy: 0.91281\n",
            "Epoch: 26/30, step: 201/364, loss: 0.22675, accuracy: 0.91247\n",
            "Epoch: 26/30, step: 202/364, loss: 0.22651, accuracy: 0.91259\n",
            "Epoch: 26/30, step: 203/364, loss: 0.22675, accuracy: 0.91248\n",
            "Epoch: 26/30, step: 204/364, loss: 0.22626, accuracy: 0.91276\n",
            "Epoch: 26/30, step: 205/364, loss: 0.22664, accuracy: 0.91250\n",
            "Epoch: 26/30, step: 206/364, loss: 0.22658, accuracy: 0.91255\n",
            "Epoch: 26/30, step: 207/364, loss: 0.22703, accuracy: 0.91229\n",
            "Epoch: 26/30, step: 208/364, loss: 0.22669, accuracy: 0.91249\n",
            "Epoch: 26/30, step: 209/364, loss: 0.22665, accuracy: 0.91260\n",
            "Epoch: 26/30, step: 210/364, loss: 0.22632, accuracy: 0.91280\n",
            "Epoch: 26/30, step: 211/364, loss: 0.22619, accuracy: 0.91284\n",
            "Epoch: 26/30, step: 212/364, loss: 0.22596, accuracy: 0.91303\n",
            "Epoch: 26/30, step: 213/364, loss: 0.22619, accuracy: 0.91307\n",
            "Epoch: 26/30, step: 214/364, loss: 0.22577, accuracy: 0.91326\n",
            "Epoch: 26/30, step: 215/364, loss: 0.22571, accuracy: 0.91323\n",
            "Epoch: 26/30, step: 216/364, loss: 0.22567, accuracy: 0.91327\n",
            "Epoch: 26/30, step: 217/364, loss: 0.22525, accuracy: 0.91359\n",
            "Epoch: 26/30, step: 218/364, loss: 0.22487, accuracy: 0.91392\n",
            "Epoch: 26/30, step: 219/364, loss: 0.22529, accuracy: 0.91374\n",
            "Epoch: 26/30, step: 220/364, loss: 0.22518, accuracy: 0.91371\n",
            "Epoch: 26/30, step: 221/364, loss: 0.22507, accuracy: 0.91382\n",
            "Epoch: 26/30, step: 222/364, loss: 0.22471, accuracy: 0.91399\n",
            "Epoch: 26/30, step: 223/364, loss: 0.22518, accuracy: 0.91382\n",
            "Epoch: 26/30, step: 224/364, loss: 0.22479, accuracy: 0.91399\n",
            "Epoch: 26/30, step: 225/364, loss: 0.22468, accuracy: 0.91410\n",
            "Epoch: 26/30, step: 226/364, loss: 0.22525, accuracy: 0.91386\n",
            "Epoch: 26/30, step: 227/364, loss: 0.22474, accuracy: 0.91410\n",
            "Epoch: 26/30, step: 228/364, loss: 0.22467, accuracy: 0.91420\n",
            "Epoch: 26/30, step: 229/364, loss: 0.22493, accuracy: 0.91396\n",
            "Epoch: 26/30, step: 230/364, loss: 0.22505, accuracy: 0.91386\n",
            "Epoch: 26/30, step: 231/364, loss: 0.22464, accuracy: 0.91403\n",
            "Epoch: 26/30, step: 232/364, loss: 0.22454, accuracy: 0.91420\n",
            "Epoch: 26/30, step: 233/364, loss: 0.22450, accuracy: 0.91423\n",
            "Epoch: 26/30, step: 234/364, loss: 0.22418, accuracy: 0.91433\n",
            "Epoch: 26/30, step: 235/364, loss: 0.22387, accuracy: 0.91443\n",
            "Epoch: 26/30, step: 236/364, loss: 0.22393, accuracy: 0.91446\n",
            "Epoch: 26/30, step: 237/364, loss: 0.22375, accuracy: 0.91443\n",
            "Epoch: 26/30, step: 238/364, loss: 0.22340, accuracy: 0.91459\n",
            "Epoch: 26/30, step: 239/364, loss: 0.22338, accuracy: 0.91462\n",
            "Epoch: 26/30, step: 240/364, loss: 0.22328, accuracy: 0.91465\n",
            "Epoch: 26/30, step: 241/364, loss: 0.22351, accuracy: 0.91429\n",
            "Epoch: 26/30, step: 242/364, loss: 0.22350, accuracy: 0.91432\n",
            "Epoch: 26/30, step: 243/364, loss: 0.22359, accuracy: 0.91429\n",
            "Epoch: 26/30, step: 244/364, loss: 0.22362, accuracy: 0.91432\n",
            "Epoch: 26/30, step: 245/364, loss: 0.22322, accuracy: 0.91454\n",
            "Epoch: 26/30, step: 246/364, loss: 0.22288, accuracy: 0.91476\n",
            "Epoch: 26/30, step: 247/364, loss: 0.22273, accuracy: 0.91473\n",
            "Epoch: 26/30, step: 248/364, loss: 0.22241, accuracy: 0.91488\n",
            "Epoch: 26/30, step: 249/364, loss: 0.22256, accuracy: 0.91478\n",
            "Epoch: 26/30, step: 250/364, loss: 0.22244, accuracy: 0.91487\n",
            "Epoch: 26/30, step: 251/364, loss: 0.22195, accuracy: 0.91509\n",
            "Epoch: 26/30, step: 252/364, loss: 0.22169, accuracy: 0.91524\n",
            "Epoch: 26/30, step: 253/364, loss: 0.22144, accuracy: 0.91539\n",
            "Epoch: 26/30, step: 254/364, loss: 0.22123, accuracy: 0.91554\n",
            "Epoch: 26/30, step: 255/364, loss: 0.22128, accuracy: 0.91562\n",
            "Epoch: 26/30, step: 256/364, loss: 0.22129, accuracy: 0.91553\n",
            "Epoch: 26/30, step: 257/364, loss: 0.22134, accuracy: 0.91561\n",
            "Epoch: 26/30, step: 258/364, loss: 0.22121, accuracy: 0.91570\n",
            "Epoch: 26/30, step: 259/364, loss: 0.22120, accuracy: 0.91572\n",
            "Epoch: 26/30, step: 260/364, loss: 0.22149, accuracy: 0.91562\n",
            "Epoch: 26/30, step: 261/364, loss: 0.22136, accuracy: 0.91571\n",
            "Epoch: 26/30, step: 262/364, loss: 0.22154, accuracy: 0.91549\n",
            "Epoch: 26/30, step: 263/364, loss: 0.22166, accuracy: 0.91534\n",
            "Epoch: 26/30, step: 264/364, loss: 0.22143, accuracy: 0.91548\n",
            "Epoch: 26/30, step: 265/364, loss: 0.22163, accuracy: 0.91533\n",
            "Epoch: 26/30, step: 266/364, loss: 0.22139, accuracy: 0.91553\n",
            "Epoch: 26/30, step: 267/364, loss: 0.22112, accuracy: 0.91573\n",
            "Epoch: 26/30, step: 268/364, loss: 0.22090, accuracy: 0.91581\n",
            "Epoch: 26/30, step: 269/364, loss: 0.22077, accuracy: 0.91595\n",
            "Epoch: 26/30, step: 270/364, loss: 0.22062, accuracy: 0.91609\n",
            "Epoch: 26/30, step: 271/364, loss: 0.22024, accuracy: 0.91634\n",
            "Epoch: 26/30, step: 272/364, loss: 0.22008, accuracy: 0.91636\n",
            "Epoch: 26/30, step: 273/364, loss: 0.21977, accuracy: 0.91661\n",
            "Epoch: 26/30, step: 274/364, loss: 0.21999, accuracy: 0.91646\n",
            "Epoch: 26/30, step: 275/364, loss: 0.22001, accuracy: 0.91636\n",
            "Epoch: 26/30, step: 276/364, loss: 0.21966, accuracy: 0.91661\n",
            "Epoch: 26/30, step: 277/364, loss: 0.21957, accuracy: 0.91674\n",
            "Epoch: 26/30, step: 278/364, loss: 0.21985, accuracy: 0.91659\n",
            "Epoch: 26/30, step: 279/364, loss: 0.21981, accuracy: 0.91661\n",
            "Epoch: 26/30, step: 280/364, loss: 0.21965, accuracy: 0.91669\n",
            "Epoch: 26/30, step: 281/364, loss: 0.21962, accuracy: 0.91659\n",
            "Epoch: 26/30, step: 282/364, loss: 0.21994, accuracy: 0.91639\n",
            "Epoch: 26/30, step: 283/364, loss: 0.21974, accuracy: 0.91652\n",
            "Epoch: 26/30, step: 284/364, loss: 0.21964, accuracy: 0.91654\n",
            "Epoch: 26/30, step: 285/364, loss: 0.22008, accuracy: 0.91628\n",
            "Epoch: 26/30, step: 286/364, loss: 0.21970, accuracy: 0.91647\n",
            "Epoch: 26/30, step: 287/364, loss: 0.21948, accuracy: 0.91670\n",
            "Epoch: 26/30, step: 288/364, loss: 0.21918, accuracy: 0.91688\n",
            "Epoch: 26/30, step: 289/364, loss: 0.21897, accuracy: 0.91701\n",
            "Epoch: 26/30, step: 290/364, loss: 0.21901, accuracy: 0.91703\n",
            "Epoch: 26/30, step: 291/364, loss: 0.21923, accuracy: 0.91698\n",
            "Epoch: 26/30, train loss: 0.21923, train accuracy: 0.91698, valid loss: 0.73139, valid accuracy: 0.68551\n",
            "Epoch: 27/30, step: 1/364, loss: 0.25378, accuracy: 0.85938\n",
            "Epoch: 27/30, step: 2/364, loss: 0.21218, accuracy: 0.90625\n",
            "Epoch: 27/30, step: 3/364, loss: 0.19410, accuracy: 0.92188\n",
            "Epoch: 27/30, step: 4/364, loss: 0.17826, accuracy: 0.93750\n",
            "Epoch: 27/30, step: 5/364, loss: 0.17005, accuracy: 0.93750\n",
            "Epoch: 27/30, step: 6/364, loss: 0.16121, accuracy: 0.94271\n",
            "Epoch: 27/30, step: 7/364, loss: 0.16392, accuracy: 0.94643\n",
            "Epoch: 27/30, step: 8/364, loss: 0.16388, accuracy: 0.94336\n",
            "Epoch: 27/30, step: 9/364, loss: 0.15600, accuracy: 0.94792\n",
            "Epoch: 27/30, step: 10/364, loss: 0.15577, accuracy: 0.95000\n",
            "Epoch: 27/30, step: 11/364, loss: 0.15519, accuracy: 0.94886\n",
            "Epoch: 27/30, step: 12/364, loss: 0.17121, accuracy: 0.94141\n",
            "Epoch: 27/30, step: 13/364, loss: 0.17680, accuracy: 0.93990\n",
            "Epoch: 27/30, step: 14/364, loss: 0.17746, accuracy: 0.94196\n",
            "Epoch: 27/30, step: 15/364, loss: 0.18080, accuracy: 0.94167\n",
            "Epoch: 27/30, step: 16/364, loss: 0.18707, accuracy: 0.93750\n",
            "Epoch: 27/30, step: 17/364, loss: 0.18678, accuracy: 0.93750\n",
            "Epoch: 27/30, step: 18/364, loss: 0.18920, accuracy: 0.93403\n",
            "Epoch: 27/30, step: 19/364, loss: 0.19028, accuracy: 0.93257\n",
            "Epoch: 27/30, step: 20/364, loss: 0.19340, accuracy: 0.93047\n",
            "Epoch: 27/30, step: 21/364, loss: 0.19083, accuracy: 0.93304\n",
            "Epoch: 27/30, step: 22/364, loss: 0.19399, accuracy: 0.93182\n",
            "Epoch: 27/30, step: 23/364, loss: 0.19608, accuracy: 0.93139\n",
            "Epoch: 27/30, step: 24/364, loss: 0.19396, accuracy: 0.93359\n",
            "Epoch: 27/30, step: 25/364, loss: 0.19553, accuracy: 0.93187\n",
            "Epoch: 27/30, step: 26/364, loss: 0.20037, accuracy: 0.92668\n",
            "Epoch: 27/30, step: 27/364, loss: 0.20045, accuracy: 0.92766\n",
            "Epoch: 27/30, step: 28/364, loss: 0.20264, accuracy: 0.92690\n",
            "Epoch: 27/30, step: 29/364, loss: 0.20320, accuracy: 0.92619\n",
            "Epoch: 27/30, step: 30/364, loss: 0.20551, accuracy: 0.92552\n",
            "Epoch: 27/30, step: 31/364, loss: 0.20722, accuracy: 0.92440\n",
            "Epoch: 27/30, step: 32/364, loss: 0.20656, accuracy: 0.92432\n",
            "Epoch: 27/30, step: 33/364, loss: 0.20621, accuracy: 0.92377\n",
            "Epoch: 27/30, step: 34/364, loss: 0.20614, accuracy: 0.92279\n",
            "Epoch: 27/30, step: 35/364, loss: 0.20485, accuracy: 0.92411\n",
            "Epoch: 27/30, step: 36/364, loss: 0.20513, accuracy: 0.92405\n",
            "Epoch: 27/30, step: 37/364, loss: 0.20385, accuracy: 0.92399\n",
            "Epoch: 27/30, step: 38/364, loss: 0.20311, accuracy: 0.92516\n",
            "Epoch: 27/30, step: 39/364, loss: 0.20816, accuracy: 0.92107\n",
            "Epoch: 27/30, step: 40/364, loss: 0.20739, accuracy: 0.92188\n",
            "Epoch: 27/30, step: 41/364, loss: 0.20855, accuracy: 0.92149\n",
            "Epoch: 27/30, step: 42/364, loss: 0.20596, accuracy: 0.92299\n",
            "Epoch: 27/30, step: 43/364, loss: 0.20428, accuracy: 0.92442\n",
            "Epoch: 27/30, step: 44/364, loss: 0.20414, accuracy: 0.92365\n",
            "Epoch: 27/30, step: 45/364, loss: 0.20528, accuracy: 0.92257\n",
            "Epoch: 27/30, step: 46/364, loss: 0.20391, accuracy: 0.92391\n",
            "Epoch: 27/30, step: 47/364, loss: 0.20440, accuracy: 0.92387\n",
            "Epoch: 27/30, step: 48/364, loss: 0.20444, accuracy: 0.92318\n",
            "Epoch: 27/30, step: 49/364, loss: 0.20420, accuracy: 0.92315\n",
            "Epoch: 27/30, step: 50/364, loss: 0.20318, accuracy: 0.92344\n",
            "Epoch: 27/30, step: 51/364, loss: 0.20336, accuracy: 0.92371\n",
            "Epoch: 27/30, step: 52/364, loss: 0.20225, accuracy: 0.92458\n",
            "Epoch: 27/30, step: 53/364, loss: 0.20304, accuracy: 0.92482\n",
            "Epoch: 27/30, step: 54/364, loss: 0.20264, accuracy: 0.92477\n",
            "Epoch: 27/30, step: 55/364, loss: 0.20577, accuracy: 0.92244\n",
            "Epoch: 27/30, step: 56/364, loss: 0.20434, accuracy: 0.92299\n",
            "Epoch: 27/30, step: 57/364, loss: 0.20269, accuracy: 0.92407\n",
            "Epoch: 27/30, step: 58/364, loss: 0.20286, accuracy: 0.92403\n",
            "Epoch: 27/30, step: 59/364, loss: 0.20342, accuracy: 0.92373\n",
            "Epoch: 27/30, step: 60/364, loss: 0.20226, accuracy: 0.92448\n",
            "Epoch: 27/30, step: 61/364, loss: 0.20768, accuracy: 0.92136\n",
            "Epoch: 27/30, step: 62/364, loss: 0.20702, accuracy: 0.92213\n",
            "Epoch: 27/30, step: 63/364, loss: 0.20734, accuracy: 0.92163\n",
            "Epoch: 27/30, step: 64/364, loss: 0.20734, accuracy: 0.92163\n",
            "Epoch: 27/30, step: 65/364, loss: 0.20774, accuracy: 0.92139\n",
            "Epoch: 27/30, step: 66/364, loss: 0.20670, accuracy: 0.92235\n",
            "Epoch: 27/30, step: 67/364, loss: 0.20560, accuracy: 0.92257\n",
            "Epoch: 27/30, step: 68/364, loss: 0.20502, accuracy: 0.92302\n",
            "Epoch: 27/30, step: 69/364, loss: 0.20497, accuracy: 0.92369\n",
            "Epoch: 27/30, step: 70/364, loss: 0.20449, accuracy: 0.92388\n",
            "Epoch: 27/30, step: 71/364, loss: 0.20421, accuracy: 0.92430\n",
            "Epoch: 27/30, step: 72/364, loss: 0.20542, accuracy: 0.92318\n",
            "Epoch: 27/30, step: 73/364, loss: 0.20578, accuracy: 0.92316\n",
            "Epoch: 27/30, step: 74/364, loss: 0.20524, accuracy: 0.92335\n",
            "Epoch: 27/30, step: 75/364, loss: 0.20518, accuracy: 0.92313\n",
            "Epoch: 27/30, step: 76/364, loss: 0.20449, accuracy: 0.92352\n",
            "Epoch: 27/30, step: 77/364, loss: 0.20358, accuracy: 0.92390\n",
            "Epoch: 27/30, step: 78/364, loss: 0.20548, accuracy: 0.92268\n",
            "Epoch: 27/30, step: 79/364, loss: 0.20638, accuracy: 0.92227\n",
            "Epoch: 27/30, step: 80/364, loss: 0.20579, accuracy: 0.92266\n",
            "Epoch: 27/30, step: 81/364, loss: 0.20584, accuracy: 0.92265\n",
            "Epoch: 27/30, step: 82/364, loss: 0.20608, accuracy: 0.92226\n",
            "Epoch: 27/30, step: 83/364, loss: 0.20656, accuracy: 0.92169\n",
            "Epoch: 27/30, step: 84/364, loss: 0.20623, accuracy: 0.92206\n",
            "Epoch: 27/30, step: 85/364, loss: 0.20639, accuracy: 0.92188\n",
            "Epoch: 27/30, step: 86/364, loss: 0.20634, accuracy: 0.92169\n",
            "Epoch: 27/30, step: 87/364, loss: 0.20628, accuracy: 0.92134\n",
            "Epoch: 27/30, step: 88/364, loss: 0.20678, accuracy: 0.92099\n",
            "Epoch: 27/30, step: 89/364, loss: 0.20643, accuracy: 0.92100\n",
            "Epoch: 27/30, step: 90/364, loss: 0.20824, accuracy: 0.92049\n",
            "Epoch: 27/30, step: 91/364, loss: 0.20825, accuracy: 0.92050\n",
            "Epoch: 27/30, step: 92/364, loss: 0.20707, accuracy: 0.92137\n",
            "Epoch: 27/30, step: 93/364, loss: 0.20693, accuracy: 0.92171\n",
            "Epoch: 27/30, step: 94/364, loss: 0.20629, accuracy: 0.92188\n",
            "Epoch: 27/30, step: 95/364, loss: 0.20599, accuracy: 0.92204\n",
            "Epoch: 27/30, step: 96/364, loss: 0.20693, accuracy: 0.92106\n",
            "Epoch: 27/30, step: 97/364, loss: 0.20806, accuracy: 0.92059\n",
            "Epoch: 27/30, step: 98/364, loss: 0.20717, accuracy: 0.92124\n",
            "Epoch: 27/30, step: 99/364, loss: 0.20732, accuracy: 0.92109\n",
            "Epoch: 27/30, step: 100/364, loss: 0.20716, accuracy: 0.92109\n",
            "Epoch: 27/30, step: 101/364, loss: 0.20667, accuracy: 0.92157\n",
            "Epoch: 27/30, step: 102/364, loss: 0.20660, accuracy: 0.92142\n",
            "Epoch: 27/30, step: 103/364, loss: 0.20763, accuracy: 0.92051\n",
            "Epoch: 27/30, step: 104/364, loss: 0.20751, accuracy: 0.92082\n",
            "Epoch: 27/30, step: 105/364, loss: 0.20816, accuracy: 0.92039\n",
            "Epoch: 27/30, step: 106/364, loss: 0.20829, accuracy: 0.92055\n",
            "Epoch: 27/30, step: 107/364, loss: 0.20846, accuracy: 0.92027\n",
            "Epoch: 27/30, step: 108/364, loss: 0.20819, accuracy: 0.92043\n",
            "Epoch: 27/30, step: 109/364, loss: 0.20840, accuracy: 0.92058\n",
            "Epoch: 27/30, step: 110/364, loss: 0.20869, accuracy: 0.92045\n",
            "Epoch: 27/30, step: 111/364, loss: 0.20771, accuracy: 0.92117\n",
            "Epoch: 27/30, step: 112/364, loss: 0.20981, accuracy: 0.92006\n",
            "Epoch: 27/30, step: 113/364, loss: 0.21198, accuracy: 0.91939\n",
            "Epoch: 27/30, step: 114/364, loss: 0.21157, accuracy: 0.91968\n",
            "Epoch: 27/30, step: 115/364, loss: 0.21153, accuracy: 0.91970\n",
            "Epoch: 27/30, step: 116/364, loss: 0.21267, accuracy: 0.91891\n",
            "Epoch: 27/30, step: 117/364, loss: 0.21252, accuracy: 0.91880\n",
            "Epoch: 27/30, step: 118/364, loss: 0.21236, accuracy: 0.91896\n",
            "Epoch: 27/30, step: 119/364, loss: 0.21169, accuracy: 0.91951\n",
            "Epoch: 27/30, step: 120/364, loss: 0.21214, accuracy: 0.91901\n",
            "Epoch: 27/30, step: 121/364, loss: 0.21169, accuracy: 0.91929\n",
            "Epoch: 27/30, step: 122/364, loss: 0.21246, accuracy: 0.91867\n",
            "Epoch: 27/30, step: 123/364, loss: 0.21314, accuracy: 0.91845\n",
            "Epoch: 27/30, step: 124/364, loss: 0.21297, accuracy: 0.91847\n",
            "Epoch: 27/30, step: 125/364, loss: 0.21206, accuracy: 0.91887\n",
            "Epoch: 27/30, step: 126/364, loss: 0.21240, accuracy: 0.91877\n",
            "Epoch: 27/30, step: 127/364, loss: 0.21244, accuracy: 0.91880\n",
            "Epoch: 27/30, step: 128/364, loss: 0.21259, accuracy: 0.91895\n",
            "Epoch: 27/30, step: 129/364, loss: 0.21258, accuracy: 0.91860\n",
            "Epoch: 27/30, step: 130/364, loss: 0.21265, accuracy: 0.91863\n",
            "Epoch: 27/30, step: 131/364, loss: 0.21271, accuracy: 0.91865\n",
            "Epoch: 27/30, step: 132/364, loss: 0.21253, accuracy: 0.91868\n",
            "Epoch: 27/30, step: 133/364, loss: 0.21237, accuracy: 0.91859\n",
            "Epoch: 27/30, step: 134/364, loss: 0.21204, accuracy: 0.91873\n",
            "Epoch: 27/30, step: 135/364, loss: 0.21207, accuracy: 0.91887\n",
            "Epoch: 27/30, step: 136/364, loss: 0.21212, accuracy: 0.91889\n",
            "Epoch: 27/30, step: 137/364, loss: 0.21196, accuracy: 0.91857\n",
            "Epoch: 27/30, step: 138/364, loss: 0.21163, accuracy: 0.91859\n",
            "Epoch: 27/30, step: 139/364, loss: 0.21322, accuracy: 0.91794\n",
            "Epoch: 27/30, step: 140/364, loss: 0.21347, accuracy: 0.91786\n",
            "Epoch: 27/30, step: 141/364, loss: 0.21318, accuracy: 0.91800\n",
            "Epoch: 27/30, step: 142/364, loss: 0.21251, accuracy: 0.91846\n",
            "Epoch: 27/30, step: 143/364, loss: 0.21240, accuracy: 0.91827\n",
            "Epoch: 27/30, step: 144/364, loss: 0.21218, accuracy: 0.91862\n",
            "Epoch: 27/30, step: 145/364, loss: 0.21201, accuracy: 0.91886\n",
            "Epoch: 27/30, step: 146/364, loss: 0.21252, accuracy: 0.91856\n",
            "Epoch: 27/30, step: 147/364, loss: 0.21231, accuracy: 0.91869\n",
            "Epoch: 27/30, step: 148/364, loss: 0.21211, accuracy: 0.91871\n",
            "Epoch: 27/30, step: 149/364, loss: 0.21193, accuracy: 0.91873\n",
            "Epoch: 27/30, step: 150/364, loss: 0.21132, accuracy: 0.91917\n",
            "Epoch: 27/30, step: 151/364, loss: 0.21109, accuracy: 0.91918\n",
            "Epoch: 27/30, step: 152/364, loss: 0.21048, accuracy: 0.91961\n",
            "Epoch: 27/30, step: 153/364, loss: 0.21059, accuracy: 0.91973\n",
            "Epoch: 27/30, step: 154/364, loss: 0.20977, accuracy: 0.92025\n",
            "Epoch: 27/30, step: 155/364, loss: 0.20946, accuracy: 0.92067\n",
            "Epoch: 27/30, step: 156/364, loss: 0.20915, accuracy: 0.92067\n",
            "Epoch: 27/30, step: 157/364, loss: 0.20899, accuracy: 0.92098\n",
            "Epoch: 27/30, step: 158/364, loss: 0.20909, accuracy: 0.92079\n",
            "Epoch: 27/30, step: 159/364, loss: 0.20977, accuracy: 0.92050\n",
            "Epoch: 27/30, step: 160/364, loss: 0.20951, accuracy: 0.92080\n",
            "Epoch: 27/30, step: 161/364, loss: 0.20913, accuracy: 0.92110\n",
            "Epoch: 27/30, step: 162/364, loss: 0.20958, accuracy: 0.92072\n",
            "Epoch: 27/30, step: 163/364, loss: 0.20923, accuracy: 0.92092\n",
            "Epoch: 27/30, step: 164/364, loss: 0.20929, accuracy: 0.92083\n",
            "Epoch: 27/30, step: 165/364, loss: 0.21023, accuracy: 0.92017\n",
            "Epoch: 27/30, step: 166/364, loss: 0.21117, accuracy: 0.91933\n",
            "Epoch: 27/30, step: 167/364, loss: 0.21135, accuracy: 0.91907\n",
            "Epoch: 27/30, step: 168/364, loss: 0.21136, accuracy: 0.91908\n",
            "Epoch: 27/30, step: 169/364, loss: 0.21129, accuracy: 0.91910\n",
            "Epoch: 27/30, step: 170/364, loss: 0.21133, accuracy: 0.91893\n",
            "Epoch: 27/30, step: 171/364, loss: 0.21150, accuracy: 0.91886\n",
            "Epoch: 27/30, step: 172/364, loss: 0.21149, accuracy: 0.91879\n",
            "Epoch: 27/30, step: 173/364, loss: 0.21105, accuracy: 0.91898\n",
            "Epoch: 27/30, step: 174/364, loss: 0.21076, accuracy: 0.91909\n",
            "Epoch: 27/30, step: 175/364, loss: 0.21060, accuracy: 0.91902\n",
            "Epoch: 27/30, step: 176/364, loss: 0.21066, accuracy: 0.91895\n",
            "Epoch: 27/30, step: 177/364, loss: 0.21081, accuracy: 0.91870\n",
            "Epoch: 27/30, step: 178/364, loss: 0.21052, accuracy: 0.91889\n",
            "Epoch: 27/30, step: 179/364, loss: 0.21043, accuracy: 0.91891\n",
            "Epoch: 27/30, step: 180/364, loss: 0.21093, accuracy: 0.91840\n",
            "Epoch: 27/30, step: 181/364, loss: 0.21095, accuracy: 0.91842\n",
            "Epoch: 27/30, step: 182/364, loss: 0.21082, accuracy: 0.91861\n",
            "Epoch: 27/30, step: 183/364, loss: 0.21048, accuracy: 0.91889\n",
            "Epoch: 27/30, step: 184/364, loss: 0.21005, accuracy: 0.91916\n",
            "Epoch: 27/30, step: 185/364, loss: 0.20999, accuracy: 0.91926\n",
            "Epoch: 27/30, step: 186/364, loss: 0.20979, accuracy: 0.91935\n",
            "Epoch: 27/30, step: 187/364, loss: 0.21007, accuracy: 0.91895\n",
            "Epoch: 27/30, step: 188/364, loss: 0.21000, accuracy: 0.91880\n",
            "Epoch: 27/30, step: 189/364, loss: 0.21015, accuracy: 0.91882\n",
            "Epoch: 27/30, step: 190/364, loss: 0.21006, accuracy: 0.91900\n",
            "Epoch: 27/30, step: 191/364, loss: 0.20995, accuracy: 0.91909\n",
            "Epoch: 27/30, step: 192/364, loss: 0.20959, accuracy: 0.91935\n",
            "Epoch: 27/30, step: 193/364, loss: 0.20947, accuracy: 0.91945\n",
            "Epoch: 27/30, step: 194/364, loss: 0.20945, accuracy: 0.91930\n",
            "Epoch: 27/30, step: 195/364, loss: 0.20940, accuracy: 0.91931\n",
            "Epoch: 27/30, step: 196/364, loss: 0.20894, accuracy: 0.91964\n",
            "Epoch: 27/30, step: 197/364, loss: 0.20863, accuracy: 0.91973\n",
            "Epoch: 27/30, step: 198/364, loss: 0.20893, accuracy: 0.91951\n",
            "Epoch: 27/30, step: 199/364, loss: 0.20885, accuracy: 0.91968\n",
            "Epoch: 27/30, step: 200/364, loss: 0.20873, accuracy: 0.91977\n",
            "Epoch: 27/30, step: 201/364, loss: 0.20848, accuracy: 0.91985\n",
            "Epoch: 27/30, step: 202/364, loss: 0.20811, accuracy: 0.91994\n",
            "Epoch: 27/30, step: 203/364, loss: 0.20799, accuracy: 0.92010\n",
            "Epoch: 27/30, step: 204/364, loss: 0.20765, accuracy: 0.92034\n",
            "Epoch: 27/30, step: 205/364, loss: 0.20746, accuracy: 0.92050\n",
            "Epoch: 27/30, step: 206/364, loss: 0.20728, accuracy: 0.92059\n",
            "Epoch: 27/30, step: 207/364, loss: 0.20708, accuracy: 0.92074\n",
            "Epoch: 27/30, step: 208/364, loss: 0.20658, accuracy: 0.92112\n",
            "Epoch: 27/30, step: 209/364, loss: 0.20640, accuracy: 0.92128\n",
            "Epoch: 27/30, step: 210/364, loss: 0.20643, accuracy: 0.92143\n",
            "Epoch: 27/30, step: 211/364, loss: 0.20665, accuracy: 0.92150\n",
            "Epoch: 27/30, step: 212/364, loss: 0.20646, accuracy: 0.92158\n",
            "Epoch: 27/30, step: 213/364, loss: 0.20686, accuracy: 0.92114\n",
            "Epoch: 27/30, step: 214/364, loss: 0.20648, accuracy: 0.92144\n",
            "Epoch: 27/30, step: 215/364, loss: 0.20686, accuracy: 0.92122\n",
            "Epoch: 27/30, step: 216/364, loss: 0.20670, accuracy: 0.92122\n",
            "Epoch: 27/30, step: 217/364, loss: 0.20725, accuracy: 0.92101\n",
            "Epoch: 27/30, step: 218/364, loss: 0.20712, accuracy: 0.92109\n",
            "Epoch: 27/30, step: 219/364, loss: 0.20703, accuracy: 0.92109\n",
            "Epoch: 27/30, step: 220/364, loss: 0.20698, accuracy: 0.92116\n",
            "Epoch: 27/30, step: 221/364, loss: 0.20693, accuracy: 0.92124\n",
            "Epoch: 27/30, step: 222/364, loss: 0.20755, accuracy: 0.92096\n",
            "Epoch: 27/30, step: 223/364, loss: 0.20785, accuracy: 0.92068\n",
            "Epoch: 27/30, step: 224/364, loss: 0.20779, accuracy: 0.92076\n",
            "Epoch: 27/30, step: 225/364, loss: 0.20752, accuracy: 0.92097\n",
            "Epoch: 27/30, step: 226/364, loss: 0.20811, accuracy: 0.92063\n",
            "Epoch: 27/30, step: 227/364, loss: 0.20805, accuracy: 0.92064\n",
            "Epoch: 27/30, step: 228/364, loss: 0.20806, accuracy: 0.92071\n",
            "Epoch: 27/30, step: 229/364, loss: 0.20776, accuracy: 0.92092\n",
            "Epoch: 27/30, step: 230/364, loss: 0.20778, accuracy: 0.92086\n",
            "Epoch: 27/30, step: 231/364, loss: 0.20769, accuracy: 0.92093\n",
            "Epoch: 27/30, step: 232/364, loss: 0.20758, accuracy: 0.92080\n",
            "Epoch: 27/30, step: 233/364, loss: 0.20778, accuracy: 0.92060\n",
            "Epoch: 27/30, step: 234/364, loss: 0.20766, accuracy: 0.92067\n",
            "Epoch: 27/30, step: 235/364, loss: 0.20794, accuracy: 0.92028\n",
            "Epoch: 27/30, step: 236/364, loss: 0.20770, accuracy: 0.92042\n",
            "Epoch: 27/30, step: 237/364, loss: 0.20799, accuracy: 0.92029\n",
            "Epoch: 27/30, step: 238/364, loss: 0.20790, accuracy: 0.92050\n",
            "Epoch: 27/30, step: 239/364, loss: 0.20800, accuracy: 0.92050\n",
            "Epoch: 27/30, step: 240/364, loss: 0.20831, accuracy: 0.92031\n",
            "Epoch: 27/30, step: 241/364, loss: 0.20811, accuracy: 0.92038\n",
            "Epoch: 27/30, step: 242/364, loss: 0.20795, accuracy: 0.92045\n",
            "Epoch: 27/30, step: 243/364, loss: 0.20776, accuracy: 0.92059\n",
            "Epoch: 27/30, step: 244/364, loss: 0.20758, accuracy: 0.92072\n",
            "Epoch: 27/30, step: 245/364, loss: 0.20871, accuracy: 0.92028\n",
            "Epoch: 27/30, step: 246/364, loss: 0.20857, accuracy: 0.92041\n",
            "Epoch: 27/30, step: 247/364, loss: 0.20863, accuracy: 0.92029\n",
            "Epoch: 27/30, step: 248/364, loss: 0.20867, accuracy: 0.92030\n",
            "Epoch: 27/30, step: 249/364, loss: 0.20858, accuracy: 0.92031\n",
            "Epoch: 27/30, step: 250/364, loss: 0.20861, accuracy: 0.92031\n",
            "Epoch: 27/30, step: 251/364, loss: 0.20905, accuracy: 0.92001\n",
            "Epoch: 27/30, step: 252/364, loss: 0.20875, accuracy: 0.92020\n",
            "Epoch: 27/30, step: 253/364, loss: 0.20846, accuracy: 0.92039\n",
            "Epoch: 27/30, step: 254/364, loss: 0.20836, accuracy: 0.92040\n",
            "Epoch: 27/30, step: 255/364, loss: 0.20861, accuracy: 0.92016\n",
            "Epoch: 27/30, step: 256/364, loss: 0.20869, accuracy: 0.92017\n",
            "Epoch: 27/30, step: 257/364, loss: 0.20854, accuracy: 0.92011\n",
            "Epoch: 27/30, step: 258/364, loss: 0.20852, accuracy: 0.92012\n",
            "Epoch: 27/30, step: 259/364, loss: 0.20853, accuracy: 0.92007\n",
            "Epoch: 27/30, step: 260/364, loss: 0.20883, accuracy: 0.91983\n",
            "Epoch: 27/30, step: 261/364, loss: 0.20890, accuracy: 0.91972\n",
            "Epoch: 27/30, step: 262/364, loss: 0.20883, accuracy: 0.91973\n",
            "Epoch: 27/30, step: 263/364, loss: 0.20903, accuracy: 0.91956\n",
            "Epoch: 27/30, step: 264/364, loss: 0.20941, accuracy: 0.91939\n",
            "Epoch: 27/30, step: 265/364, loss: 0.20934, accuracy: 0.91940\n",
            "Epoch: 27/30, step: 266/364, loss: 0.20937, accuracy: 0.91929\n",
            "Epoch: 27/30, step: 267/364, loss: 0.20963, accuracy: 0.91907\n",
            "Epoch: 27/30, step: 268/364, loss: 0.20944, accuracy: 0.91919\n",
            "Epoch: 27/30, step: 269/364, loss: 0.20936, accuracy: 0.91920\n",
            "Epoch: 27/30, step: 270/364, loss: 0.20937, accuracy: 0.91933\n",
            "Epoch: 27/30, step: 271/364, loss: 0.20935, accuracy: 0.91928\n",
            "Epoch: 27/30, step: 272/364, loss: 0.20932, accuracy: 0.91923\n",
            "Epoch: 27/30, step: 273/364, loss: 0.20899, accuracy: 0.91941\n",
            "Epoch: 27/30, step: 274/364, loss: 0.20909, accuracy: 0.91948\n",
            "Epoch: 27/30, step: 275/364, loss: 0.20907, accuracy: 0.91955\n",
            "Epoch: 27/30, step: 276/364, loss: 0.20945, accuracy: 0.91921\n",
            "Epoch: 27/30, step: 277/364, loss: 0.20976, accuracy: 0.91894\n",
            "Epoch: 27/30, step: 278/364, loss: 0.20971, accuracy: 0.91901\n",
            "Epoch: 27/30, step: 279/364, loss: 0.20959, accuracy: 0.91907\n",
            "Epoch: 27/30, step: 280/364, loss: 0.20943, accuracy: 0.91920\n",
            "Epoch: 27/30, step: 281/364, loss: 0.20942, accuracy: 0.91926\n",
            "Epoch: 27/30, step: 282/364, loss: 0.20926, accuracy: 0.91949\n",
            "Epoch: 27/30, step: 283/364, loss: 0.20903, accuracy: 0.91967\n",
            "Epoch: 27/30, step: 284/364, loss: 0.20903, accuracy: 0.91956\n",
            "Epoch: 27/30, step: 285/364, loss: 0.20902, accuracy: 0.91968\n",
            "Epoch: 27/30, step: 286/364, loss: 0.20906, accuracy: 0.91969\n",
            "Epoch: 27/30, step: 287/364, loss: 0.20938, accuracy: 0.91943\n",
            "Epoch: 27/30, step: 288/364, loss: 0.20924, accuracy: 0.91954\n",
            "Epoch: 27/30, step: 289/364, loss: 0.20923, accuracy: 0.91955\n",
            "Epoch: 27/30, step: 290/364, loss: 0.20929, accuracy: 0.91961\n",
            "Epoch: 27/30, step: 291/364, loss: 0.20975, accuracy: 0.91945\n",
            "Epoch: 27/30, train loss: 0.20975, train accuracy: 0.91945, valid loss: 0.76613, valid accuracy: 0.69003\n",
            "Epoch: 28/30, step: 1/364, loss: 0.37884, accuracy: 0.82812\n",
            "Epoch: 28/30, step: 2/364, loss: 0.26201, accuracy: 0.88281\n",
            "Epoch: 28/30, step: 3/364, loss: 0.23855, accuracy: 0.90104\n",
            "Epoch: 28/30, step: 4/364, loss: 0.21988, accuracy: 0.91797\n",
            "Epoch: 28/30, step: 5/364, loss: 0.21312, accuracy: 0.91875\n",
            "Epoch: 28/30, step: 6/364, loss: 0.19549, accuracy: 0.92969\n",
            "Epoch: 28/30, step: 7/364, loss: 0.19298, accuracy: 0.93080\n",
            "Epoch: 28/30, step: 8/364, loss: 0.18914, accuracy: 0.93750\n",
            "Epoch: 28/30, step: 9/364, loss: 0.18328, accuracy: 0.93924\n",
            "Epoch: 28/30, step: 10/364, loss: 0.18918, accuracy: 0.93437\n",
            "Epoch: 28/30, step: 11/364, loss: 0.19034, accuracy: 0.93324\n",
            "Epoch: 28/30, step: 12/364, loss: 0.19515, accuracy: 0.93229\n",
            "Epoch: 28/30, step: 13/364, loss: 0.20374, accuracy: 0.93029\n",
            "Epoch: 28/30, step: 14/364, loss: 0.20630, accuracy: 0.92857\n",
            "Epoch: 28/30, step: 15/364, loss: 0.20294, accuracy: 0.93125\n",
            "Epoch: 28/30, step: 16/364, loss: 0.20235, accuracy: 0.93164\n",
            "Epoch: 28/30, step: 17/364, loss: 0.20118, accuracy: 0.93199\n",
            "Epoch: 28/30, step: 18/364, loss: 0.19887, accuracy: 0.93142\n",
            "Epoch: 28/30, step: 19/364, loss: 0.19634, accuracy: 0.93339\n",
            "Epoch: 28/30, step: 20/364, loss: 0.19411, accuracy: 0.93359\n",
            "Epoch: 28/30, step: 21/364, loss: 0.19672, accuracy: 0.93229\n",
            "Epoch: 28/30, step: 22/364, loss: 0.19256, accuracy: 0.93537\n",
            "Epoch: 28/30, step: 23/364, loss: 0.19018, accuracy: 0.93614\n",
            "Epoch: 28/30, step: 24/364, loss: 0.18888, accuracy: 0.93620\n",
            "Epoch: 28/30, step: 25/364, loss: 0.18686, accuracy: 0.93750\n",
            "Epoch: 28/30, step: 26/364, loss: 0.18607, accuracy: 0.93750\n",
            "Epoch: 28/30, step: 27/364, loss: 0.18722, accuracy: 0.93692\n",
            "Epoch: 28/30, step: 28/364, loss: 0.19037, accuracy: 0.93471\n",
            "Epoch: 28/30, step: 29/364, loss: 0.19086, accuracy: 0.93534\n",
            "Epoch: 28/30, step: 30/364, loss: 0.18956, accuracy: 0.93542\n",
            "Epoch: 28/30, step: 31/364, loss: 0.18922, accuracy: 0.93599\n",
            "Epoch: 28/30, step: 32/364, loss: 0.18969, accuracy: 0.93506\n",
            "Epoch: 28/30, step: 33/364, loss: 0.19005, accuracy: 0.93561\n",
            "Epoch: 28/30, step: 34/364, loss: 0.19319, accuracy: 0.93428\n",
            "Epoch: 28/30, step: 35/364, loss: 0.19120, accuracy: 0.93616\n",
            "Epoch: 28/30, step: 36/364, loss: 0.19053, accuracy: 0.93707\n",
            "Epoch: 28/30, step: 37/364, loss: 0.19059, accuracy: 0.93581\n",
            "Epoch: 28/30, step: 38/364, loss: 0.19097, accuracy: 0.93586\n",
            "Epoch: 28/30, step: 39/364, loss: 0.18983, accuracy: 0.93670\n",
            "Epoch: 28/30, step: 40/364, loss: 0.18900, accuracy: 0.93711\n",
            "Epoch: 28/30, step: 41/364, loss: 0.19021, accuracy: 0.93674\n",
            "Epoch: 28/30, step: 42/364, loss: 0.18924, accuracy: 0.93750\n",
            "Epoch: 28/30, step: 43/364, loss: 0.18889, accuracy: 0.93714\n",
            "Epoch: 28/30, step: 44/364, loss: 0.18820, accuracy: 0.93714\n",
            "Epoch: 28/30, step: 45/364, loss: 0.18756, accuracy: 0.93750\n",
            "Epoch: 28/30, step: 46/364, loss: 0.18614, accuracy: 0.93886\n",
            "Epoch: 28/30, step: 47/364, loss: 0.18557, accuracy: 0.93850\n",
            "Epoch: 28/30, step: 48/364, loss: 0.18477, accuracy: 0.93848\n",
            "Epoch: 28/30, step: 49/364, loss: 0.18290, accuracy: 0.93941\n",
            "Epoch: 28/30, step: 50/364, loss: 0.18283, accuracy: 0.93937\n",
            "Epoch: 28/30, step: 51/364, loss: 0.18699, accuracy: 0.93811\n",
            "Epoch: 28/30, step: 52/364, loss: 0.18702, accuracy: 0.93810\n",
            "Epoch: 28/30, step: 53/364, loss: 0.18666, accuracy: 0.93809\n",
            "Epoch: 28/30, step: 54/364, loss: 0.18761, accuracy: 0.93808\n",
            "Epoch: 28/30, step: 55/364, loss: 0.18656, accuracy: 0.93892\n",
            "Epoch: 28/30, step: 56/364, loss: 0.18833, accuracy: 0.93778\n",
            "Epoch: 28/30, step: 57/364, loss: 0.18790, accuracy: 0.93777\n",
            "Epoch: 28/30, step: 58/364, loss: 0.18984, accuracy: 0.93696\n",
            "Epoch: 28/30, step: 59/364, loss: 0.19062, accuracy: 0.93644\n",
            "Epoch: 28/30, step: 60/364, loss: 0.19068, accuracy: 0.93620\n",
            "Epoch: 28/30, step: 61/364, loss: 0.19029, accuracy: 0.93648\n",
            "Epoch: 28/30, step: 62/364, loss: 0.19481, accuracy: 0.93448\n",
            "Epoch: 28/30, step: 63/364, loss: 0.19519, accuracy: 0.93378\n",
            "Epoch: 28/30, step: 64/364, loss: 0.19463, accuracy: 0.93408\n",
            "Epoch: 28/30, step: 65/364, loss: 0.19530, accuracy: 0.93365\n",
            "Epoch: 28/30, step: 66/364, loss: 0.19524, accuracy: 0.93324\n",
            "Epoch: 28/30, step: 67/364, loss: 0.19463, accuracy: 0.93377\n",
            "Epoch: 28/30, step: 68/364, loss: 0.19378, accuracy: 0.93451\n",
            "Epoch: 28/30, step: 69/364, loss: 0.19317, accuracy: 0.93501\n",
            "Epoch: 28/30, step: 70/364, loss: 0.19398, accuracy: 0.93393\n",
            "Epoch: 28/30, step: 71/364, loss: 0.19390, accuracy: 0.93398\n",
            "Epoch: 28/30, step: 72/364, loss: 0.19261, accuracy: 0.93446\n",
            "Epoch: 28/30, step: 73/364, loss: 0.19358, accuracy: 0.93343\n",
            "Epoch: 28/30, step: 74/364, loss: 0.19343, accuracy: 0.93307\n",
            "Epoch: 28/30, step: 75/364, loss: 0.19358, accuracy: 0.93271\n",
            "Epoch: 28/30, step: 76/364, loss: 0.19224, accuracy: 0.93339\n",
            "Epoch: 28/30, step: 77/364, loss: 0.19246, accuracy: 0.93324\n",
            "Epoch: 28/30, step: 78/364, loss: 0.19216, accuracy: 0.93349\n",
            "Epoch: 28/30, step: 79/364, loss: 0.19185, accuracy: 0.93374\n",
            "Epoch: 28/30, step: 80/364, loss: 0.19208, accuracy: 0.93359\n",
            "Epoch: 28/30, step: 81/364, loss: 0.19182, accuracy: 0.93364\n",
            "Epoch: 28/30, step: 82/364, loss: 0.19180, accuracy: 0.93369\n",
            "Epoch: 28/30, step: 83/364, loss: 0.19127, accuracy: 0.93411\n",
            "Epoch: 28/30, step: 84/364, loss: 0.19080, accuracy: 0.93434\n",
            "Epoch: 28/30, step: 85/364, loss: 0.19052, accuracy: 0.93437\n",
            "Epoch: 28/30, step: 86/364, loss: 0.19022, accuracy: 0.93459\n",
            "Epoch: 28/30, step: 87/364, loss: 0.19010, accuracy: 0.93445\n",
            "Epoch: 28/30, step: 88/364, loss: 0.19047, accuracy: 0.93377\n",
            "Epoch: 28/30, step: 89/364, loss: 0.18979, accuracy: 0.93434\n",
            "Epoch: 28/30, step: 90/364, loss: 0.18919, accuracy: 0.93472\n",
            "Epoch: 28/30, step: 91/364, loss: 0.18905, accuracy: 0.93441\n",
            "Epoch: 28/30, step: 92/364, loss: 0.18894, accuracy: 0.93461\n",
            "Epoch: 28/30, step: 93/364, loss: 0.18980, accuracy: 0.93431\n",
            "Epoch: 28/30, step: 94/364, loss: 0.18915, accuracy: 0.93484\n",
            "Epoch: 28/30, step: 95/364, loss: 0.18898, accuracy: 0.93437\n",
            "Epoch: 28/30, step: 96/364, loss: 0.18805, accuracy: 0.93506\n",
            "Epoch: 28/30, step: 97/364, loss: 0.18772, accuracy: 0.93524\n",
            "Epoch: 28/30, step: 98/364, loss: 0.18732, accuracy: 0.93527\n",
            "Epoch: 28/30, step: 99/364, loss: 0.18683, accuracy: 0.93545\n",
            "Epoch: 28/30, step: 100/364, loss: 0.18653, accuracy: 0.93563\n",
            "Epoch: 28/30, step: 101/364, loss: 0.18661, accuracy: 0.93564\n",
            "Epoch: 28/30, step: 102/364, loss: 0.18685, accuracy: 0.93551\n",
            "Epoch: 28/30, step: 103/364, loss: 0.18743, accuracy: 0.93538\n",
            "Epoch: 28/30, step: 104/364, loss: 0.18634, accuracy: 0.93600\n",
            "Epoch: 28/30, step: 105/364, loss: 0.18653, accuracy: 0.93586\n",
            "Epoch: 28/30, step: 106/364, loss: 0.18618, accuracy: 0.93603\n",
            "Epoch: 28/30, step: 107/364, loss: 0.18650, accuracy: 0.93589\n",
            "Epoch: 28/30, step: 108/364, loss: 0.18685, accuracy: 0.93576\n",
            "Epoch: 28/30, step: 109/364, loss: 0.18717, accuracy: 0.93564\n",
            "Epoch: 28/30, step: 110/364, loss: 0.18765, accuracy: 0.93551\n",
            "Epoch: 28/30, step: 111/364, loss: 0.18816, accuracy: 0.93497\n",
            "Epoch: 28/30, step: 112/364, loss: 0.18730, accuracy: 0.93541\n",
            "Epoch: 28/30, step: 113/364, loss: 0.18839, accuracy: 0.93446\n",
            "Epoch: 28/30, step: 114/364, loss: 0.18833, accuracy: 0.93435\n",
            "Epoch: 28/30, step: 115/364, loss: 0.18809, accuracy: 0.93437\n",
            "Epoch: 28/30, step: 116/364, loss: 0.18906, accuracy: 0.93373\n",
            "Epoch: 28/30, step: 117/364, loss: 0.18966, accuracy: 0.93336\n",
            "Epoch: 28/30, step: 118/364, loss: 0.18944, accuracy: 0.93340\n",
            "Epoch: 28/30, step: 119/364, loss: 0.18961, accuracy: 0.93317\n",
            "Epoch: 28/30, step: 120/364, loss: 0.18976, accuracy: 0.93294\n",
            "Epoch: 28/30, step: 121/364, loss: 0.18990, accuracy: 0.93298\n",
            "Epoch: 28/30, step: 122/364, loss: 0.18941, accuracy: 0.93327\n",
            "Epoch: 28/30, step: 123/364, loss: 0.18940, accuracy: 0.93318\n",
            "Epoch: 28/30, step: 124/364, loss: 0.18974, accuracy: 0.93296\n",
            "Epoch: 28/30, step: 125/364, loss: 0.18968, accuracy: 0.93313\n",
            "Epoch: 28/30, step: 126/364, loss: 0.18990, accuracy: 0.93316\n",
            "Epoch: 28/30, step: 127/364, loss: 0.19025, accuracy: 0.93295\n",
            "Epoch: 28/30, step: 128/364, loss: 0.18999, accuracy: 0.93323\n",
            "Epoch: 28/30, step: 129/364, loss: 0.18973, accuracy: 0.93338\n",
            "Epoch: 28/30, step: 130/364, loss: 0.18997, accuracy: 0.93341\n",
            "Epoch: 28/30, step: 131/364, loss: 0.18969, accuracy: 0.93368\n",
            "Epoch: 28/30, step: 132/364, loss: 0.18935, accuracy: 0.93383\n",
            "Epoch: 28/30, step: 133/364, loss: 0.18901, accuracy: 0.93409\n",
            "Epoch: 28/30, step: 134/364, loss: 0.18847, accuracy: 0.93435\n",
            "Epoch: 28/30, step: 135/364, loss: 0.18906, accuracy: 0.93403\n",
            "Epoch: 28/30, step: 136/364, loss: 0.18878, accuracy: 0.93417\n",
            "Epoch: 28/30, step: 137/364, loss: 0.18827, accuracy: 0.93442\n",
            "Epoch: 28/30, step: 138/364, loss: 0.18822, accuracy: 0.93456\n",
            "Epoch: 28/30, step: 139/364, loss: 0.18801, accuracy: 0.93469\n",
            "Epoch: 28/30, step: 140/364, loss: 0.18792, accuracy: 0.93460\n",
            "Epoch: 28/30, step: 141/364, loss: 0.18797, accuracy: 0.93462\n",
            "Epoch: 28/30, step: 142/364, loss: 0.18835, accuracy: 0.93420\n",
            "Epoch: 28/30, step: 143/364, loss: 0.18873, accuracy: 0.93411\n",
            "Epoch: 28/30, step: 144/364, loss: 0.18814, accuracy: 0.93446\n",
            "Epoch: 28/30, step: 145/364, loss: 0.18873, accuracy: 0.93405\n",
            "Epoch: 28/30, step: 146/364, loss: 0.18849, accuracy: 0.93429\n",
            "Epoch: 28/30, step: 147/364, loss: 0.18859, accuracy: 0.93399\n",
            "Epoch: 28/30, step: 148/364, loss: 0.18942, accuracy: 0.93338\n",
            "Epoch: 28/30, step: 149/364, loss: 0.18966, accuracy: 0.93310\n",
            "Epoch: 28/30, step: 150/364, loss: 0.18968, accuracy: 0.93302\n",
            "Epoch: 28/30, step: 151/364, loss: 0.19094, accuracy: 0.93243\n",
            "Epoch: 28/30, step: 152/364, loss: 0.19069, accuracy: 0.93257\n",
            "Epoch: 28/30, step: 153/364, loss: 0.19032, accuracy: 0.93270\n",
            "Epoch: 28/30, step: 154/364, loss: 0.19060, accuracy: 0.93243\n",
            "Epoch: 28/30, step: 155/364, loss: 0.19022, accuracy: 0.93266\n",
            "Epoch: 28/30, step: 156/364, loss: 0.18970, accuracy: 0.93299\n",
            "Epoch: 28/30, step: 157/364, loss: 0.18942, accuracy: 0.93312\n",
            "Epoch: 28/30, step: 158/364, loss: 0.18909, accuracy: 0.93325\n",
            "Epoch: 28/30, step: 159/364, loss: 0.18984, accuracy: 0.93278\n",
            "Epoch: 28/30, step: 160/364, loss: 0.19074, accuracy: 0.93223\n",
            "Epoch: 28/30, step: 161/364, loss: 0.19081, accuracy: 0.93207\n",
            "Epoch: 28/30, step: 162/364, loss: 0.19093, accuracy: 0.93200\n",
            "Epoch: 28/30, step: 163/364, loss: 0.19031, accuracy: 0.93232\n",
            "Epoch: 28/30, step: 164/364, loss: 0.19006, accuracy: 0.93245\n",
            "Epoch: 28/30, step: 165/364, loss: 0.19014, accuracy: 0.93239\n",
            "Epoch: 28/30, step: 166/364, loss: 0.19071, accuracy: 0.93195\n",
            "Epoch: 28/30, step: 167/364, loss: 0.19040, accuracy: 0.93207\n",
            "Epoch: 28/30, step: 168/364, loss: 0.19048, accuracy: 0.93201\n",
            "Epoch: 28/30, step: 169/364, loss: 0.19031, accuracy: 0.93214\n",
            "Epoch: 28/30, step: 170/364, loss: 0.19009, accuracy: 0.93217\n",
            "Epoch: 28/30, step: 171/364, loss: 0.19048, accuracy: 0.93202\n",
            "Epoch: 28/30, step: 172/364, loss: 0.19048, accuracy: 0.93205\n",
            "Epoch: 28/30, step: 173/364, loss: 0.19095, accuracy: 0.93199\n",
            "Epoch: 28/30, step: 174/364, loss: 0.19082, accuracy: 0.93202\n",
            "Epoch: 28/30, step: 175/364, loss: 0.19142, accuracy: 0.93152\n",
            "Epoch: 28/30, step: 176/364, loss: 0.19110, accuracy: 0.93191\n",
            "Epoch: 28/30, step: 177/364, loss: 0.19070, accuracy: 0.93203\n",
            "Epoch: 28/30, step: 178/364, loss: 0.19103, accuracy: 0.93179\n",
            "Epoch: 28/30, step: 179/364, loss: 0.19145, accuracy: 0.93139\n",
            "Epoch: 28/30, step: 180/364, loss: 0.19135, accuracy: 0.93142\n",
            "Epoch: 28/30, step: 181/364, loss: 0.19101, accuracy: 0.93163\n",
            "Epoch: 28/30, step: 182/364, loss: 0.19082, accuracy: 0.93175\n",
            "Epoch: 28/30, step: 183/364, loss: 0.19038, accuracy: 0.93212\n",
            "Epoch: 28/30, step: 184/364, loss: 0.19000, accuracy: 0.93240\n",
            "Epoch: 28/30, step: 185/364, loss: 0.19007, accuracy: 0.93235\n",
            "Epoch: 28/30, step: 186/364, loss: 0.18961, accuracy: 0.93263\n",
            "Epoch: 28/30, step: 187/364, loss: 0.18932, accuracy: 0.93274\n",
            "Epoch: 28/30, step: 188/364, loss: 0.18941, accuracy: 0.93268\n",
            "Epoch: 28/30, step: 189/364, loss: 0.18907, accuracy: 0.93279\n",
            "Epoch: 28/30, step: 190/364, loss: 0.18910, accuracy: 0.93273\n",
            "Epoch: 28/30, step: 191/364, loss: 0.18880, accuracy: 0.93284\n",
            "Epoch: 28/30, step: 192/364, loss: 0.18890, accuracy: 0.93270\n",
            "Epoch: 28/30, step: 193/364, loss: 0.18881, accuracy: 0.93272\n",
            "Epoch: 28/30, step: 194/364, loss: 0.18907, accuracy: 0.93259\n",
            "Epoch: 28/30, step: 195/364, loss: 0.18883, accuracy: 0.93269\n",
            "Epoch: 28/30, step: 196/364, loss: 0.18881, accuracy: 0.93264\n",
            "Epoch: 28/30, step: 197/364, loss: 0.18883, accuracy: 0.93266\n",
            "Epoch: 28/30, step: 198/364, loss: 0.18839, accuracy: 0.93300\n",
            "Epoch: 28/30, step: 199/364, loss: 0.18829, accuracy: 0.93310\n",
            "Epoch: 28/30, step: 200/364, loss: 0.18826, accuracy: 0.93320\n",
            "Epoch: 28/30, step: 201/364, loss: 0.18817, accuracy: 0.93330\n",
            "Epoch: 28/30, step: 202/364, loss: 0.18831, accuracy: 0.93325\n",
            "Epoch: 28/30, step: 203/364, loss: 0.18836, accuracy: 0.93311\n",
            "Epoch: 28/30, step: 204/364, loss: 0.18810, accuracy: 0.93329\n",
            "Epoch: 28/30, step: 205/364, loss: 0.18795, accuracy: 0.93338\n",
            "Epoch: 28/30, step: 206/364, loss: 0.18769, accuracy: 0.93363\n",
            "Epoch: 28/30, step: 207/364, loss: 0.18754, accuracy: 0.93365\n",
            "Epoch: 28/30, step: 208/364, loss: 0.18732, accuracy: 0.93359\n",
            "Epoch: 28/30, step: 209/364, loss: 0.18722, accuracy: 0.93361\n",
            "Epoch: 28/30, step: 210/364, loss: 0.18752, accuracy: 0.93333\n",
            "Epoch: 28/30, step: 211/364, loss: 0.18779, accuracy: 0.93306\n",
            "Epoch: 28/30, step: 212/364, loss: 0.18771, accuracy: 0.93308\n",
            "Epoch: 28/30, step: 213/364, loss: 0.18827, accuracy: 0.93266\n",
            "Epoch: 28/30, step: 214/364, loss: 0.18809, accuracy: 0.93275\n",
            "Epoch: 28/30, step: 215/364, loss: 0.18784, accuracy: 0.93285\n",
            "Epoch: 28/30, step: 216/364, loss: 0.18753, accuracy: 0.93302\n",
            "Epoch: 28/30, step: 217/364, loss: 0.18794, accuracy: 0.93282\n",
            "Epoch: 28/30, step: 218/364, loss: 0.18811, accuracy: 0.93277\n",
            "Epoch: 28/30, step: 219/364, loss: 0.18788, accuracy: 0.93293\n",
            "Epoch: 28/30, step: 220/364, loss: 0.18802, accuracy: 0.93288\n",
            "Epoch: 28/30, step: 221/364, loss: 0.18768, accuracy: 0.93312\n",
            "Epoch: 28/30, step: 222/364, loss: 0.18734, accuracy: 0.93328\n",
            "Epoch: 28/30, step: 223/364, loss: 0.18722, accuracy: 0.93337\n",
            "Epoch: 28/30, step: 224/364, loss: 0.18702, accuracy: 0.93338\n",
            "Epoch: 28/30, step: 225/364, loss: 0.18773, accuracy: 0.93292\n",
            "Epoch: 28/30, step: 226/364, loss: 0.18761, accuracy: 0.93301\n",
            "Epoch: 28/30, step: 227/364, loss: 0.18789, accuracy: 0.93282\n",
            "Epoch: 28/30, step: 228/364, loss: 0.18822, accuracy: 0.93270\n",
            "Epoch: 28/30, step: 229/364, loss: 0.18854, accuracy: 0.93231\n",
            "Epoch: 28/30, step: 230/364, loss: 0.18840, accuracy: 0.93247\n",
            "Epoch: 28/30, step: 231/364, loss: 0.18892, accuracy: 0.93216\n",
            "Epoch: 28/30, step: 232/364, loss: 0.18910, accuracy: 0.93204\n",
            "Epoch: 28/30, step: 233/364, loss: 0.18912, accuracy: 0.93207\n",
            "Epoch: 28/30, step: 234/364, loss: 0.18878, accuracy: 0.93236\n",
            "Epoch: 28/30, step: 235/364, loss: 0.18876, accuracy: 0.93225\n",
            "Epoch: 28/30, step: 236/364, loss: 0.18955, accuracy: 0.93194\n",
            "Epoch: 28/30, step: 237/364, loss: 0.18939, accuracy: 0.93196\n",
            "Epoch: 28/30, step: 238/364, loss: 0.18941, accuracy: 0.93192\n",
            "Epoch: 28/30, step: 239/364, loss: 0.18951, accuracy: 0.93175\n",
            "Epoch: 28/30, step: 240/364, loss: 0.18935, accuracy: 0.93184\n",
            "Epoch: 28/30, step: 241/364, loss: 0.18928, accuracy: 0.93192\n",
            "Epoch: 28/30, step: 242/364, loss: 0.18918, accuracy: 0.93195\n",
            "Epoch: 28/30, step: 243/364, loss: 0.18919, accuracy: 0.93197\n",
            "Epoch: 28/30, step: 244/364, loss: 0.18903, accuracy: 0.93212\n",
            "Epoch: 28/30, step: 245/364, loss: 0.18879, accuracy: 0.93221\n",
            "Epoch: 28/30, step: 246/364, loss: 0.18873, accuracy: 0.93229\n",
            "Epoch: 28/30, step: 247/364, loss: 0.18854, accuracy: 0.93250\n",
            "Epoch: 28/30, step: 248/364, loss: 0.18848, accuracy: 0.93259\n",
            "Epoch: 28/30, step: 249/364, loss: 0.18868, accuracy: 0.93242\n",
            "Epoch: 28/30, step: 250/364, loss: 0.18861, accuracy: 0.93231\n",
            "Epoch: 28/30, step: 251/364, loss: 0.18873, accuracy: 0.93227\n",
            "Epoch: 28/30, step: 252/364, loss: 0.18873, accuracy: 0.93235\n",
            "Epoch: 28/30, step: 253/364, loss: 0.18868, accuracy: 0.93237\n",
            "Epoch: 28/30, step: 254/364, loss: 0.18886, accuracy: 0.93239\n",
            "Epoch: 28/30, step: 255/364, loss: 0.18862, accuracy: 0.93254\n",
            "Epoch: 28/30, step: 256/364, loss: 0.18843, accuracy: 0.93268\n",
            "Epoch: 28/30, step: 257/364, loss: 0.18847, accuracy: 0.93270\n",
            "Epoch: 28/30, step: 258/364, loss: 0.18819, accuracy: 0.93284\n",
            "Epoch: 28/30, step: 259/364, loss: 0.18858, accuracy: 0.93261\n",
            "Epoch: 28/30, step: 260/364, loss: 0.18885, accuracy: 0.93233\n",
            "Epoch: 28/30, step: 261/364, loss: 0.18892, accuracy: 0.93235\n",
            "Epoch: 28/30, step: 262/364, loss: 0.18864, accuracy: 0.93249\n",
            "Epoch: 28/30, step: 263/364, loss: 0.18879, accuracy: 0.93239\n",
            "Epoch: 28/30, step: 264/364, loss: 0.18898, accuracy: 0.93229\n",
            "Epoch: 28/30, step: 265/364, loss: 0.18947, accuracy: 0.93208\n",
            "Epoch: 28/30, step: 266/364, loss: 0.18939, accuracy: 0.93204\n",
            "Epoch: 28/30, step: 267/364, loss: 0.18928, accuracy: 0.93217\n",
            "Epoch: 28/30, step: 268/364, loss: 0.18919, accuracy: 0.93225\n",
            "Epoch: 28/30, step: 269/364, loss: 0.18905, accuracy: 0.93227\n",
            "Epoch: 28/30, step: 270/364, loss: 0.18902, accuracy: 0.93229\n",
            "Epoch: 28/30, step: 271/364, loss: 0.18920, accuracy: 0.93220\n",
            "Epoch: 28/30, step: 272/364, loss: 0.18923, accuracy: 0.93227\n",
            "Epoch: 28/30, step: 273/364, loss: 0.18957, accuracy: 0.93206\n",
            "Epoch: 28/30, step: 274/364, loss: 0.19065, accuracy: 0.93168\n",
            "Epoch: 28/30, step: 275/364, loss: 0.19041, accuracy: 0.93187\n",
            "Epoch: 28/30, step: 276/364, loss: 0.19020, accuracy: 0.93195\n",
            "Epoch: 28/30, step: 277/364, loss: 0.18994, accuracy: 0.93203\n",
            "Epoch: 28/30, step: 278/364, loss: 0.18995, accuracy: 0.93216\n",
            "Epoch: 28/30, step: 279/364, loss: 0.19007, accuracy: 0.93212\n",
            "Epoch: 28/30, step: 280/364, loss: 0.18976, accuracy: 0.93231\n",
            "Epoch: 28/30, step: 281/364, loss: 0.18946, accuracy: 0.93250\n",
            "Epoch: 28/30, step: 282/364, loss: 0.18957, accuracy: 0.93235\n",
            "Epoch: 28/30, step: 283/364, loss: 0.18924, accuracy: 0.93248\n",
            "Epoch: 28/30, step: 284/364, loss: 0.18908, accuracy: 0.93249\n",
            "Epoch: 28/30, step: 285/364, loss: 0.18910, accuracy: 0.93240\n",
            "Epoch: 28/30, step: 286/364, loss: 0.18885, accuracy: 0.93258\n",
            "Epoch: 28/30, step: 287/364, loss: 0.18867, accuracy: 0.93276\n",
            "Epoch: 28/30, step: 288/364, loss: 0.18847, accuracy: 0.93294\n",
            "Epoch: 28/30, step: 289/364, loss: 0.18854, accuracy: 0.93301\n",
            "Epoch: 28/30, step: 290/364, loss: 0.18862, accuracy: 0.93303\n",
            "Epoch: 28/30, step: 291/364, loss: 0.18839, accuracy: 0.93321\n",
            "Epoch: 28/30, train loss: 0.18839, train accuracy: 0.93321, valid loss: 0.79791, valid accuracy: 0.69089\n",
            "Epoch: 29/30, step: 1/364, loss: 0.12934, accuracy: 0.96875\n",
            "Epoch: 29/30, step: 2/364, loss: 0.13109, accuracy: 0.97656\n",
            "Epoch: 29/30, step: 3/364, loss: 0.13989, accuracy: 0.97396\n",
            "Epoch: 29/30, step: 4/364, loss: 0.12614, accuracy: 0.98047\n",
            "Epoch: 29/30, step: 5/364, loss: 0.14961, accuracy: 0.97188\n",
            "Epoch: 29/30, step: 6/364, loss: 0.15993, accuracy: 0.96615\n",
            "Epoch: 29/30, step: 7/364, loss: 0.17314, accuracy: 0.95536\n",
            "Epoch: 29/30, step: 8/364, loss: 0.17246, accuracy: 0.95312\n",
            "Epoch: 29/30, step: 9/364, loss: 0.17694, accuracy: 0.94618\n",
            "Epoch: 29/30, step: 10/364, loss: 0.18831, accuracy: 0.93594\n",
            "Epoch: 29/30, step: 11/364, loss: 0.18699, accuracy: 0.93608\n",
            "Epoch: 29/30, step: 12/364, loss: 0.19381, accuracy: 0.92969\n",
            "Epoch: 29/30, step: 13/364, loss: 0.19081, accuracy: 0.93029\n",
            "Epoch: 29/30, step: 14/364, loss: 0.18989, accuracy: 0.93192\n",
            "Epoch: 29/30, step: 15/364, loss: 0.19269, accuracy: 0.93021\n",
            "Epoch: 29/30, step: 16/364, loss: 0.19704, accuracy: 0.92871\n",
            "Epoch: 29/30, step: 17/364, loss: 0.19473, accuracy: 0.93107\n",
            "Epoch: 29/30, step: 18/364, loss: 0.19217, accuracy: 0.93229\n",
            "Epoch: 29/30, step: 19/364, loss: 0.19138, accuracy: 0.93174\n",
            "Epoch: 29/30, step: 20/364, loss: 0.19144, accuracy: 0.93125\n",
            "Epoch: 29/30, step: 21/364, loss: 0.18903, accuracy: 0.93378\n",
            "Epoch: 29/30, step: 22/364, loss: 0.19390, accuracy: 0.93111\n",
            "Epoch: 29/30, step: 23/364, loss: 0.19193, accuracy: 0.93207\n",
            "Epoch: 29/30, step: 24/364, loss: 0.19202, accuracy: 0.93164\n",
            "Epoch: 29/30, step: 25/364, loss: 0.19373, accuracy: 0.93125\n",
            "Epoch: 29/30, step: 26/364, loss: 0.19619, accuracy: 0.92849\n",
            "Epoch: 29/30, step: 27/364, loss: 0.19757, accuracy: 0.92650\n",
            "Epoch: 29/30, step: 28/364, loss: 0.19974, accuracy: 0.92355\n",
            "Epoch: 29/30, step: 29/364, loss: 0.20265, accuracy: 0.92241\n",
            "Epoch: 29/30, step: 30/364, loss: 0.20008, accuracy: 0.92396\n",
            "Epoch: 29/30, step: 31/364, loss: 0.19994, accuracy: 0.92389\n",
            "Epoch: 29/30, step: 32/364, loss: 0.19753, accuracy: 0.92480\n",
            "Epoch: 29/30, step: 33/364, loss: 0.20180, accuracy: 0.92188\n",
            "Epoch: 29/30, step: 34/364, loss: 0.20167, accuracy: 0.92142\n",
            "Epoch: 29/30, step: 35/364, loss: 0.20412, accuracy: 0.92009\n",
            "Epoch: 29/30, step: 36/364, loss: 0.20112, accuracy: 0.92144\n",
            "Epoch: 29/30, step: 37/364, loss: 0.20089, accuracy: 0.92145\n",
            "Epoch: 29/30, step: 38/364, loss: 0.19850, accuracy: 0.92352\n",
            "Epoch: 29/30, step: 39/364, loss: 0.19825, accuracy: 0.92388\n",
            "Epoch: 29/30, step: 40/364, loss: 0.19537, accuracy: 0.92539\n",
            "Epoch: 29/30, step: 41/364, loss: 0.19532, accuracy: 0.92530\n",
            "Epoch: 29/30, step: 42/364, loss: 0.19649, accuracy: 0.92448\n",
            "Epoch: 29/30, step: 43/364, loss: 0.19687, accuracy: 0.92406\n",
            "Epoch: 29/30, step: 44/364, loss: 0.19424, accuracy: 0.92578\n",
            "Epoch: 29/30, step: 45/364, loss: 0.19276, accuracy: 0.92708\n",
            "Epoch: 29/30, step: 46/364, loss: 0.19167, accuracy: 0.92799\n",
            "Epoch: 29/30, step: 47/364, loss: 0.19062, accuracy: 0.92886\n",
            "Epoch: 29/30, step: 48/364, loss: 0.18951, accuracy: 0.92969\n",
            "Epoch: 29/30, step: 49/364, loss: 0.18813, accuracy: 0.93048\n",
            "Epoch: 29/30, step: 50/364, loss: 0.18924, accuracy: 0.92969\n",
            "Epoch: 29/30, step: 51/364, loss: 0.18858, accuracy: 0.92984\n",
            "Epoch: 29/30, step: 52/364, loss: 0.18822, accuracy: 0.92999\n",
            "Epoch: 29/30, step: 53/364, loss: 0.18770, accuracy: 0.92983\n",
            "Epoch: 29/30, step: 54/364, loss: 0.18752, accuracy: 0.93027\n",
            "Epoch: 29/30, step: 55/364, loss: 0.18612, accuracy: 0.93125\n",
            "Epoch: 29/30, step: 56/364, loss: 0.18517, accuracy: 0.93136\n",
            "Epoch: 29/30, step: 57/364, loss: 0.18703, accuracy: 0.93120\n",
            "Epoch: 29/30, step: 58/364, loss: 0.18740, accuracy: 0.93077\n",
            "Epoch: 29/30, step: 59/364, loss: 0.18692, accuracy: 0.93141\n",
            "Epoch: 29/30, step: 60/364, loss: 0.18597, accuracy: 0.93203\n",
            "Epoch: 29/30, step: 61/364, loss: 0.18622, accuracy: 0.93186\n",
            "Epoch: 29/30, step: 62/364, loss: 0.18515, accuracy: 0.93221\n",
            "Epoch: 29/30, step: 63/364, loss: 0.18364, accuracy: 0.93304\n",
            "Epoch: 29/30, step: 64/364, loss: 0.18579, accuracy: 0.93213\n",
            "Epoch: 29/30, step: 65/364, loss: 0.18681, accuracy: 0.93197\n",
            "Epoch: 29/30, step: 66/364, loss: 0.18647, accuracy: 0.93229\n",
            "Epoch: 29/30, step: 67/364, loss: 0.18618, accuracy: 0.93260\n",
            "Epoch: 29/30, step: 68/364, loss: 0.18590, accuracy: 0.93290\n",
            "Epoch: 29/30, step: 69/364, loss: 0.18476, accuracy: 0.93388\n",
            "Epoch: 29/30, step: 70/364, loss: 0.18448, accuracy: 0.93393\n",
            "Epoch: 29/30, step: 71/364, loss: 0.18396, accuracy: 0.93442\n",
            "Epoch: 29/30, step: 72/364, loss: 0.18399, accuracy: 0.93446\n",
            "Epoch: 29/30, step: 73/364, loss: 0.18374, accuracy: 0.93450\n",
            "Epoch: 29/30, step: 74/364, loss: 0.18396, accuracy: 0.93433\n",
            "Epoch: 29/30, step: 75/364, loss: 0.18323, accuracy: 0.93479\n",
            "Epoch: 29/30, step: 76/364, loss: 0.18316, accuracy: 0.93421\n",
            "Epoch: 29/30, step: 77/364, loss: 0.18466, accuracy: 0.93405\n",
            "Epoch: 29/30, step: 78/364, loss: 0.18567, accuracy: 0.93389\n",
            "Epoch: 29/30, step: 79/364, loss: 0.18546, accuracy: 0.93374\n",
            "Epoch: 29/30, step: 80/364, loss: 0.18647, accuracy: 0.93359\n",
            "Epoch: 29/30, step: 81/364, loss: 0.18658, accuracy: 0.93364\n",
            "Epoch: 29/30, step: 82/364, loss: 0.18673, accuracy: 0.93312\n",
            "Epoch: 29/30, step: 83/364, loss: 0.18584, accuracy: 0.93336\n",
            "Epoch: 29/30, step: 84/364, loss: 0.18512, accuracy: 0.93378\n",
            "Epoch: 29/30, step: 85/364, loss: 0.18495, accuracy: 0.93346\n",
            "Epoch: 29/30, step: 86/364, loss: 0.18428, accuracy: 0.93405\n",
            "Epoch: 29/30, step: 87/364, loss: 0.18385, accuracy: 0.93427\n",
            "Epoch: 29/30, step: 88/364, loss: 0.18434, accuracy: 0.93430\n",
            "Epoch: 29/30, step: 89/364, loss: 0.18394, accuracy: 0.93469\n",
            "Epoch: 29/30, step: 90/364, loss: 0.18407, accuracy: 0.93507\n",
            "Epoch: 29/30, step: 91/364, loss: 0.18349, accuracy: 0.93527\n",
            "Epoch: 29/30, step: 92/364, loss: 0.18523, accuracy: 0.93393\n",
            "Epoch: 29/30, step: 93/364, loss: 0.18585, accuracy: 0.93296\n",
            "Epoch: 29/30, step: 94/364, loss: 0.18530, accuracy: 0.93334\n",
            "Epoch: 29/30, step: 95/364, loss: 0.18536, accuracy: 0.93306\n",
            "Epoch: 29/30, step: 96/364, loss: 0.18502, accuracy: 0.93327\n",
            "Epoch: 29/30, step: 97/364, loss: 0.18472, accuracy: 0.93380\n",
            "Epoch: 29/30, step: 98/364, loss: 0.18429, accuracy: 0.93399\n",
            "Epoch: 29/30, step: 99/364, loss: 0.18392, accuracy: 0.93403\n",
            "Epoch: 29/30, step: 100/364, loss: 0.18664, accuracy: 0.93250\n",
            "Epoch: 29/30, step: 101/364, loss: 0.18655, accuracy: 0.93255\n",
            "Epoch: 29/30, step: 102/364, loss: 0.18773, accuracy: 0.93199\n",
            "Epoch: 29/30, step: 103/364, loss: 0.18908, accuracy: 0.93113\n",
            "Epoch: 29/30, step: 104/364, loss: 0.18949, accuracy: 0.93089\n",
            "Epoch: 29/30, step: 105/364, loss: 0.19028, accuracy: 0.92976\n",
            "Epoch: 29/30, step: 106/364, loss: 0.18933, accuracy: 0.93028\n",
            "Epoch: 29/30, step: 107/364, loss: 0.18944, accuracy: 0.93020\n",
            "Epoch: 29/30, step: 108/364, loss: 0.18926, accuracy: 0.93027\n",
            "Epoch: 29/30, step: 109/364, loss: 0.18922, accuracy: 0.93033\n",
            "Epoch: 29/30, step: 110/364, loss: 0.18901, accuracy: 0.93068\n",
            "Epoch: 29/30, step: 111/364, loss: 0.18893, accuracy: 0.93046\n",
            "Epoch: 29/30, step: 112/364, loss: 0.18870, accuracy: 0.93039\n",
            "Epoch: 29/30, step: 113/364, loss: 0.18824, accuracy: 0.93086\n",
            "Epoch: 29/30, step: 114/364, loss: 0.18774, accuracy: 0.93147\n",
            "Epoch: 29/30, step: 115/364, loss: 0.18793, accuracy: 0.93152\n",
            "Epoch: 29/30, step: 116/364, loss: 0.18830, accuracy: 0.93117\n",
            "Epoch: 29/30, step: 117/364, loss: 0.18774, accuracy: 0.93136\n",
            "Epoch: 29/30, step: 118/364, loss: 0.18767, accuracy: 0.93141\n",
            "Epoch: 29/30, step: 119/364, loss: 0.18730, accuracy: 0.93159\n",
            "Epoch: 29/30, step: 120/364, loss: 0.18757, accuracy: 0.93125\n",
            "Epoch: 29/30, step: 121/364, loss: 0.18726, accuracy: 0.93130\n",
            "Epoch: 29/30, step: 122/364, loss: 0.18729, accuracy: 0.93122\n",
            "Epoch: 29/30, step: 123/364, loss: 0.18739, accuracy: 0.93115\n",
            "Epoch: 29/30, step: 124/364, loss: 0.18720, accuracy: 0.93120\n",
            "Epoch: 29/30, step: 125/364, loss: 0.18679, accuracy: 0.93163\n",
            "Epoch: 29/30, step: 126/364, loss: 0.18680, accuracy: 0.93180\n",
            "Epoch: 29/30, step: 127/364, loss: 0.18709, accuracy: 0.93147\n",
            "Epoch: 29/30, step: 128/364, loss: 0.18666, accuracy: 0.93164\n",
            "Epoch: 29/30, step: 129/364, loss: 0.18642, accuracy: 0.93169\n",
            "Epoch: 29/30, step: 130/364, loss: 0.18614, accuracy: 0.93149\n",
            "Epoch: 29/30, step: 131/364, loss: 0.18556, accuracy: 0.93177\n",
            "Epoch: 29/30, step: 132/364, loss: 0.18671, accuracy: 0.93063\n",
            "Epoch: 29/30, step: 133/364, loss: 0.18685, accuracy: 0.93045\n",
            "Epoch: 29/30, step: 134/364, loss: 0.18759, accuracy: 0.93027\n",
            "Epoch: 29/30, step: 135/364, loss: 0.18721, accuracy: 0.93044\n",
            "Epoch: 29/30, step: 136/364, loss: 0.18721, accuracy: 0.93038\n",
            "Epoch: 29/30, step: 137/364, loss: 0.18717, accuracy: 0.93054\n",
            "Epoch: 29/30, step: 138/364, loss: 0.18698, accuracy: 0.93059\n",
            "Epoch: 29/30, step: 139/364, loss: 0.18746, accuracy: 0.93008\n",
            "Epoch: 29/30, step: 140/364, loss: 0.18679, accuracy: 0.93047\n",
            "Epoch: 29/30, step: 141/364, loss: 0.18714, accuracy: 0.93019\n",
            "Epoch: 29/30, step: 142/364, loss: 0.18643, accuracy: 0.93057\n",
            "Epoch: 29/30, step: 143/364, loss: 0.18633, accuracy: 0.93062\n",
            "Epoch: 29/30, step: 144/364, loss: 0.18689, accuracy: 0.93023\n",
            "Epoch: 29/30, step: 145/364, loss: 0.18726, accuracy: 0.93028\n",
            "Epoch: 29/30, step: 146/364, loss: 0.18699, accuracy: 0.93033\n",
            "Epoch: 29/30, step: 147/364, loss: 0.18705, accuracy: 0.93017\n",
            "Epoch: 29/30, step: 148/364, loss: 0.18700, accuracy: 0.93022\n",
            "Epoch: 29/30, step: 149/364, loss: 0.18723, accuracy: 0.93005\n",
            "Epoch: 29/30, step: 150/364, loss: 0.18699, accuracy: 0.93031\n",
            "Epoch: 29/30, step: 151/364, loss: 0.18745, accuracy: 0.93005\n",
            "Epoch: 29/30, step: 152/364, loss: 0.18730, accuracy: 0.93010\n",
            "Epoch: 29/30, step: 153/364, loss: 0.18657, accuracy: 0.93045\n",
            "Epoch: 29/30, step: 154/364, loss: 0.18646, accuracy: 0.93070\n",
            "Epoch: 29/30, step: 155/364, loss: 0.18607, accuracy: 0.93095\n",
            "Epoch: 29/30, step: 156/364, loss: 0.18580, accuracy: 0.93099\n",
            "Epoch: 29/30, step: 157/364, loss: 0.18580, accuracy: 0.93093\n",
            "Epoch: 29/30, step: 158/364, loss: 0.18561, accuracy: 0.93097\n",
            "Epoch: 29/30, step: 159/364, loss: 0.18509, accuracy: 0.93121\n",
            "Epoch: 29/30, step: 160/364, loss: 0.18537, accuracy: 0.93115\n",
            "Epoch: 29/30, step: 161/364, loss: 0.18502, accuracy: 0.93129\n",
            "Epoch: 29/30, step: 162/364, loss: 0.18487, accuracy: 0.93142\n",
            "Epoch: 29/30, step: 163/364, loss: 0.18569, accuracy: 0.93089\n",
            "Epoch: 29/30, step: 164/364, loss: 0.18604, accuracy: 0.93074\n",
            "Epoch: 29/30, step: 165/364, loss: 0.18561, accuracy: 0.93106\n",
            "Epoch: 29/30, step: 166/364, loss: 0.18592, accuracy: 0.93091\n",
            "Epoch: 29/30, step: 167/364, loss: 0.18629, accuracy: 0.93067\n",
            "Epoch: 29/30, step: 168/364, loss: 0.18652, accuracy: 0.93062\n",
            "Epoch: 29/30, step: 169/364, loss: 0.18650, accuracy: 0.93084\n",
            "Epoch: 29/30, step: 170/364, loss: 0.18646, accuracy: 0.93088\n",
            "Epoch: 29/30, step: 171/364, loss: 0.18653, accuracy: 0.93092\n",
            "Epoch: 29/30, step: 172/364, loss: 0.18651, accuracy: 0.93096\n",
            "Epoch: 29/30, step: 173/364, loss: 0.18677, accuracy: 0.93073\n",
            "Epoch: 29/30, step: 174/364, loss: 0.18656, accuracy: 0.93094\n",
            "Epoch: 29/30, step: 175/364, loss: 0.18619, accuracy: 0.93098\n",
            "Epoch: 29/30, step: 176/364, loss: 0.18574, accuracy: 0.93129\n",
            "Epoch: 29/30, step: 177/364, loss: 0.18587, accuracy: 0.93114\n",
            "Epoch: 29/30, step: 178/364, loss: 0.18598, accuracy: 0.93109\n",
            "Epoch: 29/30, step: 179/364, loss: 0.18551, accuracy: 0.93139\n",
            "Epoch: 29/30, step: 180/364, loss: 0.18505, accuracy: 0.93168\n",
            "Epoch: 29/30, step: 181/364, loss: 0.18471, accuracy: 0.93180\n",
            "Epoch: 29/30, step: 182/364, loss: 0.18464, accuracy: 0.93166\n",
            "Epoch: 29/30, step: 183/364, loss: 0.18466, accuracy: 0.93178\n",
            "Epoch: 29/30, step: 184/364, loss: 0.18423, accuracy: 0.93198\n",
            "Epoch: 29/30, step: 185/364, loss: 0.18376, accuracy: 0.93226\n",
            "Epoch: 29/30, step: 186/364, loss: 0.18343, accuracy: 0.93263\n",
            "Epoch: 29/30, step: 187/364, loss: 0.18368, accuracy: 0.93249\n",
            "Epoch: 29/30, step: 188/364, loss: 0.18379, accuracy: 0.93235\n",
            "Epoch: 29/30, step: 189/364, loss: 0.18353, accuracy: 0.93254\n",
            "Epoch: 29/30, step: 190/364, loss: 0.18386, accuracy: 0.93232\n",
            "Epoch: 29/30, step: 191/364, loss: 0.18451, accuracy: 0.93194\n",
            "Epoch: 29/30, step: 192/364, loss: 0.18452, accuracy: 0.93180\n",
            "Epoch: 29/30, step: 193/364, loss: 0.18478, accuracy: 0.93167\n",
            "Epoch: 29/30, step: 194/364, loss: 0.18457, accuracy: 0.93178\n",
            "Epoch: 29/30, step: 195/364, loss: 0.18417, accuracy: 0.93205\n",
            "Epoch: 29/30, step: 196/364, loss: 0.18404, accuracy: 0.93208\n",
            "Epoch: 29/30, step: 197/364, loss: 0.18409, accuracy: 0.93179\n",
            "Epoch: 29/30, step: 198/364, loss: 0.18415, accuracy: 0.93166\n",
            "Epoch: 29/30, step: 199/364, loss: 0.18395, accuracy: 0.93193\n",
            "Epoch: 29/30, step: 200/364, loss: 0.18386, accuracy: 0.93187\n",
            "Epoch: 29/30, step: 201/364, loss: 0.18358, accuracy: 0.93214\n",
            "Epoch: 29/30, step: 202/364, loss: 0.18417, accuracy: 0.93178\n",
            "Epoch: 29/30, step: 203/364, loss: 0.18425, accuracy: 0.93165\n",
            "Epoch: 29/30, step: 204/364, loss: 0.18433, accuracy: 0.93160\n",
            "Epoch: 29/30, step: 205/364, loss: 0.18431, accuracy: 0.93163\n",
            "Epoch: 29/30, step: 206/364, loss: 0.18427, accuracy: 0.93181\n",
            "Epoch: 29/30, step: 207/364, loss: 0.18397, accuracy: 0.93207\n",
            "Epoch: 29/30, step: 208/364, loss: 0.18381, accuracy: 0.93209\n",
            "Epoch: 29/30, step: 209/364, loss: 0.18419, accuracy: 0.93167\n",
            "Epoch: 29/30, step: 210/364, loss: 0.18421, accuracy: 0.93155\n",
            "Epoch: 29/30, step: 211/364, loss: 0.18424, accuracy: 0.93143\n",
            "Epoch: 29/30, step: 212/364, loss: 0.18386, accuracy: 0.93168\n",
            "Epoch: 29/30, step: 213/364, loss: 0.18375, accuracy: 0.93178\n",
            "Epoch: 29/30, step: 214/364, loss: 0.18349, accuracy: 0.93195\n",
            "Epoch: 29/30, step: 215/364, loss: 0.18327, accuracy: 0.93205\n",
            "Epoch: 29/30, step: 216/364, loss: 0.18333, accuracy: 0.93215\n",
            "Epoch: 29/30, step: 217/364, loss: 0.18329, accuracy: 0.93217\n",
            "Epoch: 29/30, step: 218/364, loss: 0.18304, accuracy: 0.93227\n",
            "Epoch: 29/30, step: 219/364, loss: 0.18273, accuracy: 0.93251\n",
            "Epoch: 29/30, step: 220/364, loss: 0.18249, accuracy: 0.93267\n",
            "Epoch: 29/30, step: 221/364, loss: 0.18214, accuracy: 0.93290\n",
            "Epoch: 29/30, step: 222/364, loss: 0.18202, accuracy: 0.93300\n",
            "Epoch: 29/30, step: 223/364, loss: 0.18176, accuracy: 0.93316\n",
            "Epoch: 29/30, step: 224/364, loss: 0.18145, accuracy: 0.93325\n",
            "Epoch: 29/30, step: 225/364, loss: 0.18104, accuracy: 0.93347\n",
            "Epoch: 29/30, step: 226/364, loss: 0.18101, accuracy: 0.93349\n",
            "Epoch: 29/30, step: 227/364, loss: 0.18089, accuracy: 0.93344\n",
            "Epoch: 29/30, step: 228/364, loss: 0.18106, accuracy: 0.93339\n",
            "Epoch: 29/30, step: 229/364, loss: 0.18083, accuracy: 0.93347\n",
            "Epoch: 29/30, step: 230/364, loss: 0.18073, accuracy: 0.93349\n",
            "Epoch: 29/30, step: 231/364, loss: 0.18064, accuracy: 0.93364\n",
            "Epoch: 29/30, step: 232/364, loss: 0.18086, accuracy: 0.93359\n",
            "Epoch: 29/30, step: 233/364, loss: 0.18058, accuracy: 0.93374\n",
            "Epoch: 29/30, step: 234/364, loss: 0.18040, accuracy: 0.93396\n",
            "Epoch: 29/30, step: 235/364, loss: 0.18038, accuracy: 0.93404\n",
            "Epoch: 29/30, step: 236/364, loss: 0.18024, accuracy: 0.93412\n",
            "Epoch: 29/30, step: 237/364, loss: 0.18006, accuracy: 0.93427\n",
            "Epoch: 29/30, step: 238/364, loss: 0.18000, accuracy: 0.93441\n",
            "Epoch: 29/30, step: 239/364, loss: 0.17967, accuracy: 0.93462\n",
            "Epoch: 29/30, step: 240/364, loss: 0.17955, accuracy: 0.93464\n",
            "Epoch: 29/30, step: 241/364, loss: 0.17952, accuracy: 0.93471\n",
            "Epoch: 29/30, step: 242/364, loss: 0.18028, accuracy: 0.93434\n",
            "Epoch: 29/30, step: 243/364, loss: 0.18050, accuracy: 0.93416\n",
            "Epoch: 29/30, step: 244/364, loss: 0.18068, accuracy: 0.93411\n",
            "Epoch: 29/30, step: 245/364, loss: 0.18105, accuracy: 0.93393\n",
            "Epoch: 29/30, step: 246/364, loss: 0.18091, accuracy: 0.93394\n",
            "Epoch: 29/30, step: 247/364, loss: 0.18067, accuracy: 0.93402\n",
            "Epoch: 29/30, step: 248/364, loss: 0.18033, accuracy: 0.93410\n",
            "Epoch: 29/30, step: 249/364, loss: 0.18005, accuracy: 0.93424\n",
            "Epoch: 29/30, step: 250/364, loss: 0.17987, accuracy: 0.93444\n",
            "Epoch: 29/30, step: 251/364, loss: 0.17982, accuracy: 0.93445\n",
            "Epoch: 29/30, step: 252/364, loss: 0.17978, accuracy: 0.93434\n",
            "Epoch: 29/30, step: 253/364, loss: 0.18045, accuracy: 0.93379\n",
            "Epoch: 29/30, step: 254/364, loss: 0.18026, accuracy: 0.93387\n",
            "Epoch: 29/30, step: 255/364, loss: 0.18042, accuracy: 0.93382\n",
            "Epoch: 29/30, step: 256/364, loss: 0.18018, accuracy: 0.93396\n",
            "Epoch: 29/30, step: 257/364, loss: 0.18031, accuracy: 0.93397\n",
            "Epoch: 29/30, step: 258/364, loss: 0.18054, accuracy: 0.93387\n",
            "Epoch: 29/30, step: 259/364, loss: 0.18017, accuracy: 0.93412\n",
            "Epoch: 29/30, step: 260/364, loss: 0.17999, accuracy: 0.93425\n",
            "Epoch: 29/30, step: 261/364, loss: 0.17983, accuracy: 0.93427\n",
            "Epoch: 29/30, step: 262/364, loss: 0.17983, accuracy: 0.93428\n",
            "Epoch: 29/30, step: 263/364, loss: 0.17983, accuracy: 0.93435\n",
            "Epoch: 29/30, step: 264/364, loss: 0.18003, accuracy: 0.93424\n",
            "Epoch: 29/30, step: 265/364, loss: 0.18015, accuracy: 0.93414\n",
            "Epoch: 29/30, step: 266/364, loss: 0.17989, accuracy: 0.93427\n",
            "Epoch: 29/30, step: 267/364, loss: 0.17951, accuracy: 0.93452\n",
            "Epoch: 29/30, step: 268/364, loss: 0.17973, accuracy: 0.93441\n",
            "Epoch: 29/30, step: 269/364, loss: 0.17966, accuracy: 0.93454\n",
            "Epoch: 29/30, step: 270/364, loss: 0.18024, accuracy: 0.93426\n",
            "Epoch: 29/30, step: 271/364, loss: 0.18016, accuracy: 0.93427\n",
            "Epoch: 29/30, step: 272/364, loss: 0.17998, accuracy: 0.93446\n",
            "Epoch: 29/30, step: 273/364, loss: 0.18008, accuracy: 0.93441\n",
            "Epoch: 29/30, step: 274/364, loss: 0.17995, accuracy: 0.93442\n",
            "Epoch: 29/30, step: 275/364, loss: 0.17976, accuracy: 0.93455\n",
            "Epoch: 29/30, step: 276/364, loss: 0.17989, accuracy: 0.93444\n",
            "Epoch: 29/30, step: 277/364, loss: 0.17991, accuracy: 0.93445\n",
            "Epoch: 29/30, step: 278/364, loss: 0.17967, accuracy: 0.93452\n",
            "Epoch: 29/30, step: 279/364, loss: 0.17949, accuracy: 0.93459\n",
            "Epoch: 29/30, step: 280/364, loss: 0.17926, accuracy: 0.93471\n",
            "Epoch: 29/30, step: 281/364, loss: 0.17922, accuracy: 0.93483\n",
            "Epoch: 29/30, step: 282/364, loss: 0.17906, accuracy: 0.93495\n",
            "Epoch: 29/30, step: 283/364, loss: 0.17896, accuracy: 0.93496\n",
            "Epoch: 29/30, step: 284/364, loss: 0.17915, accuracy: 0.93480\n",
            "Epoch: 29/30, step: 285/364, loss: 0.17948, accuracy: 0.93459\n",
            "Epoch: 29/30, step: 286/364, loss: 0.17979, accuracy: 0.93439\n",
            "Epoch: 29/30, step: 287/364, loss: 0.17951, accuracy: 0.93456\n",
            "Epoch: 29/30, step: 288/364, loss: 0.17946, accuracy: 0.93457\n",
            "Epoch: 29/30, step: 289/364, loss: 0.17917, accuracy: 0.93463\n",
            "Epoch: 29/30, step: 290/364, loss: 0.17913, accuracy: 0.93454\n",
            "Epoch: 29/30, step: 291/364, loss: 0.17940, accuracy: 0.93444\n",
            "Epoch: 29/30, train loss: 0.17940, train accuracy: 0.93444, valid loss: 0.98150, valid accuracy: 0.64983\n",
            "Epoch: 30/30, step: 1/364, loss: 0.13732, accuracy: 0.92188\n",
            "Epoch: 30/30, step: 2/364, loss: 0.12025, accuracy: 0.94531\n",
            "Epoch: 30/30, step: 3/364, loss: 0.15056, accuracy: 0.93750\n",
            "Epoch: 30/30, step: 4/364, loss: 0.14647, accuracy: 0.94531\n",
            "Epoch: 30/30, step: 5/364, loss: 0.13694, accuracy: 0.95312\n",
            "Epoch: 30/30, step: 6/364, loss: 0.14602, accuracy: 0.95312\n",
            "Epoch: 30/30, step: 7/364, loss: 0.16770, accuracy: 0.93750\n",
            "Epoch: 30/30, step: 8/364, loss: 0.18480, accuracy: 0.92578\n",
            "Epoch: 30/30, step: 9/364, loss: 0.17693, accuracy: 0.93056\n",
            "Epoch: 30/30, step: 10/364, loss: 0.17557, accuracy: 0.93125\n",
            "Epoch: 30/30, step: 11/364, loss: 0.16873, accuracy: 0.93466\n",
            "Epoch: 30/30, step: 12/364, loss: 0.16865, accuracy: 0.93620\n",
            "Epoch: 30/30, step: 13/364, loss: 0.16247, accuracy: 0.93990\n",
            "Epoch: 30/30, step: 14/364, loss: 0.15676, accuracy: 0.94308\n",
            "Epoch: 30/30, step: 15/364, loss: 0.15683, accuracy: 0.94375\n",
            "Epoch: 30/30, step: 16/364, loss: 0.15236, accuracy: 0.94727\n",
            "Epoch: 30/30, step: 17/364, loss: 0.15219, accuracy: 0.94761\n",
            "Epoch: 30/30, step: 18/364, loss: 0.14988, accuracy: 0.94965\n",
            "Epoch: 30/30, step: 19/364, loss: 0.15700, accuracy: 0.94572\n",
            "Epoch: 30/30, step: 20/364, loss: 0.15900, accuracy: 0.94219\n",
            "Epoch: 30/30, step: 21/364, loss: 0.15699, accuracy: 0.94345\n",
            "Epoch: 30/30, step: 22/364, loss: 0.15807, accuracy: 0.94318\n",
            "Epoch: 30/30, step: 23/364, loss: 0.15465, accuracy: 0.94565\n",
            "Epoch: 30/30, step: 24/364, loss: 0.15445, accuracy: 0.94596\n",
            "Epoch: 30/30, step: 25/364, loss: 0.15388, accuracy: 0.94563\n",
            "Epoch: 30/30, step: 26/364, loss: 0.15269, accuracy: 0.94712\n",
            "Epoch: 30/30, step: 27/364, loss: 0.15321, accuracy: 0.94734\n",
            "Epoch: 30/30, step: 28/364, loss: 0.15515, accuracy: 0.94531\n",
            "Epoch: 30/30, step: 29/364, loss: 0.15654, accuracy: 0.94343\n",
            "Epoch: 30/30, step: 30/364, loss: 0.15702, accuracy: 0.94427\n",
            "Epoch: 30/30, step: 31/364, loss: 0.15866, accuracy: 0.94456\n",
            "Epoch: 30/30, step: 32/364, loss: 0.15941, accuracy: 0.94434\n",
            "Epoch: 30/30, step: 33/364, loss: 0.15847, accuracy: 0.94460\n",
            "Epoch: 30/30, step: 34/364, loss: 0.15849, accuracy: 0.94439\n",
            "Epoch: 30/30, step: 35/364, loss: 0.15715, accuracy: 0.94509\n",
            "Epoch: 30/30, step: 36/364, loss: 0.15680, accuracy: 0.94575\n",
            "Epoch: 30/30, step: 37/364, loss: 0.15503, accuracy: 0.94721\n",
            "Epoch: 30/30, step: 38/364, loss: 0.15394, accuracy: 0.94778\n",
            "Epoch: 30/30, step: 39/364, loss: 0.15182, accuracy: 0.94872\n",
            "Epoch: 30/30, step: 40/364, loss: 0.15048, accuracy: 0.94961\n",
            "Epoch: 30/30, step: 41/364, loss: 0.15266, accuracy: 0.94931\n",
            "Epoch: 30/30, step: 42/364, loss: 0.15349, accuracy: 0.94940\n",
            "Epoch: 30/30, step: 43/364, loss: 0.15339, accuracy: 0.94949\n",
            "Epoch: 30/30, step: 44/364, loss: 0.15246, accuracy: 0.94993\n",
            "Epoch: 30/30, step: 45/364, loss: 0.15322, accuracy: 0.94896\n",
            "Epoch: 30/30, step: 46/364, loss: 0.15268, accuracy: 0.94905\n",
            "Epoch: 30/30, step: 47/364, loss: 0.15237, accuracy: 0.94947\n",
            "Epoch: 30/30, step: 48/364, loss: 0.15260, accuracy: 0.94987\n",
            "Epoch: 30/30, step: 49/364, loss: 0.15483, accuracy: 0.94802\n",
            "Epoch: 30/30, step: 50/364, loss: 0.15528, accuracy: 0.94781\n",
            "Epoch: 30/30, step: 51/364, loss: 0.15503, accuracy: 0.94761\n",
            "Epoch: 30/30, step: 52/364, loss: 0.15396, accuracy: 0.94862\n",
            "Epoch: 30/30, step: 53/364, loss: 0.15366, accuracy: 0.94900\n",
            "Epoch: 30/30, step: 54/364, loss: 0.15316, accuracy: 0.94965\n",
            "Epoch: 30/30, step: 55/364, loss: 0.15199, accuracy: 0.95028\n",
            "Epoch: 30/30, step: 56/364, loss: 0.15227, accuracy: 0.95033\n",
            "Epoch: 30/30, step: 57/364, loss: 0.15368, accuracy: 0.94984\n",
            "Epoch: 30/30, step: 58/364, loss: 0.15289, accuracy: 0.95043\n",
            "Epoch: 30/30, step: 59/364, loss: 0.15411, accuracy: 0.94968\n",
            "Epoch: 30/30, step: 60/364, loss: 0.15454, accuracy: 0.94870\n",
            "Epoch: 30/30, step: 61/364, loss: 0.15608, accuracy: 0.94800\n",
            "Epoch: 30/30, step: 62/364, loss: 0.15506, accuracy: 0.94884\n",
            "Epoch: 30/30, step: 63/364, loss: 0.15549, accuracy: 0.94891\n",
            "Epoch: 30/30, step: 64/364, loss: 0.15584, accuracy: 0.94897\n",
            "Epoch: 30/30, step: 65/364, loss: 0.15523, accuracy: 0.94928\n",
            "Epoch: 30/30, step: 66/364, loss: 0.15489, accuracy: 0.94934\n",
            "Epoch: 30/30, step: 67/364, loss: 0.15531, accuracy: 0.94869\n",
            "Epoch: 30/30, step: 68/364, loss: 0.15740, accuracy: 0.94807\n",
            "Epoch: 30/30, step: 69/364, loss: 0.15703, accuracy: 0.94837\n",
            "Epoch: 30/30, step: 70/364, loss: 0.15759, accuracy: 0.94844\n",
            "Epoch: 30/30, step: 71/364, loss: 0.15759, accuracy: 0.94828\n",
            "Epoch: 30/30, step: 72/364, loss: 0.15675, accuracy: 0.94878\n",
            "Epoch: 30/30, step: 73/364, loss: 0.15598, accuracy: 0.94949\n",
            "Epoch: 30/30, step: 74/364, loss: 0.15628, accuracy: 0.94975\n",
            "Epoch: 30/30, step: 75/364, loss: 0.15802, accuracy: 0.94854\n",
            "Epoch: 30/30, step: 76/364, loss: 0.15786, accuracy: 0.94840\n",
            "Epoch: 30/30, step: 77/364, loss: 0.15724, accuracy: 0.94866\n",
            "Epoch: 30/30, step: 78/364, loss: 0.15681, accuracy: 0.94852\n",
            "Epoch: 30/30, step: 79/364, loss: 0.15566, accuracy: 0.94897\n",
            "Epoch: 30/30, step: 80/364, loss: 0.15770, accuracy: 0.94824\n",
            "Epoch: 30/30, step: 81/364, loss: 0.15774, accuracy: 0.94830\n",
            "Epoch: 30/30, step: 82/364, loss: 0.15801, accuracy: 0.94798\n",
            "Epoch: 30/30, step: 83/364, loss: 0.15743, accuracy: 0.94804\n",
            "Epoch: 30/30, step: 84/364, loss: 0.15862, accuracy: 0.94736\n",
            "Epoch: 30/30, step: 85/364, loss: 0.15821, accuracy: 0.94761\n",
            "Epoch: 30/30, step: 86/364, loss: 0.15770, accuracy: 0.94786\n",
            "Epoch: 30/30, step: 87/364, loss: 0.15836, accuracy: 0.94720\n",
            "Epoch: 30/30, step: 88/364, loss: 0.15777, accuracy: 0.94727\n",
            "Epoch: 30/30, step: 89/364, loss: 0.15761, accuracy: 0.94733\n",
            "Epoch: 30/30, step: 90/364, loss: 0.15680, accuracy: 0.94792\n",
            "Epoch: 30/30, step: 91/364, loss: 0.15651, accuracy: 0.94815\n",
            "Epoch: 30/30, step: 92/364, loss: 0.15763, accuracy: 0.94769\n",
            "Epoch: 30/30, step: 93/364, loss: 0.15746, accuracy: 0.94758\n",
            "Epoch: 30/30, step: 94/364, loss: 0.15829, accuracy: 0.94747\n",
            "Epoch: 30/30, step: 95/364, loss: 0.15906, accuracy: 0.94638\n",
            "Epoch: 30/30, step: 96/364, loss: 0.15861, accuracy: 0.94661\n",
            "Epoch: 30/30, step: 97/364, loss: 0.15839, accuracy: 0.94700\n",
            "Epoch: 30/30, step: 98/364, loss: 0.16062, accuracy: 0.94515\n",
            "Epoch: 30/30, step: 99/364, loss: 0.16070, accuracy: 0.94508\n",
            "Epoch: 30/30, step: 100/364, loss: 0.16154, accuracy: 0.94437\n",
            "Epoch: 30/30, step: 101/364, loss: 0.16094, accuracy: 0.94493\n",
            "Epoch: 30/30, step: 102/364, loss: 0.16190, accuracy: 0.94439\n",
            "Epoch: 30/30, step: 103/364, loss: 0.16177, accuracy: 0.94478\n",
            "Epoch: 30/30, step: 104/364, loss: 0.16172, accuracy: 0.94471\n",
            "Epoch: 30/30, step: 105/364, loss: 0.16140, accuracy: 0.94494\n",
            "Epoch: 30/30, step: 106/364, loss: 0.16293, accuracy: 0.94443\n",
            "Epoch: 30/30, step: 107/364, loss: 0.16232, accuracy: 0.94480\n",
            "Epoch: 30/30, step: 108/364, loss: 0.16338, accuracy: 0.94444\n",
            "Epoch: 30/30, step: 109/364, loss: 0.16441, accuracy: 0.94366\n",
            "Epoch: 30/30, step: 110/364, loss: 0.16414, accuracy: 0.94375\n",
            "Epoch: 30/30, step: 111/364, loss: 0.16411, accuracy: 0.94383\n",
            "Epoch: 30/30, step: 112/364, loss: 0.16405, accuracy: 0.94378\n",
            "Epoch: 30/30, step: 113/364, loss: 0.16435, accuracy: 0.94372\n",
            "Epoch: 30/30, step: 114/364, loss: 0.16374, accuracy: 0.94394\n",
            "Epoch: 30/30, step: 115/364, loss: 0.16412, accuracy: 0.94389\n",
            "Epoch: 30/30, step: 116/364, loss: 0.16389, accuracy: 0.94397\n",
            "Epoch: 30/30, step: 117/364, loss: 0.16463, accuracy: 0.94338\n",
            "Epoch: 30/30, step: 118/364, loss: 0.16408, accuracy: 0.94372\n",
            "Epoch: 30/30, step: 119/364, loss: 0.16366, accuracy: 0.94407\n",
            "Epoch: 30/30, step: 120/364, loss: 0.16405, accuracy: 0.94375\n",
            "Epoch: 30/30, step: 121/364, loss: 0.16437, accuracy: 0.94370\n",
            "Epoch: 30/30, step: 122/364, loss: 0.16391, accuracy: 0.94390\n",
            "Epoch: 30/30, step: 123/364, loss: 0.16440, accuracy: 0.94372\n",
            "Epoch: 30/30, step: 124/364, loss: 0.16462, accuracy: 0.94342\n",
            "Epoch: 30/30, step: 125/364, loss: 0.16533, accuracy: 0.94300\n",
            "Epoch: 30/30, step: 126/364, loss: 0.16567, accuracy: 0.94283\n",
            "Epoch: 30/30, step: 127/364, loss: 0.16523, accuracy: 0.94328\n",
            "Epoch: 30/30, step: 128/364, loss: 0.16561, accuracy: 0.94324\n",
            "Epoch: 30/30, step: 129/364, loss: 0.16599, accuracy: 0.94295\n",
            "Epoch: 30/30, step: 130/364, loss: 0.16648, accuracy: 0.94243\n",
            "Epoch: 30/30, step: 131/364, loss: 0.16748, accuracy: 0.94179\n",
            "Epoch: 30/30, step: 132/364, loss: 0.16698, accuracy: 0.94212\n",
            "Epoch: 30/30, step: 133/364, loss: 0.16644, accuracy: 0.94243\n",
            "Epoch: 30/30, step: 134/364, loss: 0.16600, accuracy: 0.94263\n",
            "Epoch: 30/30, step: 135/364, loss: 0.16619, accuracy: 0.94248\n",
            "Epoch: 30/30, step: 136/364, loss: 0.16584, accuracy: 0.94267\n",
            "Epoch: 30/30, step: 137/364, loss: 0.16584, accuracy: 0.94275\n",
            "Epoch: 30/30, step: 138/364, loss: 0.16577, accuracy: 0.94293\n",
            "Epoch: 30/30, step: 139/364, loss: 0.16562, accuracy: 0.94278\n",
            "Epoch: 30/30, step: 140/364, loss: 0.16560, accuracy: 0.94286\n",
            "Epoch: 30/30, step: 141/364, loss: 0.16570, accuracy: 0.94293\n",
            "Epoch: 30/30, step: 142/364, loss: 0.16513, accuracy: 0.94333\n",
            "Epoch: 30/30, step: 143/364, loss: 0.16518, accuracy: 0.94318\n",
            "Epoch: 30/30, step: 144/364, loss: 0.16448, accuracy: 0.94358\n",
            "Epoch: 30/30, step: 145/364, loss: 0.16532, accuracy: 0.94321\n",
            "Epoch: 30/30, step: 146/364, loss: 0.16539, accuracy: 0.94307\n",
            "Epoch: 30/30, step: 147/364, loss: 0.16563, accuracy: 0.94281\n",
            "Epoch: 30/30, step: 148/364, loss: 0.16519, accuracy: 0.94310\n",
            "Epoch: 30/30, step: 149/364, loss: 0.16522, accuracy: 0.94306\n",
            "Epoch: 30/30, step: 150/364, loss: 0.16508, accuracy: 0.94323\n",
            "Epoch: 30/30, step: 151/364, loss: 0.16508, accuracy: 0.94309\n",
            "Epoch: 30/30, step: 152/364, loss: 0.16542, accuracy: 0.94274\n",
            "Epoch: 30/30, step: 153/364, loss: 0.16529, accuracy: 0.94261\n",
            "Epoch: 30/30, step: 154/364, loss: 0.16589, accuracy: 0.94227\n",
            "Epoch: 30/30, step: 155/364, loss: 0.16603, accuracy: 0.94224\n",
            "Epoch: 30/30, step: 156/364, loss: 0.16575, accuracy: 0.94251\n",
            "Epoch: 30/30, step: 157/364, loss: 0.16616, accuracy: 0.94228\n",
            "Epoch: 30/30, step: 158/364, loss: 0.16600, accuracy: 0.94225\n",
            "Epoch: 30/30, step: 159/364, loss: 0.16630, accuracy: 0.94192\n",
            "Epoch: 30/30, step: 160/364, loss: 0.16585, accuracy: 0.94209\n",
            "Epoch: 30/30, step: 161/364, loss: 0.16541, accuracy: 0.94245\n",
            "Epoch: 30/30, step: 162/364, loss: 0.16567, accuracy: 0.94223\n",
            "Epoch: 30/30, step: 163/364, loss: 0.16539, accuracy: 0.94239\n",
            "Epoch: 30/30, step: 164/364, loss: 0.16538, accuracy: 0.94236\n",
            "Epoch: 30/30, step: 165/364, loss: 0.16483, accuracy: 0.94271\n",
            "Epoch: 30/30, step: 166/364, loss: 0.16452, accuracy: 0.94287\n",
            "Epoch: 30/30, step: 167/364, loss: 0.16440, accuracy: 0.94293\n",
            "Epoch: 30/30, step: 168/364, loss: 0.16388, accuracy: 0.94327\n",
            "Epoch: 30/30, step: 169/364, loss: 0.16384, accuracy: 0.94323\n",
            "Epoch: 30/30, step: 170/364, loss: 0.16375, accuracy: 0.94320\n",
            "Epoch: 30/30, step: 171/364, loss: 0.16393, accuracy: 0.94298\n",
            "Epoch: 30/30, step: 172/364, loss: 0.16427, accuracy: 0.94268\n",
            "Epoch: 30/30, step: 173/364, loss: 0.16398, accuracy: 0.94283\n",
            "Epoch: 30/30, step: 174/364, loss: 0.16357, accuracy: 0.94316\n",
            "Epoch: 30/30, step: 175/364, loss: 0.16371, accuracy: 0.94313\n",
            "Epoch: 30/30, step: 176/364, loss: 0.16363, accuracy: 0.94309\n",
            "Epoch: 30/30, step: 177/364, loss: 0.16316, accuracy: 0.94333\n",
            "Epoch: 30/30, step: 178/364, loss: 0.16316, accuracy: 0.94329\n",
            "Epoch: 30/30, step: 179/364, loss: 0.16316, accuracy: 0.94335\n",
            "Epoch: 30/30, step: 180/364, loss: 0.16316, accuracy: 0.94332\n",
            "Epoch: 30/30, step: 181/364, loss: 0.16363, accuracy: 0.94320\n",
            "Epoch: 30/30, step: 182/364, loss: 0.16326, accuracy: 0.94351\n",
            "Epoch: 30/30, step: 183/364, loss: 0.16334, accuracy: 0.94348\n",
            "Epoch: 30/30, step: 184/364, loss: 0.16289, accuracy: 0.94370\n",
            "Epoch: 30/30, step: 185/364, loss: 0.16263, accuracy: 0.94392\n",
            "Epoch: 30/30, step: 186/364, loss: 0.16246, accuracy: 0.94405\n",
            "Epoch: 30/30, step: 187/364, loss: 0.16284, accuracy: 0.94393\n",
            "Epoch: 30/30, step: 188/364, loss: 0.16255, accuracy: 0.94415\n",
            "Epoch: 30/30, step: 189/364, loss: 0.16255, accuracy: 0.94403\n",
            "Epoch: 30/30, step: 190/364, loss: 0.16219, accuracy: 0.94416\n",
            "Epoch: 30/30, step: 191/364, loss: 0.16201, accuracy: 0.94429\n",
            "Epoch: 30/30, step: 192/364, loss: 0.16185, accuracy: 0.94434\n",
            "Epoch: 30/30, step: 193/364, loss: 0.16146, accuracy: 0.94462\n",
            "Epoch: 30/30, step: 194/364, loss: 0.16159, accuracy: 0.94435\n",
            "Epoch: 30/30, step: 195/364, loss: 0.16168, accuracy: 0.94431\n",
            "Epoch: 30/30, step: 196/364, loss: 0.16155, accuracy: 0.94428\n",
            "Epoch: 30/30, step: 197/364, loss: 0.16118, accuracy: 0.94448\n",
            "Epoch: 30/30, step: 198/364, loss: 0.16089, accuracy: 0.94460\n",
            "Epoch: 30/30, step: 199/364, loss: 0.16051, accuracy: 0.94488\n",
            "Epoch: 30/30, step: 200/364, loss: 0.16025, accuracy: 0.94508\n",
            "Epoch: 30/30, step: 201/364, loss: 0.16019, accuracy: 0.94520\n",
            "Epoch: 30/30, step: 202/364, loss: 0.15991, accuracy: 0.94531\n",
            "Epoch: 30/30, step: 203/364, loss: 0.15985, accuracy: 0.94543\n",
            "Epoch: 30/30, step: 204/364, loss: 0.15975, accuracy: 0.94547\n",
            "Epoch: 30/30, step: 205/364, loss: 0.15949, accuracy: 0.94558\n",
            "Epoch: 30/30, step: 206/364, loss: 0.15966, accuracy: 0.94546\n",
            "Epoch: 30/30, step: 207/364, loss: 0.15947, accuracy: 0.94550\n",
            "Epoch: 30/30, step: 208/364, loss: 0.15906, accuracy: 0.94576\n",
            "Epoch: 30/30, step: 209/364, loss: 0.15881, accuracy: 0.94595\n",
            "Epoch: 30/30, step: 210/364, loss: 0.15882, accuracy: 0.94598\n",
            "Epoch: 30/30, step: 211/364, loss: 0.15837, accuracy: 0.94624\n",
            "Epoch: 30/30, step: 212/364, loss: 0.15875, accuracy: 0.94598\n",
            "Epoch: 30/30, step: 213/364, loss: 0.15870, accuracy: 0.94601\n",
            "Epoch: 30/30, step: 214/364, loss: 0.15861, accuracy: 0.94604\n",
            "Epoch: 30/30, step: 215/364, loss: 0.15831, accuracy: 0.94615\n",
            "Epoch: 30/30, step: 216/364, loss: 0.15810, accuracy: 0.94625\n",
            "Epoch: 30/30, step: 217/364, loss: 0.15861, accuracy: 0.94607\n",
            "Epoch: 30/30, step: 218/364, loss: 0.15840, accuracy: 0.94610\n",
            "Epoch: 30/30, step: 219/364, loss: 0.15854, accuracy: 0.94599\n",
            "Epoch: 30/30, step: 220/364, loss: 0.15883, accuracy: 0.94595\n",
            "Epoch: 30/30, step: 221/364, loss: 0.15901, accuracy: 0.94570\n",
            "Epoch: 30/30, step: 222/364, loss: 0.15906, accuracy: 0.94566\n",
            "Epoch: 30/30, step: 223/364, loss: 0.15973, accuracy: 0.94514\n",
            "Epoch: 30/30, step: 224/364, loss: 0.16047, accuracy: 0.94455\n",
            "Epoch: 30/30, step: 225/364, loss: 0.16151, accuracy: 0.94389\n",
            "Epoch: 30/30, step: 226/364, loss: 0.16173, accuracy: 0.94386\n",
            "Epoch: 30/30, step: 227/364, loss: 0.16134, accuracy: 0.94411\n",
            "Epoch: 30/30, step: 228/364, loss: 0.16140, accuracy: 0.94408\n",
            "Epoch: 30/30, step: 229/364, loss: 0.16151, accuracy: 0.94398\n",
            "Epoch: 30/30, step: 230/364, loss: 0.16159, accuracy: 0.94382\n",
            "Epoch: 30/30, step: 231/364, loss: 0.16166, accuracy: 0.94372\n",
            "Epoch: 30/30, step: 232/364, loss: 0.16134, accuracy: 0.94383\n",
            "Epoch: 30/30, step: 233/364, loss: 0.16097, accuracy: 0.94407\n",
            "Epoch: 30/30, step: 234/364, loss: 0.16152, accuracy: 0.94371\n",
            "Epoch: 30/30, step: 235/364, loss: 0.16139, accuracy: 0.94382\n",
            "Epoch: 30/30, step: 236/364, loss: 0.16154, accuracy: 0.94372\n",
            "Epoch: 30/30, step: 237/364, loss: 0.16135, accuracy: 0.94370\n",
            "Epoch: 30/30, step: 238/364, loss: 0.16182, accuracy: 0.94341\n",
            "Epoch: 30/30, step: 239/364, loss: 0.16154, accuracy: 0.94358\n",
            "Epoch: 30/30, step: 240/364, loss: 0.16163, accuracy: 0.94355\n",
            "Epoch: 30/30, step: 241/364, loss: 0.16139, accuracy: 0.94372\n",
            "Epoch: 30/30, step: 242/364, loss: 0.16132, accuracy: 0.94389\n",
            "Epoch: 30/30, step: 243/364, loss: 0.16181, accuracy: 0.94374\n",
            "Epoch: 30/30, step: 244/364, loss: 0.16148, accuracy: 0.94397\n",
            "Epoch: 30/30, step: 245/364, loss: 0.16148, accuracy: 0.94401\n",
            "Epoch: 30/30, step: 246/364, loss: 0.16207, accuracy: 0.94379\n",
            "Epoch: 30/30, step: 247/364, loss: 0.16188, accuracy: 0.94395\n",
            "Epoch: 30/30, step: 248/364, loss: 0.16173, accuracy: 0.94405\n",
            "Epoch: 30/30, step: 249/364, loss: 0.16194, accuracy: 0.94396\n",
            "Epoch: 30/30, step: 250/364, loss: 0.16197, accuracy: 0.94388\n",
            "Epoch: 30/30, step: 251/364, loss: 0.16267, accuracy: 0.94341\n",
            "Epoch: 30/30, step: 252/364, loss: 0.16329, accuracy: 0.94296\n",
            "Epoch: 30/30, step: 253/364, loss: 0.16324, accuracy: 0.94300\n",
            "Epoch: 30/30, step: 254/364, loss: 0.16358, accuracy: 0.94285\n",
            "Epoch: 30/30, step: 255/364, loss: 0.16337, accuracy: 0.94301\n",
            "Epoch: 30/30, step: 256/364, loss: 0.16308, accuracy: 0.94318\n",
            "Epoch: 30/30, step: 257/364, loss: 0.16298, accuracy: 0.94321\n",
            "Epoch: 30/30, step: 258/364, loss: 0.16320, accuracy: 0.94307\n",
            "Epoch: 30/30, step: 259/364, loss: 0.16320, accuracy: 0.94305\n",
            "Epoch: 30/30, step: 260/364, loss: 0.16305, accuracy: 0.94303\n",
            "Epoch: 30/30, step: 261/364, loss: 0.16274, accuracy: 0.94319\n",
            "Epoch: 30/30, step: 262/364, loss: 0.16272, accuracy: 0.94317\n",
            "Epoch: 30/30, step: 263/364, loss: 0.16278, accuracy: 0.94320\n",
            "Epoch: 30/30, step: 264/364, loss: 0.16300, accuracy: 0.94312\n",
            "Epoch: 30/30, step: 265/364, loss: 0.16288, accuracy: 0.94310\n",
            "Epoch: 30/30, step: 266/364, loss: 0.16305, accuracy: 0.94308\n",
            "Epoch: 30/30, step: 267/364, loss: 0.16317, accuracy: 0.94288\n",
            "Epoch: 30/30, step: 268/364, loss: 0.16290, accuracy: 0.94304\n",
            "Epoch: 30/30, step: 269/364, loss: 0.16334, accuracy: 0.94296\n",
            "Epoch: 30/30, step: 270/364, loss: 0.16324, accuracy: 0.94306\n",
            "Epoch: 30/30, step: 271/364, loss: 0.16302, accuracy: 0.94309\n",
            "Epoch: 30/30, step: 272/364, loss: 0.16315, accuracy: 0.94290\n",
            "Epoch: 30/30, step: 273/364, loss: 0.16295, accuracy: 0.94299\n",
            "Epoch: 30/30, step: 274/364, loss: 0.16285, accuracy: 0.94303\n",
            "Epoch: 30/30, step: 275/364, loss: 0.16331, accuracy: 0.94284\n",
            "Epoch: 30/30, step: 276/364, loss: 0.16394, accuracy: 0.94243\n",
            "Epoch: 30/30, step: 277/364, loss: 0.16385, accuracy: 0.94241\n",
            "Epoch: 30/30, step: 278/364, loss: 0.16405, accuracy: 0.94233\n",
            "Epoch: 30/30, step: 279/364, loss: 0.16435, accuracy: 0.94209\n",
            "Epoch: 30/30, step: 280/364, loss: 0.16405, accuracy: 0.94230\n",
            "Epoch: 30/30, step: 281/364, loss: 0.16376, accuracy: 0.94250\n",
            "Epoch: 30/30, step: 282/364, loss: 0.16396, accuracy: 0.94232\n",
            "Epoch: 30/30, step: 283/364, loss: 0.16378, accuracy: 0.94236\n",
            "Epoch: 30/30, step: 284/364, loss: 0.16360, accuracy: 0.94245\n",
            "Epoch: 30/30, step: 285/364, loss: 0.16358, accuracy: 0.94249\n",
            "Epoch: 30/30, step: 286/364, loss: 0.16326, accuracy: 0.94269\n",
            "Epoch: 30/30, step: 287/364, loss: 0.16315, accuracy: 0.94267\n",
            "Epoch: 30/30, step: 288/364, loss: 0.16304, accuracy: 0.94271\n",
            "Epoch: 30/30, step: 289/364, loss: 0.16330, accuracy: 0.94258\n",
            "Epoch: 30/30, step: 290/364, loss: 0.16319, accuracy: 0.94262\n",
            "Epoch: 30/30, step: 291/364, loss: 0.16331, accuracy: 0.94256\n",
            "Epoch: 30/30, train loss: 0.16331, train accuracy: 0.94256, valid loss: 0.82241, valid accuracy: 0.68637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(type='resnet34_2'):\n",
        "    if type == 'resenet50_2':\n",
        "        model = resnet_50_2()\n",
        "    else:\n",
        "        model = resnet_34()\n",
        "    model.build(input_shape=(None, 224, 224, 3))\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "model = get_model('resnet_50_2')\n",
        "\n",
        "import math\n",
        "\n",
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(images, training=True)\n",
        "        loss = loss_object(y_true=labels, y_pred=predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(grads_and_vars=zip(gradients, model.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(labels, predictions)\n",
        "\n",
        "@tf.function\n",
        "def valid_step(images, labels):\n",
        "    predictions = model(images, training=False)\n",
        "    v_loss = loss_object(labels, predictions)\n",
        "\n",
        "    valid_loss(v_loss)\n",
        "    valid_accuracy(labels, predictions)\n",
        "\n",
        "# start training\n",
        "for epoch in range(30):\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    valid_loss.reset_states()\n",
        "    valid_accuracy.reset_states()\n",
        "    step = 0\n",
        "    for images, labels in train_batches:\n",
        "        step += 1\n",
        "        train_step(images, labels)\n",
        "        print(\"Epoch: {}/{}, step: {}/{}, loss: {:.5f}, accuracy: {:.5f}\".format(epoch + 1,\n",
        "                                                                                    30,\n",
        "                                                                                    step,\n",
        "                                                                                    math.ceil(num_examples / 64),\n",
        "                                                                                    train_loss.result(),\n",
        "                                                                                    train_accuracy.result()))\n",
        "\n",
        "    for valid_images, valid_labels in validation_batches:\n",
        "        valid_step(valid_images, valid_labels)\n",
        "\n",
        "    print(\"Epoch: {}/{}, train loss: {:.5f}, train accuracy: {:.5f}, \"\n",
        "            \"valid loss: {:.5f}, valid accuracy: {:.5f}\".format(epoch + 1,\n",
        "                                                                30,\n",
        "                                                                train_loss.result(),\n",
        "                                                                train_accuracy.result(),\n",
        "                                                                valid_loss.result(),\n",
        "                                                                valid_accuracy.result()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4kUXkdV-TQU",
        "outputId": "bbc16f3b-258f-4f48-cce3-d7cf46e24280"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch: 13/30, step: 257/364, loss: 0.46919, accuracy: 0.78247\n",
            "Epoch: 13/30, step: 258/364, loss: 0.46898, accuracy: 0.78270\n",
            "Epoch: 13/30, step: 259/364, loss: 0.46880, accuracy: 0.78282\n",
            "Epoch: 13/30, step: 260/364, loss: 0.46897, accuracy: 0.78263\n",
            "Epoch: 13/30, step: 261/364, loss: 0.46885, accuracy: 0.78281\n",
            "Epoch: 13/30, step: 262/364, loss: 0.46856, accuracy: 0.78298\n",
            "Epoch: 13/30, step: 263/364, loss: 0.46873, accuracy: 0.78291\n",
            "Epoch: 13/30, step: 264/364, loss: 0.46874, accuracy: 0.78297\n",
            "Epoch: 13/30, step: 265/364, loss: 0.46862, accuracy: 0.78314\n",
            "Epoch: 13/30, step: 266/364, loss: 0.46841, accuracy: 0.78336\n",
            "Epoch: 13/30, step: 267/364, loss: 0.46831, accuracy: 0.78365\n",
            "Epoch: 13/30, step: 268/364, loss: 0.46821, accuracy: 0.78382\n",
            "Epoch: 13/30, step: 269/364, loss: 0.46818, accuracy: 0.78375\n",
            "Epoch: 13/30, step: 270/364, loss: 0.46801, accuracy: 0.78403\n",
            "Epoch: 13/30, step: 271/364, loss: 0.46801, accuracy: 0.78408\n",
            "Epoch: 13/30, step: 272/364, loss: 0.46801, accuracy: 0.78418\n",
            "Epoch: 13/30, step: 273/364, loss: 0.46818, accuracy: 0.78405\n",
            "Epoch: 13/30, step: 274/364, loss: 0.46839, accuracy: 0.78399\n",
            "Epoch: 13/30, step: 275/364, loss: 0.46835, accuracy: 0.78398\n",
            "Epoch: 13/30, step: 276/364, loss: 0.46818, accuracy: 0.78436\n",
            "Epoch: 13/30, step: 277/364, loss: 0.46791, accuracy: 0.78458\n",
            "Epoch: 13/30, step: 278/364, loss: 0.46846, accuracy: 0.78406\n",
            "Epoch: 13/30, step: 279/364, loss: 0.46847, accuracy: 0.78416\n",
            "Epoch: 13/30, step: 280/364, loss: 0.46853, accuracy: 0.78415\n",
            "Epoch: 13/30, step: 281/364, loss: 0.46842, accuracy: 0.78425\n",
            "Epoch: 13/30, step: 282/364, loss: 0.46836, accuracy: 0.78419\n",
            "Epoch: 13/30, step: 283/364, loss: 0.46871, accuracy: 0.78379\n",
            "Epoch: 13/30, step: 284/364, loss: 0.46907, accuracy: 0.78356\n",
            "Epoch: 13/30, step: 285/364, loss: 0.46916, accuracy: 0.78344\n",
            "Epoch: 13/30, step: 286/364, loss: 0.46892, accuracy: 0.78371\n",
            "Epoch: 13/30, step: 287/364, loss: 0.46897, accuracy: 0.78359\n",
            "Epoch: 13/30, step: 288/364, loss: 0.46892, accuracy: 0.78353\n",
            "Epoch: 13/30, step: 289/364, loss: 0.46886, accuracy: 0.78363\n",
            "Epoch: 13/30, step: 290/364, loss: 0.46864, accuracy: 0.78378\n",
            "Epoch: 13/30, step: 291/364, loss: 0.46848, accuracy: 0.78383\n",
            "Epoch: 13/30, train loss: 0.46848, train accuracy: 0.78383, valid loss: 0.65123, valid accuracy: 0.65434\n",
            "Epoch: 14/30, step: 1/364, loss: 0.48330, accuracy: 0.73438\n",
            "Epoch: 14/30, step: 2/364, loss: 0.43899, accuracy: 0.78125\n",
            "Epoch: 14/30, step: 3/364, loss: 0.44569, accuracy: 0.78646\n",
            "Epoch: 14/30, step: 4/364, loss: 0.45701, accuracy: 0.79297\n",
            "Epoch: 14/30, step: 5/364, loss: 0.45818, accuracy: 0.78750\n",
            "Epoch: 14/30, step: 6/364, loss: 0.46107, accuracy: 0.78385\n",
            "Epoch: 14/30, step: 7/364, loss: 0.44897, accuracy: 0.79241\n",
            "Epoch: 14/30, step: 8/364, loss: 0.44434, accuracy: 0.78906\n",
            "Epoch: 14/30, step: 9/364, loss: 0.43578, accuracy: 0.80208\n",
            "Epoch: 14/30, step: 10/364, loss: 0.43641, accuracy: 0.80156\n",
            "Epoch: 14/30, step: 11/364, loss: 0.44396, accuracy: 0.79403\n",
            "Epoch: 14/30, step: 12/364, loss: 0.45610, accuracy: 0.78776\n",
            "Epoch: 14/30, step: 13/364, loss: 0.45424, accuracy: 0.78966\n",
            "Epoch: 14/30, step: 14/364, loss: 0.44658, accuracy: 0.79688\n",
            "Epoch: 14/30, step: 15/364, loss: 0.44548, accuracy: 0.79792\n",
            "Epoch: 14/30, step: 16/364, loss: 0.44869, accuracy: 0.79492\n",
            "Epoch: 14/30, step: 17/364, loss: 0.44874, accuracy: 0.79779\n",
            "Epoch: 14/30, step: 18/364, loss: 0.44973, accuracy: 0.79688\n",
            "Epoch: 14/30, step: 19/364, loss: 0.45504, accuracy: 0.79359\n",
            "Epoch: 14/30, step: 20/364, loss: 0.45357, accuracy: 0.79453\n",
            "Epoch: 14/30, step: 21/364, loss: 0.45065, accuracy: 0.79539\n",
            "Epoch: 14/30, step: 22/364, loss: 0.44726, accuracy: 0.79688\n",
            "Epoch: 14/30, step: 23/364, loss: 0.44873, accuracy: 0.79484\n",
            "Epoch: 14/30, step: 24/364, loss: 0.44875, accuracy: 0.79492\n",
            "Epoch: 14/30, step: 25/364, loss: 0.44634, accuracy: 0.79625\n",
            "Epoch: 14/30, step: 26/364, loss: 0.44745, accuracy: 0.79627\n",
            "Epoch: 14/30, step: 27/364, loss: 0.44781, accuracy: 0.79572\n",
            "Epoch: 14/30, step: 28/364, loss: 0.44753, accuracy: 0.79743\n",
            "Epoch: 14/30, step: 29/364, loss: 0.44524, accuracy: 0.79903\n",
            "Epoch: 14/30, step: 30/364, loss: 0.44387, accuracy: 0.80156\n",
            "Epoch: 14/30, step: 31/364, loss: 0.44255, accuracy: 0.80192\n",
            "Epoch: 14/30, step: 32/364, loss: 0.44067, accuracy: 0.80371\n",
            "Epoch: 14/30, step: 33/364, loss: 0.44065, accuracy: 0.80350\n",
            "Epoch: 14/30, step: 34/364, loss: 0.44217, accuracy: 0.80147\n",
            "Epoch: 14/30, step: 35/364, loss: 0.44231, accuracy: 0.80179\n",
            "Epoch: 14/30, step: 36/364, loss: 0.44007, accuracy: 0.80425\n",
            "Epoch: 14/30, step: 37/364, loss: 0.44139, accuracy: 0.80194\n",
            "Epoch: 14/30, step: 38/364, loss: 0.44206, accuracy: 0.80181\n",
            "Epoch: 14/30, step: 39/364, loss: 0.44103, accuracy: 0.80128\n",
            "Epoch: 14/30, step: 40/364, loss: 0.44169, accuracy: 0.80156\n",
            "Epoch: 14/30, step: 41/364, loss: 0.44233, accuracy: 0.80069\n",
            "Epoch: 14/30, step: 42/364, loss: 0.44370, accuracy: 0.79948\n",
            "Epoch: 14/30, step: 43/364, loss: 0.44505, accuracy: 0.79797\n",
            "Epoch: 14/30, step: 44/364, loss: 0.44535, accuracy: 0.79901\n",
            "Epoch: 14/30, step: 45/364, loss: 0.44620, accuracy: 0.79653\n",
            "Epoch: 14/30, step: 46/364, loss: 0.44627, accuracy: 0.79721\n",
            "Epoch: 14/30, step: 47/364, loss: 0.44601, accuracy: 0.79688\n",
            "Epoch: 14/30, step: 48/364, loss: 0.44526, accuracy: 0.79850\n",
            "Epoch: 14/30, step: 49/364, loss: 0.44695, accuracy: 0.79815\n",
            "Epoch: 14/30, step: 50/364, loss: 0.44742, accuracy: 0.79781\n",
            "Epoch: 14/30, step: 51/364, loss: 0.44713, accuracy: 0.79841\n",
            "Epoch: 14/30, step: 52/364, loss: 0.44584, accuracy: 0.79988\n",
            "Epoch: 14/30, step: 53/364, loss: 0.44413, accuracy: 0.80189\n",
            "Epoch: 14/30, step: 54/364, loss: 0.44324, accuracy: 0.80295\n",
            "Epoch: 14/30, step: 55/364, loss: 0.44295, accuracy: 0.80369\n",
            "Epoch: 14/30, step: 56/364, loss: 0.44206, accuracy: 0.80441\n",
            "Epoch: 14/30, step: 57/364, loss: 0.44407, accuracy: 0.80208\n",
            "Epoch: 14/30, step: 58/364, loss: 0.44400, accuracy: 0.80253\n",
            "Epoch: 14/30, step: 59/364, loss: 0.44394, accuracy: 0.80164\n",
            "Epoch: 14/30, step: 60/364, loss: 0.44342, accuracy: 0.80182\n",
            "Epoch: 14/30, step: 61/364, loss: 0.44402, accuracy: 0.80097\n",
            "Epoch: 14/30, step: 62/364, loss: 0.44453, accuracy: 0.80015\n",
            "Epoch: 14/30, step: 63/364, loss: 0.44449, accuracy: 0.80010\n",
            "Epoch: 14/30, step: 64/364, loss: 0.44434, accuracy: 0.80005\n",
            "Epoch: 14/30, step: 65/364, loss: 0.44290, accuracy: 0.80192\n",
            "Epoch: 14/30, step: 66/364, loss: 0.44280, accuracy: 0.80161\n",
            "Epoch: 14/30, step: 67/364, loss: 0.44354, accuracy: 0.80154\n",
            "Epoch: 14/30, step: 68/364, loss: 0.44314, accuracy: 0.80170\n",
            "Epoch: 14/30, step: 69/364, loss: 0.44390, accuracy: 0.80095\n",
            "Epoch: 14/30, step: 70/364, loss: 0.44353, accuracy: 0.80112\n",
            "Epoch: 14/30, step: 71/364, loss: 0.44364, accuracy: 0.80150\n",
            "Epoch: 14/30, step: 72/364, loss: 0.44408, accuracy: 0.80013\n",
            "Epoch: 14/30, step: 73/364, loss: 0.44341, accuracy: 0.80051\n",
            "Epoch: 14/30, step: 74/364, loss: 0.44444, accuracy: 0.79899\n",
            "Epoch: 14/30, step: 75/364, loss: 0.44381, accuracy: 0.79937\n",
            "Epoch: 14/30, step: 76/364, loss: 0.44420, accuracy: 0.79955\n",
            "Epoch: 14/30, step: 77/364, loss: 0.44368, accuracy: 0.80012\n",
            "Epoch: 14/30, step: 78/364, loss: 0.44431, accuracy: 0.79968\n",
            "Epoch: 14/30, step: 79/364, loss: 0.44464, accuracy: 0.79984\n",
            "Epoch: 14/30, step: 80/364, loss: 0.44343, accuracy: 0.80098\n",
            "Epoch: 14/30, step: 81/364, loss: 0.44273, accuracy: 0.80208\n",
            "Epoch: 14/30, step: 82/364, loss: 0.44302, accuracy: 0.80183\n",
            "Epoch: 14/30, step: 83/364, loss: 0.44305, accuracy: 0.80139\n",
            "Epoch: 14/30, step: 84/364, loss: 0.44331, accuracy: 0.80115\n",
            "Epoch: 14/30, step: 85/364, loss: 0.44222, accuracy: 0.80184\n",
            "Epoch: 14/30, step: 86/364, loss: 0.44283, accuracy: 0.80069\n",
            "Epoch: 14/30, step: 87/364, loss: 0.44275, accuracy: 0.80065\n",
            "Epoch: 14/30, step: 88/364, loss: 0.44344, accuracy: 0.80007\n",
            "Epoch: 14/30, step: 89/364, loss: 0.44414, accuracy: 0.79916\n",
            "Epoch: 14/30, step: 90/364, loss: 0.44415, accuracy: 0.79878\n",
            "Epoch: 14/30, step: 91/364, loss: 0.44440, accuracy: 0.79876\n",
            "Epoch: 14/30, step: 92/364, loss: 0.44528, accuracy: 0.79806\n",
            "Epoch: 14/30, step: 93/364, loss: 0.44563, accuracy: 0.79772\n",
            "Epoch: 14/30, step: 94/364, loss: 0.44645, accuracy: 0.79754\n",
            "Epoch: 14/30, step: 95/364, loss: 0.44639, accuracy: 0.79753\n",
            "Epoch: 14/30, step: 96/364, loss: 0.44624, accuracy: 0.79753\n",
            "Epoch: 14/30, step: 97/364, loss: 0.44621, accuracy: 0.79752\n",
            "Epoch: 14/30, step: 98/364, loss: 0.44643, accuracy: 0.79799\n",
            "Epoch: 14/30, step: 99/364, loss: 0.44722, accuracy: 0.79735\n",
            "Epoch: 14/30, step: 100/364, loss: 0.44754, accuracy: 0.79703\n",
            "Epoch: 14/30, step: 101/364, loss: 0.44759, accuracy: 0.79718\n",
            "Epoch: 14/30, step: 102/364, loss: 0.44770, accuracy: 0.79718\n",
            "Epoch: 14/30, step: 103/364, loss: 0.44885, accuracy: 0.79612\n",
            "Epoch: 14/30, step: 104/364, loss: 0.44913, accuracy: 0.79552\n",
            "Epoch: 14/30, step: 105/364, loss: 0.44961, accuracy: 0.79524\n",
            "Epoch: 14/30, step: 106/364, loss: 0.44979, accuracy: 0.79466\n",
            "Epoch: 14/30, step: 107/364, loss: 0.45033, accuracy: 0.79425\n",
            "Epoch: 14/30, step: 108/364, loss: 0.45087, accuracy: 0.79340\n",
            "Epoch: 14/30, step: 109/364, loss: 0.44986, accuracy: 0.79415\n",
            "Epoch: 14/30, step: 110/364, loss: 0.44981, accuracy: 0.79361\n",
            "Epoch: 14/30, step: 111/364, loss: 0.44992, accuracy: 0.79322\n",
            "Epoch: 14/30, step: 112/364, loss: 0.44952, accuracy: 0.79381\n",
            "Epoch: 14/30, step: 113/364, loss: 0.45007, accuracy: 0.79383\n",
            "Epoch: 14/30, step: 114/364, loss: 0.44932, accuracy: 0.79454\n",
            "Epoch: 14/30, step: 115/364, loss: 0.44977, accuracy: 0.79416\n",
            "Epoch: 14/30, step: 116/364, loss: 0.44994, accuracy: 0.79391\n",
            "Epoch: 14/30, step: 117/364, loss: 0.44997, accuracy: 0.79420\n",
            "Epoch: 14/30, step: 118/364, loss: 0.44904, accuracy: 0.79502\n",
            "Epoch: 14/30, step: 119/364, loss: 0.44909, accuracy: 0.79517\n",
            "Epoch: 14/30, step: 120/364, loss: 0.44921, accuracy: 0.79531\n",
            "Epoch: 14/30, step: 121/364, loss: 0.44964, accuracy: 0.79494\n",
            "Epoch: 14/30, step: 122/364, loss: 0.44876, accuracy: 0.79559\n",
            "Epoch: 14/30, step: 123/364, loss: 0.44882, accuracy: 0.79535\n",
            "Epoch: 14/30, step: 124/364, loss: 0.44925, accuracy: 0.79498\n",
            "Epoch: 14/30, step: 125/364, loss: 0.44900, accuracy: 0.79550\n",
            "Epoch: 14/30, step: 126/364, loss: 0.44866, accuracy: 0.79613\n",
            "Epoch: 14/30, step: 127/364, loss: 0.44921, accuracy: 0.79614\n",
            "Epoch: 14/30, step: 128/364, loss: 0.44908, accuracy: 0.79614\n",
            "Epoch: 14/30, step: 129/364, loss: 0.44975, accuracy: 0.79554\n",
            "Epoch: 14/30, step: 130/364, loss: 0.44977, accuracy: 0.79543\n",
            "Epoch: 14/30, step: 131/364, loss: 0.44920, accuracy: 0.79592\n",
            "Epoch: 14/30, step: 132/364, loss: 0.44959, accuracy: 0.79545\n",
            "Epoch: 14/30, step: 133/364, loss: 0.45012, accuracy: 0.79535\n",
            "Epoch: 14/30, step: 134/364, loss: 0.45001, accuracy: 0.79559\n",
            "Epoch: 14/30, step: 135/364, loss: 0.44938, accuracy: 0.79630\n",
            "Epoch: 14/30, step: 136/364, loss: 0.44979, accuracy: 0.79619\n",
            "Epoch: 14/30, step: 137/364, loss: 0.45016, accuracy: 0.79585\n",
            "Epoch: 14/30, step: 138/364, loss: 0.44982, accuracy: 0.79642\n",
            "Epoch: 14/30, step: 139/364, loss: 0.44978, accuracy: 0.79654\n",
            "Epoch: 14/30, step: 140/364, loss: 0.44976, accuracy: 0.79710\n",
            "Epoch: 14/30, step: 141/364, loss: 0.44983, accuracy: 0.79710\n",
            "Epoch: 14/30, step: 142/364, loss: 0.44946, accuracy: 0.79732\n",
            "Epoch: 14/30, step: 143/364, loss: 0.44946, accuracy: 0.79775\n",
            "Epoch: 14/30, step: 144/364, loss: 0.44921, accuracy: 0.79785\n",
            "Epoch: 14/30, step: 145/364, loss: 0.44933, accuracy: 0.79741\n",
            "Epoch: 14/30, step: 146/364, loss: 0.44901, accuracy: 0.79784\n",
            "Epoch: 14/30, step: 147/364, loss: 0.44894, accuracy: 0.79783\n",
            "Epoch: 14/30, step: 148/364, loss: 0.44871, accuracy: 0.79804\n",
            "Epoch: 14/30, step: 149/364, loss: 0.44929, accuracy: 0.79761\n",
            "Epoch: 14/30, step: 150/364, loss: 0.44907, accuracy: 0.79729\n",
            "Epoch: 14/30, step: 151/364, loss: 0.44877, accuracy: 0.79760\n",
            "Epoch: 14/30, step: 152/364, loss: 0.44818, accuracy: 0.79811\n",
            "Epoch: 14/30, step: 153/364, loss: 0.44819, accuracy: 0.79800\n",
            "Epoch: 14/30, step: 154/364, loss: 0.44839, accuracy: 0.79779\n",
            "Epoch: 14/30, step: 155/364, loss: 0.44874, accuracy: 0.79738\n",
            "Epoch: 14/30, step: 156/364, loss: 0.44846, accuracy: 0.79778\n",
            "Epoch: 14/30, step: 157/364, loss: 0.44870, accuracy: 0.79747\n",
            "Epoch: 14/30, step: 158/364, loss: 0.44853, accuracy: 0.79747\n",
            "Epoch: 14/30, step: 159/364, loss: 0.44832, accuracy: 0.79776\n",
            "Epoch: 14/30, step: 160/364, loss: 0.44831, accuracy: 0.79775\n",
            "Epoch: 14/30, step: 161/364, loss: 0.44858, accuracy: 0.79746\n",
            "Epoch: 14/30, step: 162/364, loss: 0.44873, accuracy: 0.79726\n",
            "Epoch: 14/30, step: 163/364, loss: 0.44860, accuracy: 0.79735\n",
            "Epoch: 14/30, step: 164/364, loss: 0.44860, accuracy: 0.79726\n",
            "Epoch: 14/30, step: 165/364, loss: 0.44891, accuracy: 0.79716\n",
            "Epoch: 14/30, step: 166/364, loss: 0.44917, accuracy: 0.79688\n",
            "Epoch: 14/30, step: 167/364, loss: 0.44881, accuracy: 0.79697\n",
            "Epoch: 14/30, step: 168/364, loss: 0.44860, accuracy: 0.79697\n",
            "Epoch: 14/30, step: 169/364, loss: 0.44871, accuracy: 0.79697\n",
            "Epoch: 14/30, step: 170/364, loss: 0.44861, accuracy: 0.79733\n",
            "Epoch: 14/30, step: 171/364, loss: 0.44839, accuracy: 0.79742\n",
            "Epoch: 14/30, step: 172/364, loss: 0.44829, accuracy: 0.79751\n",
            "Epoch: 14/30, step: 173/364, loss: 0.44792, accuracy: 0.79769\n",
            "Epoch: 14/30, step: 174/364, loss: 0.44809, accuracy: 0.79732\n",
            "Epoch: 14/30, step: 175/364, loss: 0.44806, accuracy: 0.79705\n",
            "Epoch: 14/30, step: 176/364, loss: 0.44799, accuracy: 0.79688\n",
            "Epoch: 14/30, step: 177/364, loss: 0.44794, accuracy: 0.79696\n",
            "Epoch: 14/30, step: 178/364, loss: 0.44747, accuracy: 0.79731\n",
            "Epoch: 14/30, step: 179/364, loss: 0.44738, accuracy: 0.79757\n",
            "Epoch: 14/30, step: 180/364, loss: 0.44678, accuracy: 0.79792\n",
            "Epoch: 14/30, step: 181/364, loss: 0.44662, accuracy: 0.79791\n",
            "Epoch: 14/30, step: 182/364, loss: 0.44674, accuracy: 0.79791\n",
            "Epoch: 14/30, step: 183/364, loss: 0.44647, accuracy: 0.79798\n",
            "Epoch: 14/30, step: 184/364, loss: 0.44667, accuracy: 0.79798\n",
            "Epoch: 14/30, step: 185/364, loss: 0.44664, accuracy: 0.79806\n",
            "Epoch: 14/30, step: 186/364, loss: 0.44703, accuracy: 0.79788\n",
            "Epoch: 14/30, step: 187/364, loss: 0.44727, accuracy: 0.79788\n",
            "Epoch: 14/30, step: 188/364, loss: 0.44708, accuracy: 0.79779\n",
            "Epoch: 14/30, step: 189/364, loss: 0.44706, accuracy: 0.79803\n",
            "Epoch: 14/30, step: 190/364, loss: 0.44741, accuracy: 0.79770\n",
            "Epoch: 14/30, step: 191/364, loss: 0.44774, accuracy: 0.79737\n",
            "Epoch: 14/30, step: 192/364, loss: 0.44779, accuracy: 0.79736\n",
            "Epoch: 14/30, step: 193/364, loss: 0.44759, accuracy: 0.79736\n",
            "Epoch: 14/30, step: 194/364, loss: 0.44784, accuracy: 0.79712\n",
            "Epoch: 14/30, step: 195/364, loss: 0.44783, accuracy: 0.79712\n",
            "Epoch: 14/30, step: 196/364, loss: 0.44807, accuracy: 0.79672\n",
            "Epoch: 14/30, step: 197/364, loss: 0.44803, accuracy: 0.79648\n",
            "Epoch: 14/30, step: 198/364, loss: 0.44795, accuracy: 0.79672\n",
            "Epoch: 14/30, step: 199/364, loss: 0.44767, accuracy: 0.79695\n",
            "Epoch: 14/30, step: 200/364, loss: 0.44718, accuracy: 0.79734\n",
            "Epoch: 14/30, step: 201/364, loss: 0.44724, accuracy: 0.79711\n",
            "Epoch: 14/30, step: 202/364, loss: 0.44714, accuracy: 0.79718\n",
            "Epoch: 14/30, step: 203/364, loss: 0.44676, accuracy: 0.79749\n",
            "Epoch: 14/30, step: 204/364, loss: 0.44652, accuracy: 0.79764\n",
            "Epoch: 14/30, step: 205/364, loss: 0.44618, accuracy: 0.79787\n",
            "Epoch: 14/30, step: 206/364, loss: 0.44639, accuracy: 0.79794\n",
            "Epoch: 14/30, step: 207/364, loss: 0.44635, accuracy: 0.79793\n",
            "Epoch: 14/30, step: 208/364, loss: 0.44613, accuracy: 0.79815\n",
            "Epoch: 14/30, step: 209/364, loss: 0.44681, accuracy: 0.79740\n",
            "Epoch: 14/30, step: 210/364, loss: 0.44666, accuracy: 0.79762\n",
            "Epoch: 14/30, step: 211/364, loss: 0.44653, accuracy: 0.79769\n",
            "Epoch: 14/30, step: 212/364, loss: 0.44641, accuracy: 0.79791\n",
            "Epoch: 14/30, step: 213/364, loss: 0.44604, accuracy: 0.79820\n",
            "Epoch: 14/30, step: 214/364, loss: 0.44632, accuracy: 0.79797\n",
            "Epoch: 14/30, step: 215/364, loss: 0.44634, accuracy: 0.79797\n",
            "Epoch: 14/30, step: 216/364, loss: 0.44670, accuracy: 0.79760\n",
            "Epoch: 14/30, step: 217/364, loss: 0.44665, accuracy: 0.79774\n",
            "Epoch: 14/30, step: 218/364, loss: 0.44649, accuracy: 0.79809\n",
            "Epoch: 14/30, step: 219/364, loss: 0.44638, accuracy: 0.79830\n",
            "Epoch: 14/30, step: 220/364, loss: 0.44680, accuracy: 0.79787\n",
            "Epoch: 14/30, step: 221/364, loss: 0.44681, accuracy: 0.79786\n",
            "Epoch: 14/30, step: 222/364, loss: 0.44698, accuracy: 0.79772\n",
            "Epoch: 14/30, step: 223/364, loss: 0.44683, accuracy: 0.79793\n",
            "Epoch: 14/30, step: 224/364, loss: 0.44689, accuracy: 0.79764\n",
            "Epoch: 14/30, step: 225/364, loss: 0.44659, accuracy: 0.79792\n",
            "Epoch: 14/30, step: 226/364, loss: 0.44647, accuracy: 0.79777\n",
            "Epoch: 14/30, step: 227/364, loss: 0.44629, accuracy: 0.79798\n",
            "Epoch: 14/30, step: 228/364, loss: 0.44640, accuracy: 0.79797\n",
            "Epoch: 14/30, step: 229/364, loss: 0.44625, accuracy: 0.79797\n",
            "Epoch: 14/30, step: 230/364, loss: 0.44622, accuracy: 0.79796\n",
            "Epoch: 14/30, step: 231/364, loss: 0.44595, accuracy: 0.79823\n",
            "Epoch: 14/30, step: 232/364, loss: 0.44560, accuracy: 0.79849\n",
            "Epoch: 14/30, step: 233/364, loss: 0.44584, accuracy: 0.79815\n",
            "Epoch: 14/30, step: 234/364, loss: 0.44557, accuracy: 0.79861\n",
            "Epoch: 14/30, step: 235/364, loss: 0.44554, accuracy: 0.79867\n",
            "Epoch: 14/30, step: 236/364, loss: 0.44590, accuracy: 0.79827\n",
            "Epoch: 14/30, step: 237/364, loss: 0.44607, accuracy: 0.79819\n",
            "Epoch: 14/30, step: 238/364, loss: 0.44593, accuracy: 0.79825\n",
            "Epoch: 14/30, step: 239/364, loss: 0.44586, accuracy: 0.79838\n",
            "Epoch: 14/30, step: 240/364, loss: 0.44583, accuracy: 0.79844\n",
            "Epoch: 14/30, step: 241/364, loss: 0.44582, accuracy: 0.79850\n",
            "Epoch: 14/30, step: 242/364, loss: 0.44566, accuracy: 0.79855\n",
            "Epoch: 14/30, step: 243/364, loss: 0.44543, accuracy: 0.79874\n",
            "Epoch: 14/30, step: 244/364, loss: 0.44582, accuracy: 0.79835\n",
            "Epoch: 14/30, step: 245/364, loss: 0.44566, accuracy: 0.79834\n",
            "Epoch: 14/30, step: 246/364, loss: 0.44574, accuracy: 0.79815\n",
            "Epoch: 14/30, step: 247/364, loss: 0.44552, accuracy: 0.79833\n",
            "Epoch: 14/30, step: 248/364, loss: 0.44519, accuracy: 0.79870\n",
            "Epoch: 14/30, step: 249/364, loss: 0.44514, accuracy: 0.79876\n",
            "Epoch: 14/30, step: 250/364, loss: 0.44539, accuracy: 0.79850\n",
            "Epoch: 14/30, step: 251/364, loss: 0.44523, accuracy: 0.79862\n",
            "Epoch: 14/30, step: 252/364, loss: 0.44522, accuracy: 0.79874\n",
            "Epoch: 14/30, step: 253/364, loss: 0.44521, accuracy: 0.79891\n",
            "Epoch: 14/30, step: 254/364, loss: 0.44488, accuracy: 0.79915\n",
            "Epoch: 14/30, step: 255/364, loss: 0.44483, accuracy: 0.79926\n",
            "Epoch: 14/30, step: 256/364, loss: 0.44481, accuracy: 0.79932\n",
            "Epoch: 14/30, step: 257/364, loss: 0.44490, accuracy: 0.79919\n",
            "Epoch: 14/30, step: 258/364, loss: 0.44490, accuracy: 0.79906\n",
            "Epoch: 14/30, step: 259/364, loss: 0.44466, accuracy: 0.79923\n",
            "Epoch: 14/30, step: 260/364, loss: 0.44457, accuracy: 0.79940\n",
            "Epoch: 14/30, step: 261/364, loss: 0.44474, accuracy: 0.79915\n",
            "Epoch: 14/30, step: 262/364, loss: 0.44459, accuracy: 0.79932\n",
            "Epoch: 14/30, step: 263/364, loss: 0.44455, accuracy: 0.79925\n",
            "Epoch: 14/30, step: 264/364, loss: 0.44458, accuracy: 0.79918\n",
            "Epoch: 14/30, step: 265/364, loss: 0.44449, accuracy: 0.79917\n",
            "Epoch: 14/30, step: 266/364, loss: 0.44439, accuracy: 0.79928\n",
            "Epoch: 14/30, step: 267/364, loss: 0.44418, accuracy: 0.79939\n",
            "Epoch: 14/30, step: 268/364, loss: 0.44417, accuracy: 0.79944\n",
            "Epoch: 14/30, step: 269/364, loss: 0.44421, accuracy: 0.79926\n",
            "Epoch: 14/30, step: 270/364, loss: 0.44394, accuracy: 0.79942\n",
            "Epoch: 14/30, step: 271/364, loss: 0.44344, accuracy: 0.79976\n",
            "Epoch: 14/30, step: 272/364, loss: 0.44415, accuracy: 0.79923\n",
            "Epoch: 14/30, step: 273/364, loss: 0.44452, accuracy: 0.79876\n",
            "Epoch: 14/30, step: 274/364, loss: 0.44452, accuracy: 0.79864\n",
            "Epoch: 14/30, step: 275/364, loss: 0.44459, accuracy: 0.79852\n",
            "Epoch: 14/30, step: 276/364, loss: 0.44482, accuracy: 0.79829\n",
            "Epoch: 14/30, step: 277/364, loss: 0.44493, accuracy: 0.79823\n",
            "Epoch: 14/30, step: 278/364, loss: 0.44469, accuracy: 0.79845\n",
            "Epoch: 14/30, step: 279/364, loss: 0.44450, accuracy: 0.79850\n",
            "Epoch: 14/30, step: 280/364, loss: 0.44431, accuracy: 0.79855\n",
            "Epoch: 14/30, step: 281/364, loss: 0.44447, accuracy: 0.79865\n",
            "Epoch: 14/30, step: 282/364, loss: 0.44442, accuracy: 0.79876\n",
            "Epoch: 14/30, step: 283/364, loss: 0.44433, accuracy: 0.79886\n",
            "Epoch: 14/30, step: 284/364, loss: 0.44477, accuracy: 0.79847\n",
            "Epoch: 14/30, step: 285/364, loss: 0.44491, accuracy: 0.79841\n",
            "Epoch: 14/30, step: 286/364, loss: 0.44508, accuracy: 0.79802\n",
            "Epoch: 14/30, step: 287/364, loss: 0.44505, accuracy: 0.79796\n",
            "Epoch: 14/30, step: 288/364, loss: 0.44499, accuracy: 0.79791\n",
            "Epoch: 14/30, step: 289/364, loss: 0.44520, accuracy: 0.79774\n",
            "Epoch: 14/30, step: 290/364, loss: 0.44537, accuracy: 0.79763\n",
            "Epoch: 14/30, step: 291/364, loss: 0.44518, accuracy: 0.79780\n",
            "Epoch: 14/30, train loss: 0.44518, train accuracy: 0.79780, valid loss: 0.61654, valid accuracy: 0.67648\n",
            "Epoch: 15/30, step: 1/364, loss: 0.39877, accuracy: 0.81250\n",
            "Epoch: 15/30, step: 2/364, loss: 0.35858, accuracy: 0.85938\n",
            "Epoch: 15/30, step: 3/364, loss: 0.39213, accuracy: 0.84896\n",
            "Epoch: 15/30, step: 4/364, loss: 0.37897, accuracy: 0.85156\n",
            "Epoch: 15/30, step: 5/364, loss: 0.39333, accuracy: 0.84688\n",
            "Epoch: 15/30, step: 6/364, loss: 0.41125, accuracy: 0.83594\n",
            "Epoch: 15/30, step: 7/364, loss: 0.42846, accuracy: 0.82143\n",
            "Epoch: 15/30, step: 8/364, loss: 0.42386, accuracy: 0.82617\n",
            "Epoch: 15/30, step: 9/364, loss: 0.42213, accuracy: 0.82465\n",
            "Epoch: 15/30, step: 10/364, loss: 0.42580, accuracy: 0.82344\n",
            "Epoch: 15/30, step: 11/364, loss: 0.42893, accuracy: 0.82102\n",
            "Epoch: 15/30, step: 12/364, loss: 0.43568, accuracy: 0.81510\n",
            "Epoch: 15/30, step: 13/364, loss: 0.43099, accuracy: 0.81851\n",
            "Epoch: 15/30, step: 14/364, loss: 0.43614, accuracy: 0.81920\n",
            "Epoch: 15/30, step: 15/364, loss: 0.43615, accuracy: 0.81875\n",
            "Epoch: 15/30, step: 16/364, loss: 0.43447, accuracy: 0.82129\n",
            "Epoch: 15/30, step: 17/364, loss: 0.43629, accuracy: 0.81985\n",
            "Epoch: 15/30, step: 18/364, loss: 0.43911, accuracy: 0.81771\n",
            "Epoch: 15/30, step: 19/364, loss: 0.43694, accuracy: 0.81908\n",
            "Epoch: 15/30, step: 20/364, loss: 0.44202, accuracy: 0.81563\n",
            "Epoch: 15/30, step: 21/364, loss: 0.44254, accuracy: 0.81399\n",
            "Epoch: 15/30, step: 22/364, loss: 0.44147, accuracy: 0.81250\n",
            "Epoch: 15/30, step: 23/364, loss: 0.44027, accuracy: 0.81182\n",
            "Epoch: 15/30, step: 24/364, loss: 0.44130, accuracy: 0.80924\n",
            "Epoch: 15/30, step: 25/364, loss: 0.44037, accuracy: 0.81000\n",
            "Epoch: 15/30, step: 26/364, loss: 0.44332, accuracy: 0.80589\n",
            "Epoch: 15/30, step: 27/364, loss: 0.43806, accuracy: 0.80961\n",
            "Epoch: 15/30, step: 28/364, loss: 0.43918, accuracy: 0.80915\n",
            "Epoch: 15/30, step: 29/364, loss: 0.43845, accuracy: 0.80981\n",
            "Epoch: 15/30, step: 30/364, loss: 0.44062, accuracy: 0.80573\n",
            "Epoch: 15/30, step: 31/364, loss: 0.44237, accuracy: 0.80242\n",
            "Epoch: 15/30, step: 32/364, loss: 0.44200, accuracy: 0.80420\n",
            "Epoch: 15/30, step: 33/364, loss: 0.44114, accuracy: 0.80445\n",
            "Epoch: 15/30, step: 34/364, loss: 0.43882, accuracy: 0.80561\n",
            "Epoch: 15/30, step: 35/364, loss: 0.44058, accuracy: 0.80491\n",
            "Epoch: 15/30, step: 36/364, loss: 0.43983, accuracy: 0.80599\n",
            "Epoch: 15/30, step: 37/364, loss: 0.44090, accuracy: 0.80532\n",
            "Epoch: 15/30, step: 38/364, loss: 0.44171, accuracy: 0.80387\n",
            "Epoch: 15/30, step: 39/364, loss: 0.44409, accuracy: 0.80208\n",
            "Epoch: 15/30, step: 40/364, loss: 0.44501, accuracy: 0.80117\n",
            "Epoch: 15/30, step: 41/364, loss: 0.44408, accuracy: 0.80335\n",
            "Epoch: 15/30, step: 42/364, loss: 0.44390, accuracy: 0.80357\n",
            "Epoch: 15/30, step: 43/364, loss: 0.44146, accuracy: 0.80523\n",
            "Epoch: 15/30, step: 44/364, loss: 0.44196, accuracy: 0.80469\n",
            "Epoch: 15/30, step: 45/364, loss: 0.44197, accuracy: 0.80451\n",
            "Epoch: 15/30, step: 46/364, loss: 0.44319, accuracy: 0.80299\n",
            "Epoch: 15/30, step: 47/364, loss: 0.44106, accuracy: 0.80485\n",
            "Epoch: 15/30, step: 48/364, loss: 0.44028, accuracy: 0.80534\n",
            "Epoch: 15/30, step: 49/364, loss: 0.44074, accuracy: 0.80517\n",
            "Epoch: 15/30, step: 50/364, loss: 0.43958, accuracy: 0.80625\n",
            "Epoch: 15/30, step: 51/364, loss: 0.43896, accuracy: 0.80699\n",
            "Epoch: 15/30, step: 52/364, loss: 0.44062, accuracy: 0.80619\n",
            "Epoch: 15/30, step: 53/364, loss: 0.43982, accuracy: 0.80660\n",
            "Epoch: 15/30, step: 54/364, loss: 0.43924, accuracy: 0.80758\n",
            "Epoch: 15/30, step: 55/364, loss: 0.43951, accuracy: 0.80795\n",
            "Epoch: 15/30, step: 56/364, loss: 0.43891, accuracy: 0.80804\n",
            "Epoch: 15/30, step: 57/364, loss: 0.43755, accuracy: 0.80866\n",
            "Epoch: 15/30, step: 58/364, loss: 0.43650, accuracy: 0.81008\n",
            "Epoch: 15/30, step: 59/364, loss: 0.43747, accuracy: 0.80985\n",
            "Epoch: 15/30, step: 60/364, loss: 0.43775, accuracy: 0.80937\n",
            "Epoch: 15/30, step: 61/364, loss: 0.43753, accuracy: 0.80866\n",
            "Epoch: 15/30, step: 62/364, loss: 0.43675, accuracy: 0.80948\n",
            "Epoch: 15/30, step: 63/364, loss: 0.43624, accuracy: 0.81052\n",
            "Epoch: 15/30, step: 64/364, loss: 0.43864, accuracy: 0.80835\n",
            "Epoch: 15/30, step: 65/364, loss: 0.43871, accuracy: 0.80817\n",
            "Epoch: 15/30, step: 66/364, loss: 0.43991, accuracy: 0.80682\n",
            "Epoch: 15/30, step: 67/364, loss: 0.44014, accuracy: 0.80714\n",
            "Epoch: 15/30, step: 68/364, loss: 0.44058, accuracy: 0.80813\n",
            "Epoch: 15/30, step: 69/364, loss: 0.44238, accuracy: 0.80707\n",
            "Epoch: 15/30, step: 70/364, loss: 0.44213, accuracy: 0.80714\n",
            "Epoch: 15/30, step: 71/364, loss: 0.44228, accuracy: 0.80700\n",
            "Epoch: 15/30, step: 72/364, loss: 0.44169, accuracy: 0.80751\n",
            "Epoch: 15/30, step: 73/364, loss: 0.44032, accuracy: 0.80908\n",
            "Epoch: 15/30, step: 74/364, loss: 0.44021, accuracy: 0.80933\n",
            "Epoch: 15/30, step: 75/364, loss: 0.43899, accuracy: 0.81021\n",
            "Epoch: 15/30, step: 76/364, loss: 0.43850, accuracy: 0.81106\n",
            "Epoch: 15/30, step: 77/364, loss: 0.43794, accuracy: 0.81189\n",
            "Epoch: 15/30, step: 78/364, loss: 0.43862, accuracy: 0.81050\n",
            "Epoch: 15/30, step: 79/364, loss: 0.43830, accuracy: 0.81112\n",
            "Epoch: 15/30, step: 80/364, loss: 0.43785, accuracy: 0.81094\n",
            "Epoch: 15/30, step: 81/364, loss: 0.43718, accuracy: 0.81115\n",
            "Epoch: 15/30, step: 82/364, loss: 0.43744, accuracy: 0.81136\n",
            "Epoch: 15/30, step: 83/364, loss: 0.43682, accuracy: 0.81231\n",
            "Epoch: 15/30, step: 84/364, loss: 0.43638, accuracy: 0.81250\n",
            "Epoch: 15/30, step: 85/364, loss: 0.43533, accuracy: 0.81360\n",
            "Epoch: 15/30, step: 86/364, loss: 0.43489, accuracy: 0.81377\n",
            "Epoch: 15/30, step: 87/364, loss: 0.43563, accuracy: 0.81286\n",
            "Epoch: 15/30, step: 88/364, loss: 0.43536, accuracy: 0.81268\n",
            "Epoch: 15/30, step: 89/364, loss: 0.43570, accuracy: 0.81285\n",
            "Epoch: 15/30, step: 90/364, loss: 0.43571, accuracy: 0.81302\n",
            "Epoch: 15/30, step: 91/364, loss: 0.43618, accuracy: 0.81233\n",
            "Epoch: 15/30, step: 92/364, loss: 0.43668, accuracy: 0.81250\n",
            "Epoch: 15/30, step: 93/364, loss: 0.43584, accuracy: 0.81284\n",
            "Epoch: 15/30, step: 94/364, loss: 0.43597, accuracy: 0.81250\n",
            "Epoch: 15/30, step: 95/364, loss: 0.43640, accuracy: 0.81234\n",
            "Epoch: 15/30, step: 96/364, loss: 0.43645, accuracy: 0.81234\n",
            "Epoch: 15/30, step: 97/364, loss: 0.43639, accuracy: 0.81218\n",
            "Epoch: 15/30, step: 98/364, loss: 0.43596, accuracy: 0.81266\n",
            "Epoch: 15/30, step: 99/364, loss: 0.43645, accuracy: 0.81234\n",
            "Epoch: 15/30, step: 100/364, loss: 0.43733, accuracy: 0.81156\n",
            "Epoch: 15/30, step: 101/364, loss: 0.43686, accuracy: 0.81204\n",
            "Epoch: 15/30, step: 102/364, loss: 0.43693, accuracy: 0.81204\n",
            "Epoch: 15/30, step: 103/364, loss: 0.43628, accuracy: 0.81280\n",
            "Epoch: 15/30, step: 104/364, loss: 0.43599, accuracy: 0.81265\n",
            "Epoch: 15/30, step: 105/364, loss: 0.43522, accuracy: 0.81324\n",
            "Epoch: 15/30, step: 106/364, loss: 0.43495, accuracy: 0.81368\n",
            "Epoch: 15/30, step: 107/364, loss: 0.43442, accuracy: 0.81440\n",
            "Epoch: 15/30, step: 108/364, loss: 0.43484, accuracy: 0.81424\n",
            "Epoch: 15/30, step: 109/364, loss: 0.43551, accuracy: 0.81336\n",
            "Epoch: 15/30, step: 110/364, loss: 0.43519, accuracy: 0.81392\n",
            "Epoch: 15/30, step: 111/364, loss: 0.43499, accuracy: 0.81377\n",
            "Epoch: 15/30, step: 112/364, loss: 0.43509, accuracy: 0.81417\n",
            "Epoch: 15/30, step: 113/364, loss: 0.43480, accuracy: 0.81430\n",
            "Epoch: 15/30, step: 114/364, loss: 0.43436, accuracy: 0.81456\n",
            "Epoch: 15/30, step: 115/364, loss: 0.43405, accuracy: 0.81481\n",
            "Epoch: 15/30, step: 116/364, loss: 0.43366, accuracy: 0.81506\n",
            "Epoch: 15/30, step: 117/364, loss: 0.43356, accuracy: 0.81517\n",
            "Epoch: 15/30, step: 118/364, loss: 0.43382, accuracy: 0.81502\n",
            "Epoch: 15/30, step: 119/364, loss: 0.43402, accuracy: 0.81460\n",
            "Epoch: 15/30, step: 120/364, loss: 0.43408, accuracy: 0.81406\n",
            "Epoch: 15/30, step: 121/364, loss: 0.43416, accuracy: 0.81405\n",
            "Epoch: 15/30, step: 122/364, loss: 0.43352, accuracy: 0.81442\n",
            "Epoch: 15/30, step: 123/364, loss: 0.43310, accuracy: 0.81479\n",
            "Epoch: 15/30, step: 124/364, loss: 0.43293, accuracy: 0.81477\n",
            "Epoch: 15/30, step: 125/364, loss: 0.43318, accuracy: 0.81463\n",
            "Epoch: 15/30, step: 126/364, loss: 0.43210, accuracy: 0.81548\n",
            "Epoch: 15/30, step: 127/364, loss: 0.43217, accuracy: 0.81508\n",
            "Epoch: 15/30, step: 128/364, loss: 0.43147, accuracy: 0.81555\n",
            "Epoch: 15/30, step: 129/364, loss: 0.43146, accuracy: 0.81553\n",
            "Epoch: 15/30, step: 130/364, loss: 0.43167, accuracy: 0.81514\n",
            "Epoch: 15/30, step: 131/364, loss: 0.43121, accuracy: 0.81560\n",
            "Epoch: 15/30, step: 132/364, loss: 0.43132, accuracy: 0.81546\n",
            "Epoch: 15/30, step: 133/364, loss: 0.43068, accuracy: 0.81602\n",
            "Epoch: 15/30, step: 134/364, loss: 0.43117, accuracy: 0.81530\n",
            "Epoch: 15/30, step: 135/364, loss: 0.43062, accuracy: 0.81563\n",
            "Epoch: 15/30, step: 136/364, loss: 0.43059, accuracy: 0.81537\n",
            "Epoch: 15/30, step: 137/364, loss: 0.43035, accuracy: 0.81501\n",
            "Epoch: 15/30, step: 138/364, loss: 0.43061, accuracy: 0.81499\n",
            "Epoch: 15/30, step: 139/364, loss: 0.43003, accuracy: 0.81509\n",
            "Epoch: 15/30, step: 140/364, loss: 0.43028, accuracy: 0.81440\n",
            "Epoch: 15/30, step: 141/364, loss: 0.43061, accuracy: 0.81394\n",
            "Epoch: 15/30, step: 142/364, loss: 0.43047, accuracy: 0.81371\n",
            "Epoch: 15/30, step: 143/364, loss: 0.43009, accuracy: 0.81381\n",
            "Epoch: 15/30, step: 144/364, loss: 0.42975, accuracy: 0.81424\n",
            "Epoch: 15/30, step: 145/364, loss: 0.43023, accuracy: 0.81369\n",
            "Epoch: 15/30, step: 146/364, loss: 0.43083, accuracy: 0.81368\n",
            "Epoch: 15/30, step: 147/364, loss: 0.43108, accuracy: 0.81378\n",
            "Epoch: 15/30, step: 148/364, loss: 0.43093, accuracy: 0.81377\n",
            "Epoch: 15/30, step: 149/364, loss: 0.43060, accuracy: 0.81428\n",
            "Epoch: 15/30, step: 150/364, loss: 0.43075, accuracy: 0.81406\n",
            "Epoch: 15/30, step: 151/364, loss: 0.43059, accuracy: 0.81416\n",
            "Epoch: 15/30, step: 152/364, loss: 0.43079, accuracy: 0.81394\n",
            "Epoch: 15/30, step: 153/364, loss: 0.43020, accuracy: 0.81444\n",
            "Epoch: 15/30, step: 154/364, loss: 0.42994, accuracy: 0.81463\n",
            "Epoch: 15/30, step: 155/364, loss: 0.42918, accuracy: 0.81542\n",
            "Epoch: 15/30, step: 156/364, loss: 0.42929, accuracy: 0.81520\n",
            "Epoch: 15/30, step: 157/364, loss: 0.42895, accuracy: 0.81529\n",
            "Epoch: 15/30, step: 158/364, loss: 0.42884, accuracy: 0.81507\n",
            "Epoch: 15/30, step: 159/364, loss: 0.42921, accuracy: 0.81447\n",
            "Epoch: 15/30, step: 160/364, loss: 0.42892, accuracy: 0.81494\n",
            "Epoch: 15/30, step: 161/364, loss: 0.42910, accuracy: 0.81454\n",
            "Epoch: 15/30, step: 162/364, loss: 0.42898, accuracy: 0.81453\n",
            "Epoch: 15/30, step: 163/364, loss: 0.42885, accuracy: 0.81470\n",
            "Epoch: 15/30, step: 164/364, loss: 0.42952, accuracy: 0.81412\n",
            "Epoch: 15/30, step: 165/364, loss: 0.42973, accuracy: 0.81364\n",
            "Epoch: 15/30, step: 166/364, loss: 0.42923, accuracy: 0.81382\n",
            "Epoch: 15/30, step: 167/364, loss: 0.42902, accuracy: 0.81400\n",
            "Epoch: 15/30, step: 168/364, loss: 0.42876, accuracy: 0.81390\n",
            "Epoch: 15/30, step: 169/364, loss: 0.42867, accuracy: 0.81379\n",
            "Epoch: 15/30, step: 170/364, loss: 0.42856, accuracy: 0.81397\n",
            "Epoch: 15/30, step: 171/364, loss: 0.42834, accuracy: 0.81414\n",
            "Epoch: 15/30, step: 172/364, loss: 0.42803, accuracy: 0.81414\n",
            "Epoch: 15/30, step: 173/364, loss: 0.42771, accuracy: 0.81440\n",
            "Epoch: 15/30, step: 174/364, loss: 0.42711, accuracy: 0.81492\n",
            "Epoch: 15/30, step: 175/364, loss: 0.42714, accuracy: 0.81518\n",
            "Epoch: 15/30, step: 176/364, loss: 0.42658, accuracy: 0.81578\n",
            "Epoch: 15/30, step: 177/364, loss: 0.42693, accuracy: 0.81577\n",
            "Epoch: 15/30, step: 178/364, loss: 0.42688, accuracy: 0.81566\n",
            "Epoch: 15/30, step: 179/364, loss: 0.42648, accuracy: 0.81608\n",
            "Epoch: 15/30, step: 180/364, loss: 0.42626, accuracy: 0.81632\n",
            "Epoch: 15/30, step: 181/364, loss: 0.42585, accuracy: 0.81656\n",
            "Epoch: 15/30, step: 182/364, loss: 0.42602, accuracy: 0.81611\n",
            "Epoch: 15/30, step: 183/364, loss: 0.42605, accuracy: 0.81592\n",
            "Epoch: 15/30, step: 184/364, loss: 0.42610, accuracy: 0.81573\n",
            "Epoch: 15/30, step: 185/364, loss: 0.42607, accuracy: 0.81554\n",
            "Epoch: 15/30, step: 186/364, loss: 0.42616, accuracy: 0.81536\n",
            "Epoch: 15/30, step: 187/364, loss: 0.42660, accuracy: 0.81501\n",
            "Epoch: 15/30, step: 188/364, loss: 0.42691, accuracy: 0.81458\n",
            "Epoch: 15/30, step: 189/364, loss: 0.42680, accuracy: 0.81465\n",
            "Epoch: 15/30, step: 190/364, loss: 0.42718, accuracy: 0.81431\n",
            "Epoch: 15/30, step: 191/364, loss: 0.42751, accuracy: 0.81405\n",
            "Epoch: 15/30, step: 192/364, loss: 0.42731, accuracy: 0.81437\n",
            "Epoch: 15/30, step: 193/364, loss: 0.42753, accuracy: 0.81404\n",
            "Epoch: 15/30, step: 194/364, loss: 0.42754, accuracy: 0.81411\n",
            "Epoch: 15/30, step: 195/364, loss: 0.42740, accuracy: 0.81394\n",
            "Epoch: 15/30, step: 196/364, loss: 0.42721, accuracy: 0.81401\n",
            "Epoch: 15/30, step: 197/364, loss: 0.42712, accuracy: 0.81385\n",
            "Epoch: 15/30, step: 198/364, loss: 0.42674, accuracy: 0.81432\n",
            "Epoch: 15/30, step: 199/364, loss: 0.42642, accuracy: 0.81462\n",
            "Epoch: 15/30, step: 200/364, loss: 0.42719, accuracy: 0.81391\n",
            "Epoch: 15/30, step: 201/364, loss: 0.42737, accuracy: 0.81351\n",
            "Epoch: 15/30, step: 202/364, loss: 0.42737, accuracy: 0.81358\n",
            "Epoch: 15/30, step: 203/364, loss: 0.42716, accuracy: 0.81365\n",
            "Epoch: 15/30, step: 204/364, loss: 0.42695, accuracy: 0.81380\n",
            "Epoch: 15/30, step: 205/364, loss: 0.42694, accuracy: 0.81372\n",
            "Epoch: 15/30, step: 206/364, loss: 0.42650, accuracy: 0.81394\n",
            "Epoch: 15/30, step: 207/364, loss: 0.42641, accuracy: 0.81401\n",
            "Epoch: 15/30, step: 208/364, loss: 0.42720, accuracy: 0.81340\n",
            "Epoch: 15/30, step: 209/364, loss: 0.42723, accuracy: 0.81355\n",
            "Epoch: 15/30, step: 210/364, loss: 0.42730, accuracy: 0.81347\n",
            "Epoch: 15/30, step: 211/364, loss: 0.42751, accuracy: 0.81331\n",
            "Epoch: 15/30, step: 212/364, loss: 0.42771, accuracy: 0.81302\n",
            "Epoch: 15/30, step: 213/364, loss: 0.42755, accuracy: 0.81301\n",
            "Epoch: 15/30, step: 214/364, loss: 0.42755, accuracy: 0.81323\n",
            "Epoch: 15/30, step: 215/364, loss: 0.42742, accuracy: 0.81344\n",
            "Epoch: 15/30, step: 216/364, loss: 0.42785, accuracy: 0.81286\n",
            "Epoch: 15/30, step: 217/364, loss: 0.42781, accuracy: 0.81293\n",
            "Epoch: 15/30, step: 218/364, loss: 0.42785, accuracy: 0.81271\n",
            "Epoch: 15/30, step: 219/364, loss: 0.42771, accuracy: 0.81293\n",
            "Epoch: 15/30, step: 220/364, loss: 0.42743, accuracy: 0.81328\n",
            "Epoch: 15/30, step: 221/364, loss: 0.42788, accuracy: 0.81278\n",
            "Epoch: 15/30, step: 222/364, loss: 0.42778, accuracy: 0.81292\n",
            "Epoch: 15/30, step: 223/364, loss: 0.42838, accuracy: 0.81250\n",
            "Epoch: 15/30, step: 224/364, loss: 0.42853, accuracy: 0.81243\n",
            "Epoch: 15/30, step: 225/364, loss: 0.42861, accuracy: 0.81257\n",
            "Epoch: 15/30, step: 226/364, loss: 0.42846, accuracy: 0.81264\n",
            "Epoch: 15/30, step: 227/364, loss: 0.42855, accuracy: 0.81257\n",
            "Epoch: 15/30, step: 228/364, loss: 0.42849, accuracy: 0.81277\n",
            "Epoch: 15/30, step: 229/364, loss: 0.42872, accuracy: 0.81270\n",
            "Epoch: 15/30, step: 230/364, loss: 0.42836, accuracy: 0.81298\n",
            "Epoch: 15/30, step: 231/364, loss: 0.42892, accuracy: 0.81270\n",
            "Epoch: 15/30, step: 232/364, loss: 0.42896, accuracy: 0.81263\n",
            "Epoch: 15/30, step: 233/364, loss: 0.42879, accuracy: 0.81257\n",
            "Epoch: 15/30, step: 234/364, loss: 0.42867, accuracy: 0.81283\n",
            "Epoch: 15/30, step: 235/364, loss: 0.42886, accuracy: 0.81283\n",
            "Epoch: 15/30, step: 236/364, loss: 0.42894, accuracy: 0.81296\n",
            "Epoch: 15/30, step: 237/364, loss: 0.42866, accuracy: 0.81316\n",
            "Epoch: 15/30, step: 238/364, loss: 0.42877, accuracy: 0.81296\n",
            "Epoch: 15/30, step: 239/364, loss: 0.42858, accuracy: 0.81309\n",
            "Epoch: 15/30, step: 240/364, loss: 0.42829, accuracy: 0.81341\n",
            "Epoch: 15/30, step: 241/364, loss: 0.42792, accuracy: 0.81380\n",
            "Epoch: 15/30, step: 242/364, loss: 0.42803, accuracy: 0.81353\n",
            "Epoch: 15/30, step: 243/364, loss: 0.42841, accuracy: 0.81327\n",
            "Epoch: 15/30, step: 244/364, loss: 0.42873, accuracy: 0.81295\n",
            "Epoch: 15/30, step: 245/364, loss: 0.42841, accuracy: 0.81295\n",
            "Epoch: 15/30, step: 246/364, loss: 0.42815, accuracy: 0.81307\n",
            "Epoch: 15/30, step: 247/364, loss: 0.42799, accuracy: 0.81313\n",
            "Epoch: 15/30, step: 248/364, loss: 0.42750, accuracy: 0.81357\n",
            "Epoch: 15/30, step: 249/364, loss: 0.42755, accuracy: 0.81350\n",
            "Epoch: 15/30, step: 250/364, loss: 0.42742, accuracy: 0.81356\n",
            "Epoch: 15/30, step: 251/364, loss: 0.42716, accuracy: 0.81381\n",
            "Epoch: 15/30, step: 252/364, loss: 0.42732, accuracy: 0.81380\n",
            "Epoch: 15/30, step: 253/364, loss: 0.42708, accuracy: 0.81392\n",
            "Epoch: 15/30, step: 254/364, loss: 0.42652, accuracy: 0.81428\n",
            "Epoch: 15/30, step: 255/364, loss: 0.42703, accuracy: 0.81391\n",
            "Epoch: 15/30, step: 256/364, loss: 0.42686, accuracy: 0.81396\n",
            "Epoch: 15/30, step: 257/364, loss: 0.42651, accuracy: 0.81420\n",
            "Epoch: 15/30, step: 258/364, loss: 0.42633, accuracy: 0.81420\n",
            "Epoch: 15/30, step: 259/364, loss: 0.42607, accuracy: 0.81443\n",
            "Epoch: 15/30, step: 260/364, loss: 0.42554, accuracy: 0.81490\n",
            "Epoch: 15/30, step: 261/364, loss: 0.42597, accuracy: 0.81442\n",
            "Epoch: 15/30, step: 262/364, loss: 0.42606, accuracy: 0.81423\n",
            "Epoch: 15/30, step: 263/364, loss: 0.42588, accuracy: 0.81440\n",
            "Epoch: 15/30, step: 264/364, loss: 0.42578, accuracy: 0.81451\n",
            "Epoch: 15/30, step: 265/364, loss: 0.42573, accuracy: 0.81450\n",
            "Epoch: 15/30, step: 266/364, loss: 0.42544, accuracy: 0.81473\n",
            "Epoch: 15/30, step: 267/364, loss: 0.42502, accuracy: 0.81513\n",
            "Epoch: 15/30, step: 268/364, loss: 0.42481, accuracy: 0.81530\n",
            "Epoch: 15/30, step: 269/364, loss: 0.42453, accuracy: 0.81552\n",
            "Epoch: 15/30, step: 270/364, loss: 0.42457, accuracy: 0.81528\n",
            "Epoch: 15/30, step: 271/364, loss: 0.42417, accuracy: 0.81561\n",
            "Epoch: 15/30, step: 272/364, loss: 0.42437, accuracy: 0.81543\n",
            "Epoch: 15/30, step: 273/364, loss: 0.42422, accuracy: 0.81548\n",
            "Epoch: 15/30, step: 274/364, loss: 0.42436, accuracy: 0.81524\n",
            "Epoch: 15/30, step: 275/364, loss: 0.42439, accuracy: 0.81523\n",
            "Epoch: 15/30, step: 276/364, loss: 0.42450, accuracy: 0.81505\n",
            "Epoch: 15/30, step: 277/364, loss: 0.42473, accuracy: 0.81470\n",
            "Epoch: 15/30, step: 278/364, loss: 0.42480, accuracy: 0.81452\n",
            "Epoch: 15/30, step: 279/364, loss: 0.42458, accuracy: 0.81485\n",
            "Epoch: 15/30, step: 280/364, loss: 0.42453, accuracy: 0.81484\n",
            "Epoch: 15/30, step: 281/364, loss: 0.42467, accuracy: 0.81467\n",
            "Epoch: 15/30, step: 282/364, loss: 0.42460, accuracy: 0.81455\n",
            "Epoch: 15/30, step: 283/364, loss: 0.42469, accuracy: 0.81427\n",
            "Epoch: 15/30, step: 284/364, loss: 0.42496, accuracy: 0.81410\n",
            "Epoch: 15/30, step: 285/364, loss: 0.42512, accuracy: 0.81409\n",
            "Epoch: 15/30, step: 286/364, loss: 0.42501, accuracy: 0.81425\n",
            "Epoch: 15/30, step: 287/364, loss: 0.42494, accuracy: 0.81424\n",
            "Epoch: 15/30, step: 288/364, loss: 0.42487, accuracy: 0.81424\n",
            "Epoch: 15/30, step: 289/364, loss: 0.42507, accuracy: 0.81423\n",
            "Epoch: 15/30, step: 290/364, loss: 0.42523, accuracy: 0.81406\n",
            "Epoch: 15/30, step: 291/364, loss: 0.42497, accuracy: 0.81413\n",
            "Epoch: 15/30, train loss: 0.42497, train accuracy: 0.81413, valid loss: 0.62383, valid accuracy: 0.67928\n",
            "Epoch: 16/30, step: 1/364, loss: 0.34340, accuracy: 0.89062\n",
            "Epoch: 16/30, step: 2/364, loss: 0.38153, accuracy: 0.85156\n",
            "Epoch: 16/30, step: 3/364, loss: 0.42390, accuracy: 0.81250\n",
            "Epoch: 16/30, step: 4/364, loss: 0.41082, accuracy: 0.80469\n",
            "Epoch: 16/30, step: 5/364, loss: 0.40946, accuracy: 0.80937\n",
            "Epoch: 16/30, step: 6/364, loss: 0.39578, accuracy: 0.82292\n",
            "Epoch: 16/30, step: 7/364, loss: 0.40700, accuracy: 0.81920\n",
            "Epoch: 16/30, step: 8/364, loss: 0.40497, accuracy: 0.82422\n",
            "Epoch: 16/30, step: 9/364, loss: 0.40151, accuracy: 0.82812\n",
            "Epoch: 16/30, step: 10/364, loss: 0.39345, accuracy: 0.83281\n",
            "Epoch: 16/30, step: 11/364, loss: 0.39360, accuracy: 0.83239\n",
            "Epoch: 16/30, step: 12/364, loss: 0.39554, accuracy: 0.82812\n",
            "Epoch: 16/30, step: 13/364, loss: 0.39950, accuracy: 0.82933\n",
            "Epoch: 16/30, step: 14/364, loss: 0.39429, accuracy: 0.83147\n",
            "Epoch: 16/30, step: 15/364, loss: 0.39881, accuracy: 0.82917\n",
            "Epoch: 16/30, step: 16/364, loss: 0.39964, accuracy: 0.82812\n",
            "Epoch: 16/30, step: 17/364, loss: 0.40432, accuracy: 0.82353\n",
            "Epoch: 16/30, step: 18/364, loss: 0.41286, accuracy: 0.81771\n",
            "Epoch: 16/30, step: 19/364, loss: 0.41246, accuracy: 0.81743\n",
            "Epoch: 16/30, step: 20/364, loss: 0.41348, accuracy: 0.81484\n",
            "Epoch: 16/30, step: 21/364, loss: 0.42078, accuracy: 0.81027\n",
            "Epoch: 16/30, step: 22/364, loss: 0.42106, accuracy: 0.80966\n",
            "Epoch: 16/30, step: 23/364, loss: 0.41694, accuracy: 0.81386\n",
            "Epoch: 16/30, step: 24/364, loss: 0.41840, accuracy: 0.81185\n",
            "Epoch: 16/30, step: 25/364, loss: 0.41587, accuracy: 0.81187\n",
            "Epoch: 16/30, step: 26/364, loss: 0.41483, accuracy: 0.81130\n",
            "Epoch: 16/30, step: 27/364, loss: 0.41526, accuracy: 0.81250\n",
            "Epoch: 16/30, step: 28/364, loss: 0.41480, accuracy: 0.81473\n",
            "Epoch: 16/30, step: 29/364, loss: 0.41196, accuracy: 0.81735\n",
            "Epoch: 16/30, step: 30/364, loss: 0.41376, accuracy: 0.81510\n",
            "Epoch: 16/30, step: 31/364, loss: 0.41277, accuracy: 0.81653\n",
            "Epoch: 16/30, step: 32/364, loss: 0.41481, accuracy: 0.81641\n",
            "Epoch: 16/30, step: 33/364, loss: 0.41536, accuracy: 0.81676\n",
            "Epoch: 16/30, step: 34/364, loss: 0.41283, accuracy: 0.81893\n",
            "Epoch: 16/30, step: 35/364, loss: 0.41122, accuracy: 0.82054\n",
            "Epoch: 16/30, step: 36/364, loss: 0.40926, accuracy: 0.81988\n",
            "Epoch: 16/30, step: 37/364, loss: 0.41080, accuracy: 0.81926\n",
            "Epoch: 16/30, step: 38/364, loss: 0.40997, accuracy: 0.81908\n",
            "Epoch: 16/30, step: 39/364, loss: 0.40747, accuracy: 0.82131\n",
            "Epoch: 16/30, step: 40/364, loss: 0.40795, accuracy: 0.82187\n",
            "Epoch: 16/30, step: 41/364, loss: 0.41029, accuracy: 0.81936\n",
            "Epoch: 16/30, step: 42/364, loss: 0.41070, accuracy: 0.81994\n",
            "Epoch: 16/30, step: 43/364, loss: 0.41125, accuracy: 0.82049\n",
            "Epoch: 16/30, step: 44/364, loss: 0.41111, accuracy: 0.82138\n",
            "Epoch: 16/30, step: 45/364, loss: 0.41114, accuracy: 0.82083\n",
            "Epoch: 16/30, step: 46/364, loss: 0.41029, accuracy: 0.82099\n",
            "Epoch: 16/30, step: 47/364, loss: 0.41049, accuracy: 0.82214\n",
            "Epoch: 16/30, step: 48/364, loss: 0.41174, accuracy: 0.82096\n",
            "Epoch: 16/30, step: 49/364, loss: 0.41122, accuracy: 0.82047\n",
            "Epoch: 16/30, step: 50/364, loss: 0.41126, accuracy: 0.82094\n",
            "Epoch: 16/30, step: 51/364, loss: 0.41195, accuracy: 0.82047\n",
            "Epoch: 16/30, step: 52/364, loss: 0.41253, accuracy: 0.81971\n",
            "Epoch: 16/30, step: 53/364, loss: 0.41366, accuracy: 0.81899\n",
            "Epoch: 16/30, step: 54/364, loss: 0.41277, accuracy: 0.82031\n",
            "Epoch: 16/30, step: 55/364, loss: 0.41346, accuracy: 0.81903\n",
            "Epoch: 16/30, step: 56/364, loss: 0.41211, accuracy: 0.82031\n",
            "Epoch: 16/30, step: 57/364, loss: 0.41205, accuracy: 0.82100\n",
            "Epoch: 16/30, step: 58/364, loss: 0.41158, accuracy: 0.82166\n",
            "Epoch: 16/30, step: 59/364, loss: 0.41123, accuracy: 0.82203\n",
            "Epoch: 16/30, step: 60/364, loss: 0.41379, accuracy: 0.82005\n",
            "Epoch: 16/30, step: 61/364, loss: 0.41361, accuracy: 0.82070\n",
            "Epoch: 16/30, step: 62/364, loss: 0.41448, accuracy: 0.82056\n",
            "Epoch: 16/30, step: 63/364, loss: 0.41384, accuracy: 0.82019\n",
            "Epoch: 16/30, step: 64/364, loss: 0.41287, accuracy: 0.82080\n",
            "Epoch: 16/30, step: 65/364, loss: 0.41188, accuracy: 0.82187\n",
            "Epoch: 16/30, step: 66/364, loss: 0.41094, accuracy: 0.82221\n",
            "Epoch: 16/30, step: 67/364, loss: 0.41043, accuracy: 0.82299\n",
            "Epoch: 16/30, step: 68/364, loss: 0.41059, accuracy: 0.82307\n",
            "Epoch: 16/30, step: 69/364, loss: 0.41185, accuracy: 0.82156\n",
            "Epoch: 16/30, step: 70/364, loss: 0.41193, accuracy: 0.82210\n",
            "Epoch: 16/30, step: 71/364, loss: 0.41193, accuracy: 0.82218\n",
            "Epoch: 16/30, step: 72/364, loss: 0.41086, accuracy: 0.82292\n",
            "Epoch: 16/30, step: 73/364, loss: 0.41138, accuracy: 0.82277\n",
            "Epoch: 16/30, step: 74/364, loss: 0.41128, accuracy: 0.82264\n",
            "Epoch: 16/30, step: 75/364, loss: 0.41160, accuracy: 0.82229\n",
            "Epoch: 16/30, step: 76/364, loss: 0.41091, accuracy: 0.82278\n",
            "Epoch: 16/30, step: 77/364, loss: 0.41045, accuracy: 0.82346\n",
            "Epoch: 16/30, step: 78/364, loss: 0.41093, accuracy: 0.82292\n",
            "Epoch: 16/30, step: 79/364, loss: 0.41131, accuracy: 0.82239\n",
            "Epoch: 16/30, step: 80/364, loss: 0.41149, accuracy: 0.82129\n",
            "Epoch: 16/30, step: 81/364, loss: 0.41117, accuracy: 0.82099\n",
            "Epoch: 16/30, step: 82/364, loss: 0.41064, accuracy: 0.82165\n",
            "Epoch: 16/30, step: 83/364, loss: 0.41091, accuracy: 0.82116\n",
            "Epoch: 16/30, step: 84/364, loss: 0.41106, accuracy: 0.82124\n",
            "Epoch: 16/30, step: 85/364, loss: 0.41070, accuracy: 0.82187\n",
            "Epoch: 16/30, step: 86/364, loss: 0.41107, accuracy: 0.82140\n",
            "Epoch: 16/30, step: 87/364, loss: 0.41198, accuracy: 0.82058\n",
            "Epoch: 16/30, step: 88/364, loss: 0.41136, accuracy: 0.82102\n",
            "Epoch: 16/30, step: 89/364, loss: 0.41042, accuracy: 0.82180\n",
            "Epoch: 16/30, step: 90/364, loss: 0.41070, accuracy: 0.82170\n",
            "Epoch: 16/30, step: 91/364, loss: 0.41083, accuracy: 0.82109\n",
            "Epoch: 16/30, step: 92/364, loss: 0.40999, accuracy: 0.82235\n",
            "Epoch: 16/30, step: 93/364, loss: 0.41075, accuracy: 0.82208\n",
            "Epoch: 16/30, step: 94/364, loss: 0.40997, accuracy: 0.82247\n",
            "Epoch: 16/30, step: 95/364, loss: 0.40953, accuracy: 0.82270\n",
            "Epoch: 16/30, step: 96/364, loss: 0.40981, accuracy: 0.82308\n",
            "Epoch: 16/30, step: 97/364, loss: 0.40994, accuracy: 0.82265\n",
            "Epoch: 16/30, step: 98/364, loss: 0.40930, accuracy: 0.82334\n",
            "Epoch: 16/30, step: 99/364, loss: 0.40906, accuracy: 0.82371\n",
            "Epoch: 16/30, step: 100/364, loss: 0.40919, accuracy: 0.82328\n",
            "Epoch: 16/30, step: 101/364, loss: 0.40957, accuracy: 0.82271\n",
            "Epoch: 16/30, step: 102/364, loss: 0.40825, accuracy: 0.82384\n",
            "Epoch: 16/30, step: 103/364, loss: 0.40753, accuracy: 0.82464\n",
            "Epoch: 16/30, step: 104/364, loss: 0.40733, accuracy: 0.82482\n",
            "Epoch: 16/30, step: 105/364, loss: 0.40791, accuracy: 0.82426\n",
            "Epoch: 16/30, step: 106/364, loss: 0.40812, accuracy: 0.82400\n",
            "Epoch: 16/30, step: 107/364, loss: 0.40785, accuracy: 0.82433\n",
            "Epoch: 16/30, step: 108/364, loss: 0.40803, accuracy: 0.82393\n",
            "Epoch: 16/30, step: 109/364, loss: 0.40816, accuracy: 0.82382\n",
            "Epoch: 16/30, step: 110/364, loss: 0.40975, accuracy: 0.82230\n",
            "Epoch: 16/30, step: 111/364, loss: 0.40920, accuracy: 0.82249\n",
            "Epoch: 16/30, step: 112/364, loss: 0.40907, accuracy: 0.82254\n",
            "Epoch: 16/30, step: 113/364, loss: 0.40836, accuracy: 0.82315\n",
            "Epoch: 16/30, step: 114/364, loss: 0.40791, accuracy: 0.82346\n",
            "Epoch: 16/30, step: 115/364, loss: 0.40841, accuracy: 0.82310\n",
            "Epoch: 16/30, step: 116/364, loss: 0.40800, accuracy: 0.82368\n",
            "Epoch: 16/30, step: 117/364, loss: 0.40760, accuracy: 0.82425\n",
            "Epoch: 16/30, step: 118/364, loss: 0.40764, accuracy: 0.82428\n",
            "Epoch: 16/30, step: 119/364, loss: 0.40786, accuracy: 0.82405\n",
            "Epoch: 16/30, step: 120/364, loss: 0.40791, accuracy: 0.82357\n",
            "Epoch: 16/30, step: 121/364, loss: 0.40701, accuracy: 0.82412\n",
            "Epoch: 16/30, step: 122/364, loss: 0.40718, accuracy: 0.82377\n",
            "Epoch: 16/30, step: 123/364, loss: 0.40717, accuracy: 0.82381\n",
            "Epoch: 16/30, step: 124/364, loss: 0.40645, accuracy: 0.82472\n",
            "Epoch: 16/30, step: 125/364, loss: 0.40665, accuracy: 0.82437\n",
            "Epoch: 16/30, step: 126/364, loss: 0.40628, accuracy: 0.82490\n",
            "Epoch: 16/30, step: 127/364, loss: 0.40641, accuracy: 0.82493\n",
            "Epoch: 16/30, step: 128/364, loss: 0.40643, accuracy: 0.82520\n",
            "Epoch: 16/30, step: 129/364, loss: 0.40691, accuracy: 0.82425\n",
            "Epoch: 16/30, step: 130/364, loss: 0.40626, accuracy: 0.82500\n",
            "Epoch: 16/30, step: 131/364, loss: 0.40617, accuracy: 0.82479\n",
            "Epoch: 16/30, step: 132/364, loss: 0.40563, accuracy: 0.82481\n",
            "Epoch: 16/30, step: 133/364, loss: 0.40625, accuracy: 0.82437\n",
            "Epoch: 16/30, step: 134/364, loss: 0.40680, accuracy: 0.82393\n",
            "Epoch: 16/30, step: 135/364, loss: 0.40656, accuracy: 0.82396\n",
            "Epoch: 16/30, step: 136/364, loss: 0.40675, accuracy: 0.82376\n",
            "Epoch: 16/30, step: 137/364, loss: 0.40711, accuracy: 0.82356\n",
            "Epoch: 16/30, step: 138/364, loss: 0.40668, accuracy: 0.82405\n",
            "Epoch: 16/30, step: 139/364, loss: 0.40682, accuracy: 0.82385\n",
            "Epoch: 16/30, step: 140/364, loss: 0.40683, accuracy: 0.82388\n",
            "Epoch: 16/30, step: 141/364, loss: 0.40636, accuracy: 0.82436\n",
            "Epoch: 16/30, step: 142/364, loss: 0.40611, accuracy: 0.82438\n",
            "Epoch: 16/30, step: 143/364, loss: 0.40554, accuracy: 0.82474\n",
            "Epoch: 16/30, step: 144/364, loss: 0.40525, accuracy: 0.82465\n",
            "Epoch: 16/30, step: 145/364, loss: 0.40478, accuracy: 0.82500\n",
            "Epoch: 16/30, step: 146/364, loss: 0.40464, accuracy: 0.82524\n",
            "Epoch: 16/30, step: 147/364, loss: 0.40414, accuracy: 0.82526\n",
            "Epoch: 16/30, step: 148/364, loss: 0.40497, accuracy: 0.82517\n",
            "Epoch: 16/30, step: 149/364, loss: 0.40530, accuracy: 0.82498\n",
            "Epoch: 16/30, step: 150/364, loss: 0.40536, accuracy: 0.82500\n",
            "Epoch: 16/30, step: 151/364, loss: 0.40511, accuracy: 0.82523\n",
            "Epoch: 16/30, step: 152/364, loss: 0.40549, accuracy: 0.82463\n",
            "Epoch: 16/30, step: 153/364, loss: 0.40524, accuracy: 0.82486\n",
            "Epoch: 16/30, step: 154/364, loss: 0.40522, accuracy: 0.82508\n",
            "Epoch: 16/30, step: 155/364, loss: 0.40479, accuracy: 0.82520\n",
            "Epoch: 16/30, step: 156/364, loss: 0.40494, accuracy: 0.82512\n",
            "Epoch: 16/30, step: 157/364, loss: 0.40485, accuracy: 0.82534\n",
            "Epoch: 16/30, step: 158/364, loss: 0.40441, accuracy: 0.82595\n",
            "Epoch: 16/30, step: 159/364, loss: 0.40407, accuracy: 0.82606\n",
            "Epoch: 16/30, step: 160/364, loss: 0.40430, accuracy: 0.82588\n",
            "Epoch: 16/30, step: 161/364, loss: 0.40439, accuracy: 0.82599\n",
            "Epoch: 16/30, step: 162/364, loss: 0.40421, accuracy: 0.82610\n",
            "Epoch: 16/30, step: 163/364, loss: 0.40423, accuracy: 0.82611\n",
            "Epoch: 16/30, step: 164/364, loss: 0.40429, accuracy: 0.82603\n",
            "Epoch: 16/30, step: 165/364, loss: 0.40456, accuracy: 0.82547\n",
            "Epoch: 16/30, step: 166/364, loss: 0.40431, accuracy: 0.82558\n",
            "Epoch: 16/30, step: 167/364, loss: 0.40376, accuracy: 0.82607\n",
            "Epoch: 16/30, step: 168/364, loss: 0.40355, accuracy: 0.82617\n",
            "Epoch: 16/30, step: 169/364, loss: 0.40335, accuracy: 0.82665\n",
            "Epoch: 16/30, step: 170/364, loss: 0.40365, accuracy: 0.82619\n",
            "Epoch: 16/30, step: 171/364, loss: 0.40366, accuracy: 0.82666\n",
            "Epoch: 16/30, step: 172/364, loss: 0.40365, accuracy: 0.82676\n",
            "Epoch: 16/30, step: 173/364, loss: 0.40370, accuracy: 0.82695\n",
            "Epoch: 16/30, step: 174/364, loss: 0.40434, accuracy: 0.82669\n",
            "Epoch: 16/30, step: 175/364, loss: 0.40463, accuracy: 0.82643\n",
            "Epoch: 16/30, step: 176/364, loss: 0.40411, accuracy: 0.82688\n",
            "Epoch: 16/30, step: 177/364, loss: 0.40509, accuracy: 0.82627\n",
            "Epoch: 16/30, step: 178/364, loss: 0.40501, accuracy: 0.82619\n",
            "Epoch: 16/30, step: 179/364, loss: 0.40483, accuracy: 0.82638\n",
            "Epoch: 16/30, step: 180/364, loss: 0.40502, accuracy: 0.82630\n",
            "Epoch: 16/30, step: 181/364, loss: 0.40492, accuracy: 0.82648\n",
            "Epoch: 16/30, step: 182/364, loss: 0.40530, accuracy: 0.82615\n",
            "Epoch: 16/30, step: 183/364, loss: 0.40541, accuracy: 0.82599\n",
            "Epoch: 16/30, step: 184/364, loss: 0.40495, accuracy: 0.82660\n",
            "Epoch: 16/30, step: 185/364, loss: 0.40517, accuracy: 0.82644\n",
            "Epoch: 16/30, step: 186/364, loss: 0.40468, accuracy: 0.82670\n",
            "Epoch: 16/30, step: 187/364, loss: 0.40487, accuracy: 0.82645\n",
            "Epoch: 16/30, step: 188/364, loss: 0.40428, accuracy: 0.82680\n",
            "Epoch: 16/30, step: 189/364, loss: 0.40416, accuracy: 0.82722\n",
            "Epoch: 16/30, step: 190/364, loss: 0.40401, accuracy: 0.82730\n",
            "Epoch: 16/30, step: 191/364, loss: 0.40342, accuracy: 0.82788\n",
            "Epoch: 16/30, step: 192/364, loss: 0.40335, accuracy: 0.82788\n",
            "Epoch: 16/30, step: 193/364, loss: 0.40365, accuracy: 0.82764\n",
            "Epoch: 16/30, step: 194/364, loss: 0.40335, accuracy: 0.82796\n",
            "Epoch: 16/30, step: 195/364, loss: 0.40364, accuracy: 0.82780\n",
            "Epoch: 16/30, step: 196/364, loss: 0.40303, accuracy: 0.82812\n",
            "Epoch: 16/30, step: 197/364, loss: 0.40279, accuracy: 0.82844\n",
            "Epoch: 16/30, step: 198/364, loss: 0.40290, accuracy: 0.82805\n",
            "Epoch: 16/30, step: 199/364, loss: 0.40296, accuracy: 0.82805\n",
            "Epoch: 16/30, step: 200/364, loss: 0.40244, accuracy: 0.82820\n",
            "Epoch: 16/30, step: 201/364, loss: 0.40243, accuracy: 0.82805\n",
            "Epoch: 16/30, step: 202/364, loss: 0.40221, accuracy: 0.82828\n",
            "Epoch: 16/30, step: 203/364, loss: 0.40183, accuracy: 0.82851\n",
            "Epoch: 16/30, step: 204/364, loss: 0.40233, accuracy: 0.82790\n",
            "Epoch: 16/30, step: 205/364, loss: 0.40222, accuracy: 0.82797\n",
            "Epoch: 16/30, step: 206/364, loss: 0.40188, accuracy: 0.82805\n",
            "Epoch: 16/30, step: 207/364, loss: 0.40193, accuracy: 0.82797\n",
            "Epoch: 16/30, step: 208/364, loss: 0.40176, accuracy: 0.82805\n",
            "Epoch: 16/30, step: 209/364, loss: 0.40170, accuracy: 0.82798\n",
            "Epoch: 16/30, step: 210/364, loss: 0.40181, accuracy: 0.82783\n",
            "Epoch: 16/30, step: 211/364, loss: 0.40182, accuracy: 0.82775\n",
            "Epoch: 16/30, step: 212/364, loss: 0.40166, accuracy: 0.82776\n",
            "Epoch: 16/30, step: 213/364, loss: 0.40140, accuracy: 0.82798\n",
            "Epoch: 16/30, step: 214/364, loss: 0.40116, accuracy: 0.82812\n",
            "Epoch: 16/30, step: 215/364, loss: 0.40078, accuracy: 0.82849\n",
            "Epoch: 16/30, step: 216/364, loss: 0.40124, accuracy: 0.82827\n",
            "Epoch: 16/30, step: 217/364, loss: 0.40108, accuracy: 0.82834\n",
            "Epoch: 16/30, step: 218/364, loss: 0.40136, accuracy: 0.82805\n",
            "Epoch: 16/30, step: 219/364, loss: 0.40095, accuracy: 0.82841\n",
            "Epoch: 16/30, step: 220/364, loss: 0.40112, accuracy: 0.82834\n",
            "Epoch: 16/30, step: 221/364, loss: 0.40138, accuracy: 0.82791\n",
            "Epoch: 16/30, step: 222/364, loss: 0.40176, accuracy: 0.82749\n",
            "Epoch: 16/30, step: 223/364, loss: 0.40160, accuracy: 0.82763\n",
            "Epoch: 16/30, step: 224/364, loss: 0.40150, accuracy: 0.82750\n",
            "Epoch: 16/30, step: 225/364, loss: 0.40127, accuracy: 0.82743\n",
            "Epoch: 16/30, step: 226/364, loss: 0.40136, accuracy: 0.82736\n",
            "Epoch: 16/30, step: 227/364, loss: 0.40137, accuracy: 0.82744\n",
            "Epoch: 16/30, step: 228/364, loss: 0.40148, accuracy: 0.82737\n",
            "Epoch: 16/30, step: 229/364, loss: 0.40110, accuracy: 0.82772\n",
            "Epoch: 16/30, step: 230/364, loss: 0.40087, accuracy: 0.82785\n",
            "Epoch: 16/30, step: 231/364, loss: 0.40131, accuracy: 0.82738\n",
            "Epoch: 16/30, step: 232/364, loss: 0.40121, accuracy: 0.82711\n",
            "Epoch: 16/30, step: 233/364, loss: 0.40097, accuracy: 0.82739\n",
            "Epoch: 16/30, step: 234/364, loss: 0.40125, accuracy: 0.82726\n",
            "Epoch: 16/30, step: 235/364, loss: 0.40115, accuracy: 0.82726\n",
            "Epoch: 16/30, step: 236/364, loss: 0.40099, accuracy: 0.82740\n",
            "Epoch: 16/30, step: 237/364, loss: 0.40047, accuracy: 0.82773\n",
            "Epoch: 16/30, step: 238/364, loss: 0.40039, accuracy: 0.82786\n",
            "Epoch: 16/30, step: 239/364, loss: 0.40025, accuracy: 0.82826\n",
            "Epoch: 16/30, step: 240/364, loss: 0.40007, accuracy: 0.82819\n",
            "Epoch: 16/30, step: 241/364, loss: 0.40006, accuracy: 0.82806\n",
            "Epoch: 16/30, step: 242/364, loss: 0.40032, accuracy: 0.82767\n",
            "Epoch: 16/30, step: 243/364, loss: 0.40010, accuracy: 0.82793\n",
            "Epoch: 16/30, step: 244/364, loss: 0.39994, accuracy: 0.82819\n",
            "Epoch: 16/30, step: 245/364, loss: 0.39995, accuracy: 0.82812\n",
            "Epoch: 16/30, step: 246/364, loss: 0.39992, accuracy: 0.82825\n",
            "Epoch: 16/30, step: 247/364, loss: 0.40012, accuracy: 0.82806\n",
            "Epoch: 16/30, step: 248/364, loss: 0.40009, accuracy: 0.82812\n",
            "Epoch: 16/30, step: 249/364, loss: 0.39977, accuracy: 0.82844\n",
            "Epoch: 16/30, step: 250/364, loss: 0.39983, accuracy: 0.82831\n",
            "Epoch: 16/30, step: 251/364, loss: 0.39962, accuracy: 0.82837\n",
            "Epoch: 16/30, step: 252/364, loss: 0.39965, accuracy: 0.82825\n",
            "Epoch: 16/30, step: 253/364, loss: 0.39952, accuracy: 0.82831\n",
            "Epoch: 16/30, step: 254/364, loss: 0.39917, accuracy: 0.82856\n",
            "Epoch: 16/30, step: 255/364, loss: 0.39895, accuracy: 0.82868\n",
            "Epoch: 16/30, step: 256/364, loss: 0.39916, accuracy: 0.82861\n",
            "Epoch: 16/30, step: 257/364, loss: 0.39943, accuracy: 0.82831\n",
            "Epoch: 16/30, step: 258/364, loss: 0.39929, accuracy: 0.82837\n",
            "Epoch: 16/30, step: 259/364, loss: 0.39910, accuracy: 0.82861\n",
            "Epoch: 16/30, step: 260/364, loss: 0.39920, accuracy: 0.82843\n",
            "Epoch: 16/30, step: 261/364, loss: 0.39897, accuracy: 0.82860\n",
            "Epoch: 16/30, step: 262/364, loss: 0.39903, accuracy: 0.82854\n",
            "Epoch: 16/30, step: 263/364, loss: 0.39867, accuracy: 0.82872\n",
            "Epoch: 16/30, step: 264/364, loss: 0.39897, accuracy: 0.82866\n",
            "Epoch: 16/30, step: 265/364, loss: 0.39876, accuracy: 0.82877\n",
            "Epoch: 16/30, step: 266/364, loss: 0.39851, accuracy: 0.82906\n",
            "Epoch: 16/30, step: 267/364, loss: 0.39870, accuracy: 0.82883\n",
            "Epoch: 16/30, step: 268/364, loss: 0.39890, accuracy: 0.82882\n",
            "Epoch: 16/30, step: 269/364, loss: 0.39854, accuracy: 0.82911\n",
            "Epoch: 16/30, step: 270/364, loss: 0.39836, accuracy: 0.82917\n",
            "Epoch: 16/30, step: 271/364, loss: 0.39844, accuracy: 0.82922\n",
            "Epoch: 16/30, step: 272/364, loss: 0.39843, accuracy: 0.82916\n",
            "Epoch: 16/30, step: 273/364, loss: 0.39852, accuracy: 0.82904\n",
            "Epoch: 16/30, step: 274/364, loss: 0.39850, accuracy: 0.82904\n",
            "Epoch: 16/30, step: 275/364, loss: 0.39852, accuracy: 0.82892\n",
            "Epoch: 16/30, step: 276/364, loss: 0.39876, accuracy: 0.82886\n",
            "Epoch: 16/30, step: 277/364, loss: 0.39867, accuracy: 0.82886\n",
            "Epoch: 16/30, step: 278/364, loss: 0.39835, accuracy: 0.82902\n",
            "Epoch: 16/30, step: 279/364, loss: 0.39859, accuracy: 0.82874\n",
            "Epoch: 16/30, step: 280/364, loss: 0.39845, accuracy: 0.82885\n",
            "Epoch: 16/30, step: 281/364, loss: 0.39830, accuracy: 0.82907\n",
            "Epoch: 16/30, step: 282/364, loss: 0.39837, accuracy: 0.82912\n",
            "Epoch: 16/30, step: 283/364, loss: 0.39853, accuracy: 0.82895\n",
            "Epoch: 16/30, step: 284/364, loss: 0.39859, accuracy: 0.82901\n",
            "Epoch: 16/30, step: 285/364, loss: 0.39862, accuracy: 0.82895\n",
            "Epoch: 16/30, step: 286/364, loss: 0.39881, accuracy: 0.82905\n",
            "Epoch: 16/30, step: 287/364, loss: 0.39868, accuracy: 0.82894\n",
            "Epoch: 16/30, step: 288/364, loss: 0.39900, accuracy: 0.82861\n",
            "Epoch: 16/30, step: 289/364, loss: 0.39890, accuracy: 0.82877\n",
            "Epoch: 16/30, step: 290/364, loss: 0.39909, accuracy: 0.82866\n",
            "Epoch: 16/30, step: 291/364, loss: 0.39968, accuracy: 0.82837\n",
            "Epoch: 16/30, train loss: 0.39968, train accuracy: 0.82837, valid loss: 0.63173, valid accuracy: 0.67949\n",
            "Epoch: 17/30, step: 1/364, loss: 0.30532, accuracy: 0.90625\n",
            "Epoch: 17/30, step: 2/364, loss: 0.30788, accuracy: 0.90625\n",
            "Epoch: 17/30, step: 3/364, loss: 0.33613, accuracy: 0.88021\n",
            "Epoch: 17/30, step: 4/364, loss: 0.35380, accuracy: 0.86328\n",
            "Epoch: 17/30, step: 5/364, loss: 0.36139, accuracy: 0.85938\n",
            "Epoch: 17/30, step: 6/364, loss: 0.36321, accuracy: 0.84896\n",
            "Epoch: 17/30, step: 7/364, loss: 0.37077, accuracy: 0.84152\n",
            "Epoch: 17/30, step: 8/364, loss: 0.36806, accuracy: 0.84766\n",
            "Epoch: 17/30, step: 9/364, loss: 0.37515, accuracy: 0.84028\n",
            "Epoch: 17/30, step: 10/364, loss: 0.37308, accuracy: 0.83906\n",
            "Epoch: 17/30, step: 11/364, loss: 0.36558, accuracy: 0.84801\n",
            "Epoch: 17/30, step: 12/364, loss: 0.36647, accuracy: 0.85156\n",
            "Epoch: 17/30, step: 13/364, loss: 0.36792, accuracy: 0.85096\n",
            "Epoch: 17/30, step: 14/364, loss: 0.36493, accuracy: 0.85379\n",
            "Epoch: 17/30, step: 15/364, loss: 0.37447, accuracy: 0.85000\n",
            "Epoch: 17/30, step: 16/364, loss: 0.38014, accuracy: 0.84570\n",
            "Epoch: 17/30, step: 17/364, loss: 0.38186, accuracy: 0.84191\n",
            "Epoch: 17/30, step: 18/364, loss: 0.38471, accuracy: 0.83854\n",
            "Epoch: 17/30, step: 19/364, loss: 0.37978, accuracy: 0.84128\n",
            "Epoch: 17/30, step: 20/364, loss: 0.38602, accuracy: 0.83750\n",
            "Epoch: 17/30, step: 21/364, loss: 0.38577, accuracy: 0.83854\n",
            "Epoch: 17/30, step: 22/364, loss: 0.38426, accuracy: 0.84020\n",
            "Epoch: 17/30, step: 23/364, loss: 0.38245, accuracy: 0.84171\n",
            "Epoch: 17/30, step: 24/364, loss: 0.37968, accuracy: 0.84375\n",
            "Epoch: 17/30, step: 25/364, loss: 0.37779, accuracy: 0.84562\n",
            "Epoch: 17/30, step: 26/364, loss: 0.37760, accuracy: 0.84495\n",
            "Epoch: 17/30, step: 27/364, loss: 0.37378, accuracy: 0.84896\n",
            "Epoch: 17/30, step: 28/364, loss: 0.37606, accuracy: 0.84821\n",
            "Epoch: 17/30, step: 29/364, loss: 0.37491, accuracy: 0.85022\n",
            "Epoch: 17/30, step: 30/364, loss: 0.37464, accuracy: 0.84896\n",
            "Epoch: 17/30, step: 31/364, loss: 0.37568, accuracy: 0.84778\n",
            "Epoch: 17/30, step: 32/364, loss: 0.37290, accuracy: 0.84961\n",
            "Epoch: 17/30, step: 33/364, loss: 0.37318, accuracy: 0.84943\n",
            "Epoch: 17/30, step: 34/364, loss: 0.37683, accuracy: 0.84651\n",
            "Epoch: 17/30, step: 35/364, loss: 0.37822, accuracy: 0.84330\n",
            "Epoch: 17/30, step: 36/364, loss: 0.37686, accuracy: 0.84418\n",
            "Epoch: 17/30, step: 37/364, loss: 0.37695, accuracy: 0.84333\n",
            "Epoch: 17/30, step: 38/364, loss: 0.37853, accuracy: 0.84128\n",
            "Epoch: 17/30, step: 39/364, loss: 0.37903, accuracy: 0.84095\n",
            "Epoch: 17/30, step: 40/364, loss: 0.37907, accuracy: 0.84180\n",
            "Epoch: 17/30, step: 41/364, loss: 0.37685, accuracy: 0.84375\n",
            "Epoch: 17/30, step: 42/364, loss: 0.37584, accuracy: 0.84449\n",
            "Epoch: 17/30, step: 43/364, loss: 0.37608, accuracy: 0.84411\n",
            "Epoch: 17/30, step: 44/364, loss: 0.37682, accuracy: 0.84304\n",
            "Epoch: 17/30, step: 45/364, loss: 0.37570, accuracy: 0.84375\n",
            "Epoch: 17/30, step: 46/364, loss: 0.37697, accuracy: 0.84205\n",
            "Epoch: 17/30, step: 47/364, loss: 0.37769, accuracy: 0.84109\n",
            "Epoch: 17/30, step: 48/364, loss: 0.37678, accuracy: 0.84180\n",
            "Epoch: 17/30, step: 49/364, loss: 0.37533, accuracy: 0.84279\n",
            "Epoch: 17/30, step: 50/364, loss: 0.37548, accuracy: 0.84250\n",
            "Epoch: 17/30, step: 51/364, loss: 0.37449, accuracy: 0.84406\n",
            "Epoch: 17/30, step: 52/364, loss: 0.37623, accuracy: 0.84315\n",
            "Epoch: 17/30, step: 53/364, loss: 0.37759, accuracy: 0.84110\n",
            "Epoch: 17/30, step: 54/364, loss: 0.37838, accuracy: 0.83999\n",
            "Epoch: 17/30, step: 55/364, loss: 0.37843, accuracy: 0.83949\n",
            "Epoch: 17/30, step: 56/364, loss: 0.37790, accuracy: 0.84012\n",
            "Epoch: 17/30, step: 57/364, loss: 0.37856, accuracy: 0.83827\n",
            "Epoch: 17/30, step: 58/364, loss: 0.38095, accuracy: 0.83621\n",
            "Epoch: 17/30, step: 59/364, loss: 0.38105, accuracy: 0.83633\n",
            "Epoch: 17/30, step: 60/364, loss: 0.38168, accuracy: 0.83646\n",
            "Epoch: 17/30, step: 61/364, loss: 0.38159, accuracy: 0.83683\n",
            "Epoch: 17/30, step: 62/364, loss: 0.38130, accuracy: 0.83720\n",
            "Epoch: 17/30, step: 63/364, loss: 0.38166, accuracy: 0.83730\n",
            "Epoch: 17/30, step: 64/364, loss: 0.38050, accuracy: 0.83838\n",
            "Epoch: 17/30, step: 65/364, loss: 0.38193, accuracy: 0.83726\n",
            "Epoch: 17/30, step: 66/364, loss: 0.38223, accuracy: 0.83688\n",
            "Epoch: 17/30, step: 67/364, loss: 0.38102, accuracy: 0.83769\n",
            "Epoch: 17/30, step: 68/364, loss: 0.38187, accuracy: 0.83686\n",
            "Epoch: 17/30, step: 69/364, loss: 0.38220, accuracy: 0.83650\n",
            "Epoch: 17/30, step: 70/364, loss: 0.38296, accuracy: 0.83638\n",
            "Epoch: 17/30, step: 71/364, loss: 0.38311, accuracy: 0.83539\n",
            "Epoch: 17/30, step: 72/364, loss: 0.38319, accuracy: 0.83550\n",
            "Epoch: 17/30, step: 73/364, loss: 0.38326, accuracy: 0.83583\n",
            "Epoch: 17/30, step: 74/364, loss: 0.38252, accuracy: 0.83636\n",
            "Epoch: 17/30, step: 75/364, loss: 0.38223, accuracy: 0.83667\n",
            "Epoch: 17/30, step: 76/364, loss: 0.38288, accuracy: 0.83655\n",
            "Epoch: 17/30, step: 77/364, loss: 0.38268, accuracy: 0.83705\n",
            "Epoch: 17/30, step: 78/364, loss: 0.38170, accuracy: 0.83754\n",
            "Epoch: 17/30, step: 79/364, loss: 0.38076, accuracy: 0.83782\n",
            "Epoch: 17/30, step: 80/364, loss: 0.38186, accuracy: 0.83711\n",
            "Epoch: 17/30, step: 81/364, loss: 0.38240, accuracy: 0.83719\n",
            "Epoch: 17/30, step: 82/364, loss: 0.38356, accuracy: 0.83575\n",
            "Epoch: 17/30, step: 83/364, loss: 0.38472, accuracy: 0.83453\n",
            "Epoch: 17/30, step: 84/364, loss: 0.38495, accuracy: 0.83464\n",
            "Epoch: 17/30, step: 85/364, loss: 0.38609, accuracy: 0.83346\n",
            "Epoch: 17/30, step: 86/364, loss: 0.38848, accuracy: 0.83194\n",
            "Epoch: 17/30, step: 87/364, loss: 0.38791, accuracy: 0.83208\n",
            "Epoch: 17/30, step: 88/364, loss: 0.38771, accuracy: 0.83239\n",
            "Epoch: 17/30, step: 89/364, loss: 0.38783, accuracy: 0.83251\n",
            "Epoch: 17/30, step: 90/364, loss: 0.38796, accuracy: 0.83177\n",
            "Epoch: 17/30, step: 91/364, loss: 0.38753, accuracy: 0.83242\n",
            "Epoch: 17/30, step: 92/364, loss: 0.38880, accuracy: 0.83101\n",
            "Epoch: 17/30, step: 93/364, loss: 0.38954, accuracy: 0.83081\n",
            "Epoch: 17/30, step: 94/364, loss: 0.39131, accuracy: 0.82929\n",
            "Epoch: 17/30, step: 95/364, loss: 0.39110, accuracy: 0.82977\n",
            "Epoch: 17/30, step: 96/364, loss: 0.39069, accuracy: 0.82975\n",
            "Epoch: 17/30, step: 97/364, loss: 0.39095, accuracy: 0.82957\n",
            "Epoch: 17/30, step: 98/364, loss: 0.39045, accuracy: 0.82988\n",
            "Epoch: 17/30, step: 99/364, loss: 0.39181, accuracy: 0.82891\n",
            "Epoch: 17/30, step: 100/364, loss: 0.39206, accuracy: 0.82844\n",
            "Epoch: 17/30, step: 101/364, loss: 0.39349, accuracy: 0.82766\n",
            "Epoch: 17/30, step: 102/364, loss: 0.39244, accuracy: 0.82828\n",
            "Epoch: 17/30, step: 103/364, loss: 0.39248, accuracy: 0.82858\n",
            "Epoch: 17/30, step: 104/364, loss: 0.39256, accuracy: 0.82873\n",
            "Epoch: 17/30, step: 105/364, loss: 0.39255, accuracy: 0.82872\n",
            "Epoch: 17/30, step: 106/364, loss: 0.39220, accuracy: 0.82871\n",
            "Epoch: 17/30, step: 107/364, loss: 0.39214, accuracy: 0.82856\n",
            "Epoch: 17/30, step: 108/364, loss: 0.39142, accuracy: 0.82885\n",
            "Epoch: 17/30, step: 109/364, loss: 0.39110, accuracy: 0.82899\n",
            "Epoch: 17/30, step: 110/364, loss: 0.39085, accuracy: 0.82926\n",
            "Epoch: 17/30, step: 111/364, loss: 0.39036, accuracy: 0.82995\n",
            "Epoch: 17/30, step: 112/364, loss: 0.39013, accuracy: 0.83008\n",
            "Epoch: 17/30, step: 113/364, loss: 0.38915, accuracy: 0.83089\n",
            "Epoch: 17/30, step: 114/364, loss: 0.38883, accuracy: 0.83114\n",
            "Epoch: 17/30, step: 115/364, loss: 0.38876, accuracy: 0.83111\n",
            "Epoch: 17/30, step: 116/364, loss: 0.38868, accuracy: 0.83122\n",
            "Epoch: 17/30, step: 117/364, loss: 0.38976, accuracy: 0.83040\n",
            "Epoch: 17/30, step: 118/364, loss: 0.39034, accuracy: 0.82985\n",
            "Epoch: 17/30, step: 119/364, loss: 0.39055, accuracy: 0.82983\n",
            "Epoch: 17/30, step: 120/364, loss: 0.39017, accuracy: 0.82995\n",
            "Epoch: 17/30, step: 121/364, loss: 0.39003, accuracy: 0.82993\n",
            "Epoch: 17/30, step: 122/364, loss: 0.39021, accuracy: 0.82979\n",
            "Epoch: 17/30, step: 123/364, loss: 0.39024, accuracy: 0.82990\n",
            "Epoch: 17/30, step: 124/364, loss: 0.39001, accuracy: 0.83002\n",
            "Epoch: 17/30, step: 125/364, loss: 0.38962, accuracy: 0.83025\n",
            "Epoch: 17/30, step: 126/364, loss: 0.39011, accuracy: 0.82974\n",
            "Epoch: 17/30, step: 127/364, loss: 0.39086, accuracy: 0.82911\n",
            "Epoch: 17/30, step: 128/364, loss: 0.39054, accuracy: 0.82947\n",
            "Epoch: 17/30, step: 129/364, loss: 0.38971, accuracy: 0.83006\n",
            "Epoch: 17/30, step: 130/364, loss: 0.38959, accuracy: 0.82993\n",
            "Epoch: 17/30, step: 131/364, loss: 0.38912, accuracy: 0.83003\n",
            "Epoch: 17/30, step: 132/364, loss: 0.38950, accuracy: 0.82955\n",
            "Epoch: 17/30, step: 133/364, loss: 0.38951, accuracy: 0.82953\n",
            "Epoch: 17/30, step: 134/364, loss: 0.38931, accuracy: 0.82964\n",
            "Epoch: 17/30, step: 135/364, loss: 0.38950, accuracy: 0.82951\n",
            "Epoch: 17/30, step: 136/364, loss: 0.38916, accuracy: 0.82985\n",
            "Epoch: 17/30, step: 137/364, loss: 0.38879, accuracy: 0.82984\n",
            "Epoch: 17/30, step: 138/364, loss: 0.38845, accuracy: 0.83016\n",
            "Epoch: 17/30, step: 139/364, loss: 0.38809, accuracy: 0.83049\n",
            "Epoch: 17/30, step: 140/364, loss: 0.38807, accuracy: 0.83069\n",
            "Epoch: 17/30, step: 141/364, loss: 0.38854, accuracy: 0.83045\n",
            "Epoch: 17/30, step: 142/364, loss: 0.38867, accuracy: 0.83066\n",
            "Epoch: 17/30, step: 143/364, loss: 0.38834, accuracy: 0.83086\n",
            "Epoch: 17/30, step: 144/364, loss: 0.38801, accuracy: 0.83095\n",
            "Epoch: 17/30, step: 145/364, loss: 0.38829, accuracy: 0.83060\n",
            "Epoch: 17/30, step: 146/364, loss: 0.38924, accuracy: 0.83016\n",
            "Epoch: 17/30, step: 147/364, loss: 0.38924, accuracy: 0.83025\n",
            "Epoch: 17/30, step: 148/364, loss: 0.38928, accuracy: 0.83034\n",
            "Epoch: 17/30, step: 149/364, loss: 0.38888, accuracy: 0.83075\n",
            "Epoch: 17/30, step: 150/364, loss: 0.38879, accuracy: 0.83073\n",
            "Epoch: 17/30, step: 151/364, loss: 0.38852, accuracy: 0.83102\n",
            "Epoch: 17/30, step: 152/364, loss: 0.38852, accuracy: 0.83090\n",
            "Epoch: 17/30, step: 153/364, loss: 0.38842, accuracy: 0.83109\n",
            "Epoch: 17/30, step: 154/364, loss: 0.38861, accuracy: 0.83107\n",
            "Epoch: 17/30, step: 155/364, loss: 0.38820, accuracy: 0.83135\n",
            "Epoch: 17/30, step: 156/364, loss: 0.38840, accuracy: 0.83103\n",
            "Epoch: 17/30, step: 157/364, loss: 0.38804, accuracy: 0.83121\n",
            "Epoch: 17/30, step: 158/364, loss: 0.38802, accuracy: 0.83109\n",
            "Epoch: 17/30, step: 159/364, loss: 0.38889, accuracy: 0.83078\n",
            "Epoch: 17/30, step: 160/364, loss: 0.38842, accuracy: 0.83105\n",
            "Epoch: 17/30, step: 161/364, loss: 0.38913, accuracy: 0.83045\n",
            "Epoch: 17/30, step: 162/364, loss: 0.38880, accuracy: 0.83083\n",
            "Epoch: 17/30, step: 163/364, loss: 0.38895, accuracy: 0.83043\n",
            "Epoch: 17/30, step: 164/364, loss: 0.38886, accuracy: 0.83032\n",
            "Epoch: 17/30, step: 165/364, loss: 0.38866, accuracy: 0.83049\n",
            "Epoch: 17/30, step: 166/364, loss: 0.38943, accuracy: 0.82991\n",
            "Epoch: 17/30, step: 167/364, loss: 0.38925, accuracy: 0.83018\n",
            "Epoch: 17/30, step: 168/364, loss: 0.38911, accuracy: 0.83036\n",
            "Epoch: 17/30, step: 169/364, loss: 0.38896, accuracy: 0.83025\n",
            "Epoch: 17/30, step: 170/364, loss: 0.38882, accuracy: 0.83033\n",
            "Epoch: 17/30, step: 171/364, loss: 0.38882, accuracy: 0.83032\n",
            "Epoch: 17/30, step: 172/364, loss: 0.38853, accuracy: 0.83049\n",
            "Epoch: 17/30, step: 173/364, loss: 0.38856, accuracy: 0.83047\n",
            "Epoch: 17/30, step: 174/364, loss: 0.38822, accuracy: 0.83091\n",
            "Epoch: 17/30, step: 175/364, loss: 0.38784, accuracy: 0.83107\n",
            "Epoch: 17/30, step: 176/364, loss: 0.38766, accuracy: 0.83132\n",
            "Epoch: 17/30, step: 177/364, loss: 0.38779, accuracy: 0.83121\n",
            "Epoch: 17/30, step: 178/364, loss: 0.38761, accuracy: 0.83146\n",
            "Epoch: 17/30, step: 179/364, loss: 0.38769, accuracy: 0.83170\n",
            "Epoch: 17/30, step: 180/364, loss: 0.38745, accuracy: 0.83194\n",
            "Epoch: 17/30, step: 181/364, loss: 0.38771, accuracy: 0.83201\n",
            "Epoch: 17/30, step: 182/364, loss: 0.38745, accuracy: 0.83190\n",
            "Epoch: 17/30, step: 183/364, loss: 0.38718, accuracy: 0.83197\n",
            "Epoch: 17/30, step: 184/364, loss: 0.38662, accuracy: 0.83263\n",
            "Epoch: 17/30, step: 185/364, loss: 0.38779, accuracy: 0.83167\n",
            "Epoch: 17/30, step: 186/364, loss: 0.38772, accuracy: 0.83165\n",
            "Epoch: 17/30, step: 187/364, loss: 0.38780, accuracy: 0.83155\n",
            "Epoch: 17/30, step: 188/364, loss: 0.38760, accuracy: 0.83145\n",
            "Epoch: 17/30, step: 189/364, loss: 0.38719, accuracy: 0.83168\n",
            "Epoch: 17/30, step: 190/364, loss: 0.38687, accuracy: 0.83199\n",
            "Epoch: 17/30, step: 191/364, loss: 0.38712, accuracy: 0.83172\n",
            "Epoch: 17/30, step: 192/364, loss: 0.38682, accuracy: 0.83187\n",
            "Epoch: 17/30, step: 193/364, loss: 0.38669, accuracy: 0.83193\n",
            "Epoch: 17/30, step: 194/364, loss: 0.38678, accuracy: 0.83223\n",
            "Epoch: 17/30, step: 195/364, loss: 0.38688, accuracy: 0.83213\n",
            "Epoch: 17/30, step: 196/364, loss: 0.38675, accuracy: 0.83235\n",
            "Epoch: 17/30, step: 197/364, loss: 0.38619, accuracy: 0.83265\n",
            "Epoch: 17/30, step: 198/364, loss: 0.38615, accuracy: 0.83262\n",
            "Epoch: 17/30, step: 199/364, loss: 0.38594, accuracy: 0.83284\n",
            "Epoch: 17/30, step: 200/364, loss: 0.38608, accuracy: 0.83242\n",
            "Epoch: 17/30, step: 201/364, loss: 0.38611, accuracy: 0.83248\n",
            "Epoch: 17/30, step: 202/364, loss: 0.38629, accuracy: 0.83238\n",
            "Epoch: 17/30, step: 203/364, loss: 0.38612, accuracy: 0.83236\n",
            "Epoch: 17/30, step: 204/364, loss: 0.38586, accuracy: 0.83257\n",
            "Epoch: 17/30, step: 205/364, loss: 0.38563, accuracy: 0.83285\n",
            "Epoch: 17/30, step: 206/364, loss: 0.38536, accuracy: 0.83313\n",
            "Epoch: 17/30, step: 207/364, loss: 0.38500, accuracy: 0.83326\n",
            "Epoch: 17/30, step: 208/364, loss: 0.38482, accuracy: 0.83361\n",
            "Epoch: 17/30, step: 209/364, loss: 0.38444, accuracy: 0.83381\n",
            "Epoch: 17/30, step: 210/364, loss: 0.38467, accuracy: 0.83363\n",
            "Epoch: 17/30, step: 211/364, loss: 0.38482, accuracy: 0.83338\n",
            "Epoch: 17/30, step: 212/364, loss: 0.38509, accuracy: 0.83321\n",
            "Epoch: 17/30, step: 213/364, loss: 0.38480, accuracy: 0.83355\n",
            "Epoch: 17/30, step: 214/364, loss: 0.38502, accuracy: 0.83338\n",
            "Epoch: 17/30, step: 215/364, loss: 0.38474, accuracy: 0.83365\n",
            "Epoch: 17/30, step: 216/364, loss: 0.38479, accuracy: 0.83377\n",
            "Epoch: 17/30, step: 217/364, loss: 0.38446, accuracy: 0.83389\n",
            "Epoch: 17/30, step: 218/364, loss: 0.38518, accuracy: 0.83343\n",
            "Epoch: 17/30, step: 219/364, loss: 0.38534, accuracy: 0.83312\n",
            "Epoch: 17/30, step: 220/364, loss: 0.38514, accuracy: 0.83338\n",
            "Epoch: 17/30, step: 221/364, loss: 0.38495, accuracy: 0.83357\n",
            "Epoch: 17/30, step: 222/364, loss: 0.38559, accuracy: 0.83312\n",
            "Epoch: 17/30, step: 223/364, loss: 0.38587, accuracy: 0.83275\n",
            "Epoch: 17/30, step: 224/364, loss: 0.38571, accuracy: 0.83287\n",
            "Epoch: 17/30, step: 225/364, loss: 0.38567, accuracy: 0.83292\n",
            "Epoch: 17/30, step: 226/364, loss: 0.38617, accuracy: 0.83241\n",
            "Epoch: 17/30, step: 227/364, loss: 0.38591, accuracy: 0.83246\n",
            "Epoch: 17/30, step: 228/364, loss: 0.38549, accuracy: 0.83299\n",
            "Epoch: 17/30, step: 229/364, loss: 0.38555, accuracy: 0.83311\n",
            "Epoch: 17/30, step: 230/364, loss: 0.38531, accuracy: 0.83329\n",
            "Epoch: 17/30, step: 231/364, loss: 0.38547, accuracy: 0.83300\n",
            "Epoch: 17/30, step: 232/364, loss: 0.38576, accuracy: 0.83284\n",
            "Epoch: 17/30, step: 233/364, loss: 0.38566, accuracy: 0.83302\n",
            "Epoch: 17/30, step: 234/364, loss: 0.38572, accuracy: 0.83313\n",
            "Epoch: 17/30, step: 235/364, loss: 0.38584, accuracy: 0.83285\n",
            "Epoch: 17/30, step: 236/364, loss: 0.38535, accuracy: 0.83336\n",
            "Epoch: 17/30, step: 237/364, loss: 0.38532, accuracy: 0.83320\n",
            "Epoch: 17/30, step: 238/364, loss: 0.38534, accuracy: 0.83338\n",
            "Epoch: 17/30, step: 239/364, loss: 0.38509, accuracy: 0.83355\n",
            "Epoch: 17/30, step: 240/364, loss: 0.38545, accuracy: 0.83340\n",
            "Epoch: 17/30, step: 241/364, loss: 0.38510, accuracy: 0.83383\n",
            "Epoch: 17/30, step: 242/364, loss: 0.38497, accuracy: 0.83400\n",
            "Epoch: 17/30, step: 243/364, loss: 0.38513, accuracy: 0.83410\n",
            "Epoch: 17/30, step: 244/364, loss: 0.38509, accuracy: 0.83421\n",
            "Epoch: 17/30, step: 245/364, loss: 0.38521, accuracy: 0.83425\n",
            "Epoch: 17/30, step: 246/364, loss: 0.38535, accuracy: 0.83416\n",
            "Epoch: 17/30, step: 247/364, loss: 0.38552, accuracy: 0.83407\n",
            "Epoch: 17/30, step: 248/364, loss: 0.38510, accuracy: 0.83436\n",
            "Epoch: 17/30, step: 249/364, loss: 0.38510, accuracy: 0.83427\n",
            "Epoch: 17/30, step: 250/364, loss: 0.38519, accuracy: 0.83412\n",
            "Epoch: 17/30, step: 251/364, loss: 0.38499, accuracy: 0.83441\n",
            "Epoch: 17/30, step: 252/364, loss: 0.38500, accuracy: 0.83420\n",
            "Epoch: 17/30, step: 253/364, loss: 0.38455, accuracy: 0.83455\n",
            "Epoch: 17/30, step: 254/364, loss: 0.38438, accuracy: 0.83471\n",
            "Epoch: 17/30, step: 255/364, loss: 0.38434, accuracy: 0.83474\n",
            "Epoch: 17/30, step: 256/364, loss: 0.38430, accuracy: 0.83459\n",
            "Epoch: 17/30, step: 257/364, loss: 0.38428, accuracy: 0.83463\n",
            "Epoch: 17/30, step: 258/364, loss: 0.38413, accuracy: 0.83479\n",
            "Epoch: 17/30, step: 259/364, loss: 0.38427, accuracy: 0.83476\n",
            "Epoch: 17/30, step: 260/364, loss: 0.38431, accuracy: 0.83462\n",
            "Epoch: 17/30, step: 261/364, loss: 0.38454, accuracy: 0.83459\n",
            "Epoch: 17/30, step: 262/364, loss: 0.38447, accuracy: 0.83463\n",
            "Epoch: 17/30, step: 263/364, loss: 0.38436, accuracy: 0.83484\n",
            "Epoch: 17/30, step: 264/364, loss: 0.38439, accuracy: 0.83481\n",
            "Epoch: 17/30, step: 265/364, loss: 0.38446, accuracy: 0.83485\n",
            "Epoch: 17/30, step: 266/364, loss: 0.38430, accuracy: 0.83517\n",
            "Epoch: 17/30, step: 267/364, loss: 0.38414, accuracy: 0.83532\n",
            "Epoch: 17/30, step: 268/364, loss: 0.38408, accuracy: 0.83524\n",
            "Epoch: 17/30, step: 269/364, loss: 0.38387, accuracy: 0.83539\n",
            "Epoch: 17/30, step: 270/364, loss: 0.38377, accuracy: 0.83559\n",
            "Epoch: 17/30, step: 271/364, loss: 0.38358, accuracy: 0.83574\n",
            "Epoch: 17/30, step: 272/364, loss: 0.38398, accuracy: 0.83571\n",
            "Epoch: 17/30, step: 273/364, loss: 0.38395, accuracy: 0.83557\n",
            "Epoch: 17/30, step: 274/364, loss: 0.38407, accuracy: 0.83542\n",
            "Epoch: 17/30, step: 275/364, loss: 0.38373, accuracy: 0.83580\n",
            "Epoch: 17/30, step: 276/364, loss: 0.38365, accuracy: 0.83594\n",
            "Epoch: 17/30, step: 277/364, loss: 0.38354, accuracy: 0.83608\n",
            "Epoch: 17/30, step: 278/364, loss: 0.38359, accuracy: 0.83611\n",
            "Epoch: 17/30, step: 279/364, loss: 0.38339, accuracy: 0.83619\n",
            "Epoch: 17/30, step: 280/364, loss: 0.38328, accuracy: 0.83644\n",
            "Epoch: 17/30, step: 281/364, loss: 0.38296, accuracy: 0.83663\n",
            "Epoch: 17/30, step: 282/364, loss: 0.38292, accuracy: 0.83655\n",
            "Epoch: 17/30, step: 283/364, loss: 0.38309, accuracy: 0.83641\n",
            "Epoch: 17/30, step: 284/364, loss: 0.38320, accuracy: 0.83638\n",
            "Epoch: 17/30, step: 285/364, loss: 0.38343, accuracy: 0.83629\n",
            "Epoch: 17/30, step: 286/364, loss: 0.38391, accuracy: 0.83594\n",
            "Epoch: 17/30, step: 287/364, loss: 0.38385, accuracy: 0.83607\n",
            "Epoch: 17/30, step: 288/364, loss: 0.38375, accuracy: 0.83610\n",
            "Epoch: 17/30, step: 289/364, loss: 0.38382, accuracy: 0.83602\n",
            "Epoch: 17/30, step: 290/364, loss: 0.38394, accuracy: 0.83583\n",
            "Epoch: 17/30, step: 291/364, loss: 0.38369, accuracy: 0.83589\n",
            "Epoch: 17/30, train loss: 0.38369, train accuracy: 0.83589, valid loss: 0.64633, valid accuracy: 0.67390\n",
            "Epoch: 18/30, step: 1/364, loss: 0.43733, accuracy: 0.81250\n",
            "Epoch: 18/30, step: 2/364, loss: 0.44730, accuracy: 0.80469\n",
            "Epoch: 18/30, step: 3/364, loss: 0.44539, accuracy: 0.80208\n",
            "Epoch: 18/30, step: 4/364, loss: 0.42921, accuracy: 0.80859\n",
            "Epoch: 18/30, step: 5/364, loss: 0.41866, accuracy: 0.81875\n",
            "Epoch: 18/30, step: 6/364, loss: 0.41218, accuracy: 0.82812\n",
            "Epoch: 18/30, step: 7/364, loss: 0.41366, accuracy: 0.82366\n",
            "Epoch: 18/30, step: 8/364, loss: 0.41072, accuracy: 0.82812\n",
            "Epoch: 18/30, step: 9/364, loss: 0.40241, accuracy: 0.83160\n",
            "Epoch: 18/30, step: 10/364, loss: 0.39566, accuracy: 0.83125\n",
            "Epoch: 18/30, step: 11/364, loss: 0.39694, accuracy: 0.82955\n",
            "Epoch: 18/30, step: 12/364, loss: 0.39125, accuracy: 0.83464\n",
            "Epoch: 18/30, step: 13/364, loss: 0.38126, accuracy: 0.84014\n",
            "Epoch: 18/30, step: 14/364, loss: 0.37768, accuracy: 0.84040\n",
            "Epoch: 18/30, step: 15/364, loss: 0.38296, accuracy: 0.83646\n",
            "Epoch: 18/30, step: 16/364, loss: 0.38544, accuracy: 0.83203\n",
            "Epoch: 18/30, step: 17/364, loss: 0.38255, accuracy: 0.83180\n",
            "Epoch: 18/30, step: 18/364, loss: 0.37428, accuracy: 0.83767\n",
            "Epoch: 18/30, step: 19/364, loss: 0.37870, accuracy: 0.83635\n",
            "Epoch: 18/30, step: 20/364, loss: 0.37943, accuracy: 0.83516\n",
            "Epoch: 18/30, step: 21/364, loss: 0.38487, accuracy: 0.83333\n",
            "Epoch: 18/30, step: 22/364, loss: 0.38268, accuracy: 0.83310\n",
            "Epoch: 18/30, step: 23/364, loss: 0.38076, accuracy: 0.83424\n",
            "Epoch: 18/30, step: 24/364, loss: 0.38023, accuracy: 0.83464\n",
            "Epoch: 18/30, step: 25/364, loss: 0.37864, accuracy: 0.83562\n",
            "Epoch: 18/30, step: 26/364, loss: 0.37805, accuracy: 0.83654\n",
            "Epoch: 18/30, step: 27/364, loss: 0.37702, accuracy: 0.83565\n",
            "Epoch: 18/30, step: 28/364, loss: 0.37592, accuracy: 0.83594\n",
            "Epoch: 18/30, step: 29/364, loss: 0.37793, accuracy: 0.83513\n",
            "Epoch: 18/30, step: 30/364, loss: 0.37636, accuracy: 0.83698\n",
            "Epoch: 18/30, step: 31/364, loss: 0.37848, accuracy: 0.83468\n",
            "Epoch: 18/30, step: 32/364, loss: 0.37662, accuracy: 0.83691\n",
            "Epoch: 18/30, step: 33/364, loss: 0.37749, accuracy: 0.83617\n",
            "Epoch: 18/30, step: 34/364, loss: 0.37739, accuracy: 0.83778\n",
            "Epoch: 18/30, step: 35/364, loss: 0.37486, accuracy: 0.83884\n",
            "Epoch: 18/30, step: 36/364, loss: 0.37670, accuracy: 0.83854\n",
            "Epoch: 18/30, step: 37/364, loss: 0.37928, accuracy: 0.83615\n",
            "Epoch: 18/30, step: 38/364, loss: 0.37879, accuracy: 0.83717\n",
            "Epoch: 18/30, step: 39/364, loss: 0.37819, accuracy: 0.83734\n",
            "Epoch: 18/30, step: 40/364, loss: 0.37945, accuracy: 0.83672\n",
            "Epoch: 18/30, step: 41/364, loss: 0.37826, accuracy: 0.83765\n",
            "Epoch: 18/30, step: 42/364, loss: 0.37514, accuracy: 0.84003\n",
            "Epoch: 18/30, step: 43/364, loss: 0.37497, accuracy: 0.83939\n",
            "Epoch: 18/30, step: 44/364, loss: 0.37262, accuracy: 0.84091\n",
            "Epoch: 18/30, step: 45/364, loss: 0.37362, accuracy: 0.84028\n",
            "Epoch: 18/30, step: 46/364, loss: 0.37362, accuracy: 0.84001\n",
            "Epoch: 18/30, step: 47/364, loss: 0.37379, accuracy: 0.83843\n",
            "Epoch: 18/30, step: 48/364, loss: 0.37542, accuracy: 0.83854\n",
            "Epoch: 18/30, step: 49/364, loss: 0.37260, accuracy: 0.84120\n",
            "Epoch: 18/30, step: 50/364, loss: 0.37144, accuracy: 0.84188\n",
            "Epoch: 18/30, step: 51/364, loss: 0.37098, accuracy: 0.84161\n",
            "Epoch: 18/30, step: 52/364, loss: 0.37017, accuracy: 0.84255\n",
            "Epoch: 18/30, step: 53/364, loss: 0.36982, accuracy: 0.84287\n",
            "Epoch: 18/30, step: 54/364, loss: 0.37151, accuracy: 0.84172\n",
            "Epoch: 18/30, step: 55/364, loss: 0.36878, accuracy: 0.84375\n",
            "Epoch: 18/30, step: 56/364, loss: 0.37134, accuracy: 0.84152\n",
            "Epoch: 18/30, step: 57/364, loss: 0.37107, accuracy: 0.84211\n",
            "Epoch: 18/30, step: 58/364, loss: 0.36933, accuracy: 0.84375\n",
            "Epoch: 18/30, step: 59/364, loss: 0.36891, accuracy: 0.84428\n",
            "Epoch: 18/30, step: 60/364, loss: 0.36969, accuracy: 0.84401\n",
            "Epoch: 18/30, step: 61/364, loss: 0.37308, accuracy: 0.84119\n",
            "Epoch: 18/30, step: 62/364, loss: 0.37176, accuracy: 0.84173\n",
            "Epoch: 18/30, step: 63/364, loss: 0.37182, accuracy: 0.84152\n",
            "Epoch: 18/30, step: 64/364, loss: 0.37135, accuracy: 0.84229\n",
            "Epoch: 18/30, step: 65/364, loss: 0.37197, accuracy: 0.84159\n",
            "Epoch: 18/30, step: 66/364, loss: 0.37165, accuracy: 0.84233\n",
            "Epoch: 18/30, step: 67/364, loss: 0.37046, accuracy: 0.84352\n",
            "Epoch: 18/30, step: 68/364, loss: 0.36967, accuracy: 0.84421\n",
            "Epoch: 18/30, step: 69/364, loss: 0.36916, accuracy: 0.84488\n",
            "Epoch: 18/30, step: 70/364, loss: 0.36782, accuracy: 0.84621\n",
            "Epoch: 18/30, step: 71/364, loss: 0.36820, accuracy: 0.84617\n",
            "Epoch: 18/30, step: 72/364, loss: 0.36798, accuracy: 0.84657\n",
            "Epoch: 18/30, step: 73/364, loss: 0.36888, accuracy: 0.84503\n",
            "Epoch: 18/30, step: 74/364, loss: 0.36925, accuracy: 0.84396\n",
            "Epoch: 18/30, step: 75/364, loss: 0.36912, accuracy: 0.84354\n",
            "Epoch: 18/30, step: 76/364, loss: 0.36872, accuracy: 0.84354\n",
            "Epoch: 18/30, step: 77/364, loss: 0.36818, accuracy: 0.84355\n",
            "Epoch: 18/30, step: 78/364, loss: 0.36889, accuracy: 0.84255\n",
            "Epoch: 18/30, step: 79/364, loss: 0.36915, accuracy: 0.84237\n",
            "Epoch: 18/30, step: 80/364, loss: 0.36963, accuracy: 0.84180\n",
            "Epoch: 18/30, step: 81/364, loss: 0.36943, accuracy: 0.84144\n",
            "Epoch: 18/30, step: 82/364, loss: 0.36907, accuracy: 0.84165\n",
            "Epoch: 18/30, step: 83/364, loss: 0.36809, accuracy: 0.84224\n",
            "Epoch: 18/30, step: 84/364, loss: 0.36769, accuracy: 0.84301\n",
            "Epoch: 18/30, step: 85/364, loss: 0.36736, accuracy: 0.84338\n",
            "Epoch: 18/30, step: 86/364, loss: 0.36814, accuracy: 0.84357\n",
            "Epoch: 18/30, step: 87/364, loss: 0.36714, accuracy: 0.84447\n",
            "Epoch: 18/30, step: 88/364, loss: 0.36722, accuracy: 0.84464\n",
            "Epoch: 18/30, step: 89/364, loss: 0.36731, accuracy: 0.84463\n",
            "Epoch: 18/30, step: 90/364, loss: 0.36719, accuracy: 0.84514\n",
            "Epoch: 18/30, step: 91/364, loss: 0.36745, accuracy: 0.84461\n",
            "Epoch: 18/30, step: 92/364, loss: 0.36793, accuracy: 0.84409\n",
            "Epoch: 18/30, step: 93/364, loss: 0.36658, accuracy: 0.84459\n",
            "Epoch: 18/30, step: 94/364, loss: 0.36819, accuracy: 0.84408\n",
            "Epoch: 18/30, step: 95/364, loss: 0.36825, accuracy: 0.84408\n",
            "Epoch: 18/30, step: 96/364, loss: 0.36724, accuracy: 0.84456\n",
            "Epoch: 18/30, step: 97/364, loss: 0.36777, accuracy: 0.84423\n",
            "Epoch: 18/30, step: 98/364, loss: 0.36745, accuracy: 0.84423\n",
            "Epoch: 18/30, step: 99/364, loss: 0.36732, accuracy: 0.84422\n",
            "Epoch: 18/30, step: 100/364, loss: 0.36798, accuracy: 0.84328\n",
            "Epoch: 18/30, step: 101/364, loss: 0.36821, accuracy: 0.84329\n",
            "Epoch: 18/30, step: 102/364, loss: 0.36843, accuracy: 0.84314\n",
            "Epoch: 18/30, step: 103/364, loss: 0.36800, accuracy: 0.84390\n",
            "Epoch: 18/30, step: 104/364, loss: 0.36742, accuracy: 0.84405\n",
            "Epoch: 18/30, step: 105/364, loss: 0.36712, accuracy: 0.84405\n",
            "Epoch: 18/30, step: 106/364, loss: 0.36654, accuracy: 0.84419\n",
            "Epoch: 18/30, step: 107/364, loss: 0.36664, accuracy: 0.84433\n",
            "Epoch: 18/30, step: 108/364, loss: 0.36861, accuracy: 0.84230\n",
            "Epoch: 18/30, step: 109/364, loss: 0.36845, accuracy: 0.84246\n",
            "Epoch: 18/30, step: 110/364, loss: 0.36770, accuracy: 0.84332\n",
            "Epoch: 18/30, step: 111/364, loss: 0.36746, accuracy: 0.84403\n",
            "Epoch: 18/30, step: 112/364, loss: 0.36765, accuracy: 0.84361\n",
            "Epoch: 18/30, step: 113/364, loss: 0.36703, accuracy: 0.84375\n",
            "Epoch: 18/30, step: 114/364, loss: 0.36726, accuracy: 0.84348\n",
            "Epoch: 18/30, step: 115/364, loss: 0.36751, accuracy: 0.84321\n",
            "Epoch: 18/30, step: 116/364, loss: 0.36795, accuracy: 0.84281\n",
            "Epoch: 18/30, step: 117/364, loss: 0.36726, accuracy: 0.84375\n",
            "Epoch: 18/30, step: 118/364, loss: 0.36788, accuracy: 0.84322\n",
            "Epoch: 18/30, step: 119/364, loss: 0.36747, accuracy: 0.84349\n",
            "Epoch: 18/30, step: 120/364, loss: 0.36737, accuracy: 0.84375\n",
            "Epoch: 18/30, step: 121/364, loss: 0.36701, accuracy: 0.84427\n",
            "Epoch: 18/30, step: 122/364, loss: 0.36753, accuracy: 0.84401\n",
            "Epoch: 18/30, step: 123/364, loss: 0.36775, accuracy: 0.84388\n",
            "Epoch: 18/30, step: 124/364, loss: 0.36814, accuracy: 0.84299\n",
            "Epoch: 18/30, step: 125/364, loss: 0.36865, accuracy: 0.84212\n",
            "Epoch: 18/30, step: 126/364, loss: 0.36852, accuracy: 0.84201\n",
            "Epoch: 18/30, step: 127/364, loss: 0.36857, accuracy: 0.84178\n",
            "Epoch: 18/30, step: 128/364, loss: 0.36921, accuracy: 0.84155\n",
            "Epoch: 18/30, step: 129/364, loss: 0.36904, accuracy: 0.84169\n",
            "Epoch: 18/30, step: 130/364, loss: 0.36894, accuracy: 0.84135\n",
            "Epoch: 18/30, step: 131/364, loss: 0.36864, accuracy: 0.84172\n",
            "Epoch: 18/30, step: 132/364, loss: 0.36884, accuracy: 0.84174\n",
            "Epoch: 18/30, step: 133/364, loss: 0.36928, accuracy: 0.84164\n",
            "Epoch: 18/30, step: 134/364, loss: 0.36904, accuracy: 0.84188\n",
            "Epoch: 18/30, step: 135/364, loss: 0.36903, accuracy: 0.84190\n",
            "Epoch: 18/30, step: 136/364, loss: 0.36871, accuracy: 0.84226\n",
            "Epoch: 18/30, step: 137/364, loss: 0.36894, accuracy: 0.84193\n",
            "Epoch: 18/30, step: 138/364, loss: 0.36951, accuracy: 0.84137\n",
            "Epoch: 18/30, step: 139/364, loss: 0.36940, accuracy: 0.84116\n",
            "Epoch: 18/30, step: 140/364, loss: 0.36918, accuracy: 0.84141\n",
            "Epoch: 18/30, step: 141/364, loss: 0.36958, accuracy: 0.84120\n",
            "Epoch: 18/30, step: 142/364, loss: 0.36871, accuracy: 0.84199\n",
            "Epoch: 18/30, step: 143/364, loss: 0.36885, accuracy: 0.84211\n",
            "Epoch: 18/30, step: 144/364, loss: 0.36898, accuracy: 0.84201\n",
            "Epoch: 18/30, step: 145/364, loss: 0.36866, accuracy: 0.84213\n",
            "Epoch: 18/30, step: 146/364, loss: 0.36899, accuracy: 0.84161\n",
            "Epoch: 18/30, step: 147/364, loss: 0.36903, accuracy: 0.84162\n",
            "Epoch: 18/30, step: 148/364, loss: 0.36867, accuracy: 0.84217\n",
            "Epoch: 18/30, step: 149/364, loss: 0.36862, accuracy: 0.84218\n",
            "Epoch: 18/30, step: 150/364, loss: 0.36864, accuracy: 0.84208\n",
            "Epoch: 18/30, step: 151/364, loss: 0.36835, accuracy: 0.84240\n",
            "Epoch: 18/30, step: 152/364, loss: 0.36814, accuracy: 0.84241\n",
            "Epoch: 18/30, step: 153/364, loss: 0.36818, accuracy: 0.84232\n",
            "Epoch: 18/30, step: 154/364, loss: 0.36827, accuracy: 0.84233\n",
            "Epoch: 18/30, step: 155/364, loss: 0.36809, accuracy: 0.84244\n",
            "Epoch: 18/30, step: 156/364, loss: 0.36761, accuracy: 0.84265\n",
            "Epoch: 18/30, step: 157/364, loss: 0.36702, accuracy: 0.84305\n",
            "Epoch: 18/30, step: 158/364, loss: 0.36714, accuracy: 0.84286\n",
            "Epoch: 18/30, step: 159/364, loss: 0.36714, accuracy: 0.84277\n",
            "Epoch: 18/30, step: 160/364, loss: 0.36650, accuracy: 0.84307\n",
            "Epoch: 18/30, step: 161/364, loss: 0.36660, accuracy: 0.84288\n",
            "Epoch: 18/30, step: 162/364, loss: 0.36636, accuracy: 0.84307\n",
            "Epoch: 18/30, step: 163/364, loss: 0.36659, accuracy: 0.84289\n",
            "Epoch: 18/30, step: 164/364, loss: 0.36601, accuracy: 0.84337\n",
            "Epoch: 18/30, step: 165/364, loss: 0.36601, accuracy: 0.84328\n",
            "Epoch: 18/30, step: 166/364, loss: 0.36577, accuracy: 0.84347\n",
            "Epoch: 18/30, step: 167/364, loss: 0.36570, accuracy: 0.84347\n",
            "Epoch: 18/30, step: 168/364, loss: 0.36576, accuracy: 0.84356\n",
            "Epoch: 18/30, step: 169/364, loss: 0.36592, accuracy: 0.84329\n",
            "Epoch: 18/30, step: 170/364, loss: 0.36568, accuracy: 0.84347\n",
            "Epoch: 18/30, step: 171/364, loss: 0.36514, accuracy: 0.84384\n",
            "Epoch: 18/30, step: 172/364, loss: 0.36534, accuracy: 0.84366\n",
            "Epoch: 18/30, step: 173/364, loss: 0.36518, accuracy: 0.84366\n",
            "Epoch: 18/30, step: 174/364, loss: 0.36503, accuracy: 0.84384\n",
            "Epoch: 18/30, step: 175/364, loss: 0.36475, accuracy: 0.84402\n",
            "Epoch: 18/30, step: 176/364, loss: 0.36509, accuracy: 0.84375\n",
            "Epoch: 18/30, step: 177/364, loss: 0.36469, accuracy: 0.84401\n",
            "Epoch: 18/30, step: 178/364, loss: 0.36426, accuracy: 0.84445\n",
            "Epoch: 18/30, step: 179/364, loss: 0.36418, accuracy: 0.84454\n",
            "Epoch: 18/30, step: 180/364, loss: 0.36450, accuracy: 0.84453\n",
            "Epoch: 18/30, step: 181/364, loss: 0.36384, accuracy: 0.84504\n",
            "Epoch: 18/30, step: 182/364, loss: 0.36379, accuracy: 0.84521\n",
            "Epoch: 18/30, step: 183/364, loss: 0.36301, accuracy: 0.84580\n",
            "Epoch: 18/30, step: 184/364, loss: 0.36321, accuracy: 0.84596\n",
            "Epoch: 18/30, step: 185/364, loss: 0.36322, accuracy: 0.84611\n",
            "Epoch: 18/30, step: 186/364, loss: 0.36363, accuracy: 0.84593\n",
            "Epoch: 18/30, step: 187/364, loss: 0.36330, accuracy: 0.84634\n",
            "Epoch: 18/30, step: 188/364, loss: 0.36445, accuracy: 0.84558\n",
            "Epoch: 18/30, step: 189/364, loss: 0.36444, accuracy: 0.84582\n",
            "Epoch: 18/30, step: 190/364, loss: 0.36492, accuracy: 0.84539\n",
            "Epoch: 18/30, step: 191/364, loss: 0.36484, accuracy: 0.84514\n",
            "Epoch: 18/30, step: 192/364, loss: 0.36437, accuracy: 0.84554\n",
            "Epoch: 18/30, step: 193/364, loss: 0.36431, accuracy: 0.84569\n",
            "Epoch: 18/30, step: 194/364, loss: 0.36410, accuracy: 0.84552\n",
            "Epoch: 18/30, step: 195/364, loss: 0.36411, accuracy: 0.84567\n",
            "Epoch: 18/30, step: 196/364, loss: 0.36426, accuracy: 0.84518\n",
            "Epoch: 18/30, step: 197/364, loss: 0.36399, accuracy: 0.84526\n",
            "Epoch: 18/30, step: 198/364, loss: 0.36422, accuracy: 0.84517\n",
            "Epoch: 18/30, step: 199/364, loss: 0.36406, accuracy: 0.84524\n",
            "Epoch: 18/30, step: 200/364, loss: 0.36411, accuracy: 0.84547\n",
            "Epoch: 18/30, step: 201/364, loss: 0.36407, accuracy: 0.84569\n",
            "Epoch: 18/30, step: 202/364, loss: 0.36399, accuracy: 0.84561\n",
            "Epoch: 18/30, step: 203/364, loss: 0.36481, accuracy: 0.84498\n",
            "Epoch: 18/30, step: 204/364, loss: 0.36498, accuracy: 0.84490\n",
            "Epoch: 18/30, step: 205/364, loss: 0.36438, accuracy: 0.84535\n",
            "Epoch: 18/30, step: 206/364, loss: 0.36448, accuracy: 0.84519\n",
            "Epoch: 18/30, step: 207/364, loss: 0.36425, accuracy: 0.84556\n",
            "Epoch: 18/30, step: 208/364, loss: 0.36411, accuracy: 0.84563\n",
            "Epoch: 18/30, step: 209/364, loss: 0.36356, accuracy: 0.84584\n",
            "Epoch: 18/30, step: 210/364, loss: 0.36375, accuracy: 0.84583\n",
            "Epoch: 18/30, step: 211/364, loss: 0.36368, accuracy: 0.84590\n",
            "Epoch: 18/30, step: 212/364, loss: 0.36341, accuracy: 0.84611\n",
            "Epoch: 18/30, step: 213/364, loss: 0.36320, accuracy: 0.84595\n",
            "Epoch: 18/30, step: 214/364, loss: 0.36313, accuracy: 0.84609\n",
            "Epoch: 18/30, step: 215/364, loss: 0.36271, accuracy: 0.84622\n",
            "Epoch: 18/30, step: 216/364, loss: 0.36298, accuracy: 0.84606\n",
            "Epoch: 18/30, step: 217/364, loss: 0.36267, accuracy: 0.84627\n",
            "Epoch: 18/30, step: 218/364, loss: 0.36261, accuracy: 0.84619\n",
            "Epoch: 18/30, step: 219/364, loss: 0.36264, accuracy: 0.84596\n",
            "Epoch: 18/30, step: 220/364, loss: 0.36284, accuracy: 0.84588\n",
            "Epoch: 18/30, step: 221/364, loss: 0.36284, accuracy: 0.84580\n",
            "Epoch: 18/30, step: 222/364, loss: 0.36265, accuracy: 0.84593\n",
            "Epoch: 18/30, step: 223/364, loss: 0.36297, accuracy: 0.84550\n",
            "Epoch: 18/30, step: 224/364, loss: 0.36277, accuracy: 0.84570\n",
            "Epoch: 18/30, step: 225/364, loss: 0.36242, accuracy: 0.84597\n",
            "Epoch: 18/30, step: 226/364, loss: 0.36254, accuracy: 0.84582\n",
            "Epoch: 18/30, step: 227/364, loss: 0.36263, accuracy: 0.84588\n",
            "Epoch: 18/30, step: 228/364, loss: 0.36349, accuracy: 0.84498\n",
            "Epoch: 18/30, step: 229/364, loss: 0.36354, accuracy: 0.84505\n",
            "Epoch: 18/30, step: 230/364, loss: 0.36353, accuracy: 0.84490\n",
            "Epoch: 18/30, step: 231/364, loss: 0.36379, accuracy: 0.84463\n",
            "Epoch: 18/30, step: 232/364, loss: 0.36377, accuracy: 0.84476\n",
            "Epoch: 18/30, step: 233/364, loss: 0.36353, accuracy: 0.84496\n",
            "Epoch: 18/30, step: 234/364, loss: 0.36358, accuracy: 0.84489\n",
            "Epoch: 18/30, step: 235/364, loss: 0.36371, accuracy: 0.84495\n",
            "Epoch: 18/30, step: 236/364, loss: 0.36362, accuracy: 0.84507\n",
            "Epoch: 18/30, step: 237/364, loss: 0.36369, accuracy: 0.84507\n",
            "Epoch: 18/30, step: 238/364, loss: 0.36377, accuracy: 0.84506\n",
            "Epoch: 18/30, step: 239/364, loss: 0.36358, accuracy: 0.84499\n",
            "Epoch: 18/30, step: 240/364, loss: 0.36345, accuracy: 0.84492\n",
            "Epoch: 18/30, step: 241/364, loss: 0.36342, accuracy: 0.84505\n",
            "Epoch: 18/30, step: 242/364, loss: 0.36367, accuracy: 0.84504\n",
            "Epoch: 18/30, step: 243/364, loss: 0.36368, accuracy: 0.84510\n",
            "Epoch: 18/30, step: 244/364, loss: 0.36366, accuracy: 0.84516\n",
            "Epoch: 18/30, step: 245/364, loss: 0.36307, accuracy: 0.84560\n",
            "Epoch: 18/30, step: 246/364, loss: 0.36312, accuracy: 0.84572\n",
            "Epoch: 18/30, step: 247/364, loss: 0.36360, accuracy: 0.84552\n",
            "Epoch: 18/30, step: 248/364, loss: 0.36355, accuracy: 0.84558\n",
            "Epoch: 18/30, step: 249/364, loss: 0.36301, accuracy: 0.84607\n",
            "Epoch: 18/30, step: 250/364, loss: 0.36321, accuracy: 0.84613\n",
            "Epoch: 18/30, step: 251/364, loss: 0.36303, accuracy: 0.84624\n",
            "Epoch: 18/30, step: 252/364, loss: 0.36300, accuracy: 0.84611\n",
            "Epoch: 18/30, step: 253/364, loss: 0.36311, accuracy: 0.84591\n",
            "Epoch: 18/30, step: 254/364, loss: 0.36301, accuracy: 0.84603\n",
            "Epoch: 18/30, step: 255/364, loss: 0.36322, accuracy: 0.84583\n",
            "Epoch: 18/30, step: 256/364, loss: 0.36345, accuracy: 0.84558\n",
            "Epoch: 18/30, step: 257/364, loss: 0.36324, accuracy: 0.84576\n",
            "Epoch: 18/30, step: 258/364, loss: 0.36380, accuracy: 0.84545\n",
            "Epoch: 18/30, step: 259/364, loss: 0.36367, accuracy: 0.84544\n",
            "Epoch: 18/30, step: 260/364, loss: 0.36323, accuracy: 0.84561\n",
            "Epoch: 18/30, step: 261/364, loss: 0.36303, accuracy: 0.84585\n",
            "Epoch: 18/30, step: 262/364, loss: 0.36307, accuracy: 0.84578\n",
            "Epoch: 18/30, step: 263/364, loss: 0.36297, accuracy: 0.84577\n",
            "Epoch: 18/30, step: 264/364, loss: 0.36308, accuracy: 0.84570\n",
            "Epoch: 18/30, step: 265/364, loss: 0.36298, accuracy: 0.84570\n",
            "Epoch: 18/30, step: 266/364, loss: 0.36269, accuracy: 0.84592\n",
            "Epoch: 18/30, step: 267/364, loss: 0.36257, accuracy: 0.84592\n",
            "Epoch: 18/30, step: 268/364, loss: 0.36265, accuracy: 0.84567\n",
            "Epoch: 18/30, step: 269/364, loss: 0.36251, accuracy: 0.84567\n",
            "Epoch: 18/30, step: 270/364, loss: 0.36240, accuracy: 0.84566\n",
            "Epoch: 18/30, step: 271/364, loss: 0.36233, accuracy: 0.84583\n",
            "Epoch: 18/30, step: 272/364, loss: 0.36232, accuracy: 0.84593\n",
            "Epoch: 18/30, step: 273/364, loss: 0.36206, accuracy: 0.84592\n",
            "Epoch: 18/30, step: 274/364, loss: 0.36207, accuracy: 0.84586\n",
            "Epoch: 18/30, step: 275/364, loss: 0.36206, accuracy: 0.84580\n",
            "Epoch: 18/30, step: 276/364, loss: 0.36171, accuracy: 0.84601\n",
            "Epoch: 18/30, step: 277/364, loss: 0.36199, accuracy: 0.84572\n",
            "Epoch: 18/30, step: 278/364, loss: 0.36184, accuracy: 0.84583\n",
            "Epoch: 18/30, step: 279/364, loss: 0.36136, accuracy: 0.84616\n",
            "Epoch: 18/30, step: 280/364, loss: 0.36125, accuracy: 0.84621\n",
            "Epoch: 18/30, step: 281/364, loss: 0.36130, accuracy: 0.84625\n",
            "Epoch: 18/30, step: 282/364, loss: 0.36141, accuracy: 0.84613\n",
            "Epoch: 18/30, step: 283/364, loss: 0.36126, accuracy: 0.84618\n",
            "Epoch: 18/30, step: 284/364, loss: 0.36143, accuracy: 0.84617\n",
            "Epoch: 18/30, step: 285/364, loss: 0.36166, accuracy: 0.84616\n",
            "Epoch: 18/30, step: 286/364, loss: 0.36144, accuracy: 0.84643\n",
            "Epoch: 18/30, step: 287/364, loss: 0.36134, accuracy: 0.84658\n",
            "Epoch: 18/30, step: 288/364, loss: 0.36170, accuracy: 0.84635\n",
            "Epoch: 18/30, step: 289/364, loss: 0.36154, accuracy: 0.84651\n",
            "Epoch: 18/30, step: 290/364, loss: 0.36158, accuracy: 0.84644\n",
            "Epoch: 18/30, step: 291/364, loss: 0.36124, accuracy: 0.84664\n",
            "Epoch: 18/30, train loss: 0.36124, train accuracy: 0.84664, valid loss: 0.65518, valid accuracy: 0.67949\n",
            "Epoch: 19/30, step: 1/364, loss: 0.35494, accuracy: 0.87500\n",
            "Epoch: 19/30, step: 2/364, loss: 0.38219, accuracy: 0.84375\n",
            "Epoch: 19/30, step: 3/364, loss: 0.35512, accuracy: 0.85938\n",
            "Epoch: 19/30, step: 4/364, loss: 0.35651, accuracy: 0.83984\n",
            "Epoch: 19/30, step: 5/364, loss: 0.33579, accuracy: 0.86250\n",
            "Epoch: 19/30, step: 6/364, loss: 0.33584, accuracy: 0.85677\n",
            "Epoch: 19/30, step: 7/364, loss: 0.33482, accuracy: 0.86384\n",
            "Epoch: 19/30, step: 8/364, loss: 0.34125, accuracy: 0.85742\n",
            "Epoch: 19/30, step: 9/364, loss: 0.34281, accuracy: 0.85938\n",
            "Epoch: 19/30, step: 10/364, loss: 0.34675, accuracy: 0.85781\n",
            "Epoch: 19/30, step: 11/364, loss: 0.34559, accuracy: 0.86080\n",
            "Epoch: 19/30, step: 12/364, loss: 0.34386, accuracy: 0.86068\n",
            "Epoch: 19/30, step: 13/364, loss: 0.35059, accuracy: 0.85577\n",
            "Epoch: 19/30, step: 14/364, loss: 0.34517, accuracy: 0.85938\n",
            "Epoch: 19/30, step: 15/364, loss: 0.34555, accuracy: 0.85938\n",
            "Epoch: 19/30, step: 16/364, loss: 0.33990, accuracy: 0.86328\n",
            "Epoch: 19/30, step: 17/364, loss: 0.34459, accuracy: 0.86305\n",
            "Epoch: 19/30, step: 18/364, loss: 0.35143, accuracy: 0.85417\n",
            "Epoch: 19/30, step: 19/364, loss: 0.34688, accuracy: 0.85855\n",
            "Epoch: 19/30, step: 20/364, loss: 0.34376, accuracy: 0.86172\n",
            "Epoch: 19/30, step: 21/364, loss: 0.35007, accuracy: 0.85789\n",
            "Epoch: 19/30, step: 22/364, loss: 0.34825, accuracy: 0.85866\n",
            "Epoch: 19/30, step: 23/364, loss: 0.34676, accuracy: 0.85870\n",
            "Epoch: 19/30, step: 24/364, loss: 0.35149, accuracy: 0.85742\n",
            "Epoch: 19/30, step: 25/364, loss: 0.35196, accuracy: 0.85625\n",
            "Epoch: 19/30, step: 26/364, loss: 0.35120, accuracy: 0.85757\n",
            "Epoch: 19/30, step: 27/364, loss: 0.35308, accuracy: 0.85648\n",
            "Epoch: 19/30, step: 28/364, loss: 0.35166, accuracy: 0.85714\n",
            "Epoch: 19/30, step: 29/364, loss: 0.34951, accuracy: 0.85938\n",
            "Epoch: 19/30, step: 30/364, loss: 0.34730, accuracy: 0.86042\n",
            "Epoch: 19/30, step: 31/364, loss: 0.35101, accuracy: 0.85736\n",
            "Epoch: 19/30, step: 32/364, loss: 0.34909, accuracy: 0.85840\n",
            "Epoch: 19/30, step: 33/364, loss: 0.34846, accuracy: 0.85890\n",
            "Epoch: 19/30, step: 34/364, loss: 0.34739, accuracy: 0.85938\n",
            "Epoch: 19/30, step: 35/364, loss: 0.34755, accuracy: 0.85982\n",
            "Epoch: 19/30, step: 36/364, loss: 0.34889, accuracy: 0.85981\n",
            "Epoch: 19/30, step: 37/364, loss: 0.34674, accuracy: 0.86106\n",
            "Epoch: 19/30, step: 38/364, loss: 0.34996, accuracy: 0.85896\n",
            "Epoch: 19/30, step: 39/364, loss: 0.34862, accuracy: 0.85938\n",
            "Epoch: 19/30, step: 40/364, loss: 0.34891, accuracy: 0.85859\n",
            "Epoch: 19/30, step: 41/364, loss: 0.34618, accuracy: 0.86014\n",
            "Epoch: 19/30, step: 42/364, loss: 0.34576, accuracy: 0.86049\n",
            "Epoch: 19/30, step: 43/364, loss: 0.34624, accuracy: 0.86010\n",
            "Epoch: 19/30, step: 44/364, loss: 0.34704, accuracy: 0.86080\n",
            "Epoch: 19/30, step: 45/364, loss: 0.34552, accuracy: 0.86146\n",
            "Epoch: 19/30, step: 46/364, loss: 0.34577, accuracy: 0.86073\n",
            "Epoch: 19/30, step: 47/364, loss: 0.34484, accuracy: 0.86137\n",
            "Epoch: 19/30, step: 48/364, loss: 0.34510, accuracy: 0.86035\n",
            "Epoch: 19/30, step: 49/364, loss: 0.34520, accuracy: 0.86033\n",
            "Epoch: 19/30, step: 50/364, loss: 0.34447, accuracy: 0.86000\n",
            "Epoch: 19/30, step: 51/364, loss: 0.34372, accuracy: 0.86060\n",
            "Epoch: 19/30, step: 52/364, loss: 0.34247, accuracy: 0.86118\n",
            "Epoch: 19/30, step: 53/364, loss: 0.34122, accuracy: 0.86144\n",
            "Epoch: 19/30, step: 54/364, loss: 0.34217, accuracy: 0.86140\n",
            "Epoch: 19/30, step: 55/364, loss: 0.34246, accuracy: 0.86023\n",
            "Epoch: 19/30, step: 56/364, loss: 0.34391, accuracy: 0.85965\n",
            "Epoch: 19/30, step: 57/364, loss: 0.34407, accuracy: 0.85938\n",
            "Epoch: 19/30, step: 58/364, loss: 0.34452, accuracy: 0.85857\n",
            "Epoch: 19/30, step: 59/364, loss: 0.34380, accuracy: 0.85885\n",
            "Epoch: 19/30, step: 60/364, loss: 0.34378, accuracy: 0.85938\n",
            "Epoch: 19/30, step: 61/364, loss: 0.34454, accuracy: 0.85989\n",
            "Epoch: 19/30, step: 62/364, loss: 0.34331, accuracy: 0.86114\n",
            "Epoch: 19/30, step: 63/364, loss: 0.34313, accuracy: 0.86086\n",
            "Epoch: 19/30, step: 64/364, loss: 0.34248, accuracy: 0.86108\n",
            "Epoch: 19/30, step: 65/364, loss: 0.34147, accuracy: 0.86130\n",
            "Epoch: 19/30, step: 66/364, loss: 0.34174, accuracy: 0.86151\n",
            "Epoch: 19/30, step: 67/364, loss: 0.34124, accuracy: 0.86217\n",
            "Epoch: 19/30, step: 68/364, loss: 0.33965, accuracy: 0.86305\n",
            "Epoch: 19/30, step: 69/364, loss: 0.34090, accuracy: 0.86187\n",
            "Epoch: 19/30, step: 70/364, loss: 0.33991, accuracy: 0.86228\n",
            "Epoch: 19/30, step: 71/364, loss: 0.33973, accuracy: 0.86180\n",
            "Epoch: 19/30, step: 72/364, loss: 0.33865, accuracy: 0.86241\n",
            "Epoch: 19/30, step: 73/364, loss: 0.33909, accuracy: 0.86173\n",
            "Epoch: 19/30, step: 74/364, loss: 0.34080, accuracy: 0.86022\n",
            "Epoch: 19/30, step: 75/364, loss: 0.34248, accuracy: 0.85896\n",
            "Epoch: 19/30, step: 76/364, loss: 0.34209, accuracy: 0.85876\n",
            "Epoch: 19/30, step: 77/364, loss: 0.34129, accuracy: 0.85897\n",
            "Epoch: 19/30, step: 78/364, loss: 0.34132, accuracy: 0.85917\n",
            "Epoch: 19/30, step: 79/364, loss: 0.34096, accuracy: 0.85898\n",
            "Epoch: 19/30, step: 80/364, loss: 0.34020, accuracy: 0.85938\n",
            "Epoch: 19/30, step: 81/364, loss: 0.33946, accuracy: 0.85995\n",
            "Epoch: 19/30, step: 82/364, loss: 0.33998, accuracy: 0.85957\n",
            "Epoch: 19/30, step: 83/364, loss: 0.33946, accuracy: 0.85975\n",
            "Epoch: 19/30, step: 84/364, loss: 0.33869, accuracy: 0.86012\n",
            "Epoch: 19/30, step: 85/364, loss: 0.33846, accuracy: 0.85993\n",
            "Epoch: 19/30, step: 86/364, loss: 0.33825, accuracy: 0.86047\n",
            "Epoch: 19/30, step: 87/364, loss: 0.33896, accuracy: 0.85991\n",
            "Epoch: 19/30, step: 88/364, loss: 0.33870, accuracy: 0.86026\n",
            "Epoch: 19/30, step: 89/364, loss: 0.33914, accuracy: 0.85955\n",
            "Epoch: 19/30, step: 90/364, loss: 0.33903, accuracy: 0.86007\n",
            "Epoch: 19/30, step: 91/364, loss: 0.33806, accuracy: 0.86075\n",
            "Epoch: 19/30, step: 92/364, loss: 0.33893, accuracy: 0.85971\n",
            "Epoch: 19/30, step: 93/364, loss: 0.33819, accuracy: 0.86022\n",
            "Epoch: 19/30, step: 94/364, loss: 0.33840, accuracy: 0.85987\n",
            "Epoch: 19/30, step: 95/364, loss: 0.33934, accuracy: 0.85921\n",
            "Epoch: 19/30, step: 96/364, loss: 0.33911, accuracy: 0.85938\n",
            "Epoch: 19/30, step: 97/364, loss: 0.33876, accuracy: 0.86002\n",
            "Epoch: 19/30, step: 98/364, loss: 0.33839, accuracy: 0.86049\n",
            "Epoch: 19/30, step: 99/364, loss: 0.33864, accuracy: 0.86016\n",
            "Epoch: 19/30, step: 100/364, loss: 0.33923, accuracy: 0.85922\n",
            "Epoch: 19/30, step: 101/364, loss: 0.33940, accuracy: 0.85876\n",
            "Epoch: 19/30, step: 102/364, loss: 0.33864, accuracy: 0.85907\n",
            "Epoch: 19/30, step: 103/364, loss: 0.33845, accuracy: 0.85938\n",
            "Epoch: 19/30, step: 104/364, loss: 0.33940, accuracy: 0.85772\n",
            "Epoch: 19/30, step: 105/364, loss: 0.33868, accuracy: 0.85818\n",
            "Epoch: 19/30, step: 106/364, loss: 0.33872, accuracy: 0.85805\n",
            "Epoch: 19/30, step: 107/364, loss: 0.34021, accuracy: 0.85675\n",
            "Epoch: 19/30, step: 108/364, loss: 0.34071, accuracy: 0.85634\n",
            "Epoch: 19/30, step: 109/364, loss: 0.34059, accuracy: 0.85651\n",
            "Epoch: 19/30, step: 110/364, loss: 0.34084, accuracy: 0.85611\n",
            "Epoch: 19/30, step: 111/364, loss: 0.34120, accuracy: 0.85600\n",
            "Epoch: 19/30, step: 112/364, loss: 0.34091, accuracy: 0.85617\n",
            "Epoch: 19/30, step: 113/364, loss: 0.34131, accuracy: 0.85578\n",
            "Epoch: 19/30, step: 114/364, loss: 0.34079, accuracy: 0.85622\n",
            "Epoch: 19/30, step: 115/364, loss: 0.34127, accuracy: 0.85639\n",
            "Epoch: 19/30, step: 116/364, loss: 0.34160, accuracy: 0.85614\n",
            "Epoch: 19/30, step: 117/364, loss: 0.34143, accuracy: 0.85617\n",
            "Epoch: 19/30, step: 118/364, loss: 0.34150, accuracy: 0.85606\n",
            "Epoch: 19/30, step: 119/364, loss: 0.34149, accuracy: 0.85596\n",
            "Epoch: 19/30, step: 120/364, loss: 0.34099, accuracy: 0.85625\n",
            "Epoch: 19/30, step: 121/364, loss: 0.34133, accuracy: 0.85589\n",
            "Epoch: 19/30, step: 122/364, loss: 0.34211, accuracy: 0.85553\n",
            "Epoch: 19/30, step: 123/364, loss: 0.34225, accuracy: 0.85531\n",
            "Epoch: 19/30, step: 124/364, loss: 0.34205, accuracy: 0.85547\n",
            "Epoch: 19/30, step: 125/364, loss: 0.34177, accuracy: 0.85562\n",
            "Epoch: 19/30, step: 126/364, loss: 0.34185, accuracy: 0.85578\n",
            "Epoch: 19/30, step: 127/364, loss: 0.34214, accuracy: 0.85556\n",
            "Epoch: 19/30, step: 128/364, loss: 0.34208, accuracy: 0.85596\n",
            "Epoch: 19/30, step: 129/364, loss: 0.34199, accuracy: 0.85647\n",
            "Epoch: 19/30, step: 130/364, loss: 0.34225, accuracy: 0.85565\n",
            "Epoch: 19/30, step: 131/364, loss: 0.34259, accuracy: 0.85520\n",
            "Epoch: 19/30, step: 132/364, loss: 0.34288, accuracy: 0.85500\n",
            "Epoch: 19/30, step: 133/364, loss: 0.34314, accuracy: 0.85491\n",
            "Epoch: 19/30, step: 134/364, loss: 0.34299, accuracy: 0.85529\n",
            "Epoch: 19/30, step: 135/364, loss: 0.34304, accuracy: 0.85532\n",
            "Epoch: 19/30, step: 136/364, loss: 0.34374, accuracy: 0.85501\n",
            "Epoch: 19/30, step: 137/364, loss: 0.34318, accuracy: 0.85516\n",
            "Epoch: 19/30, step: 138/364, loss: 0.34269, accuracy: 0.85541\n",
            "Epoch: 19/30, step: 139/364, loss: 0.34354, accuracy: 0.85488\n",
            "Epoch: 19/30, step: 140/364, loss: 0.34288, accuracy: 0.85536\n",
            "Epoch: 19/30, step: 141/364, loss: 0.34280, accuracy: 0.85527\n",
            "Epoch: 19/30, step: 142/364, loss: 0.34226, accuracy: 0.85541\n",
            "Epoch: 19/30, step: 143/364, loss: 0.34259, accuracy: 0.85522\n",
            "Epoch: 19/30, step: 144/364, loss: 0.34230, accuracy: 0.85558\n",
            "Epoch: 19/30, step: 145/364, loss: 0.34196, accuracy: 0.85582\n",
            "Epoch: 19/30, step: 146/364, loss: 0.34162, accuracy: 0.85616\n",
            "Epoch: 19/30, step: 147/364, loss: 0.34208, accuracy: 0.85597\n",
            "Epoch: 19/30, step: 148/364, loss: 0.34209, accuracy: 0.85610\n",
            "Epoch: 19/30, step: 149/364, loss: 0.34187, accuracy: 0.85602\n",
            "Epoch: 19/30, step: 150/364, loss: 0.34126, accuracy: 0.85646\n",
            "Epoch: 19/30, step: 151/364, loss: 0.34139, accuracy: 0.85606\n",
            "Epoch: 19/30, step: 152/364, loss: 0.34096, accuracy: 0.85629\n",
            "Epoch: 19/30, step: 153/364, loss: 0.34090, accuracy: 0.85621\n",
            "Epoch: 19/30, step: 154/364, loss: 0.34161, accuracy: 0.85552\n",
            "Epoch: 19/30, step: 155/364, loss: 0.34166, accuracy: 0.85585\n",
            "Epoch: 19/30, step: 156/364, loss: 0.34210, accuracy: 0.85547\n",
            "Epoch: 19/30, step: 157/364, loss: 0.34215, accuracy: 0.85549\n",
            "Epoch: 19/30, step: 158/364, loss: 0.34183, accuracy: 0.85581\n",
            "Epoch: 19/30, step: 159/364, loss: 0.34176, accuracy: 0.85554\n",
            "Epoch: 19/30, step: 160/364, loss: 0.34104, accuracy: 0.85605\n",
            "Epoch: 19/30, step: 161/364, loss: 0.34074, accuracy: 0.85627\n",
            "Epoch: 19/30, step: 162/364, loss: 0.34038, accuracy: 0.85687\n",
            "Epoch: 19/30, step: 163/364, loss: 0.34028, accuracy: 0.85679\n",
            "Epoch: 19/30, step: 164/364, loss: 0.33982, accuracy: 0.85747\n",
            "Epoch: 19/30, step: 165/364, loss: 0.34024, accuracy: 0.85729\n",
            "Epoch: 19/30, step: 166/364, loss: 0.34009, accuracy: 0.85768\n",
            "Epoch: 19/30, step: 167/364, loss: 0.33943, accuracy: 0.85797\n",
            "Epoch: 19/30, step: 168/364, loss: 0.33878, accuracy: 0.85835\n",
            "Epoch: 19/30, step: 169/364, loss: 0.33862, accuracy: 0.85854\n",
            "Epoch: 19/30, step: 170/364, loss: 0.33835, accuracy: 0.85892\n",
            "Epoch: 19/30, step: 171/364, loss: 0.33815, accuracy: 0.85901\n",
            "Epoch: 19/30, step: 172/364, loss: 0.33792, accuracy: 0.85919\n",
            "Epoch: 19/30, step: 173/364, loss: 0.33800, accuracy: 0.85919\n",
            "Epoch: 19/30, step: 174/364, loss: 0.33745, accuracy: 0.85946\n",
            "Epoch: 19/30, step: 175/364, loss: 0.33717, accuracy: 0.85973\n",
            "Epoch: 19/30, step: 176/364, loss: 0.33735, accuracy: 0.85964\n",
            "Epoch: 19/30, step: 177/364, loss: 0.33722, accuracy: 0.85946\n",
            "Epoch: 19/30, step: 178/364, loss: 0.33745, accuracy: 0.85929\n",
            "Epoch: 19/30, step: 179/364, loss: 0.33718, accuracy: 0.85929\n",
            "Epoch: 19/30, step: 180/364, loss: 0.33784, accuracy: 0.85894\n",
            "Epoch: 19/30, step: 181/364, loss: 0.33807, accuracy: 0.85868\n",
            "Epoch: 19/30, step: 182/364, loss: 0.33814, accuracy: 0.85843\n",
            "Epoch: 19/30, step: 183/364, loss: 0.33803, accuracy: 0.85844\n",
            "Epoch: 19/30, step: 184/364, loss: 0.33760, accuracy: 0.85887\n",
            "Epoch: 19/30, step: 185/364, loss: 0.33771, accuracy: 0.85887\n",
            "Epoch: 19/30, step: 186/364, loss: 0.33703, accuracy: 0.85921\n",
            "Epoch: 19/30, step: 187/364, loss: 0.33659, accuracy: 0.85954\n",
            "Epoch: 19/30, step: 188/364, loss: 0.33606, accuracy: 0.85996\n",
            "Epoch: 19/30, step: 189/364, loss: 0.33556, accuracy: 0.86037\n",
            "Epoch: 19/30, step: 190/364, loss: 0.33554, accuracy: 0.86028\n",
            "Epoch: 19/30, step: 191/364, loss: 0.33576, accuracy: 0.86019\n",
            "Epoch: 19/30, step: 192/364, loss: 0.33550, accuracy: 0.86027\n",
            "Epoch: 19/30, step: 193/364, loss: 0.33532, accuracy: 0.86059\n",
            "Epoch: 19/30, step: 194/364, loss: 0.33518, accuracy: 0.86050\n",
            "Epoch: 19/30, step: 195/364, loss: 0.33574, accuracy: 0.86026\n",
            "Epoch: 19/30, step: 196/364, loss: 0.33590, accuracy: 0.86001\n",
            "Epoch: 19/30, step: 197/364, loss: 0.33586, accuracy: 0.85993\n",
            "Epoch: 19/30, step: 198/364, loss: 0.33558, accuracy: 0.86009\n",
            "Epoch: 19/30, step: 199/364, loss: 0.33551, accuracy: 0.86000\n",
            "Epoch: 19/30, step: 200/364, loss: 0.33527, accuracy: 0.86023\n",
            "Epoch: 19/30, step: 201/364, loss: 0.33551, accuracy: 0.86039\n",
            "Epoch: 19/30, step: 202/364, loss: 0.33528, accuracy: 0.86046\n",
            "Epoch: 19/30, step: 203/364, loss: 0.33512, accuracy: 0.86068\n",
            "Epoch: 19/30, step: 204/364, loss: 0.33502, accuracy: 0.86068\n",
            "Epoch: 19/30, step: 205/364, loss: 0.33479, accuracy: 0.86082\n",
            "Epoch: 19/30, step: 206/364, loss: 0.33445, accuracy: 0.86104\n",
            "Epoch: 19/30, step: 207/364, loss: 0.33456, accuracy: 0.86088\n",
            "Epoch: 19/30, step: 208/364, loss: 0.33459, accuracy: 0.86095\n",
            "Epoch: 19/30, step: 209/364, loss: 0.33433, accuracy: 0.86117\n",
            "Epoch: 19/30, step: 210/364, loss: 0.33449, accuracy: 0.86109\n",
            "Epoch: 19/30, step: 211/364, loss: 0.33422, accuracy: 0.86123\n",
            "Epoch: 19/30, step: 212/364, loss: 0.33445, accuracy: 0.86092\n",
            "Epoch: 19/30, step: 213/364, loss: 0.33496, accuracy: 0.86048\n",
            "Epoch: 19/30, step: 214/364, loss: 0.33493, accuracy: 0.86047\n",
            "Epoch: 19/30, step: 215/364, loss: 0.33500, accuracy: 0.86054\n",
            "Epoch: 19/30, step: 216/364, loss: 0.33522, accuracy: 0.86039\n",
            "Epoch: 19/30, step: 217/364, loss: 0.33480, accuracy: 0.86053\n",
            "Epoch: 19/30, step: 218/364, loss: 0.33462, accuracy: 0.86059\n",
            "Epoch: 19/30, step: 219/364, loss: 0.33517, accuracy: 0.86030\n",
            "Epoch: 19/30, step: 220/364, loss: 0.33472, accuracy: 0.86065\n",
            "Epoch: 19/30, step: 221/364, loss: 0.33484, accuracy: 0.86044\n",
            "Epoch: 19/30, step: 222/364, loss: 0.33485, accuracy: 0.86036\n",
            "Epoch: 19/30, step: 223/364, loss: 0.33482, accuracy: 0.86050\n",
            "Epoch: 19/30, step: 224/364, loss: 0.33495, accuracy: 0.86049\n",
            "Epoch: 19/30, step: 225/364, loss: 0.33507, accuracy: 0.86007\n",
            "Epoch: 19/30, step: 226/364, loss: 0.33524, accuracy: 0.86014\n",
            "Epoch: 19/30, step: 227/364, loss: 0.33548, accuracy: 0.85993\n",
            "Epoch: 19/30, step: 228/364, loss: 0.33540, accuracy: 0.85985\n",
            "Epoch: 19/30, step: 229/364, loss: 0.33526, accuracy: 0.85978\n",
            "Epoch: 19/30, step: 230/364, loss: 0.33537, accuracy: 0.85958\n",
            "Epoch: 19/30, step: 231/364, loss: 0.33535, accuracy: 0.85965\n",
            "Epoch: 19/30, step: 232/364, loss: 0.33541, accuracy: 0.85971\n",
            "Epoch: 19/30, step: 233/364, loss: 0.33540, accuracy: 0.85978\n",
            "Epoch: 19/30, step: 234/364, loss: 0.33564, accuracy: 0.85951\n",
            "Epoch: 19/30, step: 235/364, loss: 0.33544, accuracy: 0.85964\n",
            "Epoch: 19/30, step: 236/364, loss: 0.33524, accuracy: 0.85984\n",
            "Epoch: 19/30, step: 237/364, loss: 0.33512, accuracy: 0.85984\n",
            "Epoch: 19/30, step: 238/364, loss: 0.33497, accuracy: 0.85983\n",
            "Epoch: 19/30, step: 239/364, loss: 0.33534, accuracy: 0.85951\n",
            "Epoch: 19/30, step: 240/364, loss: 0.33504, accuracy: 0.85977\n",
            "Epoch: 19/30, step: 241/364, loss: 0.33494, accuracy: 0.85976\n",
            "Epoch: 19/30, step: 242/364, loss: 0.33498, accuracy: 0.85970\n",
            "Epoch: 19/30, step: 243/364, loss: 0.33488, accuracy: 0.85976\n",
            "Epoch: 19/30, step: 244/364, loss: 0.33456, accuracy: 0.85995\n",
            "Epoch: 19/30, step: 245/364, loss: 0.33458, accuracy: 0.85982\n",
            "Epoch: 19/30, step: 246/364, loss: 0.33506, accuracy: 0.85963\n",
            "Epoch: 19/30, step: 247/364, loss: 0.33496, accuracy: 0.85982\n",
            "Epoch: 19/30, step: 248/364, loss: 0.33531, accuracy: 0.85950\n",
            "Epoch: 19/30, step: 249/364, loss: 0.33540, accuracy: 0.85944\n",
            "Epoch: 19/30, step: 250/364, loss: 0.33525, accuracy: 0.85969\n",
            "Epoch: 19/30, step: 251/364, loss: 0.33503, accuracy: 0.85981\n",
            "Epoch: 19/30, step: 252/364, loss: 0.33522, accuracy: 0.85962\n",
            "Epoch: 19/30, step: 253/364, loss: 0.33545, accuracy: 0.85956\n",
            "Epoch: 19/30, step: 254/364, loss: 0.33564, accuracy: 0.85944\n",
            "Epoch: 19/30, step: 255/364, loss: 0.33517, accuracy: 0.85987\n",
            "Epoch: 19/30, step: 256/364, loss: 0.33512, accuracy: 0.85992\n",
            "Epoch: 19/30, step: 257/364, loss: 0.33575, accuracy: 0.85956\n",
            "Epoch: 19/30, step: 258/364, loss: 0.33565, accuracy: 0.85968\n",
            "Epoch: 19/30, step: 259/364, loss: 0.33545, accuracy: 0.85968\n",
            "Epoch: 19/30, step: 260/364, loss: 0.33561, accuracy: 0.85956\n",
            "Epoch: 19/30, step: 261/364, loss: 0.33547, accuracy: 0.85973\n",
            "Epoch: 19/30, step: 262/364, loss: 0.33554, accuracy: 0.85967\n",
            "Epoch: 19/30, step: 263/364, loss: 0.33525, accuracy: 0.85985\n",
            "Epoch: 19/30, step: 264/364, loss: 0.33523, accuracy: 0.85985\n",
            "Epoch: 19/30, step: 265/364, loss: 0.33532, accuracy: 0.85985\n",
            "Epoch: 19/30, step: 266/364, loss: 0.33490, accuracy: 0.86008\n",
            "Epoch: 19/30, step: 267/364, loss: 0.33529, accuracy: 0.85973\n",
            "Epoch: 19/30, step: 268/364, loss: 0.33499, accuracy: 0.86002\n",
            "Epoch: 19/30, step: 269/364, loss: 0.33501, accuracy: 0.85996\n",
            "Epoch: 19/30, step: 270/364, loss: 0.33507, accuracy: 0.85972\n",
            "Epoch: 19/30, step: 271/364, loss: 0.33475, accuracy: 0.86001\n",
            "Epoch: 19/30, step: 272/364, loss: 0.33461, accuracy: 0.86024\n",
            "Epoch: 19/30, step: 273/364, loss: 0.33462, accuracy: 0.86023\n",
            "Epoch: 19/30, step: 274/364, loss: 0.33471, accuracy: 0.86023\n",
            "Epoch: 19/30, step: 275/364, loss: 0.33438, accuracy: 0.86045\n",
            "Epoch: 19/30, step: 276/364, loss: 0.33432, accuracy: 0.86056\n",
            "Epoch: 19/30, step: 277/364, loss: 0.33415, accuracy: 0.86073\n",
            "Epoch: 19/30, step: 278/364, loss: 0.33366, accuracy: 0.86117\n",
            "Epoch: 19/30, step: 279/364, loss: 0.33380, accuracy: 0.86106\n",
            "Epoch: 19/30, step: 280/364, loss: 0.33370, accuracy: 0.86110\n",
            "Epoch: 19/30, step: 281/364, loss: 0.33388, accuracy: 0.86104\n",
            "Epoch: 19/30, step: 282/364, loss: 0.33388, accuracy: 0.86098\n",
            "Epoch: 19/30, step: 283/364, loss: 0.33427, accuracy: 0.86076\n",
            "Epoch: 19/30, step: 284/364, loss: 0.33427, accuracy: 0.86092\n",
            "Epoch: 19/30, step: 285/364, loss: 0.33422, accuracy: 0.86113\n",
            "Epoch: 19/30, step: 286/364, loss: 0.33402, accuracy: 0.86145\n",
            "Epoch: 19/30, step: 287/364, loss: 0.33375, accuracy: 0.86172\n",
            "Epoch: 19/30, step: 288/364, loss: 0.33353, accuracy: 0.86192\n",
            "Epoch: 19/30, step: 289/364, loss: 0.33316, accuracy: 0.86208\n",
            "Epoch: 19/30, step: 290/364, loss: 0.33308, accuracy: 0.86212\n",
            "Epoch: 19/30, step: 291/364, loss: 0.33325, accuracy: 0.86196\n",
            "Epoch: 19/30, train loss: 0.33325, train accuracy: 0.86196, valid loss: 0.71264, valid accuracy: 0.65843\n",
            "Epoch: 20/30, step: 1/364, loss: 0.39672, accuracy: 0.81250\n",
            "Epoch: 20/30, step: 2/364, loss: 0.33637, accuracy: 0.87500\n",
            "Epoch: 20/30, step: 3/364, loss: 0.30132, accuracy: 0.89583\n",
            "Epoch: 20/30, step: 4/364, loss: 0.31358, accuracy: 0.88281\n",
            "Epoch: 20/30, step: 5/364, loss: 0.30381, accuracy: 0.89062\n",
            "Epoch: 20/30, step: 6/364, loss: 0.31998, accuracy: 0.87760\n",
            "Epoch: 20/30, step: 7/364, loss: 0.32718, accuracy: 0.86830\n",
            "Epoch: 20/30, step: 8/364, loss: 0.34053, accuracy: 0.85938\n",
            "Epoch: 20/30, step: 9/364, loss: 0.33495, accuracy: 0.86111\n",
            "Epoch: 20/30, step: 10/364, loss: 0.33448, accuracy: 0.86250\n",
            "Epoch: 20/30, step: 11/364, loss: 0.32587, accuracy: 0.86790\n",
            "Epoch: 20/30, step: 12/364, loss: 0.33191, accuracy: 0.86589\n",
            "Epoch: 20/30, step: 13/364, loss: 0.33272, accuracy: 0.87019\n",
            "Epoch: 20/30, step: 14/364, loss: 0.32778, accuracy: 0.87500\n",
            "Epoch: 20/30, step: 15/364, loss: 0.32972, accuracy: 0.87187\n",
            "Epoch: 20/30, step: 16/364, loss: 0.32796, accuracy: 0.87207\n",
            "Epoch: 20/30, step: 17/364, loss: 0.32946, accuracy: 0.86949\n",
            "Epoch: 20/30, step: 18/364, loss: 0.32492, accuracy: 0.87153\n",
            "Epoch: 20/30, step: 19/364, loss: 0.33004, accuracy: 0.86760\n",
            "Epoch: 20/30, step: 20/364, loss: 0.33092, accuracy: 0.86328\n",
            "Epoch: 20/30, step: 21/364, loss: 0.33094, accuracy: 0.86310\n",
            "Epoch: 20/30, step: 22/364, loss: 0.33319, accuracy: 0.86009\n",
            "Epoch: 20/30, step: 23/364, loss: 0.33541, accuracy: 0.85870\n",
            "Epoch: 20/30, step: 24/364, loss: 0.33547, accuracy: 0.85807\n",
            "Epoch: 20/30, step: 25/364, loss: 0.33388, accuracy: 0.85812\n",
            "Epoch: 20/30, step: 26/364, loss: 0.33399, accuracy: 0.85938\n",
            "Epoch: 20/30, step: 27/364, loss: 0.33063, accuracy: 0.86227\n",
            "Epoch: 20/30, step: 28/364, loss: 0.33011, accuracy: 0.86272\n",
            "Epoch: 20/30, step: 29/364, loss: 0.33056, accuracy: 0.86261\n",
            "Epoch: 20/30, step: 30/364, loss: 0.33488, accuracy: 0.85729\n",
            "Epoch: 20/30, step: 31/364, loss: 0.33436, accuracy: 0.85635\n",
            "Epoch: 20/30, step: 32/364, loss: 0.33598, accuracy: 0.85547\n",
            "Epoch: 20/30, step: 33/364, loss: 0.33393, accuracy: 0.85795\n",
            "Epoch: 20/30, step: 34/364, loss: 0.33696, accuracy: 0.85754\n",
            "Epoch: 20/30, step: 35/364, loss: 0.33623, accuracy: 0.85893\n",
            "Epoch: 20/30, step: 36/364, loss: 0.33717, accuracy: 0.85894\n",
            "Epoch: 20/30, step: 37/364, loss: 0.33562, accuracy: 0.86064\n",
            "Epoch: 20/30, step: 38/364, loss: 0.33477, accuracy: 0.86102\n",
            "Epoch: 20/30, step: 39/364, loss: 0.33485, accuracy: 0.86058\n",
            "Epoch: 20/30, step: 40/364, loss: 0.33310, accuracy: 0.86094\n",
            "Epoch: 20/30, step: 41/364, loss: 0.33252, accuracy: 0.86166\n",
            "Epoch: 20/30, step: 42/364, loss: 0.33089, accuracy: 0.86235\n",
            "Epoch: 20/30, step: 43/364, loss: 0.33102, accuracy: 0.86119\n",
            "Epoch: 20/30, step: 44/364, loss: 0.33057, accuracy: 0.86151\n",
            "Epoch: 20/30, step: 45/364, loss: 0.32918, accuracy: 0.86319\n",
            "Epoch: 20/30, step: 46/364, loss: 0.32729, accuracy: 0.86549\n",
            "Epoch: 20/30, step: 47/364, loss: 0.32631, accuracy: 0.86669\n",
            "Epoch: 20/30, step: 48/364, loss: 0.32606, accuracy: 0.86621\n",
            "Epoch: 20/30, step: 49/364, loss: 0.32669, accuracy: 0.86607\n",
            "Epoch: 20/30, step: 50/364, loss: 0.32767, accuracy: 0.86531\n",
            "Epoch: 20/30, step: 51/364, loss: 0.32961, accuracy: 0.86305\n",
            "Epoch: 20/30, step: 52/364, loss: 0.32974, accuracy: 0.86298\n",
            "Epoch: 20/30, step: 53/364, loss: 0.32865, accuracy: 0.86409\n",
            "Epoch: 20/30, step: 54/364, loss: 0.32970, accuracy: 0.86314\n",
            "Epoch: 20/30, step: 55/364, loss: 0.32935, accuracy: 0.86278\n",
            "Epoch: 20/30, step: 56/364, loss: 0.32965, accuracy: 0.86189\n",
            "Epoch: 20/30, step: 57/364, loss: 0.32821, accuracy: 0.86349\n",
            "Epoch: 20/30, step: 58/364, loss: 0.32712, accuracy: 0.86503\n",
            "Epoch: 20/30, step: 59/364, loss: 0.32909, accuracy: 0.86335\n",
            "Epoch: 20/30, step: 60/364, loss: 0.32958, accuracy: 0.86250\n",
            "Epoch: 20/30, step: 61/364, loss: 0.33263, accuracy: 0.86091\n",
            "Epoch: 20/30, step: 62/364, loss: 0.33037, accuracy: 0.86265\n",
            "Epoch: 20/30, step: 63/364, loss: 0.33055, accuracy: 0.86235\n",
            "Epoch: 20/30, step: 64/364, loss: 0.33011, accuracy: 0.86230\n",
            "Epoch: 20/30, step: 65/364, loss: 0.32963, accuracy: 0.86250\n",
            "Epoch: 20/30, step: 66/364, loss: 0.32881, accuracy: 0.86316\n",
            "Epoch: 20/30, step: 67/364, loss: 0.32837, accuracy: 0.86311\n",
            "Epoch: 20/30, step: 68/364, loss: 0.32882, accuracy: 0.86305\n",
            "Epoch: 20/30, step: 69/364, loss: 0.33038, accuracy: 0.86164\n",
            "Epoch: 20/30, step: 70/364, loss: 0.33018, accuracy: 0.86161\n",
            "Epoch: 20/30, step: 71/364, loss: 0.32872, accuracy: 0.86312\n",
            "Epoch: 20/30, step: 72/364, loss: 0.32871, accuracy: 0.86306\n",
            "Epoch: 20/30, step: 73/364, loss: 0.32727, accuracy: 0.86408\n",
            "Epoch: 20/30, step: 74/364, loss: 0.32603, accuracy: 0.86486\n",
            "Epoch: 20/30, step: 75/364, loss: 0.32468, accuracy: 0.86583\n",
            "Epoch: 20/30, step: 76/364, loss: 0.32583, accuracy: 0.86575\n",
            "Epoch: 20/30, step: 77/364, loss: 0.32577, accuracy: 0.86567\n",
            "Epoch: 20/30, step: 78/364, loss: 0.32606, accuracy: 0.86558\n",
            "Epoch: 20/30, step: 79/364, loss: 0.32630, accuracy: 0.86491\n",
            "Epoch: 20/30, step: 80/364, loss: 0.32550, accuracy: 0.86504\n",
            "Epoch: 20/30, step: 81/364, loss: 0.32566, accuracy: 0.86497\n",
            "Epoch: 20/30, step: 82/364, loss: 0.32552, accuracy: 0.86528\n",
            "Epoch: 20/30, step: 83/364, loss: 0.32595, accuracy: 0.86483\n",
            "Epoch: 20/30, step: 84/364, loss: 0.32659, accuracy: 0.86440\n",
            "Epoch: 20/30, step: 85/364, loss: 0.32621, accuracy: 0.86471\n",
            "Epoch: 20/30, step: 86/364, loss: 0.32625, accuracy: 0.86446\n",
            "Epoch: 20/30, step: 87/364, loss: 0.32722, accuracy: 0.86404\n",
            "Epoch: 20/30, step: 88/364, loss: 0.32714, accuracy: 0.86364\n",
            "Epoch: 20/30, step: 89/364, loss: 0.32645, accuracy: 0.86429\n",
            "Epoch: 20/30, step: 90/364, loss: 0.32575, accuracy: 0.86493\n",
            "Epoch: 20/30, step: 91/364, loss: 0.32639, accuracy: 0.86435\n",
            "Epoch: 20/30, step: 92/364, loss: 0.32556, accuracy: 0.86481\n",
            "Epoch: 20/30, step: 93/364, loss: 0.32522, accuracy: 0.86509\n",
            "Epoch: 20/30, step: 94/364, loss: 0.32603, accuracy: 0.86453\n",
            "Epoch: 20/30, step: 95/364, loss: 0.32470, accuracy: 0.86530\n",
            "Epoch: 20/30, step: 96/364, loss: 0.32361, accuracy: 0.86621\n",
            "Epoch: 20/30, step: 97/364, loss: 0.32294, accuracy: 0.86695\n",
            "Epoch: 20/30, step: 98/364, loss: 0.32181, accuracy: 0.86783\n",
            "Epoch: 20/30, step: 99/364, loss: 0.32246, accuracy: 0.86711\n",
            "Epoch: 20/30, step: 100/364, loss: 0.32136, accuracy: 0.86781\n",
            "Epoch: 20/30, step: 101/364, loss: 0.32190, accuracy: 0.86742\n",
            "Epoch: 20/30, step: 102/364, loss: 0.32302, accuracy: 0.86688\n",
            "Epoch: 20/30, step: 103/364, loss: 0.32376, accuracy: 0.86666\n",
            "Epoch: 20/30, step: 104/364, loss: 0.32366, accuracy: 0.86674\n",
            "Epoch: 20/30, step: 105/364, loss: 0.32422, accuracy: 0.86637\n",
            "Epoch: 20/30, step: 106/364, loss: 0.32372, accuracy: 0.86675\n",
            "Epoch: 20/30, step: 107/364, loss: 0.32421, accuracy: 0.86638\n",
            "Epoch: 20/30, step: 108/364, loss: 0.32455, accuracy: 0.86632\n",
            "Epoch: 20/30, step: 109/364, loss: 0.32495, accuracy: 0.86597\n",
            "Epoch: 20/30, step: 110/364, loss: 0.32495, accuracy: 0.86591\n",
            "Epoch: 20/30, step: 111/364, loss: 0.32520, accuracy: 0.86613\n",
            "Epoch: 20/30, step: 112/364, loss: 0.32495, accuracy: 0.86621\n",
            "Epoch: 20/30, step: 113/364, loss: 0.32491, accuracy: 0.86629\n",
            "Epoch: 20/30, step: 114/364, loss: 0.32590, accuracy: 0.86527\n",
            "Epoch: 20/30, step: 115/364, loss: 0.32535, accuracy: 0.86576\n",
            "Epoch: 20/30, step: 116/364, loss: 0.32536, accuracy: 0.86557\n",
            "Epoch: 20/30, step: 117/364, loss: 0.32716, accuracy: 0.86418\n",
            "Epoch: 20/30, step: 118/364, loss: 0.32853, accuracy: 0.86335\n",
            "Epoch: 20/30, step: 119/364, loss: 0.32849, accuracy: 0.86292\n",
            "Epoch: 20/30, step: 120/364, loss: 0.32912, accuracy: 0.86211\n",
            "Epoch: 20/30, step: 121/364, loss: 0.32835, accuracy: 0.86260\n",
            "Epoch: 20/30, step: 122/364, loss: 0.32941, accuracy: 0.86194\n",
            "Epoch: 20/30, step: 123/364, loss: 0.32924, accuracy: 0.86217\n",
            "Epoch: 20/30, step: 124/364, loss: 0.32913, accuracy: 0.86240\n",
            "Epoch: 20/30, step: 125/364, loss: 0.32945, accuracy: 0.86238\n",
            "Epoch: 20/30, step: 126/364, loss: 0.32936, accuracy: 0.86248\n",
            "Epoch: 20/30, step: 127/364, loss: 0.32914, accuracy: 0.86294\n",
            "Epoch: 20/30, step: 128/364, loss: 0.32884, accuracy: 0.86304\n",
            "Epoch: 20/30, step: 129/364, loss: 0.32881, accuracy: 0.86289\n",
            "Epoch: 20/30, step: 130/364, loss: 0.33066, accuracy: 0.86190\n",
            "Epoch: 20/30, step: 131/364, loss: 0.33047, accuracy: 0.86212\n",
            "Epoch: 20/30, step: 132/364, loss: 0.33002, accuracy: 0.86257\n",
            "Epoch: 20/30, step: 133/364, loss: 0.32968, accuracy: 0.86266\n",
            "Epoch: 20/30, step: 134/364, loss: 0.33066, accuracy: 0.86206\n",
            "Epoch: 20/30, step: 135/364, loss: 0.33049, accuracy: 0.86227\n",
            "Epoch: 20/30, step: 136/364, loss: 0.32977, accuracy: 0.86282\n",
            "Epoch: 20/30, step: 137/364, loss: 0.32957, accuracy: 0.86325\n",
            "Epoch: 20/30, step: 138/364, loss: 0.32957, accuracy: 0.86334\n",
            "Epoch: 20/30, step: 139/364, loss: 0.32940, accuracy: 0.86297\n",
            "Epoch: 20/30, step: 140/364, loss: 0.32889, accuracy: 0.86328\n",
            "Epoch: 20/30, step: 141/364, loss: 0.32853, accuracy: 0.86359\n",
            "Epoch: 20/30, step: 142/364, loss: 0.32836, accuracy: 0.86389\n",
            "Epoch: 20/30, step: 143/364, loss: 0.32862, accuracy: 0.86396\n",
            "Epoch: 20/30, step: 144/364, loss: 0.32872, accuracy: 0.86393\n",
            "Epoch: 20/30, step: 145/364, loss: 0.32893, accuracy: 0.86369\n",
            "Epoch: 20/30, step: 146/364, loss: 0.32939, accuracy: 0.86355\n",
            "Epoch: 20/30, step: 147/364, loss: 0.32959, accuracy: 0.86341\n",
            "Epoch: 20/30, step: 148/364, loss: 0.32927, accuracy: 0.86370\n",
            "Epoch: 20/30, step: 149/364, loss: 0.32900, accuracy: 0.86378\n",
            "Epoch: 20/30, step: 150/364, loss: 0.32894, accuracy: 0.86417\n",
            "Epoch: 20/30, step: 151/364, loss: 0.32866, accuracy: 0.86455\n",
            "Epoch: 20/30, step: 152/364, loss: 0.32909, accuracy: 0.86431\n",
            "Epoch: 20/30, step: 153/364, loss: 0.32916, accuracy: 0.86407\n",
            "Epoch: 20/30, step: 154/364, loss: 0.32857, accuracy: 0.86445\n",
            "Epoch: 20/30, step: 155/364, loss: 0.32838, accuracy: 0.86452\n",
            "Epoch: 20/30, step: 156/364, loss: 0.32881, accuracy: 0.86398\n",
            "Epoch: 20/30, step: 157/364, loss: 0.32897, accuracy: 0.86375\n",
            "Epoch: 20/30, step: 158/364, loss: 0.32878, accuracy: 0.86392\n",
            "Epoch: 20/30, step: 159/364, loss: 0.32944, accuracy: 0.86350\n",
            "Epoch: 20/30, step: 160/364, loss: 0.32892, accuracy: 0.86387\n",
            "Epoch: 20/30, step: 161/364, loss: 0.32911, accuracy: 0.86384\n",
            "Epoch: 20/30, step: 162/364, loss: 0.32911, accuracy: 0.86400\n",
            "Epoch: 20/30, step: 163/364, loss: 0.32887, accuracy: 0.86426\n",
            "Epoch: 20/30, step: 164/364, loss: 0.32941, accuracy: 0.86414\n",
            "Epoch: 20/30, step: 165/364, loss: 0.32933, accuracy: 0.86430\n",
            "Epoch: 20/30, step: 166/364, loss: 0.32874, accuracy: 0.86483\n",
            "Epoch: 20/30, step: 167/364, loss: 0.32865, accuracy: 0.86490\n",
            "Epoch: 20/30, step: 168/364, loss: 0.32799, accuracy: 0.86523\n",
            "Epoch: 20/30, step: 169/364, loss: 0.32748, accuracy: 0.86548\n",
            "Epoch: 20/30, step: 170/364, loss: 0.32708, accuracy: 0.86572\n",
            "Epoch: 20/30, step: 171/364, loss: 0.32717, accuracy: 0.86577\n",
            "Epoch: 20/30, step: 172/364, loss: 0.32771, accuracy: 0.86537\n",
            "Epoch: 20/30, step: 173/364, loss: 0.32799, accuracy: 0.86488\n",
            "Epoch: 20/30, step: 174/364, loss: 0.32785, accuracy: 0.86494\n",
            "Epoch: 20/30, step: 175/364, loss: 0.32790, accuracy: 0.86491\n",
            "Epoch: 20/30, step: 176/364, loss: 0.32734, accuracy: 0.86550\n",
            "Epoch: 20/30, step: 177/364, loss: 0.32744, accuracy: 0.86547\n",
            "Epoch: 20/30, step: 178/364, loss: 0.32697, accuracy: 0.86578\n",
            "Epoch: 20/30, step: 179/364, loss: 0.32702, accuracy: 0.86583\n",
            "Epoch: 20/30, step: 180/364, loss: 0.32743, accuracy: 0.86554\n",
            "Epoch: 20/30, step: 181/364, loss: 0.32765, accuracy: 0.86533\n",
            "Epoch: 20/30, step: 182/364, loss: 0.32781, accuracy: 0.86521\n",
            "Epoch: 20/30, step: 183/364, loss: 0.32780, accuracy: 0.86518\n",
            "Epoch: 20/30, step: 184/364, loss: 0.32776, accuracy: 0.86515\n",
            "Epoch: 20/30, step: 185/364, loss: 0.32740, accuracy: 0.86537\n",
            "Epoch: 20/30, step: 186/364, loss: 0.32704, accuracy: 0.86559\n",
            "Epoch: 20/30, step: 187/364, loss: 0.32687, accuracy: 0.86573\n",
            "Epoch: 20/30, step: 188/364, loss: 0.32652, accuracy: 0.86569\n",
            "Epoch: 20/30, step: 189/364, loss: 0.32681, accuracy: 0.86574\n",
            "Epoch: 20/30, step: 190/364, loss: 0.32655, accuracy: 0.86612\n",
            "Epoch: 20/30, step: 191/364, loss: 0.32655, accuracy: 0.86600\n",
            "Epoch: 20/30, step: 192/364, loss: 0.32624, accuracy: 0.86629\n",
            "Epoch: 20/30, step: 193/364, loss: 0.32616, accuracy: 0.86666\n",
            "Epoch: 20/30, step: 194/364, loss: 0.32597, accuracy: 0.86687\n",
            "Epoch: 20/30, step: 195/364, loss: 0.32619, accuracy: 0.86659\n",
            "Epoch: 20/30, step: 196/364, loss: 0.32586, accuracy: 0.86671\n",
            "Epoch: 20/30, step: 197/364, loss: 0.32559, accuracy: 0.86691\n",
            "Epoch: 20/30, step: 198/364, loss: 0.32545, accuracy: 0.86703\n",
            "Epoch: 20/30, step: 199/364, loss: 0.32495, accuracy: 0.86738\n",
            "Epoch: 20/30, step: 200/364, loss: 0.32511, accuracy: 0.86711\n",
            "Epoch: 20/30, step: 201/364, loss: 0.32542, accuracy: 0.86707\n",
            "Epoch: 20/30, step: 202/364, loss: 0.32548, accuracy: 0.86703\n",
            "Epoch: 20/30, step: 203/364, loss: 0.32557, accuracy: 0.86700\n",
            "Epoch: 20/30, step: 204/364, loss: 0.32531, accuracy: 0.86719\n",
            "Epoch: 20/30, step: 205/364, loss: 0.32526, accuracy: 0.86730\n",
            "Epoch: 20/30, step: 206/364, loss: 0.32487, accuracy: 0.86757\n",
            "Epoch: 20/30, step: 207/364, loss: 0.32426, accuracy: 0.86790\n",
            "Epoch: 20/30, step: 208/364, loss: 0.32423, accuracy: 0.86786\n",
            "Epoch: 20/30, step: 209/364, loss: 0.32416, accuracy: 0.86797\n",
            "Epoch: 20/30, step: 210/364, loss: 0.32385, accuracy: 0.86808\n",
            "Epoch: 20/30, step: 211/364, loss: 0.32396, accuracy: 0.86797\n",
            "Epoch: 20/30, step: 212/364, loss: 0.32417, accuracy: 0.86778\n",
            "Epoch: 20/30, step: 213/364, loss: 0.32397, accuracy: 0.86788\n",
            "Epoch: 20/30, step: 214/364, loss: 0.32371, accuracy: 0.86806\n",
            "Epoch: 20/30, step: 215/364, loss: 0.32394, accuracy: 0.86802\n",
            "Epoch: 20/30, step: 216/364, loss: 0.32350, accuracy: 0.86849\n",
            "Epoch: 20/30, step: 217/364, loss: 0.32386, accuracy: 0.86809\n",
            "Epoch: 20/30, step: 218/364, loss: 0.32360, accuracy: 0.86841\n",
            "Epoch: 20/30, step: 219/364, loss: 0.32341, accuracy: 0.86865\n",
            "Epoch: 20/30, step: 220/364, loss: 0.32321, accuracy: 0.86882\n",
            "Epoch: 20/30, step: 221/364, loss: 0.32321, accuracy: 0.86878\n",
            "Epoch: 20/30, step: 222/364, loss: 0.32307, accuracy: 0.86895\n",
            "Epoch: 20/30, step: 223/364, loss: 0.32292, accuracy: 0.86904\n",
            "Epoch: 20/30, step: 224/364, loss: 0.32274, accuracy: 0.86914\n",
            "Epoch: 20/30, step: 225/364, loss: 0.32267, accuracy: 0.86937\n",
            "Epoch: 20/30, step: 226/364, loss: 0.32231, accuracy: 0.86968\n",
            "Epoch: 20/30, step: 227/364, loss: 0.32188, accuracy: 0.86998\n",
            "Epoch: 20/30, step: 228/364, loss: 0.32153, accuracy: 0.87013\n",
            "Epoch: 20/30, step: 229/364, loss: 0.32148, accuracy: 0.87022\n",
            "Epoch: 20/30, step: 230/364, loss: 0.32106, accuracy: 0.87052\n",
            "Epoch: 20/30, step: 231/364, loss: 0.32098, accuracy: 0.87054\n",
            "Epoch: 20/30, step: 232/364, loss: 0.32120, accuracy: 0.87022\n",
            "Epoch: 20/30, step: 233/364, loss: 0.32078, accuracy: 0.87064\n",
            "Epoch: 20/30, step: 234/364, loss: 0.32082, accuracy: 0.87053\n",
            "Epoch: 20/30, step: 235/364, loss: 0.32061, accuracy: 0.87068\n",
            "Epoch: 20/30, step: 236/364, loss: 0.32020, accuracy: 0.87083\n",
            "Epoch: 20/30, step: 237/364, loss: 0.31979, accuracy: 0.87118\n",
            "Epoch: 20/30, step: 238/364, loss: 0.31975, accuracy: 0.87119\n",
            "Epoch: 20/30, step: 239/364, loss: 0.31964, accuracy: 0.87101\n",
            "Epoch: 20/30, step: 240/364, loss: 0.31938, accuracy: 0.87129\n",
            "Epoch: 20/30, step: 241/364, loss: 0.31923, accuracy: 0.87137\n",
            "Epoch: 20/30, step: 242/364, loss: 0.31921, accuracy: 0.87138\n",
            "Epoch: 20/30, step: 243/364, loss: 0.31886, accuracy: 0.87140\n",
            "Epoch: 20/30, step: 244/364, loss: 0.31864, accuracy: 0.87161\n",
            "Epoch: 20/30, step: 245/364, loss: 0.31820, accuracy: 0.87181\n",
            "Epoch: 20/30, step: 246/364, loss: 0.31854, accuracy: 0.87157\n",
            "Epoch: 20/30, step: 247/364, loss: 0.31843, accuracy: 0.87171\n",
            "Epoch: 20/30, step: 248/364, loss: 0.31850, accuracy: 0.87153\n",
            "Epoch: 20/30, step: 249/364, loss: 0.31799, accuracy: 0.87180\n",
            "Epoch: 20/30, step: 250/364, loss: 0.31771, accuracy: 0.87200\n",
            "Epoch: 20/30, step: 251/364, loss: 0.31773, accuracy: 0.87195\n",
            "Epoch: 20/30, step: 252/364, loss: 0.31789, accuracy: 0.87190\n",
            "Epoch: 20/30, step: 253/364, loss: 0.31818, accuracy: 0.87173\n",
            "Epoch: 20/30, step: 254/364, loss: 0.31792, accuracy: 0.87186\n",
            "Epoch: 20/30, step: 255/364, loss: 0.31769, accuracy: 0.87200\n",
            "Epoch: 20/30, step: 256/364, loss: 0.31767, accuracy: 0.87213\n",
            "Epoch: 20/30, step: 257/364, loss: 0.31786, accuracy: 0.87196\n",
            "Epoch: 20/30, step: 258/364, loss: 0.31800, accuracy: 0.87173\n",
            "Epoch: 20/30, step: 259/364, loss: 0.31787, accuracy: 0.87174\n",
            "Epoch: 20/30, step: 260/364, loss: 0.31781, accuracy: 0.87181\n",
            "Epoch: 20/30, step: 261/364, loss: 0.31781, accuracy: 0.87189\n",
            "Epoch: 20/30, step: 262/364, loss: 0.31795, accuracy: 0.87184\n",
            "Epoch: 20/30, step: 263/364, loss: 0.31812, accuracy: 0.87173\n",
            "Epoch: 20/30, step: 264/364, loss: 0.31824, accuracy: 0.87157\n",
            "Epoch: 20/30, step: 265/364, loss: 0.31813, accuracy: 0.87170\n",
            "Epoch: 20/30, step: 266/364, loss: 0.31825, accuracy: 0.87142\n",
            "Epoch: 20/30, step: 267/364, loss: 0.31823, accuracy: 0.87137\n",
            "Epoch: 20/30, step: 268/364, loss: 0.31802, accuracy: 0.87156\n",
            "Epoch: 20/30, step: 269/364, loss: 0.31806, accuracy: 0.87146\n",
            "Epoch: 20/30, step: 270/364, loss: 0.31787, accuracy: 0.87170\n",
            "Epoch: 20/30, step: 271/364, loss: 0.31761, accuracy: 0.87194\n",
            "Epoch: 20/30, step: 272/364, loss: 0.31758, accuracy: 0.87190\n",
            "Epoch: 20/30, step: 273/364, loss: 0.31761, accuracy: 0.87191\n",
            "Epoch: 20/30, step: 274/364, loss: 0.31759, accuracy: 0.87198\n",
            "Epoch: 20/30, step: 275/364, loss: 0.31742, accuracy: 0.87205\n",
            "Epoch: 20/30, step: 276/364, loss: 0.31721, accuracy: 0.87217\n",
            "Epoch: 20/30, step: 277/364, loss: 0.31707, accuracy: 0.87229\n",
            "Epoch: 20/30, step: 278/364, loss: 0.31736, accuracy: 0.87208\n",
            "Epoch: 20/30, step: 279/364, loss: 0.31763, accuracy: 0.87186\n",
            "Epoch: 20/30, step: 280/364, loss: 0.31786, accuracy: 0.87176\n",
            "Epoch: 20/30, step: 281/364, loss: 0.31779, accuracy: 0.87172\n",
            "Epoch: 20/30, step: 282/364, loss: 0.31804, accuracy: 0.87134\n",
            "Epoch: 20/30, step: 283/364, loss: 0.31791, accuracy: 0.87136\n",
            "Epoch: 20/30, step: 284/364, loss: 0.31779, accuracy: 0.87148\n",
            "Epoch: 20/30, step: 285/364, loss: 0.31772, accuracy: 0.87144\n",
            "Epoch: 20/30, step: 286/364, loss: 0.31744, accuracy: 0.87161\n",
            "Epoch: 20/30, step: 287/364, loss: 0.31737, accuracy: 0.87152\n",
            "Epoch: 20/30, step: 288/364, loss: 0.31719, accuracy: 0.87158\n",
            "Epoch: 20/30, step: 289/364, loss: 0.31691, accuracy: 0.87181\n",
            "Epoch: 20/30, step: 290/364, loss: 0.31662, accuracy: 0.87193\n",
            "Epoch: 20/30, step: 291/364, loss: 0.31715, accuracy: 0.87168\n",
            "Epoch: 20/30, train loss: 0.31715, train accuracy: 0.87168, valid loss: 0.67807, valid accuracy: 0.67648\n",
            "Epoch: 21/30, step: 1/364, loss: 0.31852, accuracy: 0.87500\n",
            "Epoch: 21/30, step: 2/364, loss: 0.31850, accuracy: 0.85938\n",
            "Epoch: 21/30, step: 3/364, loss: 0.31281, accuracy: 0.86458\n",
            "Epoch: 21/30, step: 4/364, loss: 0.31725, accuracy: 0.85938\n",
            "Epoch: 21/30, step: 5/364, loss: 0.29670, accuracy: 0.88125\n",
            "Epoch: 21/30, step: 6/364, loss: 0.29976, accuracy: 0.88021\n",
            "Epoch: 21/30, step: 7/364, loss: 0.30199, accuracy: 0.88170\n",
            "Epoch: 21/30, step: 8/364, loss: 0.29396, accuracy: 0.88477\n",
            "Epoch: 21/30, step: 9/364, loss: 0.28893, accuracy: 0.89062\n",
            "Epoch: 21/30, step: 10/364, loss: 0.28662, accuracy: 0.89219\n",
            "Epoch: 21/30, step: 11/364, loss: 0.27964, accuracy: 0.89347\n",
            "Epoch: 21/30, step: 12/364, loss: 0.28307, accuracy: 0.88542\n",
            "Epoch: 21/30, step: 13/364, loss: 0.29001, accuracy: 0.88462\n",
            "Epoch: 21/30, step: 14/364, loss: 0.28952, accuracy: 0.88170\n",
            "Epoch: 21/30, step: 15/364, loss: 0.29235, accuracy: 0.88125\n",
            "Epoch: 21/30, step: 16/364, loss: 0.30522, accuracy: 0.87695\n",
            "Epoch: 21/30, step: 17/364, loss: 0.30766, accuracy: 0.87500\n",
            "Epoch: 21/30, step: 18/364, loss: 0.30780, accuracy: 0.87413\n",
            "Epoch: 21/30, step: 19/364, loss: 0.30415, accuracy: 0.87664\n",
            "Epoch: 21/30, step: 20/364, loss: 0.30333, accuracy: 0.87734\n",
            "Epoch: 21/30, step: 21/364, loss: 0.30386, accuracy: 0.87500\n",
            "Epoch: 21/30, step: 22/364, loss: 0.30042, accuracy: 0.87713\n",
            "Epoch: 21/30, step: 23/364, loss: 0.30138, accuracy: 0.87772\n",
            "Epoch: 21/30, step: 24/364, loss: 0.29964, accuracy: 0.87891\n",
            "Epoch: 21/30, step: 25/364, loss: 0.30194, accuracy: 0.87750\n",
            "Epoch: 21/30, step: 26/364, loss: 0.30194, accuracy: 0.87680\n",
            "Epoch: 21/30, step: 27/364, loss: 0.30670, accuracy: 0.87558\n",
            "Epoch: 21/30, step: 28/364, loss: 0.30598, accuracy: 0.87556\n",
            "Epoch: 21/30, step: 29/364, loss: 0.30743, accuracy: 0.87554\n",
            "Epoch: 21/30, step: 30/364, loss: 0.30562, accuracy: 0.87656\n",
            "Epoch: 21/30, step: 31/364, loss: 0.30373, accuracy: 0.87752\n",
            "Epoch: 21/30, step: 32/364, loss: 0.30214, accuracy: 0.87939\n",
            "Epoch: 21/30, step: 33/364, loss: 0.30105, accuracy: 0.88021\n",
            "Epoch: 21/30, step: 34/364, loss: 0.30184, accuracy: 0.87960\n",
            "Epoch: 21/30, step: 35/364, loss: 0.30077, accuracy: 0.88036\n",
            "Epoch: 21/30, step: 36/364, loss: 0.29921, accuracy: 0.88021\n",
            "Epoch: 21/30, step: 37/364, loss: 0.29801, accuracy: 0.88049\n",
            "Epoch: 21/30, step: 38/364, loss: 0.29805, accuracy: 0.88117\n",
            "Epoch: 21/30, step: 39/364, loss: 0.29814, accuracy: 0.88141\n",
            "Epoch: 21/30, step: 40/364, loss: 0.30037, accuracy: 0.87969\n",
            "Epoch: 21/30, step: 41/364, loss: 0.30117, accuracy: 0.87881\n",
            "Epoch: 21/30, step: 42/364, loss: 0.30140, accuracy: 0.87872\n",
            "Epoch: 21/30, step: 43/364, loss: 0.30093, accuracy: 0.87900\n",
            "Epoch: 21/30, step: 44/364, loss: 0.30198, accuracy: 0.87749\n",
            "Epoch: 21/30, step: 45/364, loss: 0.30120, accuracy: 0.87882\n",
            "Epoch: 21/30, step: 46/364, loss: 0.30099, accuracy: 0.87942\n",
            "Epoch: 21/30, step: 47/364, loss: 0.29938, accuracy: 0.88065\n",
            "Epoch: 21/30, step: 48/364, loss: 0.30094, accuracy: 0.87956\n",
            "Epoch: 21/30, step: 49/364, loss: 0.30251, accuracy: 0.87883\n",
            "Epoch: 21/30, step: 50/364, loss: 0.30714, accuracy: 0.87625\n",
            "Epoch: 21/30, step: 51/364, loss: 0.30827, accuracy: 0.87561\n",
            "Epoch: 21/30, step: 52/364, loss: 0.30850, accuracy: 0.87650\n",
            "Epoch: 21/30, step: 53/364, loss: 0.30944, accuracy: 0.87647\n",
            "Epoch: 21/30, step: 54/364, loss: 0.30795, accuracy: 0.87789\n",
            "Epoch: 21/30, step: 55/364, loss: 0.30925, accuracy: 0.87756\n",
            "Epoch: 21/30, step: 56/364, loss: 0.31139, accuracy: 0.87612\n",
            "Epoch: 21/30, step: 57/364, loss: 0.31004, accuracy: 0.87719\n",
            "Epoch: 21/30, step: 58/364, loss: 0.31025, accuracy: 0.87742\n",
            "Epoch: 21/30, step: 59/364, loss: 0.31005, accuracy: 0.87685\n",
            "Epoch: 21/30, step: 60/364, loss: 0.31009, accuracy: 0.87708\n",
            "Epoch: 21/30, step: 61/364, loss: 0.30860, accuracy: 0.87859\n",
            "Epoch: 21/30, step: 62/364, loss: 0.30922, accuracy: 0.87752\n",
            "Epoch: 21/30, step: 63/364, loss: 0.30974, accuracy: 0.87649\n",
            "Epoch: 21/30, step: 64/364, loss: 0.31074, accuracy: 0.87622\n",
            "Epoch: 21/30, step: 65/364, loss: 0.31218, accuracy: 0.87476\n",
            "Epoch: 21/30, step: 66/364, loss: 0.31126, accuracy: 0.87500\n",
            "Epoch: 21/30, step: 67/364, loss: 0.31178, accuracy: 0.87453\n",
            "Epoch: 21/30, step: 68/364, loss: 0.31050, accuracy: 0.87569\n",
            "Epoch: 21/30, step: 69/364, loss: 0.31046, accuracy: 0.87545\n",
            "Epoch: 21/30, step: 70/364, loss: 0.31002, accuracy: 0.87567\n",
            "Epoch: 21/30, step: 71/364, loss: 0.30965, accuracy: 0.87610\n",
            "Epoch: 21/30, step: 72/364, loss: 0.30914, accuracy: 0.87652\n",
            "Epoch: 21/30, step: 73/364, loss: 0.30990, accuracy: 0.87564\n",
            "Epoch: 21/30, step: 74/364, loss: 0.30961, accuracy: 0.87584\n",
            "Epoch: 21/30, step: 75/364, loss: 0.30864, accuracy: 0.87667\n",
            "Epoch: 21/30, step: 76/364, loss: 0.30833, accuracy: 0.87706\n",
            "Epoch: 21/30, step: 77/364, loss: 0.30809, accuracy: 0.87703\n",
            "Epoch: 21/30, step: 78/364, loss: 0.30766, accuracy: 0.87700\n",
            "Epoch: 21/30, step: 79/364, loss: 0.30837, accuracy: 0.87619\n",
            "Epoch: 21/30, step: 80/364, loss: 0.30779, accuracy: 0.87656\n",
            "Epoch: 21/30, step: 81/364, loss: 0.30681, accuracy: 0.87731\n",
            "Epoch: 21/30, step: 82/364, loss: 0.30610, accuracy: 0.87767\n",
            "Epoch: 21/30, step: 83/364, loss: 0.30589, accuracy: 0.87782\n",
            "Epoch: 21/30, step: 84/364, loss: 0.30478, accuracy: 0.87835\n",
            "Epoch: 21/30, step: 85/364, loss: 0.30495, accuracy: 0.87794\n",
            "Epoch: 21/30, step: 86/364, loss: 0.30512, accuracy: 0.87773\n",
            "Epoch: 21/30, step: 87/364, loss: 0.30523, accuracy: 0.87716\n",
            "Epoch: 21/30, step: 88/364, loss: 0.30610, accuracy: 0.87624\n",
            "Epoch: 21/30, step: 89/364, loss: 0.30662, accuracy: 0.87588\n",
            "Epoch: 21/30, step: 90/364, loss: 0.30631, accuracy: 0.87639\n",
            "Epoch: 21/30, step: 91/364, loss: 0.30671, accuracy: 0.87552\n",
            "Epoch: 21/30, step: 92/364, loss: 0.30673, accuracy: 0.87534\n",
            "Epoch: 21/30, step: 93/364, loss: 0.30615, accuracy: 0.87550\n",
            "Epoch: 21/30, step: 94/364, loss: 0.30560, accuracy: 0.87566\n",
            "Epoch: 21/30, step: 95/364, loss: 0.30538, accuracy: 0.87615\n",
            "Epoch: 21/30, step: 96/364, loss: 0.30522, accuracy: 0.87614\n",
            "Epoch: 21/30, step: 97/364, loss: 0.30561, accuracy: 0.87597\n",
            "Epoch: 21/30, step: 98/364, loss: 0.30471, accuracy: 0.87675\n",
            "Epoch: 21/30, step: 99/364, loss: 0.30502, accuracy: 0.87674\n",
            "Epoch: 21/30, step: 100/364, loss: 0.30401, accuracy: 0.87750\n",
            "Epoch: 21/30, step: 101/364, loss: 0.30573, accuracy: 0.87608\n",
            "Epoch: 21/30, step: 102/364, loss: 0.30575, accuracy: 0.87607\n",
            "Epoch: 21/30, step: 103/364, loss: 0.30472, accuracy: 0.87682\n",
            "Epoch: 21/30, step: 104/364, loss: 0.30363, accuracy: 0.87740\n",
            "Epoch: 21/30, step: 105/364, loss: 0.30384, accuracy: 0.87723\n",
            "Epoch: 21/30, step: 106/364, loss: 0.30412, accuracy: 0.87721\n",
            "Epoch: 21/30, step: 107/364, loss: 0.30390, accuracy: 0.87734\n",
            "Epoch: 21/30, step: 108/364, loss: 0.30368, accuracy: 0.87760\n",
            "Epoch: 21/30, step: 109/364, loss: 0.30399, accuracy: 0.87729\n",
            "Epoch: 21/30, step: 110/364, loss: 0.30397, accuracy: 0.87741\n",
            "Epoch: 21/30, step: 111/364, loss: 0.30290, accuracy: 0.87810\n",
            "Epoch: 21/30, step: 112/364, loss: 0.30296, accuracy: 0.87793\n",
            "Epoch: 21/30, step: 113/364, loss: 0.30325, accuracy: 0.87790\n",
            "Epoch: 21/30, step: 114/364, loss: 0.30327, accuracy: 0.87774\n",
            "Epoch: 21/30, step: 115/364, loss: 0.30312, accuracy: 0.87745\n",
            "Epoch: 21/30, step: 116/364, loss: 0.30365, accuracy: 0.87716\n",
            "Epoch: 21/30, step: 117/364, loss: 0.30285, accuracy: 0.87767\n",
            "Epoch: 21/30, step: 118/364, loss: 0.30259, accuracy: 0.87791\n",
            "Epoch: 21/30, step: 119/364, loss: 0.30212, accuracy: 0.87802\n",
            "Epoch: 21/30, step: 120/364, loss: 0.30146, accuracy: 0.87839\n",
            "Epoch: 21/30, step: 121/364, loss: 0.30126, accuracy: 0.87874\n",
            "Epoch: 21/30, step: 122/364, loss: 0.30086, accuracy: 0.87923\n",
            "Epoch: 21/30, step: 123/364, loss: 0.30012, accuracy: 0.87970\n",
            "Epoch: 21/30, step: 124/364, loss: 0.29985, accuracy: 0.87966\n",
            "Epoch: 21/30, step: 125/364, loss: 0.29970, accuracy: 0.87988\n",
            "Epoch: 21/30, step: 126/364, loss: 0.29982, accuracy: 0.87959\n",
            "Epoch: 21/30, step: 127/364, loss: 0.29927, accuracy: 0.88004\n",
            "Epoch: 21/30, step: 128/364, loss: 0.29919, accuracy: 0.88037\n",
            "Epoch: 21/30, step: 129/364, loss: 0.29947, accuracy: 0.88033\n",
            "Epoch: 21/30, step: 130/364, loss: 0.29883, accuracy: 0.88077\n",
            "Epoch: 21/30, step: 131/364, loss: 0.29857, accuracy: 0.88096\n",
            "Epoch: 21/30, step: 132/364, loss: 0.29887, accuracy: 0.88080\n",
            "Epoch: 21/30, step: 133/364, loss: 0.29891, accuracy: 0.88076\n",
            "Epoch: 21/30, step: 134/364, loss: 0.29889, accuracy: 0.88048\n",
            "Epoch: 21/30, step: 135/364, loss: 0.29946, accuracy: 0.88009\n",
            "Epoch: 21/30, step: 136/364, loss: 0.29989, accuracy: 0.87983\n",
            "Epoch: 21/30, step: 137/364, loss: 0.29927, accuracy: 0.88013\n",
            "Epoch: 21/30, step: 138/364, loss: 0.29896, accuracy: 0.88032\n",
            "Epoch: 21/30, step: 139/364, loss: 0.29885, accuracy: 0.88085\n",
            "Epoch: 21/30, step: 140/364, loss: 0.29906, accuracy: 0.88069\n",
            "Epoch: 21/30, step: 141/364, loss: 0.29894, accuracy: 0.88087\n",
            "Epoch: 21/30, step: 142/364, loss: 0.29931, accuracy: 0.88072\n",
            "Epoch: 21/30, step: 143/364, loss: 0.29927, accuracy: 0.88068\n",
            "Epoch: 21/30, step: 144/364, loss: 0.30008, accuracy: 0.88021\n",
            "Epoch: 21/30, step: 145/364, loss: 0.29969, accuracy: 0.88039\n",
            "Epoch: 21/30, step: 146/364, loss: 0.29939, accuracy: 0.88046\n",
            "Epoch: 21/30, step: 147/364, loss: 0.29924, accuracy: 0.88063\n",
            "Epoch: 21/30, step: 148/364, loss: 0.29869, accuracy: 0.88112\n",
            "Epoch: 21/30, step: 149/364, loss: 0.29900, accuracy: 0.88077\n",
            "Epoch: 21/30, step: 150/364, loss: 0.29932, accuracy: 0.88073\n",
            "Epoch: 21/30, step: 151/364, loss: 0.29933, accuracy: 0.88069\n",
            "Epoch: 21/30, step: 152/364, loss: 0.29928, accuracy: 0.88096\n",
            "Epoch: 21/30, step: 153/364, loss: 0.29997, accuracy: 0.88062\n",
            "Epoch: 21/30, step: 154/364, loss: 0.29977, accuracy: 0.88068\n",
            "Epoch: 21/30, step: 155/364, loss: 0.29926, accuracy: 0.88085\n",
            "Epoch: 21/30, step: 156/364, loss: 0.29989, accuracy: 0.88041\n",
            "Epoch: 21/30, step: 157/364, loss: 0.29953, accuracy: 0.88057\n",
            "Epoch: 21/30, step: 158/364, loss: 0.29980, accuracy: 0.88044\n",
            "Epoch: 21/30, step: 159/364, loss: 0.29992, accuracy: 0.88021\n",
            "Epoch: 21/30, step: 160/364, loss: 0.29943, accuracy: 0.88047\n",
            "Epoch: 21/30, step: 161/364, loss: 0.29896, accuracy: 0.88082\n",
            "Epoch: 21/30, step: 162/364, loss: 0.29901, accuracy: 0.88079\n",
            "Epoch: 21/30, step: 163/364, loss: 0.29928, accuracy: 0.88056\n",
            "Epoch: 21/30, step: 164/364, loss: 0.29940, accuracy: 0.88034\n",
            "Epoch: 21/30, step: 165/364, loss: 0.29900, accuracy: 0.88078\n",
            "Epoch: 21/30, step: 166/364, loss: 0.29887, accuracy: 0.88074\n",
            "Epoch: 21/30, step: 167/364, loss: 0.29921, accuracy: 0.88033\n",
            "Epoch: 21/30, step: 168/364, loss: 0.29919, accuracy: 0.88039\n",
            "Epoch: 21/30, step: 169/364, loss: 0.29879, accuracy: 0.88064\n",
            "Epoch: 21/30, step: 170/364, loss: 0.29844, accuracy: 0.88070\n",
            "Epoch: 21/30, step: 171/364, loss: 0.29801, accuracy: 0.88103\n",
            "Epoch: 21/30, step: 172/364, loss: 0.29801, accuracy: 0.88090\n",
            "Epoch: 21/30, step: 173/364, loss: 0.29799, accuracy: 0.88114\n",
            "Epoch: 21/30, step: 174/364, loss: 0.29748, accuracy: 0.88156\n",
            "Epoch: 21/30, step: 175/364, loss: 0.29739, accuracy: 0.88170\n",
            "Epoch: 21/30, step: 176/364, loss: 0.29707, accuracy: 0.88192\n",
            "Epoch: 21/30, step: 177/364, loss: 0.29685, accuracy: 0.88206\n",
            "Epoch: 21/30, step: 178/364, loss: 0.29669, accuracy: 0.88211\n",
            "Epoch: 21/30, step: 179/364, loss: 0.29608, accuracy: 0.88259\n",
            "Epoch: 21/30, step: 180/364, loss: 0.29585, accuracy: 0.88281\n",
            "Epoch: 21/30, step: 181/364, loss: 0.29615, accuracy: 0.88260\n",
            "Epoch: 21/30, step: 182/364, loss: 0.29574, accuracy: 0.88290\n",
            "Epoch: 21/30, step: 183/364, loss: 0.29557, accuracy: 0.88286\n",
            "Epoch: 21/30, step: 184/364, loss: 0.29588, accuracy: 0.88264\n",
            "Epoch: 21/30, step: 185/364, loss: 0.29593, accuracy: 0.88252\n",
            "Epoch: 21/30, step: 186/364, loss: 0.29586, accuracy: 0.88248\n",
            "Epoch: 21/30, step: 187/364, loss: 0.29612, accuracy: 0.88235\n",
            "Epoch: 21/30, step: 188/364, loss: 0.29557, accuracy: 0.88281\n",
            "Epoch: 21/30, step: 189/364, loss: 0.29567, accuracy: 0.88261\n",
            "Epoch: 21/30, step: 190/364, loss: 0.29550, accuracy: 0.88281\n",
            "Epoch: 21/30, step: 191/364, loss: 0.29540, accuracy: 0.88285\n",
            "Epoch: 21/30, step: 192/364, loss: 0.29559, accuracy: 0.88265\n",
            "Epoch: 21/30, step: 193/364, loss: 0.29561, accuracy: 0.88269\n",
            "Epoch: 21/30, step: 194/364, loss: 0.29542, accuracy: 0.88257\n",
            "Epoch: 21/30, step: 195/364, loss: 0.29560, accuracy: 0.88261\n",
            "Epoch: 21/30, step: 196/364, loss: 0.29543, accuracy: 0.88273\n",
            "Epoch: 21/30, step: 197/364, loss: 0.29519, accuracy: 0.88293\n",
            "Epoch: 21/30, step: 198/364, loss: 0.29485, accuracy: 0.88313\n",
            "Epoch: 21/30, step: 199/364, loss: 0.29428, accuracy: 0.88356\n",
            "Epoch: 21/30, step: 200/364, loss: 0.29373, accuracy: 0.88383\n",
            "Epoch: 21/30, step: 201/364, loss: 0.29347, accuracy: 0.88402\n",
            "Epoch: 21/30, step: 202/364, loss: 0.29356, accuracy: 0.88382\n",
            "Epoch: 21/30, step: 203/364, loss: 0.29385, accuracy: 0.88377\n",
            "Epoch: 21/30, step: 204/364, loss: 0.29405, accuracy: 0.88366\n",
            "Epoch: 21/30, step: 205/364, loss: 0.29354, accuracy: 0.88392\n",
            "Epoch: 21/30, step: 206/364, loss: 0.29312, accuracy: 0.88425\n",
            "Epoch: 21/30, step: 207/364, loss: 0.29302, accuracy: 0.88421\n",
            "Epoch: 21/30, step: 208/364, loss: 0.29275, accuracy: 0.88447\n",
            "Epoch: 21/30, step: 209/364, loss: 0.29258, accuracy: 0.88442\n",
            "Epoch: 21/30, step: 210/364, loss: 0.29278, accuracy: 0.88452\n",
            "Epoch: 21/30, step: 211/364, loss: 0.29220, accuracy: 0.88492\n",
            "Epoch: 21/30, step: 212/364, loss: 0.29211, accuracy: 0.88495\n",
            "Epoch: 21/30, step: 213/364, loss: 0.29222, accuracy: 0.88483\n",
            "Epoch: 21/30, step: 214/364, loss: 0.29227, accuracy: 0.88449\n",
            "Epoch: 21/30, step: 215/364, loss: 0.29285, accuracy: 0.88430\n",
            "Epoch: 21/30, step: 216/364, loss: 0.29286, accuracy: 0.88419\n",
            "Epoch: 21/30, step: 217/364, loss: 0.29281, accuracy: 0.88414\n",
            "Epoch: 21/30, step: 218/364, loss: 0.29263, accuracy: 0.88417\n",
            "Epoch: 21/30, step: 219/364, loss: 0.29232, accuracy: 0.88442\n",
            "Epoch: 21/30, step: 220/364, loss: 0.29216, accuracy: 0.88459\n",
            "Epoch: 21/30, step: 221/364, loss: 0.29234, accuracy: 0.88447\n",
            "Epoch: 21/30, step: 222/364, loss: 0.29198, accuracy: 0.88471\n",
            "Epoch: 21/30, step: 223/364, loss: 0.29199, accuracy: 0.88474\n",
            "Epoch: 21/30, step: 224/364, loss: 0.29225, accuracy: 0.88470\n",
            "Epoch: 21/30, step: 225/364, loss: 0.29241, accuracy: 0.88444\n",
            "Epoch: 21/30, step: 226/364, loss: 0.29246, accuracy: 0.88447\n",
            "Epoch: 21/30, step: 227/364, loss: 0.29203, accuracy: 0.88484\n",
            "Epoch: 21/30, step: 228/364, loss: 0.29188, accuracy: 0.88487\n",
            "Epoch: 21/30, step: 229/364, loss: 0.29168, accuracy: 0.88496\n",
            "Epoch: 21/30, step: 230/364, loss: 0.29175, accuracy: 0.88505\n",
            "Epoch: 21/30, step: 231/364, loss: 0.29168, accuracy: 0.88494\n",
            "Epoch: 21/30, step: 232/364, loss: 0.29169, accuracy: 0.88483\n",
            "Epoch: 21/30, step: 233/364, loss: 0.29142, accuracy: 0.88492\n",
            "Epoch: 21/30, step: 234/364, loss: 0.29178, accuracy: 0.88468\n",
            "Epoch: 21/30, step: 235/364, loss: 0.29210, accuracy: 0.88451\n",
            "Epoch: 21/30, step: 236/364, loss: 0.29186, accuracy: 0.88460\n",
            "Epoch: 21/30, step: 237/364, loss: 0.29180, accuracy: 0.88476\n",
            "Epoch: 21/30, step: 238/364, loss: 0.29166, accuracy: 0.88485\n",
            "Epoch: 21/30, step: 239/364, loss: 0.29282, accuracy: 0.88402\n",
            "Epoch: 21/30, step: 240/364, loss: 0.29313, accuracy: 0.88379\n",
            "Epoch: 21/30, step: 241/364, loss: 0.29299, accuracy: 0.88369\n",
            "Epoch: 21/30, step: 242/364, loss: 0.29299, accuracy: 0.88365\n",
            "Epoch: 21/30, step: 243/364, loss: 0.29279, accuracy: 0.88381\n",
            "Epoch: 21/30, step: 244/364, loss: 0.29272, accuracy: 0.88384\n",
            "Epoch: 21/30, step: 245/364, loss: 0.29287, accuracy: 0.88374\n",
            "Epoch: 21/30, step: 246/364, loss: 0.29280, accuracy: 0.88383\n",
            "Epoch: 21/30, step: 247/364, loss: 0.29266, accuracy: 0.88386\n",
            "Epoch: 21/30, step: 248/364, loss: 0.29284, accuracy: 0.88363\n",
            "Epoch: 21/30, step: 249/364, loss: 0.29282, accuracy: 0.88360\n",
            "Epoch: 21/30, step: 250/364, loss: 0.29290, accuracy: 0.88356\n",
            "Epoch: 21/30, step: 251/364, loss: 0.29270, accuracy: 0.88384\n",
            "Epoch: 21/30, step: 252/364, loss: 0.29245, accuracy: 0.88405\n",
            "Epoch: 21/30, step: 253/364, loss: 0.29223, accuracy: 0.88433\n",
            "Epoch: 21/30, step: 254/364, loss: 0.29198, accuracy: 0.88453\n",
            "Epoch: 21/30, step: 255/364, loss: 0.29164, accuracy: 0.88474\n",
            "Epoch: 21/30, step: 256/364, loss: 0.29138, accuracy: 0.88489\n",
            "Epoch: 21/30, step: 257/364, loss: 0.29163, accuracy: 0.88467\n",
            "Epoch: 21/30, step: 258/364, loss: 0.29142, accuracy: 0.88475\n",
            "Epoch: 21/30, step: 259/364, loss: 0.29123, accuracy: 0.88489\n",
            "Epoch: 21/30, step: 260/364, loss: 0.29100, accuracy: 0.88504\n",
            "Epoch: 21/30, step: 261/364, loss: 0.29098, accuracy: 0.88518\n",
            "Epoch: 21/30, step: 262/364, loss: 0.29088, accuracy: 0.88520\n",
            "Epoch: 21/30, step: 263/364, loss: 0.29083, accuracy: 0.88528\n",
            "Epoch: 21/30, step: 264/364, loss: 0.29076, accuracy: 0.88530\n",
            "Epoch: 21/30, step: 265/364, loss: 0.29076, accuracy: 0.88520\n",
            "Epoch: 21/30, step: 266/364, loss: 0.29059, accuracy: 0.88528\n",
            "Epoch: 21/30, step: 267/364, loss: 0.29030, accuracy: 0.88548\n",
            "Epoch: 21/30, step: 268/364, loss: 0.29012, accuracy: 0.88555\n",
            "Epoch: 21/30, step: 269/364, loss: 0.29002, accuracy: 0.88563\n",
            "Epoch: 21/30, step: 270/364, loss: 0.29063, accuracy: 0.88530\n",
            "Epoch: 21/30, step: 271/364, loss: 0.29069, accuracy: 0.88515\n",
            "Epoch: 21/30, step: 272/364, loss: 0.29053, accuracy: 0.88517\n",
            "Epoch: 21/30, step: 273/364, loss: 0.29025, accuracy: 0.88536\n",
            "Epoch: 21/30, step: 274/364, loss: 0.29049, accuracy: 0.88532\n",
            "Epoch: 21/30, step: 275/364, loss: 0.29066, accuracy: 0.88523\n",
            "Epoch: 21/30, step: 276/364, loss: 0.29032, accuracy: 0.88542\n",
            "Epoch: 21/30, step: 277/364, loss: 0.29038, accuracy: 0.88538\n",
            "Epoch: 21/30, step: 278/364, loss: 0.29009, accuracy: 0.88557\n",
            "Epoch: 21/30, step: 279/364, loss: 0.29070, accuracy: 0.88519\n",
            "Epoch: 21/30, step: 280/364, loss: 0.29038, accuracy: 0.88544\n",
            "Epoch: 21/30, step: 281/364, loss: 0.29059, accuracy: 0.88529\n",
            "Epoch: 21/30, step: 282/364, loss: 0.29063, accuracy: 0.88525\n",
            "Epoch: 21/30, step: 283/364, loss: 0.29076, accuracy: 0.88510\n",
            "Epoch: 21/30, step: 284/364, loss: 0.29057, accuracy: 0.88507\n",
            "Epoch: 21/30, step: 285/364, loss: 0.29054, accuracy: 0.88509\n",
            "Epoch: 21/30, step: 286/364, loss: 0.29084, accuracy: 0.88489\n",
            "Epoch: 21/30, step: 287/364, loss: 0.29106, accuracy: 0.88464\n",
            "Epoch: 21/30, step: 288/364, loss: 0.29102, accuracy: 0.88455\n",
            "Epoch: 21/30, step: 289/364, loss: 0.29093, accuracy: 0.88468\n",
            "Epoch: 21/30, step: 290/364, loss: 0.29124, accuracy: 0.88443\n",
            "Epoch: 21/30, step: 291/364, loss: 0.29106, accuracy: 0.88452\n",
            "Epoch: 21/30, train loss: 0.29106, train accuracy: 0.88452, valid loss: 0.70267, valid accuracy: 0.67347\n",
            "Epoch: 22/30, step: 1/364, loss: 0.24834, accuracy: 0.93750\n",
            "Epoch: 22/30, step: 2/364, loss: 0.27207, accuracy: 0.89844\n",
            "Epoch: 22/30, step: 3/364, loss: 0.26365, accuracy: 0.90104\n",
            "Epoch: 22/30, step: 4/364, loss: 0.27605, accuracy: 0.89453\n",
            "Epoch: 22/30, step: 5/364, loss: 0.27313, accuracy: 0.88750\n",
            "Epoch: 22/30, step: 6/364, loss: 0.27631, accuracy: 0.88021\n",
            "Epoch: 22/30, step: 7/364, loss: 0.26801, accuracy: 0.88839\n",
            "Epoch: 22/30, step: 8/364, loss: 0.26651, accuracy: 0.89062\n",
            "Epoch: 22/30, step: 9/364, loss: 0.26862, accuracy: 0.89236\n",
            "Epoch: 22/30, step: 10/364, loss: 0.26529, accuracy: 0.89844\n",
            "Epoch: 22/30, step: 11/364, loss: 0.26522, accuracy: 0.90057\n",
            "Epoch: 22/30, step: 12/364, loss: 0.26325, accuracy: 0.90365\n",
            "Epoch: 22/30, step: 13/364, loss: 0.26243, accuracy: 0.90264\n",
            "Epoch: 22/30, step: 14/364, loss: 0.26670, accuracy: 0.89955\n",
            "Epoch: 22/30, step: 15/364, loss: 0.26636, accuracy: 0.90208\n",
            "Epoch: 22/30, step: 16/364, loss: 0.26483, accuracy: 0.90234\n",
            "Epoch: 22/30, step: 17/364, loss: 0.26356, accuracy: 0.90533\n",
            "Epoch: 22/30, step: 18/364, loss: 0.26891, accuracy: 0.90104\n",
            "Epoch: 22/30, step: 19/364, loss: 0.27187, accuracy: 0.89885\n",
            "Epoch: 22/30, step: 20/364, loss: 0.27204, accuracy: 0.89844\n",
            "Epoch: 22/30, step: 21/364, loss: 0.27481, accuracy: 0.89658\n",
            "Epoch: 22/30, step: 22/364, loss: 0.27021, accuracy: 0.89844\n",
            "Epoch: 22/30, step: 23/364, loss: 0.27474, accuracy: 0.89402\n",
            "Epoch: 22/30, step: 24/364, loss: 0.27560, accuracy: 0.89193\n",
            "Epoch: 22/30, step: 25/364, loss: 0.28131, accuracy: 0.88750\n",
            "Epoch: 22/30, step: 26/364, loss: 0.28271, accuracy: 0.88762\n",
            "Epoch: 22/30, step: 27/364, loss: 0.28069, accuracy: 0.88773\n",
            "Epoch: 22/30, step: 28/364, loss: 0.28168, accuracy: 0.88783\n",
            "Epoch: 22/30, step: 29/364, loss: 0.28390, accuracy: 0.88685\n",
            "Epoch: 22/30, step: 30/364, loss: 0.28335, accuracy: 0.88802\n",
            "Epoch: 22/30, step: 31/364, loss: 0.28139, accuracy: 0.88962\n",
            "Epoch: 22/30, step: 32/364, loss: 0.28111, accuracy: 0.89014\n",
            "Epoch: 22/30, step: 33/364, loss: 0.28311, accuracy: 0.88778\n",
            "Epoch: 22/30, step: 34/364, loss: 0.28097, accuracy: 0.88971\n",
            "Epoch: 22/30, step: 35/364, loss: 0.28160, accuracy: 0.89062\n",
            "Epoch: 22/30, step: 36/364, loss: 0.28211, accuracy: 0.89019\n",
            "Epoch: 22/30, step: 37/364, loss: 0.28157, accuracy: 0.88936\n",
            "Epoch: 22/30, step: 38/364, loss: 0.28127, accuracy: 0.88939\n",
            "Epoch: 22/30, step: 39/364, loss: 0.28207, accuracy: 0.88942\n",
            "Epoch: 22/30, step: 40/364, loss: 0.28195, accuracy: 0.88945\n",
            "Epoch: 22/30, step: 41/364, loss: 0.28043, accuracy: 0.89062\n",
            "Epoch: 22/30, step: 42/364, loss: 0.27943, accuracy: 0.89100\n",
            "Epoch: 22/30, step: 43/364, loss: 0.27767, accuracy: 0.89208\n",
            "Epoch: 22/30, step: 44/364, loss: 0.27628, accuracy: 0.89240\n",
            "Epoch: 22/30, step: 45/364, loss: 0.27856, accuracy: 0.89062\n",
            "Epoch: 22/30, step: 46/364, loss: 0.27861, accuracy: 0.89062\n",
            "Epoch: 22/30, step: 47/364, loss: 0.28031, accuracy: 0.89062\n",
            "Epoch: 22/30, step: 48/364, loss: 0.27948, accuracy: 0.89160\n",
            "Epoch: 22/30, step: 49/364, loss: 0.27723, accuracy: 0.89318\n",
            "Epoch: 22/30, step: 50/364, loss: 0.27683, accuracy: 0.89250\n",
            "Epoch: 22/30, step: 51/364, loss: 0.27839, accuracy: 0.89154\n",
            "Epoch: 22/30, step: 52/364, loss: 0.27852, accuracy: 0.89123\n",
            "Epoch: 22/30, step: 53/364, loss: 0.27758, accuracy: 0.89210\n",
            "Epoch: 22/30, step: 54/364, loss: 0.27533, accuracy: 0.89410\n",
            "Epoch: 22/30, step: 55/364, loss: 0.27470, accuracy: 0.89460\n",
            "Epoch: 22/30, step: 56/364, loss: 0.27450, accuracy: 0.89453\n",
            "Epoch: 22/30, step: 57/364, loss: 0.27491, accuracy: 0.89474\n",
            "Epoch: 22/30, step: 58/364, loss: 0.27517, accuracy: 0.89359\n",
            "Epoch: 22/30, step: 59/364, loss: 0.27618, accuracy: 0.89195\n",
            "Epoch: 22/30, step: 60/364, loss: 0.27524, accuracy: 0.89271\n",
            "Epoch: 22/30, step: 61/364, loss: 0.27590, accuracy: 0.89293\n",
            "Epoch: 22/30, step: 62/364, loss: 0.27563, accuracy: 0.89239\n",
            "Epoch: 22/30, step: 63/364, loss: 0.27566, accuracy: 0.89236\n",
            "Epoch: 22/30, step: 64/364, loss: 0.27545, accuracy: 0.89209\n",
            "Epoch: 22/30, step: 65/364, loss: 0.27403, accuracy: 0.89303\n",
            "Epoch: 22/30, step: 66/364, loss: 0.27378, accuracy: 0.89323\n",
            "Epoch: 22/30, step: 67/364, loss: 0.27294, accuracy: 0.89319\n",
            "Epoch: 22/30, step: 68/364, loss: 0.27191, accuracy: 0.89430\n",
            "Epoch: 22/30, step: 69/364, loss: 0.27398, accuracy: 0.89266\n",
            "Epoch: 22/30, step: 70/364, loss: 0.27408, accuracy: 0.89263\n",
            "Epoch: 22/30, step: 71/364, loss: 0.27519, accuracy: 0.89217\n",
            "Epoch: 22/30, step: 72/364, loss: 0.27555, accuracy: 0.89214\n",
            "Epoch: 22/30, step: 73/364, loss: 0.27582, accuracy: 0.89191\n",
            "Epoch: 22/30, step: 74/364, loss: 0.27493, accuracy: 0.89253\n",
            "Epoch: 22/30, step: 75/364, loss: 0.27406, accuracy: 0.89271\n",
            "Epoch: 22/30, step: 76/364, loss: 0.27467, accuracy: 0.89186\n",
            "Epoch: 22/30, step: 77/364, loss: 0.27493, accuracy: 0.89205\n",
            "Epoch: 22/30, step: 78/364, loss: 0.27444, accuracy: 0.89263\n",
            "Epoch: 22/30, step: 79/364, loss: 0.27523, accuracy: 0.89201\n",
            "Epoch: 22/30, step: 80/364, loss: 0.27508, accuracy: 0.89219\n",
            "Epoch: 22/30, step: 81/364, loss: 0.27561, accuracy: 0.89198\n",
            "Epoch: 22/30, step: 82/364, loss: 0.27525, accuracy: 0.89215\n",
            "Epoch: 22/30, step: 83/364, loss: 0.27519, accuracy: 0.89194\n",
            "Epoch: 22/30, step: 84/364, loss: 0.27518, accuracy: 0.89193\n",
            "Epoch: 22/30, step: 85/364, loss: 0.27603, accuracy: 0.89136\n",
            "Epoch: 22/30, step: 86/364, loss: 0.27642, accuracy: 0.89117\n",
            "Epoch: 22/30, step: 87/364, loss: 0.27639, accuracy: 0.89098\n",
            "Epoch: 22/30, step: 88/364, loss: 0.27623, accuracy: 0.89134\n",
            "Epoch: 22/30, step: 89/364, loss: 0.27607, accuracy: 0.89168\n",
            "Epoch: 22/30, step: 90/364, loss: 0.27663, accuracy: 0.89132\n",
            "Epoch: 22/30, step: 91/364, loss: 0.27642, accuracy: 0.89183\n",
            "Epoch: 22/30, step: 92/364, loss: 0.27734, accuracy: 0.89096\n",
            "Epoch: 22/30, step: 93/364, loss: 0.27789, accuracy: 0.89079\n",
            "Epoch: 22/30, step: 94/364, loss: 0.27774, accuracy: 0.89062\n",
            "Epoch: 22/30, step: 95/364, loss: 0.27742, accuracy: 0.89046\n",
            "Epoch: 22/30, step: 96/364, loss: 0.27890, accuracy: 0.88997\n",
            "Epoch: 22/30, step: 97/364, loss: 0.27933, accuracy: 0.88982\n",
            "Epoch: 22/30, step: 98/364, loss: 0.27873, accuracy: 0.89015\n",
            "Epoch: 22/30, step: 99/364, loss: 0.27893, accuracy: 0.88984\n",
            "Epoch: 22/30, step: 100/364, loss: 0.27936, accuracy: 0.88969\n",
            "Epoch: 22/30, step: 101/364, loss: 0.27971, accuracy: 0.88985\n",
            "Epoch: 22/30, step: 102/364, loss: 0.27981, accuracy: 0.88986\n",
            "Epoch: 22/30, step: 103/364, loss: 0.27971, accuracy: 0.88987\n",
            "Epoch: 22/30, step: 104/364, loss: 0.28005, accuracy: 0.88957\n",
            "Epoch: 22/30, step: 105/364, loss: 0.27888, accuracy: 0.89033\n",
            "Epoch: 22/30, step: 106/364, loss: 0.27867, accuracy: 0.89077\n",
            "Epoch: 22/30, step: 107/364, loss: 0.27895, accuracy: 0.89077\n",
            "Epoch: 22/30, step: 108/364, loss: 0.27936, accuracy: 0.89048\n",
            "Epoch: 22/30, step: 109/364, loss: 0.27877, accuracy: 0.89077\n",
            "Epoch: 22/30, step: 110/364, loss: 0.27891, accuracy: 0.89077\n",
            "Epoch: 22/30, step: 111/364, loss: 0.27880, accuracy: 0.89091\n",
            "Epoch: 22/30, step: 112/364, loss: 0.27866, accuracy: 0.89076\n",
            "Epoch: 22/30, step: 113/364, loss: 0.27863, accuracy: 0.89035\n",
            "Epoch: 22/30, step: 114/364, loss: 0.27909, accuracy: 0.88994\n",
            "Epoch: 22/30, step: 115/364, loss: 0.27857, accuracy: 0.89022\n",
            "Epoch: 22/30, step: 116/364, loss: 0.27923, accuracy: 0.88982\n",
            "Epoch: 22/30, step: 117/364, loss: 0.27856, accuracy: 0.89009\n",
            "Epoch: 22/30, step: 118/364, loss: 0.27775, accuracy: 0.89076\n",
            "Epoch: 22/30, step: 119/364, loss: 0.27742, accuracy: 0.89102\n",
            "Epoch: 22/30, step: 120/364, loss: 0.27742, accuracy: 0.89115\n",
            "Epoch: 22/30, step: 121/364, loss: 0.27747, accuracy: 0.89101\n",
            "Epoch: 22/30, step: 122/364, loss: 0.27802, accuracy: 0.89088\n",
            "Epoch: 22/30, step: 123/364, loss: 0.27800, accuracy: 0.89075\n",
            "Epoch: 22/30, step: 124/364, loss: 0.27786, accuracy: 0.89050\n",
            "Epoch: 22/30, step: 125/364, loss: 0.27813, accuracy: 0.89012\n",
            "Epoch: 22/30, step: 126/364, loss: 0.27738, accuracy: 0.89062\n",
            "Epoch: 22/30, step: 127/364, loss: 0.27758, accuracy: 0.89112\n",
            "Epoch: 22/30, step: 128/364, loss: 0.27756, accuracy: 0.89099\n",
            "Epoch: 22/30, step: 129/364, loss: 0.27739, accuracy: 0.89087\n",
            "Epoch: 22/30, step: 130/364, loss: 0.27731, accuracy: 0.89087\n",
            "Epoch: 22/30, step: 131/364, loss: 0.27656, accuracy: 0.89134\n",
            "Epoch: 22/30, step: 132/364, loss: 0.27646, accuracy: 0.89110\n",
            "Epoch: 22/30, step: 133/364, loss: 0.27782, accuracy: 0.89027\n",
            "Epoch: 22/30, step: 134/364, loss: 0.27733, accuracy: 0.89062\n",
            "Epoch: 22/30, step: 135/364, loss: 0.27677, accuracy: 0.89109\n",
            "Epoch: 22/30, step: 136/364, loss: 0.27708, accuracy: 0.89051\n",
            "Epoch: 22/30, step: 137/364, loss: 0.27706, accuracy: 0.89051\n",
            "Epoch: 22/30, step: 138/364, loss: 0.27628, accuracy: 0.89119\n",
            "Epoch: 22/30, step: 139/364, loss: 0.27545, accuracy: 0.89197\n",
            "Epoch: 22/30, step: 140/364, loss: 0.27557, accuracy: 0.89163\n",
            "Epoch: 22/30, step: 141/364, loss: 0.27544, accuracy: 0.89173\n",
            "Epoch: 22/30, step: 142/364, loss: 0.27549, accuracy: 0.89173\n",
            "Epoch: 22/30, step: 143/364, loss: 0.27506, accuracy: 0.89215\n",
            "Epoch: 22/30, step: 144/364, loss: 0.27524, accuracy: 0.89149\n",
            "Epoch: 22/30, step: 145/364, loss: 0.27550, accuracy: 0.89116\n",
            "Epoch: 22/30, step: 146/364, loss: 0.27541, accuracy: 0.89137\n",
            "Epoch: 22/30, step: 147/364, loss: 0.27486, accuracy: 0.89179\n",
            "Epoch: 22/30, step: 148/364, loss: 0.27494, accuracy: 0.89147\n",
            "Epoch: 22/30, step: 149/364, loss: 0.27527, accuracy: 0.89104\n",
            "Epoch: 22/30, step: 150/364, loss: 0.27495, accuracy: 0.89135\n",
            "Epoch: 22/30, step: 151/364, loss: 0.27437, accuracy: 0.89156\n",
            "Epoch: 22/30, step: 152/364, loss: 0.27441, accuracy: 0.89165\n",
            "Epoch: 22/30, step: 153/364, loss: 0.27484, accuracy: 0.89154\n",
            "Epoch: 22/30, step: 154/364, loss: 0.27487, accuracy: 0.89154\n",
            "Epoch: 22/30, step: 155/364, loss: 0.27459, accuracy: 0.89173\n",
            "Epoch: 22/30, step: 156/364, loss: 0.27551, accuracy: 0.89093\n",
            "Epoch: 22/30, step: 157/364, loss: 0.27540, accuracy: 0.89112\n",
            "Epoch: 22/30, step: 158/364, loss: 0.27482, accuracy: 0.89161\n",
            "Epoch: 22/30, step: 159/364, loss: 0.27456, accuracy: 0.89180\n",
            "Epoch: 22/30, step: 160/364, loss: 0.27504, accuracy: 0.89160\n",
            "Epoch: 22/30, step: 161/364, loss: 0.27500, accuracy: 0.89179\n",
            "Epoch: 22/30, step: 162/364, loss: 0.27491, accuracy: 0.89178\n",
            "Epoch: 22/30, step: 163/364, loss: 0.27473, accuracy: 0.89206\n",
            "Epoch: 22/30, step: 164/364, loss: 0.27532, accuracy: 0.89177\n",
            "Epoch: 22/30, step: 165/364, loss: 0.27505, accuracy: 0.89214\n",
            "Epoch: 22/30, step: 166/364, loss: 0.27495, accuracy: 0.89232\n",
            "Epoch: 22/30, step: 167/364, loss: 0.27445, accuracy: 0.89259\n",
            "Epoch: 22/30, step: 168/364, loss: 0.27416, accuracy: 0.89295\n",
            "Epoch: 22/30, step: 169/364, loss: 0.27391, accuracy: 0.89312\n",
            "Epoch: 22/30, step: 170/364, loss: 0.27352, accuracy: 0.89357\n",
            "Epoch: 22/30, step: 171/364, loss: 0.27397, accuracy: 0.89327\n",
            "Epoch: 22/30, step: 172/364, loss: 0.27406, accuracy: 0.89317\n",
            "Epoch: 22/30, step: 173/364, loss: 0.27373, accuracy: 0.89352\n",
            "Epoch: 22/30, step: 174/364, loss: 0.27363, accuracy: 0.89368\n",
            "Epoch: 22/30, step: 175/364, loss: 0.27337, accuracy: 0.89393\n",
            "Epoch: 22/30, step: 176/364, loss: 0.27345, accuracy: 0.89409\n",
            "Epoch: 22/30, step: 177/364, loss: 0.27404, accuracy: 0.89398\n",
            "Epoch: 22/30, step: 178/364, loss: 0.27354, accuracy: 0.89431\n",
            "Epoch: 22/30, step: 179/364, loss: 0.27364, accuracy: 0.89438\n",
            "Epoch: 22/30, step: 180/364, loss: 0.27405, accuracy: 0.89392\n",
            "Epoch: 22/30, step: 181/364, loss: 0.27378, accuracy: 0.89399\n",
            "Epoch: 22/30, step: 182/364, loss: 0.27419, accuracy: 0.89380\n",
            "Epoch: 22/30, step: 183/364, loss: 0.27370, accuracy: 0.89421\n",
            "Epoch: 22/30, step: 184/364, loss: 0.27352, accuracy: 0.89445\n",
            "Epoch: 22/30, step: 185/364, loss: 0.27316, accuracy: 0.89451\n",
            "Epoch: 22/30, step: 186/364, loss: 0.27331, accuracy: 0.89441\n",
            "Epoch: 22/30, step: 187/364, loss: 0.27344, accuracy: 0.89397\n",
            "Epoch: 22/30, step: 188/364, loss: 0.27361, accuracy: 0.89403\n",
            "Epoch: 22/30, step: 189/364, loss: 0.27331, accuracy: 0.89393\n",
            "Epoch: 22/30, step: 190/364, loss: 0.27322, accuracy: 0.89400\n",
            "Epoch: 22/30, step: 191/364, loss: 0.27317, accuracy: 0.89390\n",
            "Epoch: 22/30, step: 192/364, loss: 0.27333, accuracy: 0.89421\n",
            "Epoch: 22/30, step: 193/364, loss: 0.27425, accuracy: 0.89370\n",
            "Epoch: 22/30, step: 194/364, loss: 0.27493, accuracy: 0.89328\n",
            "Epoch: 22/30, step: 195/364, loss: 0.27438, accuracy: 0.89367\n",
            "Epoch: 22/30, step: 196/364, loss: 0.27430, accuracy: 0.89365\n",
            "Epoch: 22/30, step: 197/364, loss: 0.27417, accuracy: 0.89364\n",
            "Epoch: 22/30, step: 198/364, loss: 0.27356, accuracy: 0.89402\n",
            "Epoch: 22/30, step: 199/364, loss: 0.27353, accuracy: 0.89392\n",
            "Epoch: 22/30, step: 200/364, loss: 0.27352, accuracy: 0.89398\n",
            "Epoch: 22/30, step: 201/364, loss: 0.27343, accuracy: 0.89397\n",
            "Epoch: 22/30, step: 202/364, loss: 0.27349, accuracy: 0.89387\n",
            "Epoch: 22/30, step: 203/364, loss: 0.27328, accuracy: 0.89409\n",
            "Epoch: 22/30, step: 204/364, loss: 0.27292, accuracy: 0.89422\n",
            "Epoch: 22/30, step: 205/364, loss: 0.27257, accuracy: 0.89444\n",
            "Epoch: 22/30, step: 206/364, loss: 0.27296, accuracy: 0.89427\n",
            "Epoch: 22/30, step: 207/364, loss: 0.27261, accuracy: 0.89440\n",
            "Epoch: 22/30, step: 208/364, loss: 0.27266, accuracy: 0.89438\n",
            "Epoch: 22/30, step: 209/364, loss: 0.27226, accuracy: 0.89466\n",
            "Epoch: 22/30, step: 210/364, loss: 0.27239, accuracy: 0.89457\n",
            "Epoch: 22/30, step: 211/364, loss: 0.27237, accuracy: 0.89462\n",
            "Epoch: 22/30, step: 212/364, loss: 0.27253, accuracy: 0.89460\n",
            "Epoch: 22/30, step: 213/364, loss: 0.27255, accuracy: 0.89451\n",
            "Epoch: 22/30, step: 214/364, loss: 0.27229, accuracy: 0.89457\n",
            "Epoch: 22/30, step: 215/364, loss: 0.27190, accuracy: 0.89484\n",
            "Epoch: 22/30, step: 216/364, loss: 0.27179, accuracy: 0.89482\n",
            "Epoch: 22/30, step: 217/364, loss: 0.27143, accuracy: 0.89495\n",
            "Epoch: 22/30, step: 218/364, loss: 0.27129, accuracy: 0.89471\n",
            "Epoch: 22/30, step: 219/364, loss: 0.27192, accuracy: 0.89405\n",
            "Epoch: 22/30, step: 220/364, loss: 0.27167, accuracy: 0.89403\n",
            "Epoch: 22/30, step: 221/364, loss: 0.27244, accuracy: 0.89352\n",
            "Epoch: 22/30, step: 222/364, loss: 0.27212, accuracy: 0.89379\n",
            "Epoch: 22/30, step: 223/364, loss: 0.27226, accuracy: 0.89343\n",
            "Epoch: 22/30, step: 224/364, loss: 0.27217, accuracy: 0.89335\n",
            "Epoch: 22/30, step: 225/364, loss: 0.27202, accuracy: 0.89347\n",
            "Epoch: 22/30, step: 226/364, loss: 0.27197, accuracy: 0.89360\n",
            "Epoch: 22/30, step: 227/364, loss: 0.27220, accuracy: 0.89358\n",
            "Epoch: 22/30, step: 228/364, loss: 0.27184, accuracy: 0.89378\n",
            "Epoch: 22/30, step: 229/364, loss: 0.27167, accuracy: 0.89376\n",
            "Epoch: 22/30, step: 230/364, loss: 0.27206, accuracy: 0.89361\n",
            "Epoch: 22/30, step: 231/364, loss: 0.27225, accuracy: 0.89360\n",
            "Epoch: 22/30, step: 232/364, loss: 0.27207, accuracy: 0.89359\n",
            "Epoch: 22/30, step: 233/364, loss: 0.27206, accuracy: 0.89391\n",
            "Epoch: 22/30, step: 234/364, loss: 0.27167, accuracy: 0.89416\n",
            "Epoch: 22/30, step: 235/364, loss: 0.27172, accuracy: 0.89408\n",
            "Epoch: 22/30, step: 236/364, loss: 0.27182, accuracy: 0.89387\n",
            "Epoch: 22/30, step: 237/364, loss: 0.27172, accuracy: 0.89372\n",
            "Epoch: 22/30, step: 238/364, loss: 0.27146, accuracy: 0.89378\n",
            "Epoch: 22/30, step: 239/364, loss: 0.27118, accuracy: 0.89402\n",
            "Epoch: 22/30, step: 240/364, loss: 0.27126, accuracy: 0.89388\n",
            "Epoch: 22/30, step: 241/364, loss: 0.27149, accuracy: 0.89387\n",
            "Epoch: 22/30, step: 242/364, loss: 0.27142, accuracy: 0.89385\n",
            "Epoch: 22/30, step: 243/364, loss: 0.27133, accuracy: 0.89384\n",
            "Epoch: 22/30, step: 244/364, loss: 0.27119, accuracy: 0.89389\n",
            "Epoch: 22/30, step: 245/364, loss: 0.27114, accuracy: 0.89394\n",
            "Epoch: 22/30, step: 246/364, loss: 0.27110, accuracy: 0.89393\n",
            "Epoch: 22/30, step: 247/364, loss: 0.27137, accuracy: 0.89391\n",
            "Epoch: 22/30, step: 248/364, loss: 0.27144, accuracy: 0.89396\n",
            "Epoch: 22/30, step: 249/364, loss: 0.27122, accuracy: 0.89408\n",
            "Epoch: 22/30, step: 250/364, loss: 0.27159, accuracy: 0.89388\n",
            "Epoch: 22/30, step: 251/364, loss: 0.27214, accuracy: 0.89355\n",
            "Epoch: 22/30, step: 252/364, loss: 0.27238, accuracy: 0.89342\n",
            "Epoch: 22/30, step: 253/364, loss: 0.27233, accuracy: 0.89359\n",
            "Epoch: 22/30, step: 254/364, loss: 0.27209, accuracy: 0.89370\n",
            "Epoch: 22/30, step: 255/364, loss: 0.27225, accuracy: 0.89363\n",
            "Epoch: 22/30, step: 256/364, loss: 0.27224, accuracy: 0.89374\n",
            "Epoch: 22/30, step: 257/364, loss: 0.27191, accuracy: 0.89397\n",
            "Epoch: 22/30, step: 258/364, loss: 0.27222, accuracy: 0.89371\n",
            "Epoch: 22/30, step: 259/364, loss: 0.27196, accuracy: 0.89382\n",
            "Epoch: 22/30, step: 260/364, loss: 0.27161, accuracy: 0.89411\n",
            "Epoch: 22/30, step: 261/364, loss: 0.27189, accuracy: 0.89392\n",
            "Epoch: 22/30, step: 262/364, loss: 0.27171, accuracy: 0.89414\n",
            "Epoch: 22/30, step: 263/364, loss: 0.27216, accuracy: 0.89365\n",
            "Epoch: 22/30, step: 264/364, loss: 0.27192, accuracy: 0.89388\n",
            "Epoch: 22/30, step: 265/364, loss: 0.27158, accuracy: 0.89410\n",
            "Epoch: 22/30, step: 266/364, loss: 0.27150, accuracy: 0.89409\n",
            "Epoch: 22/30, step: 267/364, loss: 0.27122, accuracy: 0.89431\n",
            "Epoch: 22/30, step: 268/364, loss: 0.27157, accuracy: 0.89412\n",
            "Epoch: 22/30, step: 269/364, loss: 0.27149, accuracy: 0.89417\n",
            "Epoch: 22/30, step: 270/364, loss: 0.27142, accuracy: 0.89421\n",
            "Epoch: 22/30, step: 271/364, loss: 0.27130, accuracy: 0.89432\n",
            "Epoch: 22/30, step: 272/364, loss: 0.27156, accuracy: 0.89424\n",
            "Epoch: 22/30, step: 273/364, loss: 0.27127, accuracy: 0.89446\n",
            "Epoch: 22/30, step: 274/364, loss: 0.27120, accuracy: 0.89450\n",
            "Epoch: 22/30, step: 275/364, loss: 0.27143, accuracy: 0.89449\n",
            "Epoch: 22/30, step: 276/364, loss: 0.27126, accuracy: 0.89459\n",
            "Epoch: 22/30, step: 277/364, loss: 0.27120, accuracy: 0.89469\n",
            "Epoch: 22/30, step: 278/364, loss: 0.27093, accuracy: 0.89490\n",
            "Epoch: 22/30, step: 279/364, loss: 0.27072, accuracy: 0.89499\n",
            "Epoch: 22/30, step: 280/364, loss: 0.27066, accuracy: 0.89492\n",
            "Epoch: 22/30, step: 281/364, loss: 0.27052, accuracy: 0.89496\n",
            "Epoch: 22/30, step: 282/364, loss: 0.27077, accuracy: 0.89478\n",
            "Epoch: 22/30, step: 283/364, loss: 0.27092, accuracy: 0.89471\n",
            "Epoch: 22/30, step: 284/364, loss: 0.27102, accuracy: 0.89481\n",
            "Epoch: 22/30, step: 285/364, loss: 0.27092, accuracy: 0.89485\n",
            "Epoch: 22/30, step: 286/364, loss: 0.27103, accuracy: 0.89472\n",
            "Epoch: 22/30, step: 287/364, loss: 0.27104, accuracy: 0.89471\n",
            "Epoch: 22/30, step: 288/364, loss: 0.27092, accuracy: 0.89486\n",
            "Epoch: 22/30, step: 289/364, loss: 0.27098, accuracy: 0.89484\n",
            "Epoch: 22/30, step: 290/364, loss: 0.27055, accuracy: 0.89510\n",
            "Epoch: 22/30, step: 291/364, loss: 0.27028, accuracy: 0.89522\n",
            "Epoch: 22/30, train loss: 0.27028, train accuracy: 0.89522, valid loss: 0.72419, valid accuracy: 0.67992\n",
            "Epoch: 23/30, step: 1/364, loss: 0.24838, accuracy: 0.90625\n",
            "Epoch: 23/30, step: 2/364, loss: 0.21869, accuracy: 0.92188\n",
            "Epoch: 23/30, step: 3/364, loss: 0.20566, accuracy: 0.92708\n",
            "Epoch: 23/30, step: 4/364, loss: 0.24999, accuracy: 0.89844\n",
            "Epoch: 23/30, step: 5/364, loss: 0.25601, accuracy: 0.89688\n",
            "Epoch: 23/30, step: 6/364, loss: 0.25109, accuracy: 0.90104\n",
            "Epoch: 23/30, step: 7/364, loss: 0.25078, accuracy: 0.90179\n",
            "Epoch: 23/30, step: 8/364, loss: 0.24493, accuracy: 0.90234\n",
            "Epoch: 23/30, step: 9/364, loss: 0.24495, accuracy: 0.90278\n",
            "Epoch: 23/30, step: 10/364, loss: 0.25016, accuracy: 0.90469\n",
            "Epoch: 23/30, step: 11/364, loss: 0.25471, accuracy: 0.90057\n",
            "Epoch: 23/30, step: 12/364, loss: 0.25914, accuracy: 0.89714\n",
            "Epoch: 23/30, step: 13/364, loss: 0.25793, accuracy: 0.89904\n",
            "Epoch: 23/30, step: 14/364, loss: 0.25268, accuracy: 0.90290\n",
            "Epoch: 23/30, step: 15/364, loss: 0.25252, accuracy: 0.90312\n",
            "Epoch: 23/30, step: 16/364, loss: 0.25085, accuracy: 0.90430\n",
            "Epoch: 23/30, step: 17/364, loss: 0.25644, accuracy: 0.90349\n",
            "Epoch: 23/30, step: 18/364, loss: 0.25667, accuracy: 0.90191\n",
            "Epoch: 23/30, step: 19/364, loss: 0.26037, accuracy: 0.89967\n",
            "Epoch: 23/30, step: 20/364, loss: 0.26281, accuracy: 0.89609\n",
            "Epoch: 23/30, step: 21/364, loss: 0.26781, accuracy: 0.89435\n",
            "Epoch: 23/30, step: 22/364, loss: 0.26846, accuracy: 0.89631\n",
            "Epoch: 23/30, step: 23/364, loss: 0.26458, accuracy: 0.89946\n",
            "Epoch: 23/30, step: 24/364, loss: 0.26262, accuracy: 0.89844\n",
            "Epoch: 23/30, step: 25/364, loss: 0.26148, accuracy: 0.89938\n",
            "Epoch: 23/30, step: 26/364, loss: 0.25946, accuracy: 0.90024\n",
            "Epoch: 23/30, step: 27/364, loss: 0.25959, accuracy: 0.90046\n",
            "Epoch: 23/30, step: 28/364, loss: 0.26030, accuracy: 0.89955\n",
            "Epoch: 23/30, step: 29/364, loss: 0.26013, accuracy: 0.90032\n",
            "Epoch: 23/30, step: 30/364, loss: 0.26085, accuracy: 0.89948\n",
            "Epoch: 23/30, step: 31/364, loss: 0.25927, accuracy: 0.90121\n",
            "Epoch: 23/30, step: 32/364, loss: 0.25897, accuracy: 0.90088\n",
            "Epoch: 23/30, step: 33/364, loss: 0.25802, accuracy: 0.90104\n",
            "Epoch: 23/30, step: 34/364, loss: 0.26174, accuracy: 0.89890\n",
            "Epoch: 23/30, step: 35/364, loss: 0.25866, accuracy: 0.90134\n",
            "Epoch: 23/30, step: 36/364, loss: 0.25809, accuracy: 0.90148\n",
            "Epoch: 23/30, step: 37/364, loss: 0.25759, accuracy: 0.90287\n",
            "Epoch: 23/30, step: 38/364, loss: 0.25798, accuracy: 0.90296\n",
            "Epoch: 23/30, step: 39/364, loss: 0.25928, accuracy: 0.90184\n",
            "Epoch: 23/30, step: 40/364, loss: 0.25937, accuracy: 0.90195\n",
            "Epoch: 23/30, step: 41/364, loss: 0.25866, accuracy: 0.90206\n",
            "Epoch: 23/30, step: 42/364, loss: 0.26036, accuracy: 0.89993\n",
            "Epoch: 23/30, step: 43/364, loss: 0.26229, accuracy: 0.89862\n",
            "Epoch: 23/30, step: 44/364, loss: 0.26325, accuracy: 0.89844\n",
            "Epoch: 23/30, step: 45/364, loss: 0.26358, accuracy: 0.89861\n",
            "Epoch: 23/30, step: 46/364, loss: 0.26133, accuracy: 0.90014\n",
            "Epoch: 23/30, step: 47/364, loss: 0.26170, accuracy: 0.89960\n",
            "Epoch: 23/30, step: 48/364, loss: 0.26047, accuracy: 0.90007\n",
            "Epoch: 23/30, step: 49/364, loss: 0.26051, accuracy: 0.89987\n",
            "Epoch: 23/30, step: 50/364, loss: 0.26017, accuracy: 0.90031\n",
            "Epoch: 23/30, step: 51/364, loss: 0.26059, accuracy: 0.90135\n",
            "Epoch: 23/30, step: 52/364, loss: 0.26226, accuracy: 0.90144\n",
            "Epoch: 23/30, step: 53/364, loss: 0.26166, accuracy: 0.90183\n",
            "Epoch: 23/30, step: 54/364, loss: 0.25972, accuracy: 0.90278\n",
            "Epoch: 23/30, step: 55/364, loss: 0.25773, accuracy: 0.90398\n",
            "Epoch: 23/30, step: 56/364, loss: 0.25844, accuracy: 0.90402\n",
            "Epoch: 23/30, step: 57/364, loss: 0.26015, accuracy: 0.90323\n",
            "Epoch: 23/30, step: 58/364, loss: 0.26044, accuracy: 0.90329\n",
            "Epoch: 23/30, step: 59/364, loss: 0.26232, accuracy: 0.90201\n",
            "Epoch: 23/30, step: 60/364, loss: 0.26147, accuracy: 0.90260\n",
            "Epoch: 23/30, step: 61/364, loss: 0.26068, accuracy: 0.90318\n",
            "Epoch: 23/30, step: 62/364, loss: 0.26074, accuracy: 0.90297\n",
            "Epoch: 23/30, step: 63/364, loss: 0.26031, accuracy: 0.90303\n",
            "Epoch: 23/30, step: 64/364, loss: 0.25953, accuracy: 0.90405\n",
            "Epoch: 23/30, step: 65/364, loss: 0.25992, accuracy: 0.90337\n",
            "Epoch: 23/30, step: 66/364, loss: 0.26000, accuracy: 0.90341\n",
            "Epoch: 23/30, step: 67/364, loss: 0.25923, accuracy: 0.90392\n",
            "Epoch: 23/30, step: 68/364, loss: 0.25933, accuracy: 0.90372\n",
            "Epoch: 23/30, step: 69/364, loss: 0.25965, accuracy: 0.90308\n",
            "Epoch: 23/30, step: 70/364, loss: 0.26029, accuracy: 0.90246\n",
            "Epoch: 23/30, step: 71/364, loss: 0.25993, accuracy: 0.90207\n",
            "Epoch: 23/30, step: 72/364, loss: 0.26054, accuracy: 0.90126\n",
            "Epoch: 23/30, step: 73/364, loss: 0.25913, accuracy: 0.90240\n",
            "Epoch: 23/30, step: 74/364, loss: 0.26022, accuracy: 0.90160\n",
            "Epoch: 23/30, step: 75/364, loss: 0.25895, accuracy: 0.90250\n",
            "Epoch: 23/30, step: 76/364, loss: 0.25875, accuracy: 0.90296\n",
            "Epoch: 23/30, step: 77/364, loss: 0.25870, accuracy: 0.90280\n",
            "Epoch: 23/30, step: 78/364, loss: 0.25829, accuracy: 0.90284\n",
            "Epoch: 23/30, step: 79/364, loss: 0.25849, accuracy: 0.90210\n",
            "Epoch: 23/30, step: 80/364, loss: 0.25825, accuracy: 0.90195\n",
            "Epoch: 23/30, step: 81/364, loss: 0.25764, accuracy: 0.90258\n",
            "Epoch: 23/30, step: 82/364, loss: 0.25896, accuracy: 0.90206\n",
            "Epoch: 23/30, step: 83/364, loss: 0.25862, accuracy: 0.90267\n",
            "Epoch: 23/30, step: 84/364, loss: 0.25807, accuracy: 0.90309\n",
            "Epoch: 23/30, step: 85/364, loss: 0.25745, accuracy: 0.90349\n",
            "Epoch: 23/30, step: 86/364, loss: 0.25734, accuracy: 0.90352\n",
            "Epoch: 23/30, step: 87/364, loss: 0.25764, accuracy: 0.90266\n",
            "Epoch: 23/30, step: 88/364, loss: 0.25811, accuracy: 0.90234\n",
            "Epoch: 23/30, step: 89/364, loss: 0.25731, accuracy: 0.90274\n",
            "Epoch: 23/30, step: 90/364, loss: 0.25797, accuracy: 0.90243\n",
            "Epoch: 23/30, step: 91/364, loss: 0.25942, accuracy: 0.90179\n",
            "Epoch: 23/30, step: 92/364, loss: 0.25835, accuracy: 0.90251\n",
            "Epoch: 23/30, step: 93/364, loss: 0.25777, accuracy: 0.90272\n",
            "Epoch: 23/30, step: 94/364, loss: 0.25852, accuracy: 0.90276\n",
            "Epoch: 23/30, step: 95/364, loss: 0.25807, accuracy: 0.90329\n",
            "Epoch: 23/30, step: 96/364, loss: 0.25809, accuracy: 0.90348\n",
            "Epoch: 23/30, step: 97/364, loss: 0.25773, accuracy: 0.90351\n",
            "Epoch: 23/30, step: 98/364, loss: 0.25729, accuracy: 0.90354\n",
            "Epoch: 23/30, step: 99/364, loss: 0.25724, accuracy: 0.90372\n",
            "Epoch: 23/30, step: 100/364, loss: 0.25633, accuracy: 0.90391\n",
            "Epoch: 23/30, step: 101/364, loss: 0.25571, accuracy: 0.90455\n",
            "Epoch: 23/30, step: 102/364, loss: 0.25604, accuracy: 0.90411\n",
            "Epoch: 23/30, step: 103/364, loss: 0.25611, accuracy: 0.90397\n",
            "Epoch: 23/30, step: 104/364, loss: 0.25621, accuracy: 0.90370\n",
            "Epoch: 23/30, step: 105/364, loss: 0.25661, accuracy: 0.90312\n",
            "Epoch: 23/30, step: 106/364, loss: 0.25718, accuracy: 0.90286\n",
            "Epoch: 23/30, step: 107/364, loss: 0.25692, accuracy: 0.90289\n",
            "Epoch: 23/30, step: 108/364, loss: 0.25789, accuracy: 0.90205\n",
            "Epoch: 23/30, step: 109/364, loss: 0.25811, accuracy: 0.90209\n",
            "Epoch: 23/30, step: 110/364, loss: 0.25729, accuracy: 0.90256\n",
            "Epoch: 23/30, step: 111/364, loss: 0.25704, accuracy: 0.90231\n",
            "Epoch: 23/30, step: 112/364, loss: 0.25712, accuracy: 0.90262\n",
            "Epoch: 23/30, step: 113/364, loss: 0.25824, accuracy: 0.90196\n",
            "Epoch: 23/30, step: 114/364, loss: 0.25803, accuracy: 0.90200\n",
            "Epoch: 23/30, step: 115/364, loss: 0.25783, accuracy: 0.90245\n",
            "Epoch: 23/30, step: 116/364, loss: 0.25800, accuracy: 0.90261\n",
            "Epoch: 23/30, step: 117/364, loss: 0.25738, accuracy: 0.90291\n",
            "Epoch: 23/30, step: 118/364, loss: 0.25736, accuracy: 0.90254\n",
            "Epoch: 23/30, step: 119/364, loss: 0.25799, accuracy: 0.90231\n",
            "Epoch: 23/30, step: 120/364, loss: 0.25796, accuracy: 0.90260\n",
            "Epoch: 23/30, step: 121/364, loss: 0.25764, accuracy: 0.90263\n",
            "Epoch: 23/30, step: 122/364, loss: 0.25727, accuracy: 0.90292\n",
            "Epoch: 23/30, step: 123/364, loss: 0.25721, accuracy: 0.90282\n",
            "Epoch: 23/30, step: 124/364, loss: 0.25697, accuracy: 0.90297\n",
            "Epoch: 23/30, step: 125/364, loss: 0.25725, accuracy: 0.90237\n",
            "Epoch: 23/30, step: 126/364, loss: 0.25747, accuracy: 0.90216\n",
            "Epoch: 23/30, step: 127/364, loss: 0.25707, accuracy: 0.90231\n",
            "Epoch: 23/30, step: 128/364, loss: 0.25695, accuracy: 0.90234\n",
            "Epoch: 23/30, step: 129/364, loss: 0.25705, accuracy: 0.90213\n",
            "Epoch: 23/30, step: 130/364, loss: 0.25718, accuracy: 0.90192\n",
            "Epoch: 23/30, step: 131/364, loss: 0.25743, accuracy: 0.90172\n",
            "Epoch: 23/30, step: 132/364, loss: 0.25780, accuracy: 0.90152\n",
            "Epoch: 23/30, step: 133/364, loss: 0.25773, accuracy: 0.90155\n",
            "Epoch: 23/30, step: 134/364, loss: 0.25714, accuracy: 0.90205\n",
            "Epoch: 23/30, step: 135/364, loss: 0.25664, accuracy: 0.90220\n",
            "Epoch: 23/30, step: 136/364, loss: 0.25634, accuracy: 0.90234\n",
            "Epoch: 23/30, step: 137/364, loss: 0.25571, accuracy: 0.90249\n",
            "Epoch: 23/30, step: 138/364, loss: 0.25586, accuracy: 0.90240\n",
            "Epoch: 23/30, step: 139/364, loss: 0.25534, accuracy: 0.90299\n",
            "Epoch: 23/30, step: 140/364, loss: 0.25523, accuracy: 0.90279\n",
            "Epoch: 23/30, step: 141/364, loss: 0.25510, accuracy: 0.90270\n",
            "Epoch: 23/30, step: 142/364, loss: 0.25532, accuracy: 0.90284\n",
            "Epoch: 23/30, step: 143/364, loss: 0.25507, accuracy: 0.90308\n",
            "Epoch: 23/30, step: 144/364, loss: 0.25552, accuracy: 0.90289\n",
            "Epoch: 23/30, step: 145/364, loss: 0.25547, accuracy: 0.90291\n",
            "Epoch: 23/30, step: 146/364, loss: 0.25525, accuracy: 0.90315\n",
            "Epoch: 23/30, step: 147/364, loss: 0.25589, accuracy: 0.90274\n",
            "Epoch: 23/30, step: 148/364, loss: 0.25554, accuracy: 0.90298\n",
            "Epoch: 23/30, step: 149/364, loss: 0.25502, accuracy: 0.90342\n",
            "Epoch: 23/30, step: 150/364, loss: 0.25486, accuracy: 0.90344\n",
            "Epoch: 23/30, step: 151/364, loss: 0.25460, accuracy: 0.90335\n",
            "Epoch: 23/30, step: 152/364, loss: 0.25412, accuracy: 0.90368\n",
            "Epoch: 23/30, step: 153/364, loss: 0.25469, accuracy: 0.90339\n",
            "Epoch: 23/30, step: 154/364, loss: 0.25460, accuracy: 0.90341\n",
            "Epoch: 23/30, step: 155/364, loss: 0.25400, accuracy: 0.90383\n",
            "Epoch: 23/30, step: 156/364, loss: 0.25382, accuracy: 0.90395\n",
            "Epoch: 23/30, step: 157/364, loss: 0.25392, accuracy: 0.90386\n",
            "Epoch: 23/30, step: 158/364, loss: 0.25432, accuracy: 0.90378\n",
            "Epoch: 23/30, step: 159/364, loss: 0.25469, accuracy: 0.90350\n",
            "Epoch: 23/30, step: 160/364, loss: 0.25454, accuracy: 0.90342\n",
            "Epoch: 23/30, step: 161/364, loss: 0.25486, accuracy: 0.90324\n",
            "Epoch: 23/30, step: 162/364, loss: 0.25463, accuracy: 0.90336\n",
            "Epoch: 23/30, step: 163/364, loss: 0.25482, accuracy: 0.90328\n",
            "Epoch: 23/30, step: 164/364, loss: 0.25455, accuracy: 0.90349\n",
            "Epoch: 23/30, step: 165/364, loss: 0.25424, accuracy: 0.90341\n",
            "Epoch: 23/30, step: 166/364, loss: 0.25350, accuracy: 0.90380\n",
            "Epoch: 23/30, step: 167/364, loss: 0.25312, accuracy: 0.90419\n",
            "Epoch: 23/30, step: 168/364, loss: 0.25328, accuracy: 0.90392\n",
            "Epoch: 23/30, step: 169/364, loss: 0.25339, accuracy: 0.90366\n",
            "Epoch: 23/30, step: 170/364, loss: 0.25376, accuracy: 0.90368\n",
            "Epoch: 23/30, step: 171/364, loss: 0.25332, accuracy: 0.90406\n",
            "Epoch: 23/30, step: 172/364, loss: 0.25309, accuracy: 0.90434\n",
            "Epoch: 23/30, step: 173/364, loss: 0.25352, accuracy: 0.90390\n",
            "Epoch: 23/30, step: 174/364, loss: 0.25317, accuracy: 0.90418\n",
            "Epoch: 23/30, step: 175/364, loss: 0.25294, accuracy: 0.90429\n",
            "Epoch: 23/30, step: 176/364, loss: 0.25255, accuracy: 0.90447\n",
            "Epoch: 23/30, step: 177/364, loss: 0.25293, accuracy: 0.90413\n",
            "Epoch: 23/30, step: 178/364, loss: 0.25273, accuracy: 0.90441\n",
            "Epoch: 23/30, step: 179/364, loss: 0.25321, accuracy: 0.90389\n",
            "Epoch: 23/30, step: 180/364, loss: 0.25312, accuracy: 0.90391\n",
            "Epoch: 23/30, step: 181/364, loss: 0.25260, accuracy: 0.90435\n",
            "Epoch: 23/30, step: 182/364, loss: 0.25231, accuracy: 0.90436\n",
            "Epoch: 23/30, step: 183/364, loss: 0.25174, accuracy: 0.90471\n",
            "Epoch: 23/30, step: 184/364, loss: 0.25144, accuracy: 0.90481\n",
            "Epoch: 23/30, step: 185/364, loss: 0.25109, accuracy: 0.90507\n",
            "Epoch: 23/30, step: 186/364, loss: 0.25045, accuracy: 0.90533\n",
            "Epoch: 23/30, step: 187/364, loss: 0.25087, accuracy: 0.90500\n",
            "Epoch: 23/30, step: 188/364, loss: 0.25087, accuracy: 0.90500\n",
            "Epoch: 23/30, step: 189/364, loss: 0.25108, accuracy: 0.90460\n",
            "Epoch: 23/30, step: 190/364, loss: 0.25169, accuracy: 0.90428\n",
            "Epoch: 23/30, step: 191/364, loss: 0.25178, accuracy: 0.90429\n",
            "Epoch: 23/30, step: 192/364, loss: 0.25165, accuracy: 0.90446\n",
            "Epoch: 23/30, step: 193/364, loss: 0.25153, accuracy: 0.90447\n",
            "Epoch: 23/30, step: 194/364, loss: 0.25136, accuracy: 0.90472\n",
            "Epoch: 23/30, step: 195/364, loss: 0.25176, accuracy: 0.90441\n",
            "Epoch: 23/30, step: 196/364, loss: 0.25190, accuracy: 0.90434\n",
            "Epoch: 23/30, step: 197/364, loss: 0.25171, accuracy: 0.90451\n",
            "Epoch: 23/30, step: 198/364, loss: 0.25160, accuracy: 0.90451\n",
            "Epoch: 23/30, step: 199/364, loss: 0.25149, accuracy: 0.90452\n",
            "Epoch: 23/30, step: 200/364, loss: 0.25135, accuracy: 0.90469\n",
            "Epoch: 23/30, step: 201/364, loss: 0.25134, accuracy: 0.90485\n",
            "Epoch: 23/30, step: 202/364, loss: 0.25092, accuracy: 0.90501\n",
            "Epoch: 23/30, step: 203/364, loss: 0.25065, accuracy: 0.90502\n",
            "Epoch: 23/30, step: 204/364, loss: 0.25038, accuracy: 0.90518\n",
            "Epoch: 23/30, step: 205/364, loss: 0.25038, accuracy: 0.90511\n",
            "Epoch: 23/30, step: 206/364, loss: 0.25029, accuracy: 0.90519\n",
            "Epoch: 23/30, step: 207/364, loss: 0.25033, accuracy: 0.90512\n",
            "Epoch: 23/30, step: 208/364, loss: 0.25069, accuracy: 0.90490\n",
            "Epoch: 23/30, step: 209/364, loss: 0.25080, accuracy: 0.90475\n",
            "Epoch: 23/30, step: 210/364, loss: 0.25043, accuracy: 0.90499\n",
            "Epoch: 23/30, step: 211/364, loss: 0.25037, accuracy: 0.90484\n",
            "Epoch: 23/30, step: 212/364, loss: 0.25036, accuracy: 0.90463\n",
            "Epoch: 23/30, step: 213/364, loss: 0.25052, accuracy: 0.90449\n",
            "Epoch: 23/30, step: 214/364, loss: 0.25030, accuracy: 0.90457\n",
            "Epoch: 23/30, step: 215/364, loss: 0.25043, accuracy: 0.90451\n",
            "Epoch: 23/30, step: 216/364, loss: 0.25004, accuracy: 0.90473\n",
            "Epoch: 23/30, step: 217/364, loss: 0.24979, accuracy: 0.90481\n",
            "Epoch: 23/30, step: 218/364, loss: 0.24983, accuracy: 0.90482\n",
            "Epoch: 23/30, step: 219/364, loss: 0.24995, accuracy: 0.90468\n",
            "Epoch: 23/30, step: 220/364, loss: 0.25016, accuracy: 0.90447\n",
            "Epoch: 23/30, step: 221/364, loss: 0.25088, accuracy: 0.90385\n",
            "Epoch: 23/30, step: 222/364, loss: 0.25037, accuracy: 0.90414\n",
            "Epoch: 23/30, step: 223/364, loss: 0.25035, accuracy: 0.90408\n",
            "Epoch: 23/30, step: 224/364, loss: 0.25039, accuracy: 0.90416\n",
            "Epoch: 23/30, step: 225/364, loss: 0.25016, accuracy: 0.90424\n",
            "Epoch: 23/30, step: 226/364, loss: 0.24986, accuracy: 0.90452\n",
            "Epoch: 23/30, step: 227/364, loss: 0.24972, accuracy: 0.90467\n",
            "Epoch: 23/30, step: 228/364, loss: 0.25052, accuracy: 0.90406\n",
            "Epoch: 23/30, step: 229/364, loss: 0.25027, accuracy: 0.90420\n",
            "Epoch: 23/30, step: 230/364, loss: 0.24991, accuracy: 0.90442\n",
            "Epoch: 23/30, step: 231/364, loss: 0.24989, accuracy: 0.90449\n",
            "Epoch: 23/30, step: 232/364, loss: 0.24947, accuracy: 0.90470\n",
            "Epoch: 23/30, step: 233/364, loss: 0.24931, accuracy: 0.90484\n",
            "Epoch: 23/30, step: 234/364, loss: 0.24952, accuracy: 0.90471\n",
            "Epoch: 23/30, step: 235/364, loss: 0.24940, accuracy: 0.90472\n",
            "Epoch: 23/30, step: 236/364, loss: 0.24905, accuracy: 0.90486\n",
            "Epoch: 23/30, step: 237/364, loss: 0.24914, accuracy: 0.90473\n",
            "Epoch: 23/30, step: 238/364, loss: 0.24908, accuracy: 0.90481\n",
            "Epoch: 23/30, step: 239/364, loss: 0.24895, accuracy: 0.90501\n",
            "Epoch: 23/30, step: 240/364, loss: 0.24903, accuracy: 0.90488\n",
            "Epoch: 23/30, step: 241/364, loss: 0.24906, accuracy: 0.90476\n",
            "Epoch: 23/30, step: 242/364, loss: 0.24919, accuracy: 0.90489\n",
            "Epoch: 23/30, step: 243/364, loss: 0.24889, accuracy: 0.90516\n",
            "Epoch: 23/30, step: 244/364, loss: 0.24879, accuracy: 0.90516\n",
            "Epoch: 23/30, step: 245/364, loss: 0.24846, accuracy: 0.90542\n",
            "Epoch: 23/30, step: 246/364, loss: 0.24829, accuracy: 0.90561\n",
            "Epoch: 23/30, step: 247/364, loss: 0.24845, accuracy: 0.90549\n",
            "Epoch: 23/30, step: 248/364, loss: 0.24839, accuracy: 0.90556\n",
            "Epoch: 23/30, step: 249/364, loss: 0.24866, accuracy: 0.90531\n",
            "Epoch: 23/30, step: 250/364, loss: 0.24880, accuracy: 0.90513\n",
            "Epoch: 23/30, step: 251/364, loss: 0.24863, accuracy: 0.90519\n",
            "Epoch: 23/30, step: 252/364, loss: 0.24875, accuracy: 0.90507\n",
            "Epoch: 23/30, step: 253/364, loss: 0.24851, accuracy: 0.90514\n",
            "Epoch: 23/30, step: 254/364, loss: 0.24907, accuracy: 0.90484\n",
            "Epoch: 23/30, step: 255/364, loss: 0.24892, accuracy: 0.90490\n",
            "Epoch: 23/30, step: 256/364, loss: 0.24891, accuracy: 0.90485\n",
            "Epoch: 23/30, step: 257/364, loss: 0.24892, accuracy: 0.90479\n",
            "Epoch: 23/30, step: 258/364, loss: 0.24939, accuracy: 0.90431\n",
            "Epoch: 23/30, step: 259/364, loss: 0.24933, accuracy: 0.90432\n",
            "Epoch: 23/30, step: 260/364, loss: 0.24932, accuracy: 0.90433\n",
            "Epoch: 23/30, step: 261/364, loss: 0.24963, accuracy: 0.90409\n",
            "Epoch: 23/30, step: 262/364, loss: 0.24933, accuracy: 0.90428\n",
            "Epoch: 23/30, step: 263/364, loss: 0.24963, accuracy: 0.90411\n",
            "Epoch: 23/30, step: 264/364, loss: 0.24977, accuracy: 0.90406\n",
            "Epoch: 23/30, step: 265/364, loss: 0.25038, accuracy: 0.90383\n",
            "Epoch: 23/30, step: 266/364, loss: 0.25059, accuracy: 0.90372\n",
            "Epoch: 23/30, step: 267/364, loss: 0.25012, accuracy: 0.90403\n",
            "Epoch: 23/30, step: 268/364, loss: 0.25009, accuracy: 0.90409\n",
            "Epoch: 23/30, step: 269/364, loss: 0.25028, accuracy: 0.90393\n",
            "Epoch: 23/30, step: 270/364, loss: 0.25011, accuracy: 0.90399\n",
            "Epoch: 23/30, step: 271/364, loss: 0.24967, accuracy: 0.90429\n",
            "Epoch: 23/30, step: 272/364, loss: 0.24952, accuracy: 0.90447\n",
            "Epoch: 23/30, step: 273/364, loss: 0.24946, accuracy: 0.90448\n",
            "Epoch: 23/30, step: 274/364, loss: 0.24951, accuracy: 0.90460\n",
            "Epoch: 23/30, step: 275/364, loss: 0.24938, accuracy: 0.90460\n",
            "Epoch: 23/30, step: 276/364, loss: 0.24927, accuracy: 0.90461\n",
            "Epoch: 23/30, step: 277/364, loss: 0.24930, accuracy: 0.90461\n",
            "Epoch: 23/30, step: 278/364, loss: 0.24935, accuracy: 0.90473\n",
            "Epoch: 23/30, step: 279/364, loss: 0.24950, accuracy: 0.90457\n",
            "Epoch: 23/30, step: 280/364, loss: 0.24930, accuracy: 0.90469\n",
            "Epoch: 23/30, step: 281/364, loss: 0.24925, accuracy: 0.90480\n",
            "Epoch: 23/30, step: 282/364, loss: 0.24933, accuracy: 0.90464\n",
            "Epoch: 23/30, step: 283/364, loss: 0.24903, accuracy: 0.90498\n",
            "Epoch: 23/30, step: 284/364, loss: 0.24938, accuracy: 0.90471\n",
            "Epoch: 23/30, step: 285/364, loss: 0.24918, accuracy: 0.90477\n",
            "Epoch: 23/30, step: 286/364, loss: 0.24905, accuracy: 0.90494\n",
            "Epoch: 23/30, step: 287/364, loss: 0.24940, accuracy: 0.90483\n",
            "Epoch: 23/30, step: 288/364, loss: 0.24942, accuracy: 0.90479\n",
            "Epoch: 23/30, step: 289/364, loss: 0.24954, accuracy: 0.90468\n",
            "Epoch: 23/30, step: 290/364, loss: 0.24998, accuracy: 0.90431\n",
            "Epoch: 23/30, step: 291/364, loss: 0.24993, accuracy: 0.90441\n",
            "Epoch: 23/30, train loss: 0.24993, train accuracy: 0.90441, valid loss: 0.75783, valid accuracy: 0.66488\n",
            "Epoch: 24/30, step: 1/364, loss: 0.42655, accuracy: 0.75000\n",
            "Epoch: 24/30, step: 2/364, loss: 0.32262, accuracy: 0.84375\n",
            "Epoch: 24/30, step: 3/364, loss: 0.27977, accuracy: 0.87500\n",
            "Epoch: 24/30, step: 4/364, loss: 0.26536, accuracy: 0.88672\n",
            "Epoch: 24/30, step: 5/364, loss: 0.26341, accuracy: 0.88437\n",
            "Epoch: 24/30, step: 6/364, loss: 0.26448, accuracy: 0.88281\n",
            "Epoch: 24/30, step: 7/364, loss: 0.26156, accuracy: 0.88616\n",
            "Epoch: 24/30, step: 8/364, loss: 0.25603, accuracy: 0.88867\n",
            "Epoch: 24/30, step: 9/364, loss: 0.25512, accuracy: 0.89062\n",
            "Epoch: 24/30, step: 10/364, loss: 0.24128, accuracy: 0.90000\n",
            "Epoch: 24/30, step: 11/364, loss: 0.23698, accuracy: 0.90625\n",
            "Epoch: 24/30, step: 12/364, loss: 0.24969, accuracy: 0.89453\n",
            "Epoch: 24/30, step: 13/364, loss: 0.24878, accuracy: 0.89543\n",
            "Epoch: 24/30, step: 14/364, loss: 0.24761, accuracy: 0.89509\n",
            "Epoch: 24/30, step: 15/364, loss: 0.24640, accuracy: 0.89688\n",
            "Epoch: 24/30, step: 16/364, loss: 0.24813, accuracy: 0.89551\n",
            "Epoch: 24/30, step: 17/364, loss: 0.24914, accuracy: 0.89246\n",
            "Epoch: 24/30, step: 18/364, loss: 0.24974, accuracy: 0.89323\n",
            "Epoch: 24/30, step: 19/364, loss: 0.24781, accuracy: 0.89556\n",
            "Epoch: 24/30, step: 20/364, loss: 0.24630, accuracy: 0.89688\n",
            "Epoch: 24/30, step: 21/364, loss: 0.24685, accuracy: 0.89509\n",
            "Epoch: 24/30, step: 22/364, loss: 0.24835, accuracy: 0.89560\n",
            "Epoch: 24/30, step: 23/364, loss: 0.24462, accuracy: 0.89878\n",
            "Epoch: 24/30, step: 24/364, loss: 0.24209, accuracy: 0.90104\n",
            "Epoch: 24/30, step: 25/364, loss: 0.23945, accuracy: 0.90312\n",
            "Epoch: 24/30, step: 26/364, loss: 0.24045, accuracy: 0.90204\n",
            "Epoch: 24/30, step: 27/364, loss: 0.23999, accuracy: 0.90278\n",
            "Epoch: 24/30, step: 28/364, loss: 0.24299, accuracy: 0.90179\n",
            "Epoch: 24/30, step: 29/364, loss: 0.23946, accuracy: 0.90463\n",
            "Epoch: 24/30, step: 30/364, loss: 0.24060, accuracy: 0.90417\n",
            "Epoch: 24/30, step: 31/364, loss: 0.24085, accuracy: 0.90373\n",
            "Epoch: 24/30, step: 32/364, loss: 0.24378, accuracy: 0.90137\n",
            "Epoch: 24/30, step: 33/364, loss: 0.24280, accuracy: 0.90246\n",
            "Epoch: 24/30, step: 34/364, loss: 0.24615, accuracy: 0.90028\n",
            "Epoch: 24/30, step: 35/364, loss: 0.24514, accuracy: 0.90134\n",
            "Epoch: 24/30, step: 36/364, loss: 0.24599, accuracy: 0.90148\n",
            "Epoch: 24/30, step: 37/364, loss: 0.24553, accuracy: 0.90160\n",
            "Epoch: 24/30, step: 38/364, loss: 0.24527, accuracy: 0.90090\n",
            "Epoch: 24/30, step: 39/364, loss: 0.24612, accuracy: 0.90024\n",
            "Epoch: 24/30, step: 40/364, loss: 0.24569, accuracy: 0.90000\n",
            "Epoch: 24/30, step: 41/364, loss: 0.24599, accuracy: 0.90091\n",
            "Epoch: 24/30, step: 42/364, loss: 0.24499, accuracy: 0.90179\n",
            "Epoch: 24/30, step: 43/364, loss: 0.24433, accuracy: 0.90262\n",
            "Epoch: 24/30, step: 44/364, loss: 0.24433, accuracy: 0.90163\n",
            "Epoch: 24/30, step: 45/364, loss: 0.24357, accuracy: 0.90174\n",
            "Epoch: 24/30, step: 46/364, loss: 0.24212, accuracy: 0.90319\n",
            "Epoch: 24/30, step: 47/364, loss: 0.24151, accuracy: 0.90359\n",
            "Epoch: 24/30, step: 48/364, loss: 0.24318, accuracy: 0.90234\n",
            "Epoch: 24/30, step: 49/364, loss: 0.24374, accuracy: 0.90306\n",
            "Epoch: 24/30, step: 50/364, loss: 0.24274, accuracy: 0.90406\n",
            "Epoch: 24/30, step: 51/364, loss: 0.24171, accuracy: 0.90472\n",
            "Epoch: 24/30, step: 52/364, loss: 0.24092, accuracy: 0.90505\n",
            "Epoch: 24/30, step: 53/364, loss: 0.24087, accuracy: 0.90478\n",
            "Epoch: 24/30, step: 54/364, loss: 0.23952, accuracy: 0.90538\n",
            "Epoch: 24/30, step: 55/364, loss: 0.23942, accuracy: 0.90540\n",
            "Epoch: 24/30, step: 56/364, loss: 0.23960, accuracy: 0.90569\n",
            "Epoch: 24/30, step: 57/364, loss: 0.24027, accuracy: 0.90543\n",
            "Epoch: 24/30, step: 58/364, loss: 0.23942, accuracy: 0.90598\n",
            "Epoch: 24/30, step: 59/364, loss: 0.23980, accuracy: 0.90599\n",
            "Epoch: 24/30, step: 60/364, loss: 0.24076, accuracy: 0.90469\n",
            "Epoch: 24/30, step: 61/364, loss: 0.24098, accuracy: 0.90446\n",
            "Epoch: 24/30, step: 62/364, loss: 0.24037, accuracy: 0.90499\n",
            "Epoch: 24/30, step: 63/364, loss: 0.24125, accuracy: 0.90402\n",
            "Epoch: 24/30, step: 64/364, loss: 0.24005, accuracy: 0.90503\n",
            "Epoch: 24/30, step: 65/364, loss: 0.24129, accuracy: 0.90457\n",
            "Epoch: 24/30, step: 66/364, loss: 0.24234, accuracy: 0.90365\n",
            "Epoch: 24/30, step: 67/364, loss: 0.24253, accuracy: 0.90345\n",
            "Epoch: 24/30, step: 68/364, loss: 0.24210, accuracy: 0.90418\n",
            "Epoch: 24/30, step: 69/364, loss: 0.24391, accuracy: 0.90285\n",
            "Epoch: 24/30, step: 70/364, loss: 0.24533, accuracy: 0.90179\n",
            "Epoch: 24/30, step: 71/364, loss: 0.24701, accuracy: 0.90119\n",
            "Epoch: 24/30, step: 72/364, loss: 0.24627, accuracy: 0.90169\n",
            "Epoch: 24/30, step: 73/364, loss: 0.24547, accuracy: 0.90218\n",
            "Epoch: 24/30, step: 74/364, loss: 0.24526, accuracy: 0.90245\n",
            "Epoch: 24/30, step: 75/364, loss: 0.24474, accuracy: 0.90271\n",
            "Epoch: 24/30, step: 76/364, loss: 0.24383, accuracy: 0.90358\n",
            "Epoch: 24/30, step: 77/364, loss: 0.24359, accuracy: 0.90381\n",
            "Epoch: 24/30, step: 78/364, loss: 0.24352, accuracy: 0.90385\n",
            "Epoch: 24/30, step: 79/364, loss: 0.24326, accuracy: 0.90427\n",
            "Epoch: 24/30, step: 80/364, loss: 0.24262, accuracy: 0.90469\n",
            "Epoch: 24/30, step: 81/364, loss: 0.24418, accuracy: 0.90394\n",
            "Epoch: 24/30, step: 82/364, loss: 0.24332, accuracy: 0.90454\n",
            "Epoch: 24/30, step: 83/364, loss: 0.24258, accuracy: 0.90512\n",
            "Epoch: 24/30, step: 84/364, loss: 0.24273, accuracy: 0.90495\n",
            "Epoch: 24/30, step: 85/364, loss: 0.24285, accuracy: 0.90478\n",
            "Epoch: 24/30, step: 86/364, loss: 0.24394, accuracy: 0.90425\n",
            "Epoch: 24/30, step: 87/364, loss: 0.24350, accuracy: 0.90445\n",
            "Epoch: 24/30, step: 88/364, loss: 0.24305, accuracy: 0.90465\n",
            "Epoch: 24/30, step: 89/364, loss: 0.24268, accuracy: 0.90467\n",
            "Epoch: 24/30, step: 90/364, loss: 0.24376, accuracy: 0.90399\n",
            "Epoch: 24/30, step: 91/364, loss: 0.24393, accuracy: 0.90402\n",
            "Epoch: 24/30, step: 92/364, loss: 0.24323, accuracy: 0.90455\n",
            "Epoch: 24/30, step: 93/364, loss: 0.24237, accuracy: 0.90524\n",
            "Epoch: 24/30, step: 94/364, loss: 0.24207, accuracy: 0.90559\n",
            "Epoch: 24/30, step: 95/364, loss: 0.24210, accuracy: 0.90543\n",
            "Epoch: 24/30, step: 96/364, loss: 0.24133, accuracy: 0.90592\n",
            "Epoch: 24/30, step: 97/364, loss: 0.24079, accuracy: 0.90609\n",
            "Epoch: 24/30, step: 98/364, loss: 0.24105, accuracy: 0.90609\n",
            "Epoch: 24/30, step: 99/364, loss: 0.24006, accuracy: 0.90672\n",
            "Epoch: 24/30, step: 100/364, loss: 0.23994, accuracy: 0.90703\n",
            "Epoch: 24/30, step: 101/364, loss: 0.23962, accuracy: 0.90733\n",
            "Epoch: 24/30, step: 102/364, loss: 0.23968, accuracy: 0.90748\n",
            "Epoch: 24/30, step: 103/364, loss: 0.24094, accuracy: 0.90701\n",
            "Epoch: 24/30, step: 104/364, loss: 0.24049, accuracy: 0.90730\n",
            "Epoch: 24/30, step: 105/364, loss: 0.24083, accuracy: 0.90685\n",
            "Epoch: 24/30, step: 106/364, loss: 0.24208, accuracy: 0.90596\n",
            "Epoch: 24/30, step: 107/364, loss: 0.24163, accuracy: 0.90610\n",
            "Epoch: 24/30, step: 108/364, loss: 0.24168, accuracy: 0.90611\n",
            "Epoch: 24/30, step: 109/364, loss: 0.24103, accuracy: 0.90668\n",
            "Epoch: 24/30, step: 110/364, loss: 0.24059, accuracy: 0.90682\n",
            "Epoch: 24/30, step: 111/364, loss: 0.24187, accuracy: 0.90611\n",
            "Epoch: 24/30, step: 112/364, loss: 0.24177, accuracy: 0.90611\n",
            "Epoch: 24/30, step: 113/364, loss: 0.24092, accuracy: 0.90666\n",
            "Epoch: 24/30, step: 114/364, loss: 0.24030, accuracy: 0.90694\n",
            "Epoch: 24/30, step: 115/364, loss: 0.24024, accuracy: 0.90679\n",
            "Epoch: 24/30, step: 116/364, loss: 0.24100, accuracy: 0.90625\n",
            "Epoch: 24/30, step: 117/364, loss: 0.24079, accuracy: 0.90652\n",
            "Epoch: 24/30, step: 118/364, loss: 0.24111, accuracy: 0.90625\n",
            "Epoch: 24/30, step: 119/364, loss: 0.24174, accuracy: 0.90572\n",
            "Epoch: 24/30, step: 120/364, loss: 0.24255, accuracy: 0.90547\n",
            "Epoch: 24/30, step: 121/364, loss: 0.24249, accuracy: 0.90560\n",
            "Epoch: 24/30, step: 122/364, loss: 0.24205, accuracy: 0.90574\n",
            "Epoch: 24/30, step: 123/364, loss: 0.24196, accuracy: 0.90574\n",
            "Epoch: 24/30, step: 124/364, loss: 0.24305, accuracy: 0.90499\n",
            "Epoch: 24/30, step: 125/364, loss: 0.24261, accuracy: 0.90550\n",
            "Epoch: 24/30, step: 126/364, loss: 0.24232, accuracy: 0.90538\n",
            "Epoch: 24/30, step: 127/364, loss: 0.24217, accuracy: 0.90551\n",
            "Epoch: 24/30, step: 128/364, loss: 0.24166, accuracy: 0.90588\n",
            "Epoch: 24/30, step: 129/364, loss: 0.24241, accuracy: 0.90552\n",
            "Epoch: 24/30, step: 130/364, loss: 0.24251, accuracy: 0.90529\n",
            "Epoch: 24/30, step: 131/364, loss: 0.24285, accuracy: 0.90518\n",
            "Epoch: 24/30, step: 132/364, loss: 0.24220, accuracy: 0.90589\n",
            "Epoch: 24/30, step: 133/364, loss: 0.24175, accuracy: 0.90637\n",
            "Epoch: 24/30, step: 134/364, loss: 0.24216, accuracy: 0.90602\n",
            "Epoch: 24/30, step: 135/364, loss: 0.24312, accuracy: 0.90532\n",
            "Epoch: 24/30, step: 136/364, loss: 0.24345, accuracy: 0.90522\n",
            "Epoch: 24/30, step: 137/364, loss: 0.24321, accuracy: 0.90522\n",
            "Epoch: 24/30, step: 138/364, loss: 0.24323, accuracy: 0.90512\n",
            "Epoch: 24/30, step: 139/364, loss: 0.24344, accuracy: 0.90501\n",
            "Epoch: 24/30, step: 140/364, loss: 0.24319, accuracy: 0.90513\n",
            "Epoch: 24/30, step: 141/364, loss: 0.24268, accuracy: 0.90559\n",
            "Epoch: 24/30, step: 142/364, loss: 0.24243, accuracy: 0.90570\n",
            "Epoch: 24/30, step: 143/364, loss: 0.24326, accuracy: 0.90538\n",
            "Epoch: 24/30, step: 144/364, loss: 0.24332, accuracy: 0.90495\n",
            "Epoch: 24/30, step: 145/364, loss: 0.24356, accuracy: 0.90485\n",
            "Epoch: 24/30, step: 146/364, loss: 0.24397, accuracy: 0.90443\n",
            "Epoch: 24/30, step: 147/364, loss: 0.24334, accuracy: 0.90476\n",
            "Epoch: 24/30, step: 148/364, loss: 0.24384, accuracy: 0.90424\n",
            "Epoch: 24/30, step: 149/364, loss: 0.24371, accuracy: 0.90436\n",
            "Epoch: 24/30, step: 150/364, loss: 0.24362, accuracy: 0.90438\n",
            "Epoch: 24/30, step: 151/364, loss: 0.24387, accuracy: 0.90418\n",
            "Epoch: 24/30, step: 152/364, loss: 0.24438, accuracy: 0.90389\n",
            "Epoch: 24/30, step: 153/364, loss: 0.24372, accuracy: 0.90431\n",
            "Epoch: 24/30, step: 154/364, loss: 0.24396, accuracy: 0.90412\n",
            "Epoch: 24/30, step: 155/364, loss: 0.24399, accuracy: 0.90423\n",
            "Epoch: 24/30, step: 156/364, loss: 0.24417, accuracy: 0.90405\n",
            "Epoch: 24/30, step: 157/364, loss: 0.24470, accuracy: 0.90376\n",
            "Epoch: 24/30, step: 158/364, loss: 0.24429, accuracy: 0.90407\n",
            "Epoch: 24/30, step: 159/364, loss: 0.24449, accuracy: 0.90369\n",
            "Epoch: 24/30, step: 160/364, loss: 0.24446, accuracy: 0.90381\n",
            "Epoch: 24/30, step: 161/364, loss: 0.24420, accuracy: 0.90392\n",
            "Epoch: 24/30, step: 162/364, loss: 0.24386, accuracy: 0.90403\n",
            "Epoch: 24/30, step: 163/364, loss: 0.24335, accuracy: 0.90433\n",
            "Epoch: 24/30, step: 164/364, loss: 0.24341, accuracy: 0.90444\n",
            "Epoch: 24/30, step: 165/364, loss: 0.24315, accuracy: 0.90455\n",
            "Epoch: 24/30, step: 166/364, loss: 0.24351, accuracy: 0.90446\n",
            "Epoch: 24/30, step: 167/364, loss: 0.24363, accuracy: 0.90438\n",
            "Epoch: 24/30, step: 168/364, loss: 0.24291, accuracy: 0.90476\n",
            "Epoch: 24/30, step: 169/364, loss: 0.24279, accuracy: 0.90486\n",
            "Epoch: 24/30, step: 170/364, loss: 0.24311, accuracy: 0.90478\n",
            "Epoch: 24/30, step: 171/364, loss: 0.24243, accuracy: 0.90524\n",
            "Epoch: 24/30, step: 172/364, loss: 0.24231, accuracy: 0.90525\n",
            "Epoch: 24/30, step: 173/364, loss: 0.24250, accuracy: 0.90517\n",
            "Epoch: 24/30, step: 174/364, loss: 0.24281, accuracy: 0.90517\n",
            "Epoch: 24/30, step: 175/364, loss: 0.24235, accuracy: 0.90554\n",
            "Epoch: 24/30, step: 176/364, loss: 0.24266, accuracy: 0.90554\n",
            "Epoch: 24/30, step: 177/364, loss: 0.24211, accuracy: 0.90599\n",
            "Epoch: 24/30, step: 178/364, loss: 0.24275, accuracy: 0.90528\n",
            "Epoch: 24/30, step: 179/364, loss: 0.24225, accuracy: 0.90555\n",
            "Epoch: 24/30, step: 180/364, loss: 0.24212, accuracy: 0.90556\n",
            "Epoch: 24/30, step: 181/364, loss: 0.24207, accuracy: 0.90565\n",
            "Epoch: 24/30, step: 182/364, loss: 0.24222, accuracy: 0.90539\n",
            "Epoch: 24/30, step: 183/364, loss: 0.24166, accuracy: 0.90557\n",
            "Epoch: 24/30, step: 184/364, loss: 0.24304, accuracy: 0.90506\n",
            "Epoch: 24/30, step: 185/364, loss: 0.24287, accuracy: 0.90524\n",
            "Epoch: 24/30, step: 186/364, loss: 0.24302, accuracy: 0.90516\n",
            "Epoch: 24/30, step: 187/364, loss: 0.24328, accuracy: 0.90483\n",
            "Epoch: 24/30, step: 188/364, loss: 0.24331, accuracy: 0.90467\n",
            "Epoch: 24/30, step: 189/364, loss: 0.24286, accuracy: 0.90484\n",
            "Epoch: 24/30, step: 190/364, loss: 0.24272, accuracy: 0.90502\n",
            "Epoch: 24/30, step: 191/364, loss: 0.24297, accuracy: 0.90486\n",
            "Epoch: 24/30, step: 192/364, loss: 0.24283, accuracy: 0.90503\n",
            "Epoch: 24/30, step: 193/364, loss: 0.24251, accuracy: 0.90528\n",
            "Epoch: 24/30, step: 194/364, loss: 0.24241, accuracy: 0.90536\n",
            "Epoch: 24/30, step: 195/364, loss: 0.24243, accuracy: 0.90521\n",
            "Epoch: 24/30, step: 196/364, loss: 0.24270, accuracy: 0.90482\n",
            "Epoch: 24/30, step: 197/364, loss: 0.24280, accuracy: 0.90482\n",
            "Epoch: 24/30, step: 198/364, loss: 0.24337, accuracy: 0.90459\n",
            "Epoch: 24/30, step: 199/364, loss: 0.24304, accuracy: 0.90476\n",
            "Epoch: 24/30, step: 200/364, loss: 0.24314, accuracy: 0.90461\n",
            "Epoch: 24/30, step: 201/364, loss: 0.24284, accuracy: 0.90470\n",
            "Epoch: 24/30, step: 202/364, loss: 0.24285, accuracy: 0.90478\n",
            "Epoch: 24/30, step: 203/364, loss: 0.24270, accuracy: 0.90494\n",
            "Epoch: 24/30, step: 204/364, loss: 0.24245, accuracy: 0.90525\n",
            "Epoch: 24/30, step: 205/364, loss: 0.24239, accuracy: 0.90518\n",
            "Epoch: 24/30, step: 206/364, loss: 0.24204, accuracy: 0.90534\n",
            "Epoch: 24/30, step: 207/364, loss: 0.24168, accuracy: 0.90550\n",
            "Epoch: 24/30, step: 208/364, loss: 0.24180, accuracy: 0.90550\n",
            "Epoch: 24/30, step: 209/364, loss: 0.24132, accuracy: 0.90565\n",
            "Epoch: 24/30, step: 210/364, loss: 0.24132, accuracy: 0.90558\n",
            "Epoch: 24/30, step: 211/364, loss: 0.24115, accuracy: 0.90573\n",
            "Epoch: 24/30, step: 212/364, loss: 0.24177, accuracy: 0.90544\n",
            "Epoch: 24/30, step: 213/364, loss: 0.24145, accuracy: 0.90574\n",
            "Epoch: 24/30, step: 214/364, loss: 0.24101, accuracy: 0.90596\n",
            "Epoch: 24/30, step: 215/364, loss: 0.24070, accuracy: 0.90610\n",
            "Epoch: 24/30, step: 216/364, loss: 0.24050, accuracy: 0.90603\n",
            "Epoch: 24/30, step: 217/364, loss: 0.24043, accuracy: 0.90603\n",
            "Epoch: 24/30, step: 218/364, loss: 0.24019, accuracy: 0.90618\n",
            "Epoch: 24/30, step: 219/364, loss: 0.24077, accuracy: 0.90589\n",
            "Epoch: 24/30, step: 220/364, loss: 0.24108, accuracy: 0.90575\n",
            "Epoch: 24/30, step: 221/364, loss: 0.24160, accuracy: 0.90533\n",
            "Epoch: 24/30, step: 222/364, loss: 0.24167, accuracy: 0.90534\n",
            "Epoch: 24/30, step: 223/364, loss: 0.24175, accuracy: 0.90499\n",
            "Epoch: 24/30, step: 224/364, loss: 0.24129, accuracy: 0.90534\n",
            "Epoch: 24/30, step: 225/364, loss: 0.24081, accuracy: 0.90562\n",
            "Epoch: 24/30, step: 226/364, loss: 0.24030, accuracy: 0.90604\n",
            "Epoch: 24/30, step: 227/364, loss: 0.24065, accuracy: 0.90591\n",
            "Epoch: 24/30, step: 228/364, loss: 0.24064, accuracy: 0.90577\n",
            "Epoch: 24/30, step: 229/364, loss: 0.24122, accuracy: 0.90550\n",
            "Epoch: 24/30, step: 230/364, loss: 0.24143, accuracy: 0.90530\n",
            "Epoch: 24/30, step: 231/364, loss: 0.24103, accuracy: 0.90544\n",
            "Epoch: 24/30, step: 232/364, loss: 0.24099, accuracy: 0.90544\n",
            "Epoch: 24/30, step: 233/364, loss: 0.24076, accuracy: 0.90558\n",
            "Epoch: 24/30, step: 234/364, loss: 0.24062, accuracy: 0.90558\n",
            "Epoch: 24/30, step: 235/364, loss: 0.24043, accuracy: 0.90572\n",
            "Epoch: 24/30, step: 236/364, loss: 0.24016, accuracy: 0.90592\n",
            "Epoch: 24/30, step: 237/364, loss: 0.24043, accuracy: 0.90566\n",
            "Epoch: 24/30, step: 238/364, loss: 0.24108, accuracy: 0.90487\n",
            "Epoch: 24/30, step: 239/364, loss: 0.24132, accuracy: 0.90468\n",
            "Epoch: 24/30, step: 240/364, loss: 0.24167, accuracy: 0.90443\n",
            "Epoch: 24/30, step: 241/364, loss: 0.24164, accuracy: 0.90450\n",
            "Epoch: 24/30, step: 242/364, loss: 0.24183, accuracy: 0.90425\n",
            "Epoch: 24/30, step: 243/364, loss: 0.24171, accuracy: 0.90426\n",
            "Epoch: 24/30, step: 244/364, loss: 0.24155, accuracy: 0.90439\n",
            "Epoch: 24/30, step: 245/364, loss: 0.24129, accuracy: 0.90459\n",
            "Epoch: 24/30, step: 246/364, loss: 0.24100, accuracy: 0.90460\n",
            "Epoch: 24/30, step: 247/364, loss: 0.24107, accuracy: 0.90473\n",
            "Epoch: 24/30, step: 248/364, loss: 0.24090, accuracy: 0.90493\n",
            "Epoch: 24/30, step: 249/364, loss: 0.24067, accuracy: 0.90499\n",
            "Epoch: 24/30, step: 250/364, loss: 0.24104, accuracy: 0.90475\n",
            "Epoch: 24/30, step: 251/364, loss: 0.24091, accuracy: 0.90488\n",
            "Epoch: 24/30, step: 252/364, loss: 0.24070, accuracy: 0.90513\n",
            "Epoch: 24/30, step: 253/364, loss: 0.24130, accuracy: 0.90489\n",
            "Epoch: 24/30, step: 254/364, loss: 0.24123, accuracy: 0.90502\n",
            "Epoch: 24/30, step: 255/364, loss: 0.24104, accuracy: 0.90521\n",
            "Epoch: 24/30, step: 256/364, loss: 0.24106, accuracy: 0.90515\n",
            "Epoch: 24/30, step: 257/364, loss: 0.24087, accuracy: 0.90528\n",
            "Epoch: 24/30, step: 258/364, loss: 0.24107, accuracy: 0.90516\n",
            "Epoch: 24/30, step: 259/364, loss: 0.24090, accuracy: 0.90528\n",
            "Epoch: 24/30, step: 260/364, loss: 0.24067, accuracy: 0.90541\n",
            "Epoch: 24/30, step: 261/364, loss: 0.24033, accuracy: 0.90553\n",
            "Epoch: 24/30, step: 262/364, loss: 0.24037, accuracy: 0.90530\n",
            "Epoch: 24/30, step: 263/364, loss: 0.24053, accuracy: 0.90530\n",
            "Epoch: 24/30, step: 264/364, loss: 0.24059, accuracy: 0.90542\n",
            "Epoch: 24/30, step: 265/364, loss: 0.24071, accuracy: 0.90531\n",
            "Epoch: 24/30, step: 266/364, loss: 0.24048, accuracy: 0.90543\n",
            "Epoch: 24/30, step: 267/364, loss: 0.24047, accuracy: 0.90537\n",
            "Epoch: 24/30, step: 268/364, loss: 0.24100, accuracy: 0.90514\n",
            "Epoch: 24/30, step: 269/364, loss: 0.24080, accuracy: 0.90520\n",
            "Epoch: 24/30, step: 270/364, loss: 0.24143, accuracy: 0.90480\n",
            "Epoch: 24/30, step: 271/364, loss: 0.24139, accuracy: 0.90481\n",
            "Epoch: 24/30, step: 272/364, loss: 0.24115, accuracy: 0.90493\n",
            "Epoch: 24/30, step: 273/364, loss: 0.24114, accuracy: 0.90499\n",
            "Epoch: 24/30, step: 274/364, loss: 0.24100, accuracy: 0.90505\n",
            "Epoch: 24/30, step: 275/364, loss: 0.24071, accuracy: 0.90517\n",
            "Epoch: 24/30, step: 276/364, loss: 0.24069, accuracy: 0.90517\n",
            "Epoch: 24/30, step: 277/364, loss: 0.24045, accuracy: 0.90529\n",
            "Epoch: 24/30, step: 278/364, loss: 0.24095, accuracy: 0.90496\n",
            "Epoch: 24/30, step: 279/364, loss: 0.24098, accuracy: 0.90502\n",
            "Epoch: 24/30, step: 280/364, loss: 0.24076, accuracy: 0.90513\n",
            "Epoch: 24/30, step: 281/364, loss: 0.24152, accuracy: 0.90469\n",
            "Epoch: 24/30, step: 282/364, loss: 0.24148, accuracy: 0.90470\n",
            "Epoch: 24/30, step: 283/364, loss: 0.24128, accuracy: 0.90470\n",
            "Epoch: 24/30, step: 284/364, loss: 0.24127, accuracy: 0.90476\n",
            "Epoch: 24/30, step: 285/364, loss: 0.24125, accuracy: 0.90477\n",
            "Epoch: 24/30, step: 286/364, loss: 0.24093, accuracy: 0.90494\n",
            "Epoch: 24/30, step: 287/364, loss: 0.24076, accuracy: 0.90505\n",
            "Epoch: 24/30, step: 288/364, loss: 0.24108, accuracy: 0.90489\n",
            "Epoch: 24/30, step: 289/364, loss: 0.24099, accuracy: 0.90490\n",
            "Epoch: 24/30, step: 290/364, loss: 0.24088, accuracy: 0.90485\n",
            "Epoch: 24/30, step: 291/364, loss: 0.24071, accuracy: 0.90500\n",
            "Epoch: 24/30, train loss: 0.24071, train accuracy: 0.90500, valid loss: 0.75687, valid accuracy: 0.66574\n",
            "Epoch: 25/30, step: 1/364, loss: 0.25841, accuracy: 0.89062\n",
            "Epoch: 25/30, step: 2/364, loss: 0.31237, accuracy: 0.86719\n",
            "Epoch: 25/30, step: 3/364, loss: 0.25901, accuracy: 0.90625\n",
            "Epoch: 25/30, step: 4/364, loss: 0.23715, accuracy: 0.91797\n",
            "Epoch: 25/30, step: 5/364, loss: 0.22783, accuracy: 0.92188\n",
            "Epoch: 25/30, step: 6/364, loss: 0.24096, accuracy: 0.91406\n",
            "Epoch: 25/30, step: 7/364, loss: 0.23072, accuracy: 0.91964\n",
            "Epoch: 25/30, step: 8/364, loss: 0.22507, accuracy: 0.92383\n",
            "Epoch: 25/30, step: 9/364, loss: 0.22585, accuracy: 0.92361\n",
            "Epoch: 25/30, step: 10/364, loss: 0.22341, accuracy: 0.92500\n",
            "Epoch: 25/30, step: 11/364, loss: 0.23789, accuracy: 0.91477\n",
            "Epoch: 25/30, step: 12/364, loss: 0.23586, accuracy: 0.91276\n",
            "Epoch: 25/30, step: 13/364, loss: 0.22950, accuracy: 0.91707\n",
            "Epoch: 25/30, step: 14/364, loss: 0.22439, accuracy: 0.91853\n",
            "Epoch: 25/30, step: 15/364, loss: 0.22012, accuracy: 0.92083\n",
            "Epoch: 25/30, step: 16/364, loss: 0.21609, accuracy: 0.92285\n",
            "Epoch: 25/30, step: 17/364, loss: 0.21415, accuracy: 0.92463\n",
            "Epoch: 25/30, step: 18/364, loss: 0.21695, accuracy: 0.92361\n",
            "Epoch: 25/30, step: 19/364, loss: 0.21484, accuracy: 0.92516\n",
            "Epoch: 25/30, step: 20/364, loss: 0.21045, accuracy: 0.92734\n",
            "Epoch: 25/30, step: 21/364, loss: 0.21585, accuracy: 0.92336\n",
            "Epoch: 25/30, step: 22/364, loss: 0.21614, accuracy: 0.92259\n",
            "Epoch: 25/30, step: 23/364, loss: 0.21652, accuracy: 0.92323\n",
            "Epoch: 25/30, step: 24/364, loss: 0.21928, accuracy: 0.92253\n",
            "Epoch: 25/30, step: 25/364, loss: 0.21655, accuracy: 0.92375\n",
            "Epoch: 25/30, step: 26/364, loss: 0.21633, accuracy: 0.92308\n",
            "Epoch: 25/30, step: 27/364, loss: 0.21586, accuracy: 0.92361\n",
            "Epoch: 25/30, step: 28/364, loss: 0.21805, accuracy: 0.92299\n",
            "Epoch: 25/30, step: 29/364, loss: 0.21812, accuracy: 0.92241\n",
            "Epoch: 25/30, step: 30/364, loss: 0.21963, accuracy: 0.92135\n",
            "Epoch: 25/30, step: 31/364, loss: 0.21866, accuracy: 0.92137\n",
            "Epoch: 25/30, step: 32/364, loss: 0.22412, accuracy: 0.91895\n",
            "Epoch: 25/30, step: 33/364, loss: 0.22138, accuracy: 0.92093\n",
            "Epoch: 25/30, step: 34/364, loss: 0.21997, accuracy: 0.92188\n",
            "Epoch: 25/30, step: 35/364, loss: 0.22090, accuracy: 0.91964\n",
            "Epoch: 25/30, step: 36/364, loss: 0.21814, accuracy: 0.92101\n",
            "Epoch: 25/30, step: 37/364, loss: 0.21627, accuracy: 0.92230\n",
            "Epoch: 25/30, step: 38/364, loss: 0.21876, accuracy: 0.92023\n",
            "Epoch: 25/30, step: 39/364, loss: 0.21772, accuracy: 0.92107\n",
            "Epoch: 25/30, step: 40/364, loss: 0.22124, accuracy: 0.91836\n",
            "Epoch: 25/30, step: 41/364, loss: 0.22117, accuracy: 0.91806\n",
            "Epoch: 25/30, step: 42/364, loss: 0.22066, accuracy: 0.91741\n",
            "Epoch: 25/30, step: 43/364, loss: 0.22208, accuracy: 0.91570\n",
            "Epoch: 25/30, step: 44/364, loss: 0.22336, accuracy: 0.91513\n",
            "Epoch: 25/30, step: 45/364, loss: 0.22306, accuracy: 0.91528\n",
            "Epoch: 25/30, step: 46/364, loss: 0.23028, accuracy: 0.91202\n",
            "Epoch: 25/30, step: 47/364, loss: 0.22951, accuracy: 0.91323\n",
            "Epoch: 25/30, step: 48/364, loss: 0.22954, accuracy: 0.91309\n",
            "Epoch: 25/30, step: 49/364, loss: 0.23188, accuracy: 0.91103\n",
            "Epoch: 25/30, step: 50/364, loss: 0.23002, accuracy: 0.91188\n",
            "Epoch: 25/30, step: 51/364, loss: 0.22961, accuracy: 0.91238\n",
            "Epoch: 25/30, step: 52/364, loss: 0.23000, accuracy: 0.91166\n",
            "Epoch: 25/30, step: 53/364, loss: 0.22789, accuracy: 0.91303\n",
            "Epoch: 25/30, step: 54/364, loss: 0.22797, accuracy: 0.91377\n",
            "Epoch: 25/30, step: 55/364, loss: 0.22716, accuracy: 0.91449\n",
            "Epoch: 25/30, step: 56/364, loss: 0.22761, accuracy: 0.91434\n",
            "Epoch: 25/30, step: 57/364, loss: 0.22715, accuracy: 0.91502\n",
            "Epoch: 25/30, step: 58/364, loss: 0.22862, accuracy: 0.91460\n",
            "Epoch: 25/30, step: 59/364, loss: 0.22813, accuracy: 0.91525\n",
            "Epoch: 25/30, step: 60/364, loss: 0.22717, accuracy: 0.91589\n",
            "Epoch: 25/30, step: 61/364, loss: 0.22954, accuracy: 0.91368\n",
            "Epoch: 25/30, step: 62/364, loss: 0.22895, accuracy: 0.91381\n",
            "Epoch: 25/30, step: 63/364, loss: 0.22954, accuracy: 0.91319\n",
            "Epoch: 25/30, step: 64/364, loss: 0.22970, accuracy: 0.91284\n",
            "Epoch: 25/30, step: 65/364, loss: 0.22845, accuracy: 0.91370\n",
            "Epoch: 25/30, step: 66/364, loss: 0.22788, accuracy: 0.91430\n",
            "Epoch: 25/30, step: 67/364, loss: 0.22617, accuracy: 0.91535\n",
            "Epoch: 25/30, step: 68/364, loss: 0.22594, accuracy: 0.91521\n",
            "Epoch: 25/30, step: 69/364, loss: 0.22580, accuracy: 0.91531\n",
            "Epoch: 25/30, step: 70/364, loss: 0.22529, accuracy: 0.91562\n",
            "Epoch: 25/30, step: 71/364, loss: 0.22364, accuracy: 0.91637\n",
            "Epoch: 25/30, step: 72/364, loss: 0.22303, accuracy: 0.91667\n",
            "Epoch: 25/30, step: 73/364, loss: 0.22394, accuracy: 0.91631\n",
            "Epoch: 25/30, step: 74/364, loss: 0.22388, accuracy: 0.91660\n",
            "Epoch: 25/30, step: 75/364, loss: 0.22372, accuracy: 0.91646\n",
            "Epoch: 25/30, step: 76/364, loss: 0.22392, accuracy: 0.91653\n",
            "Epoch: 25/30, step: 77/364, loss: 0.22344, accuracy: 0.91680\n",
            "Epoch: 25/30, step: 78/364, loss: 0.22413, accuracy: 0.91627\n",
            "Epoch: 25/30, step: 79/364, loss: 0.22549, accuracy: 0.91535\n",
            "Epoch: 25/30, step: 80/364, loss: 0.22570, accuracy: 0.91543\n",
            "Epoch: 25/30, step: 81/364, loss: 0.22688, accuracy: 0.91532\n",
            "Epoch: 25/30, step: 82/364, loss: 0.22609, accuracy: 0.91578\n",
            "Epoch: 25/30, step: 83/364, loss: 0.22577, accuracy: 0.91566\n",
            "Epoch: 25/30, step: 84/364, loss: 0.22593, accuracy: 0.91555\n",
            "Epoch: 25/30, step: 85/364, loss: 0.22605, accuracy: 0.91562\n",
            "Epoch: 25/30, step: 86/364, loss: 0.22539, accuracy: 0.91588\n",
            "Epoch: 25/30, step: 87/364, loss: 0.22685, accuracy: 0.91487\n",
            "Epoch: 25/30, step: 88/364, loss: 0.22666, accuracy: 0.91513\n",
            "Epoch: 25/30, step: 89/364, loss: 0.22831, accuracy: 0.91362\n",
            "Epoch: 25/30, step: 90/364, loss: 0.22795, accuracy: 0.91372\n",
            "Epoch: 25/30, step: 91/364, loss: 0.22819, accuracy: 0.91363\n",
            "Epoch: 25/30, step: 92/364, loss: 0.22780, accuracy: 0.91389\n",
            "Epoch: 25/30, step: 93/364, loss: 0.22745, accuracy: 0.91431\n",
            "Epoch: 25/30, step: 94/364, loss: 0.22857, accuracy: 0.91373\n",
            "Epoch: 25/30, step: 95/364, loss: 0.22851, accuracy: 0.91349\n",
            "Epoch: 25/30, step: 96/364, loss: 0.22737, accuracy: 0.91423\n",
            "Epoch: 25/30, step: 97/364, loss: 0.22654, accuracy: 0.91479\n",
            "Epoch: 25/30, step: 98/364, loss: 0.22705, accuracy: 0.91422\n",
            "Epoch: 25/30, step: 99/364, loss: 0.22698, accuracy: 0.91446\n",
            "Epoch: 25/30, step: 100/364, loss: 0.22592, accuracy: 0.91500\n",
            "Epoch: 25/30, step: 101/364, loss: 0.22543, accuracy: 0.91553\n",
            "Epoch: 25/30, step: 102/364, loss: 0.22459, accuracy: 0.91621\n",
            "Epoch: 25/30, step: 103/364, loss: 0.22409, accuracy: 0.91657\n",
            "Epoch: 25/30, step: 104/364, loss: 0.22379, accuracy: 0.91692\n",
            "Epoch: 25/30, step: 105/364, loss: 0.22288, accuracy: 0.91756\n",
            "Epoch: 25/30, step: 106/364, loss: 0.22274, accuracy: 0.91760\n",
            "Epoch: 25/30, step: 107/364, loss: 0.22271, accuracy: 0.91764\n",
            "Epoch: 25/30, step: 108/364, loss: 0.22181, accuracy: 0.91811\n",
            "Epoch: 25/30, step: 109/364, loss: 0.22166, accuracy: 0.91800\n",
            "Epoch: 25/30, step: 110/364, loss: 0.22128, accuracy: 0.91847\n",
            "Epoch: 25/30, step: 111/364, loss: 0.22048, accuracy: 0.91878\n",
            "Epoch: 25/30, step: 112/364, loss: 0.22103, accuracy: 0.91867\n",
            "Epoch: 25/30, step: 113/364, loss: 0.22067, accuracy: 0.91911\n",
            "Epoch: 25/30, step: 114/364, loss: 0.22086, accuracy: 0.91886\n",
            "Epoch: 25/30, step: 115/364, loss: 0.22071, accuracy: 0.91861\n",
            "Epoch: 25/30, step: 116/364, loss: 0.22087, accuracy: 0.91891\n",
            "Epoch: 25/30, step: 117/364, loss: 0.22065, accuracy: 0.91894\n",
            "Epoch: 25/30, step: 118/364, loss: 0.22021, accuracy: 0.91909\n",
            "Epoch: 25/30, step: 119/364, loss: 0.22028, accuracy: 0.91912\n",
            "Epoch: 25/30, step: 120/364, loss: 0.22007, accuracy: 0.91940\n",
            "Epoch: 25/30, step: 121/364, loss: 0.22042, accuracy: 0.91890\n",
            "Epoch: 25/30, step: 122/364, loss: 0.21974, accuracy: 0.91931\n",
            "Epoch: 25/30, step: 123/364, loss: 0.21922, accuracy: 0.91972\n",
            "Epoch: 25/30, step: 124/364, loss: 0.21962, accuracy: 0.91923\n",
            "Epoch: 25/30, step: 125/364, loss: 0.21960, accuracy: 0.91913\n",
            "Epoch: 25/30, step: 126/364, loss: 0.21914, accuracy: 0.91927\n",
            "Epoch: 25/30, step: 127/364, loss: 0.22032, accuracy: 0.91905\n",
            "Epoch: 25/30, step: 128/364, loss: 0.22001, accuracy: 0.91907\n",
            "Epoch: 25/30, step: 129/364, loss: 0.22019, accuracy: 0.91873\n",
            "Epoch: 25/30, step: 130/364, loss: 0.22001, accuracy: 0.91887\n",
            "Epoch: 25/30, step: 131/364, loss: 0.22066, accuracy: 0.91877\n",
            "Epoch: 25/30, step: 132/364, loss: 0.22022, accuracy: 0.91892\n",
            "Epoch: 25/30, step: 133/364, loss: 0.21978, accuracy: 0.91941\n",
            "Epoch: 25/30, step: 134/364, loss: 0.21945, accuracy: 0.91966\n",
            "Epoch: 25/30, step: 135/364, loss: 0.21964, accuracy: 0.91944\n",
            "Epoch: 25/30, step: 136/364, loss: 0.22083, accuracy: 0.91877\n",
            "Epoch: 25/30, step: 137/364, loss: 0.22081, accuracy: 0.91857\n",
            "Epoch: 25/30, step: 138/364, loss: 0.22025, accuracy: 0.91870\n",
            "Epoch: 25/30, step: 139/364, loss: 0.22006, accuracy: 0.91873\n",
            "Epoch: 25/30, step: 140/364, loss: 0.21938, accuracy: 0.91908\n",
            "Epoch: 25/30, step: 141/364, loss: 0.21986, accuracy: 0.91877\n",
            "Epoch: 25/30, step: 142/364, loss: 0.21955, accuracy: 0.91879\n",
            "Epoch: 25/30, step: 143/364, loss: 0.21952, accuracy: 0.91892\n",
            "Epoch: 25/30, step: 144/364, loss: 0.21894, accuracy: 0.91927\n",
            "Epoch: 25/30, step: 145/364, loss: 0.21908, accuracy: 0.91918\n",
            "Epoch: 25/30, step: 146/364, loss: 0.21988, accuracy: 0.91877\n",
            "Epoch: 25/30, step: 147/364, loss: 0.21972, accuracy: 0.91879\n",
            "Epoch: 25/30, step: 148/364, loss: 0.21953, accuracy: 0.91881\n",
            "Epoch: 25/30, step: 149/364, loss: 0.21892, accuracy: 0.91915\n",
            "Epoch: 25/30, step: 150/364, loss: 0.21900, accuracy: 0.91927\n",
            "Epoch: 25/30, step: 151/364, loss: 0.21874, accuracy: 0.91939\n",
            "Epoch: 25/30, step: 152/364, loss: 0.21837, accuracy: 0.91961\n",
            "Epoch: 25/30, step: 153/364, loss: 0.21813, accuracy: 0.91983\n",
            "Epoch: 25/30, step: 154/364, loss: 0.21846, accuracy: 0.91954\n",
            "Epoch: 25/30, step: 155/364, loss: 0.21809, accuracy: 0.91996\n",
            "Epoch: 25/30, step: 156/364, loss: 0.21837, accuracy: 0.91977\n",
            "Epoch: 25/30, step: 157/364, loss: 0.21838, accuracy: 0.91969\n",
            "Epoch: 25/30, step: 158/364, loss: 0.21858, accuracy: 0.91980\n",
            "Epoch: 25/30, step: 159/364, loss: 0.21805, accuracy: 0.91991\n",
            "Epoch: 25/30, step: 160/364, loss: 0.21784, accuracy: 0.91992\n",
            "Epoch: 25/30, step: 161/364, loss: 0.21748, accuracy: 0.92003\n",
            "Epoch: 25/30, step: 162/364, loss: 0.21765, accuracy: 0.91985\n",
            "Epoch: 25/30, step: 163/364, loss: 0.21789, accuracy: 0.91977\n",
            "Epoch: 25/30, step: 164/364, loss: 0.21729, accuracy: 0.92006\n",
            "Epoch: 25/30, step: 165/364, loss: 0.21781, accuracy: 0.91979\n",
            "Epoch: 25/30, step: 166/364, loss: 0.21752, accuracy: 0.91999\n",
            "Epoch: 25/30, step: 167/364, loss: 0.21797, accuracy: 0.91982\n",
            "Epoch: 25/30, step: 168/364, loss: 0.21780, accuracy: 0.91983\n",
            "Epoch: 25/30, step: 169/364, loss: 0.21817, accuracy: 0.91947\n",
            "Epoch: 25/30, step: 170/364, loss: 0.21778, accuracy: 0.91939\n",
            "Epoch: 25/30, step: 171/364, loss: 0.21766, accuracy: 0.91950\n",
            "Epoch: 25/30, step: 172/364, loss: 0.21769, accuracy: 0.91942\n",
            "Epoch: 25/30, step: 173/364, loss: 0.21713, accuracy: 0.91962\n",
            "Epoch: 25/30, step: 174/364, loss: 0.21700, accuracy: 0.91954\n",
            "Epoch: 25/30, step: 175/364, loss: 0.21689, accuracy: 0.91938\n",
            "Epoch: 25/30, step: 176/364, loss: 0.21701, accuracy: 0.91912\n",
            "Epoch: 25/30, step: 177/364, loss: 0.21730, accuracy: 0.91896\n",
            "Epoch: 25/30, step: 178/364, loss: 0.21756, accuracy: 0.91880\n",
            "Epoch: 25/30, step: 179/364, loss: 0.21703, accuracy: 0.91908\n",
            "Epoch: 25/30, step: 180/364, loss: 0.21703, accuracy: 0.91918\n",
            "Epoch: 25/30, step: 181/364, loss: 0.21734, accuracy: 0.91894\n",
            "Epoch: 25/30, step: 182/364, loss: 0.21718, accuracy: 0.91870\n",
            "Epoch: 25/30, step: 183/364, loss: 0.21745, accuracy: 0.91846\n",
            "Epoch: 25/30, step: 184/364, loss: 0.21696, accuracy: 0.91890\n",
            "Epoch: 25/30, step: 185/364, loss: 0.21678, accuracy: 0.91909\n",
            "Epoch: 25/30, step: 186/364, loss: 0.21666, accuracy: 0.91935\n",
            "Epoch: 25/30, step: 187/364, loss: 0.21629, accuracy: 0.91970\n",
            "Epoch: 25/30, step: 188/364, loss: 0.21589, accuracy: 0.91988\n",
            "Epoch: 25/30, step: 189/364, loss: 0.21574, accuracy: 0.91981\n",
            "Epoch: 25/30, step: 190/364, loss: 0.21554, accuracy: 0.91998\n",
            "Epoch: 25/30, step: 191/364, loss: 0.21515, accuracy: 0.92032\n",
            "Epoch: 25/30, step: 192/364, loss: 0.21487, accuracy: 0.92049\n",
            "Epoch: 25/30, step: 193/364, loss: 0.21442, accuracy: 0.92082\n",
            "Epoch: 25/30, step: 194/364, loss: 0.21426, accuracy: 0.92099\n",
            "Epoch: 25/30, step: 195/364, loss: 0.21391, accuracy: 0.92123\n",
            "Epoch: 25/30, step: 196/364, loss: 0.21383, accuracy: 0.92116\n",
            "Epoch: 25/30, step: 197/364, loss: 0.21361, accuracy: 0.92108\n",
            "Epoch: 25/30, step: 198/364, loss: 0.21352, accuracy: 0.92116\n",
            "Epoch: 25/30, step: 199/364, loss: 0.21321, accuracy: 0.92140\n",
            "Epoch: 25/30, step: 200/364, loss: 0.21346, accuracy: 0.92141\n",
            "Epoch: 25/30, step: 201/364, loss: 0.21358, accuracy: 0.92141\n",
            "Epoch: 25/30, step: 202/364, loss: 0.21373, accuracy: 0.92133\n",
            "Epoch: 25/30, step: 203/364, loss: 0.21318, accuracy: 0.92164\n",
            "Epoch: 25/30, step: 204/364, loss: 0.21298, accuracy: 0.92180\n",
            "Epoch: 25/30, step: 205/364, loss: 0.21283, accuracy: 0.92188\n",
            "Epoch: 25/30, step: 206/364, loss: 0.21341, accuracy: 0.92150\n",
            "Epoch: 25/30, step: 207/364, loss: 0.21364, accuracy: 0.92142\n",
            "Epoch: 25/30, step: 208/364, loss: 0.21310, accuracy: 0.92180\n",
            "Epoch: 25/30, step: 209/364, loss: 0.21285, accuracy: 0.92173\n",
            "Epoch: 25/30, step: 210/364, loss: 0.21236, accuracy: 0.92202\n",
            "Epoch: 25/30, step: 211/364, loss: 0.21206, accuracy: 0.92210\n",
            "Epoch: 25/30, step: 212/364, loss: 0.21214, accuracy: 0.92202\n",
            "Epoch: 25/30, step: 213/364, loss: 0.21208, accuracy: 0.92195\n",
            "Epoch: 25/30, step: 214/364, loss: 0.21190, accuracy: 0.92217\n",
            "Epoch: 25/30, step: 215/364, loss: 0.21178, accuracy: 0.92224\n",
            "Epoch: 25/30, step: 216/364, loss: 0.21149, accuracy: 0.92238\n",
            "Epoch: 25/30, step: 217/364, loss: 0.21189, accuracy: 0.92238\n",
            "Epoch: 25/30, step: 218/364, loss: 0.21207, accuracy: 0.92216\n",
            "Epoch: 25/30, step: 219/364, loss: 0.21192, accuracy: 0.92216\n",
            "Epoch: 25/30, step: 220/364, loss: 0.21223, accuracy: 0.92188\n",
            "Epoch: 25/30, step: 221/364, loss: 0.21216, accuracy: 0.92202\n",
            "Epoch: 25/30, step: 222/364, loss: 0.21169, accuracy: 0.92230\n",
            "Epoch: 25/30, step: 223/364, loss: 0.21172, accuracy: 0.92223\n",
            "Epoch: 25/30, step: 224/364, loss: 0.21167, accuracy: 0.92229\n",
            "Epoch: 25/30, step: 225/364, loss: 0.21166, accuracy: 0.92222\n",
            "Epoch: 25/30, step: 226/364, loss: 0.21149, accuracy: 0.92229\n",
            "Epoch: 25/30, step: 227/364, loss: 0.21128, accuracy: 0.92249\n",
            "Epoch: 25/30, step: 228/364, loss: 0.21130, accuracy: 0.92235\n",
            "Epoch: 25/30, step: 229/364, loss: 0.21100, accuracy: 0.92256\n",
            "Epoch: 25/30, step: 230/364, loss: 0.21066, accuracy: 0.92283\n",
            "Epoch: 25/30, step: 231/364, loss: 0.21059, accuracy: 0.92282\n",
            "Epoch: 25/30, step: 232/364, loss: 0.21071, accuracy: 0.92275\n",
            "Epoch: 25/30, step: 233/364, loss: 0.21062, accuracy: 0.92275\n",
            "Epoch: 25/30, step: 234/364, loss: 0.21024, accuracy: 0.92294\n",
            "Epoch: 25/30, step: 235/364, loss: 0.21031, accuracy: 0.92294\n",
            "Epoch: 25/30, step: 236/364, loss: 0.21076, accuracy: 0.92267\n",
            "Epoch: 25/30, step: 237/364, loss: 0.21139, accuracy: 0.92240\n",
            "Epoch: 25/30, step: 238/364, loss: 0.21169, accuracy: 0.92227\n",
            "Epoch: 25/30, step: 239/364, loss: 0.21207, accuracy: 0.92188\n",
            "Epoch: 25/30, step: 240/364, loss: 0.21233, accuracy: 0.92181\n",
            "Epoch: 25/30, step: 241/364, loss: 0.21268, accuracy: 0.92181\n",
            "Epoch: 25/30, step: 242/364, loss: 0.21266, accuracy: 0.92181\n",
            "Epoch: 25/30, step: 243/364, loss: 0.21260, accuracy: 0.92194\n",
            "Epoch: 25/30, step: 244/364, loss: 0.21301, accuracy: 0.92175\n",
            "Epoch: 25/30, step: 245/364, loss: 0.21293, accuracy: 0.92181\n",
            "Epoch: 25/30, step: 246/364, loss: 0.21278, accuracy: 0.92175\n",
            "Epoch: 25/30, step: 247/364, loss: 0.21244, accuracy: 0.92188\n",
            "Epoch: 25/30, step: 248/364, loss: 0.21228, accuracy: 0.92194\n",
            "Epoch: 25/30, step: 249/364, loss: 0.21212, accuracy: 0.92200\n",
            "Epoch: 25/30, step: 250/364, loss: 0.21197, accuracy: 0.92212\n",
            "Epoch: 25/30, step: 251/364, loss: 0.21172, accuracy: 0.92219\n",
            "Epoch: 25/30, step: 252/364, loss: 0.21157, accuracy: 0.92225\n",
            "Epoch: 25/30, step: 253/364, loss: 0.21126, accuracy: 0.92237\n",
            "Epoch: 25/30, step: 254/364, loss: 0.21112, accuracy: 0.92243\n",
            "Epoch: 25/30, step: 255/364, loss: 0.21100, accuracy: 0.92255\n",
            "Epoch: 25/30, step: 256/364, loss: 0.21104, accuracy: 0.92249\n",
            "Epoch: 25/30, step: 257/364, loss: 0.21094, accuracy: 0.92242\n",
            "Epoch: 25/30, step: 258/364, loss: 0.21094, accuracy: 0.92248\n",
            "Epoch: 25/30, step: 259/364, loss: 0.21075, accuracy: 0.92254\n",
            "Epoch: 25/30, step: 260/364, loss: 0.21070, accuracy: 0.92254\n",
            "Epoch: 25/30, step: 261/364, loss: 0.21048, accuracy: 0.92271\n",
            "Epoch: 25/30, step: 262/364, loss: 0.21036, accuracy: 0.92271\n",
            "Epoch: 25/30, step: 263/364, loss: 0.21043, accuracy: 0.92259\n",
            "Epoch: 25/30, step: 264/364, loss: 0.21049, accuracy: 0.92241\n",
            "Epoch: 25/30, step: 265/364, loss: 0.21054, accuracy: 0.92246\n",
            "Epoch: 25/30, step: 266/364, loss: 0.21073, accuracy: 0.92240\n",
            "Epoch: 25/30, step: 267/364, loss: 0.21094, accuracy: 0.92240\n",
            "Epoch: 25/30, step: 268/364, loss: 0.21086, accuracy: 0.92240\n",
            "Epoch: 25/30, step: 269/364, loss: 0.21092, accuracy: 0.92234\n",
            "Epoch: 25/30, step: 270/364, loss: 0.21106, accuracy: 0.92222\n",
            "Epoch: 25/30, step: 271/364, loss: 0.21085, accuracy: 0.92234\n",
            "Epoch: 25/30, step: 272/364, loss: 0.21073, accuracy: 0.92239\n",
            "Epoch: 25/30, step: 273/364, loss: 0.21055, accuracy: 0.92239\n",
            "Epoch: 25/30, step: 274/364, loss: 0.21062, accuracy: 0.92239\n",
            "Epoch: 25/30, step: 275/364, loss: 0.21066, accuracy: 0.92244\n",
            "Epoch: 25/30, step: 276/364, loss: 0.21062, accuracy: 0.92244\n",
            "Epoch: 25/30, step: 277/364, loss: 0.21072, accuracy: 0.92244\n",
            "Epoch: 25/30, step: 278/364, loss: 0.21121, accuracy: 0.92210\n",
            "Epoch: 25/30, step: 279/364, loss: 0.21127, accuracy: 0.92199\n",
            "Epoch: 25/30, step: 280/364, loss: 0.21080, accuracy: 0.92227\n",
            "Epoch: 25/30, step: 281/364, loss: 0.21074, accuracy: 0.92232\n",
            "Epoch: 25/30, step: 282/364, loss: 0.21047, accuracy: 0.92254\n",
            "Epoch: 25/30, step: 283/364, loss: 0.21045, accuracy: 0.92254\n",
            "Epoch: 25/30, step: 284/364, loss: 0.21026, accuracy: 0.92265\n",
            "Epoch: 25/30, step: 285/364, loss: 0.21032, accuracy: 0.92264\n",
            "Epoch: 25/30, step: 286/364, loss: 0.21089, accuracy: 0.92231\n",
            "Epoch: 25/30, step: 287/364, loss: 0.21101, accuracy: 0.92226\n",
            "Epoch: 25/30, step: 288/364, loss: 0.21185, accuracy: 0.92193\n",
            "Epoch: 25/30, step: 289/364, loss: 0.21166, accuracy: 0.92204\n",
            "Epoch: 25/30, step: 290/364, loss: 0.21161, accuracy: 0.92204\n",
            "Epoch: 25/30, step: 291/364, loss: 0.21149, accuracy: 0.92198\n",
            "Epoch: 25/30, train loss: 0.21149, train accuracy: 0.92198, valid loss: 0.88968, valid accuracy: 0.64402\n",
            "Epoch: 26/30, step: 1/364, loss: 0.38573, accuracy: 0.84375\n",
            "Epoch: 26/30, step: 2/364, loss: 0.27906, accuracy: 0.89844\n",
            "Epoch: 26/30, step: 3/364, loss: 0.24432, accuracy: 0.91146\n",
            "Epoch: 26/30, step: 4/364, loss: 0.20861, accuracy: 0.92969\n",
            "Epoch: 26/30, step: 5/364, loss: 0.23503, accuracy: 0.92188\n",
            "Epoch: 26/30, step: 6/364, loss: 0.22494, accuracy: 0.92188\n",
            "Epoch: 26/30, step: 7/364, loss: 0.22764, accuracy: 0.91295\n",
            "Epoch: 26/30, step: 8/364, loss: 0.23346, accuracy: 0.91406\n",
            "Epoch: 26/30, step: 9/364, loss: 0.23346, accuracy: 0.91319\n",
            "Epoch: 26/30, step: 10/364, loss: 0.23814, accuracy: 0.90625\n",
            "Epoch: 26/30, step: 11/364, loss: 0.23763, accuracy: 0.90483\n",
            "Epoch: 26/30, step: 12/364, loss: 0.23694, accuracy: 0.90495\n",
            "Epoch: 26/30, step: 13/364, loss: 0.23502, accuracy: 0.90625\n",
            "Epoch: 26/30, step: 14/364, loss: 0.23490, accuracy: 0.90737\n",
            "Epoch: 26/30, step: 15/364, loss: 0.24026, accuracy: 0.90625\n",
            "Epoch: 26/30, step: 16/364, loss: 0.23213, accuracy: 0.91211\n",
            "Epoch: 26/30, step: 17/364, loss: 0.23341, accuracy: 0.91085\n",
            "Epoch: 26/30, step: 18/364, loss: 0.23127, accuracy: 0.91233\n",
            "Epoch: 26/30, step: 19/364, loss: 0.23047, accuracy: 0.91118\n",
            "Epoch: 26/30, step: 20/364, loss: 0.22638, accuracy: 0.91328\n",
            "Epoch: 26/30, step: 21/364, loss: 0.22374, accuracy: 0.91443\n",
            "Epoch: 26/30, step: 22/364, loss: 0.22274, accuracy: 0.91690\n",
            "Epoch: 26/30, step: 23/364, loss: 0.23015, accuracy: 0.91304\n",
            "Epoch: 26/30, step: 24/364, loss: 0.23432, accuracy: 0.91081\n",
            "Epoch: 26/30, step: 25/364, loss: 0.23274, accuracy: 0.91250\n",
            "Epoch: 26/30, step: 26/364, loss: 0.22957, accuracy: 0.91406\n",
            "Epoch: 26/30, step: 27/364, loss: 0.23116, accuracy: 0.91377\n",
            "Epoch: 26/30, step: 28/364, loss: 0.22971, accuracy: 0.91406\n",
            "Epoch: 26/30, step: 29/364, loss: 0.22760, accuracy: 0.91595\n",
            "Epoch: 26/30, step: 30/364, loss: 0.22460, accuracy: 0.91719\n",
            "Epoch: 26/30, step: 31/364, loss: 0.22211, accuracy: 0.91935\n",
            "Epoch: 26/30, step: 32/364, loss: 0.22043, accuracy: 0.92041\n",
            "Epoch: 26/30, step: 33/364, loss: 0.21846, accuracy: 0.92188\n",
            "Epoch: 26/30, step: 34/364, loss: 0.21611, accuracy: 0.92325\n",
            "Epoch: 26/30, step: 35/364, loss: 0.21459, accuracy: 0.92366\n",
            "Epoch: 26/30, step: 36/364, loss: 0.21247, accuracy: 0.92535\n",
            "Epoch: 26/30, step: 37/364, loss: 0.21171, accuracy: 0.92525\n",
            "Epoch: 26/30, step: 38/364, loss: 0.20916, accuracy: 0.92722\n",
            "Epoch: 26/30, step: 39/364, loss: 0.20893, accuracy: 0.92668\n",
            "Epoch: 26/30, step: 40/364, loss: 0.20640, accuracy: 0.92773\n",
            "Epoch: 26/30, step: 41/364, loss: 0.20576, accuracy: 0.92797\n",
            "Epoch: 26/30, step: 42/364, loss: 0.20501, accuracy: 0.92820\n",
            "Epoch: 26/30, step: 43/364, loss: 0.20625, accuracy: 0.92805\n",
            "Epoch: 26/30, step: 44/364, loss: 0.20504, accuracy: 0.92862\n",
            "Epoch: 26/30, step: 45/364, loss: 0.20381, accuracy: 0.92951\n",
            "Epoch: 26/30, step: 46/364, loss: 0.20418, accuracy: 0.92901\n",
            "Epoch: 26/30, step: 47/364, loss: 0.20222, accuracy: 0.93019\n",
            "Epoch: 26/30, step: 48/364, loss: 0.20004, accuracy: 0.93132\n",
            "Epoch: 26/30, step: 49/364, loss: 0.20070, accuracy: 0.93080\n",
            "Epoch: 26/30, step: 50/364, loss: 0.20154, accuracy: 0.92937\n",
            "Epoch: 26/30, step: 51/364, loss: 0.20088, accuracy: 0.92953\n",
            "Epoch: 26/30, step: 52/364, loss: 0.20111, accuracy: 0.92939\n",
            "Epoch: 26/30, step: 53/364, loss: 0.20236, accuracy: 0.92807\n",
            "Epoch: 26/30, step: 54/364, loss: 0.20150, accuracy: 0.92853\n",
            "Epoch: 26/30, step: 55/364, loss: 0.20143, accuracy: 0.92869\n",
            "Epoch: 26/30, step: 56/364, loss: 0.20069, accuracy: 0.92941\n",
            "Epoch: 26/30, step: 57/364, loss: 0.20096, accuracy: 0.92900\n",
            "Epoch: 26/30, step: 58/364, loss: 0.19971, accuracy: 0.92996\n",
            "Epoch: 26/30, step: 59/364, loss: 0.20004, accuracy: 0.92929\n",
            "Epoch: 26/30, step: 60/364, loss: 0.19956, accuracy: 0.92943\n",
            "Epoch: 26/30, step: 61/364, loss: 0.19848, accuracy: 0.93058\n",
            "Epoch: 26/30, step: 62/364, loss: 0.19865, accuracy: 0.93019\n",
            "Epoch: 26/30, step: 63/364, loss: 0.19952, accuracy: 0.92981\n",
            "Epoch: 26/30, step: 64/364, loss: 0.19986, accuracy: 0.92969\n",
            "Epoch: 26/30, step: 65/364, loss: 0.20039, accuracy: 0.92933\n",
            "Epoch: 26/30, step: 66/364, loss: 0.19998, accuracy: 0.92969\n",
            "Epoch: 26/30, step: 67/364, loss: 0.20227, accuracy: 0.92840\n",
            "Epoch: 26/30, step: 68/364, loss: 0.20128, accuracy: 0.92900\n",
            "Epoch: 26/30, step: 69/364, loss: 0.20108, accuracy: 0.92889\n",
            "Epoch: 26/30, step: 70/364, loss: 0.20058, accuracy: 0.92924\n",
            "Epoch: 26/30, step: 71/364, loss: 0.20045, accuracy: 0.92892\n",
            "Epoch: 26/30, step: 72/364, loss: 0.19972, accuracy: 0.92969\n",
            "Epoch: 26/30, step: 73/364, loss: 0.19958, accuracy: 0.92958\n",
            "Epoch: 26/30, step: 74/364, loss: 0.19995, accuracy: 0.92905\n",
            "Epoch: 26/30, step: 75/364, loss: 0.20022, accuracy: 0.92896\n",
            "Epoch: 26/30, step: 76/364, loss: 0.20026, accuracy: 0.92866\n",
            "Epoch: 26/30, step: 77/364, loss: 0.20110, accuracy: 0.92817\n",
            "Epoch: 26/30, step: 78/364, loss: 0.20154, accuracy: 0.92768\n",
            "Epoch: 26/30, step: 79/364, loss: 0.20233, accuracy: 0.92722\n",
            "Epoch: 26/30, step: 80/364, loss: 0.20345, accuracy: 0.92676\n",
            "Epoch: 26/30, step: 81/364, loss: 0.20291, accuracy: 0.92689\n",
            "Epoch: 26/30, step: 82/364, loss: 0.20474, accuracy: 0.92569\n",
            "Epoch: 26/30, step: 83/364, loss: 0.20523, accuracy: 0.92526\n",
            "Epoch: 26/30, step: 84/364, loss: 0.20448, accuracy: 0.92578\n",
            "Epoch: 26/30, step: 85/364, loss: 0.20456, accuracy: 0.92592\n",
            "Epoch: 26/30, step: 86/364, loss: 0.20449, accuracy: 0.92569\n",
            "Epoch: 26/30, step: 87/364, loss: 0.20387, accuracy: 0.92619\n",
            "Epoch: 26/30, step: 88/364, loss: 0.20352, accuracy: 0.92649\n",
            "Epoch: 26/30, step: 89/364, loss: 0.20502, accuracy: 0.92626\n",
            "Epoch: 26/30, step: 90/364, loss: 0.20502, accuracy: 0.92622\n",
            "Epoch: 26/30, step: 91/364, loss: 0.20586, accuracy: 0.92582\n",
            "Epoch: 26/30, step: 92/364, loss: 0.20549, accuracy: 0.92612\n",
            "Epoch: 26/30, step: 93/364, loss: 0.20527, accuracy: 0.92608\n",
            "Epoch: 26/30, step: 94/364, loss: 0.20751, accuracy: 0.92487\n",
            "Epoch: 26/30, step: 95/364, loss: 0.20711, accuracy: 0.92500\n",
            "Epoch: 26/30, step: 96/364, loss: 0.20698, accuracy: 0.92513\n",
            "Epoch: 26/30, step: 97/364, loss: 0.20712, accuracy: 0.92510\n",
            "Epoch: 26/30, step: 98/364, loss: 0.20687, accuracy: 0.92506\n",
            "Epoch: 26/30, step: 99/364, loss: 0.20879, accuracy: 0.92424\n",
            "Epoch: 26/30, step: 100/364, loss: 0.20909, accuracy: 0.92406\n",
            "Epoch: 26/30, step: 101/364, loss: 0.20863, accuracy: 0.92420\n",
            "Epoch: 26/30, step: 102/364, loss: 0.20812, accuracy: 0.92463\n",
            "Epoch: 26/30, step: 103/364, loss: 0.20807, accuracy: 0.92445\n",
            "Epoch: 26/30, step: 104/364, loss: 0.20783, accuracy: 0.92443\n",
            "Epoch: 26/30, step: 105/364, loss: 0.20760, accuracy: 0.92440\n",
            "Epoch: 26/30, step: 106/364, loss: 0.20748, accuracy: 0.92453\n",
            "Epoch: 26/30, step: 107/364, loss: 0.20723, accuracy: 0.92465\n",
            "Epoch: 26/30, step: 108/364, loss: 0.20719, accuracy: 0.92477\n",
            "Epoch: 26/30, step: 109/364, loss: 0.20739, accuracy: 0.92431\n",
            "Epoch: 26/30, step: 110/364, loss: 0.20836, accuracy: 0.92344\n",
            "Epoch: 26/30, step: 111/364, loss: 0.20891, accuracy: 0.92328\n",
            "Epoch: 26/30, step: 112/364, loss: 0.20800, accuracy: 0.92369\n",
            "Epoch: 26/30, step: 113/364, loss: 0.20739, accuracy: 0.92409\n",
            "Epoch: 26/30, step: 114/364, loss: 0.20722, accuracy: 0.92421\n",
            "Epoch: 26/30, step: 115/364, loss: 0.20708, accuracy: 0.92446\n",
            "Epoch: 26/30, step: 116/364, loss: 0.20658, accuracy: 0.92484\n",
            "Epoch: 26/30, step: 117/364, loss: 0.20659, accuracy: 0.92455\n",
            "Epoch: 26/30, step: 118/364, loss: 0.20667, accuracy: 0.92452\n",
            "Epoch: 26/30, step: 119/364, loss: 0.20630, accuracy: 0.92489\n",
            "Epoch: 26/30, step: 120/364, loss: 0.20633, accuracy: 0.92487\n",
            "Epoch: 26/30, step: 121/364, loss: 0.20625, accuracy: 0.92510\n",
            "Epoch: 26/30, step: 122/364, loss: 0.20617, accuracy: 0.92508\n",
            "Epoch: 26/30, step: 123/364, loss: 0.20716, accuracy: 0.92454\n",
            "Epoch: 26/30, step: 124/364, loss: 0.20657, accuracy: 0.92503\n",
            "Epoch: 26/30, step: 125/364, loss: 0.20741, accuracy: 0.92437\n",
            "Epoch: 26/30, step: 126/364, loss: 0.20695, accuracy: 0.92460\n",
            "Epoch: 26/30, step: 127/364, loss: 0.20685, accuracy: 0.92458\n",
            "Epoch: 26/30, step: 128/364, loss: 0.20623, accuracy: 0.92505\n",
            "Epoch: 26/30, step: 129/364, loss: 0.20620, accuracy: 0.92478\n",
            "Epoch: 26/30, step: 130/364, loss: 0.20625, accuracy: 0.92452\n",
            "Epoch: 26/30, step: 131/364, loss: 0.20568, accuracy: 0.92474\n",
            "Epoch: 26/30, step: 132/364, loss: 0.20542, accuracy: 0.92483\n",
            "Epoch: 26/30, step: 133/364, loss: 0.20543, accuracy: 0.92469\n",
            "Epoch: 26/30, step: 134/364, loss: 0.20584, accuracy: 0.92467\n",
            "Epoch: 26/30, step: 135/364, loss: 0.20519, accuracy: 0.92488\n",
            "Epoch: 26/30, step: 136/364, loss: 0.20544, accuracy: 0.92440\n",
            "Epoch: 26/30, step: 137/364, loss: 0.20522, accuracy: 0.92450\n",
            "Epoch: 26/30, step: 138/364, loss: 0.20588, accuracy: 0.92391\n",
            "Epoch: 26/30, step: 139/364, loss: 0.20579, accuracy: 0.92390\n",
            "Epoch: 26/30, step: 140/364, loss: 0.20672, accuracy: 0.92310\n",
            "Epoch: 26/30, step: 141/364, loss: 0.20643, accuracy: 0.92309\n",
            "Epoch: 26/30, step: 142/364, loss: 0.20653, accuracy: 0.92320\n",
            "Epoch: 26/30, step: 143/364, loss: 0.20627, accuracy: 0.92330\n",
            "Epoch: 26/30, step: 144/364, loss: 0.20644, accuracy: 0.92350\n",
            "Epoch: 26/30, step: 145/364, loss: 0.20608, accuracy: 0.92381\n",
            "Epoch: 26/30, step: 146/364, loss: 0.20631, accuracy: 0.92359\n",
            "Epoch: 26/30, step: 147/364, loss: 0.20599, accuracy: 0.92368\n",
            "Epoch: 26/30, step: 148/364, loss: 0.20637, accuracy: 0.92356\n",
            "Epoch: 26/30, step: 149/364, loss: 0.20611, accuracy: 0.92376\n",
            "Epoch: 26/30, step: 150/364, loss: 0.20608, accuracy: 0.92375\n",
            "Epoch: 26/30, step: 151/364, loss: 0.20580, accuracy: 0.92384\n",
            "Epoch: 26/30, step: 152/364, loss: 0.20527, accuracy: 0.92403\n",
            "Epoch: 26/30, step: 153/364, loss: 0.20565, accuracy: 0.92402\n",
            "Epoch: 26/30, step: 154/364, loss: 0.20487, accuracy: 0.92451\n",
            "Epoch: 26/30, step: 155/364, loss: 0.20447, accuracy: 0.92470\n",
            "Epoch: 26/30, step: 156/364, loss: 0.20365, accuracy: 0.92518\n",
            "Epoch: 26/30, step: 157/364, loss: 0.20310, accuracy: 0.92546\n",
            "Epoch: 26/30, step: 158/364, loss: 0.20347, accuracy: 0.92524\n",
            "Epoch: 26/30, step: 159/364, loss: 0.20329, accuracy: 0.92541\n",
            "Epoch: 26/30, step: 160/364, loss: 0.20316, accuracy: 0.92559\n",
            "Epoch: 26/30, step: 161/364, loss: 0.20319, accuracy: 0.92566\n",
            "Epoch: 26/30, step: 162/364, loss: 0.20260, accuracy: 0.92602\n",
            "Epoch: 26/30, step: 163/364, loss: 0.20218, accuracy: 0.92619\n",
            "Epoch: 26/30, step: 164/364, loss: 0.20229, accuracy: 0.92597\n",
            "Epoch: 26/30, step: 165/364, loss: 0.20214, accuracy: 0.92585\n",
            "Epoch: 26/30, step: 166/364, loss: 0.20176, accuracy: 0.92602\n",
            "Epoch: 26/30, step: 167/364, loss: 0.20131, accuracy: 0.92637\n",
            "Epoch: 26/30, step: 168/364, loss: 0.20079, accuracy: 0.92680\n",
            "Epoch: 26/30, step: 169/364, loss: 0.20109, accuracy: 0.92668\n",
            "Epoch: 26/30, step: 170/364, loss: 0.20072, accuracy: 0.92675\n",
            "Epoch: 26/30, step: 171/364, loss: 0.20029, accuracy: 0.92699\n",
            "Epoch: 26/30, step: 172/364, loss: 0.20086, accuracy: 0.92678\n",
            "Epoch: 26/30, step: 173/364, loss: 0.20046, accuracy: 0.92693\n",
            "Epoch: 26/30, step: 174/364, loss: 0.20000, accuracy: 0.92708\n",
            "Epoch: 26/30, step: 175/364, loss: 0.20002, accuracy: 0.92714\n",
            "Epoch: 26/30, step: 176/364, loss: 0.20019, accuracy: 0.92676\n",
            "Epoch: 26/30, step: 177/364, loss: 0.20037, accuracy: 0.92664\n",
            "Epoch: 26/30, step: 178/364, loss: 0.20028, accuracy: 0.92662\n",
            "Epoch: 26/30, step: 179/364, loss: 0.19973, accuracy: 0.92694\n",
            "Epoch: 26/30, step: 180/364, loss: 0.19923, accuracy: 0.92717\n",
            "Epoch: 26/30, step: 181/364, loss: 0.19970, accuracy: 0.92688\n",
            "Epoch: 26/30, step: 182/364, loss: 0.20041, accuracy: 0.92668\n",
            "Epoch: 26/30, step: 183/364, loss: 0.19998, accuracy: 0.92691\n",
            "Epoch: 26/30, step: 184/364, loss: 0.19974, accuracy: 0.92706\n",
            "Epoch: 26/30, step: 185/364, loss: 0.19977, accuracy: 0.92711\n",
            "Epoch: 26/30, step: 186/364, loss: 0.19996, accuracy: 0.92692\n",
            "Epoch: 26/30, step: 187/364, loss: 0.20001, accuracy: 0.92689\n",
            "Epoch: 26/30, step: 188/364, loss: 0.20008, accuracy: 0.92686\n",
            "Epoch: 26/30, step: 189/364, loss: 0.20014, accuracy: 0.92692\n",
            "Epoch: 26/30, step: 190/364, loss: 0.20011, accuracy: 0.92689\n",
            "Epoch: 26/30, step: 191/364, loss: 0.20010, accuracy: 0.92662\n",
            "Epoch: 26/30, step: 192/364, loss: 0.20009, accuracy: 0.92651\n",
            "Epoch: 26/30, step: 193/364, loss: 0.19951, accuracy: 0.92673\n",
            "Epoch: 26/30, step: 194/364, loss: 0.20005, accuracy: 0.92647\n",
            "Epoch: 26/30, step: 195/364, loss: 0.19971, accuracy: 0.92668\n",
            "Epoch: 26/30, step: 196/364, loss: 0.19936, accuracy: 0.92698\n",
            "Epoch: 26/30, step: 197/364, loss: 0.19941, accuracy: 0.92687\n",
            "Epoch: 26/30, step: 198/364, loss: 0.19899, accuracy: 0.92700\n",
            "Epoch: 26/30, step: 199/364, loss: 0.19911, accuracy: 0.92698\n",
            "Epoch: 26/30, step: 200/364, loss: 0.19864, accuracy: 0.92727\n",
            "Epoch: 26/30, step: 201/364, loss: 0.19871, accuracy: 0.92724\n",
            "Epoch: 26/30, step: 202/364, loss: 0.19934, accuracy: 0.92698\n",
            "Epoch: 26/30, step: 203/364, loss: 0.19900, accuracy: 0.92726\n",
            "Epoch: 26/30, step: 204/364, loss: 0.19920, accuracy: 0.92701\n",
            "Epoch: 26/30, step: 205/364, loss: 0.19960, accuracy: 0.92683\n",
            "Epoch: 26/30, step: 206/364, loss: 0.19932, accuracy: 0.92703\n",
            "Epoch: 26/30, step: 207/364, loss: 0.19909, accuracy: 0.92716\n",
            "Epoch: 26/30, step: 208/364, loss: 0.19921, accuracy: 0.92713\n",
            "Epoch: 26/30, step: 209/364, loss: 0.20000, accuracy: 0.92658\n",
            "Epoch: 26/30, step: 210/364, loss: 0.19986, accuracy: 0.92641\n",
            "Epoch: 26/30, step: 211/364, loss: 0.19994, accuracy: 0.92647\n",
            "Epoch: 26/30, step: 212/364, loss: 0.19979, accuracy: 0.92659\n",
            "Epoch: 26/30, step: 213/364, loss: 0.19939, accuracy: 0.92686\n",
            "Epoch: 26/30, step: 214/364, loss: 0.19966, accuracy: 0.92669\n",
            "Epoch: 26/30, step: 215/364, loss: 0.19976, accuracy: 0.92667\n",
            "Epoch: 26/30, step: 216/364, loss: 0.19994, accuracy: 0.92643\n",
            "Epoch: 26/30, step: 217/364, loss: 0.20013, accuracy: 0.92627\n",
            "Epoch: 26/30, step: 218/364, loss: 0.19990, accuracy: 0.92646\n",
            "Epoch: 26/30, step: 219/364, loss: 0.19961, accuracy: 0.92658\n",
            "Epoch: 26/30, step: 220/364, loss: 0.19945, accuracy: 0.92670\n",
            "Epoch: 26/30, step: 221/364, loss: 0.19932, accuracy: 0.92689\n",
            "Epoch: 26/30, step: 222/364, loss: 0.19912, accuracy: 0.92708\n",
            "Epoch: 26/30, step: 223/364, loss: 0.19916, accuracy: 0.92699\n",
            "Epoch: 26/30, step: 224/364, loss: 0.19919, accuracy: 0.92676\n",
            "Epoch: 26/30, step: 225/364, loss: 0.19909, accuracy: 0.92681\n",
            "Epoch: 26/30, step: 226/364, loss: 0.19863, accuracy: 0.92706\n",
            "Epoch: 26/30, step: 227/364, loss: 0.19833, accuracy: 0.92718\n",
            "Epoch: 26/30, step: 228/364, loss: 0.19871, accuracy: 0.92695\n",
            "Epoch: 26/30, step: 229/364, loss: 0.19859, accuracy: 0.92699\n",
            "Epoch: 26/30, step: 230/364, loss: 0.19920, accuracy: 0.92670\n",
            "Epoch: 26/30, step: 231/364, loss: 0.19904, accuracy: 0.92668\n",
            "Epoch: 26/30, step: 232/364, loss: 0.19863, accuracy: 0.92699\n",
            "Epoch: 26/30, step: 233/364, loss: 0.19877, accuracy: 0.92690\n",
            "Epoch: 26/30, step: 234/364, loss: 0.19868, accuracy: 0.92702\n",
            "Epoch: 26/30, step: 235/364, loss: 0.19858, accuracy: 0.92713\n",
            "Epoch: 26/30, step: 236/364, loss: 0.19844, accuracy: 0.92711\n",
            "Epoch: 26/30, step: 237/364, loss: 0.19845, accuracy: 0.92722\n",
            "Epoch: 26/30, step: 238/364, loss: 0.19828, accuracy: 0.92726\n",
            "Epoch: 26/30, step: 239/364, loss: 0.19793, accuracy: 0.92750\n",
            "Epoch: 26/30, step: 240/364, loss: 0.19780, accuracy: 0.92767\n",
            "Epoch: 26/30, step: 241/364, loss: 0.19822, accuracy: 0.92745\n",
            "Epoch: 26/30, step: 242/364, loss: 0.19819, accuracy: 0.92736\n",
            "Epoch: 26/30, step: 243/364, loss: 0.19794, accuracy: 0.92753\n",
            "Epoch: 26/30, step: 244/364, loss: 0.19775, accuracy: 0.92751\n",
            "Epoch: 26/30, step: 245/364, loss: 0.19776, accuracy: 0.92736\n",
            "Epoch: 26/30, step: 246/364, loss: 0.19770, accuracy: 0.92740\n",
            "Epoch: 26/30, step: 247/364, loss: 0.19799, accuracy: 0.92725\n",
            "Epoch: 26/30, step: 248/364, loss: 0.19778, accuracy: 0.92742\n",
            "Epoch: 26/30, step: 249/364, loss: 0.19736, accuracy: 0.92759\n",
            "Epoch: 26/30, step: 250/364, loss: 0.19724, accuracy: 0.92756\n",
            "Epoch: 26/30, step: 251/364, loss: 0.19743, accuracy: 0.92729\n",
            "Epoch: 26/30, step: 252/364, loss: 0.19720, accuracy: 0.92746\n",
            "Epoch: 26/30, step: 253/364, loss: 0.19717, accuracy: 0.92737\n",
            "Epoch: 26/30, step: 254/364, loss: 0.19685, accuracy: 0.92753\n",
            "Epoch: 26/30, step: 255/364, loss: 0.19711, accuracy: 0.92733\n",
            "Epoch: 26/30, step: 256/364, loss: 0.19691, accuracy: 0.92743\n",
            "Epoch: 26/30, step: 257/364, loss: 0.19664, accuracy: 0.92759\n",
            "Epoch: 26/30, step: 258/364, loss: 0.19662, accuracy: 0.92763\n",
            "Epoch: 26/30, step: 259/364, loss: 0.19682, accuracy: 0.92749\n",
            "Epoch: 26/30, step: 260/364, loss: 0.19650, accuracy: 0.92764\n",
            "Epoch: 26/30, step: 261/364, loss: 0.19677, accuracy: 0.92750\n",
            "Epoch: 26/30, step: 262/364, loss: 0.19661, accuracy: 0.92760\n",
            "Epoch: 26/30, step: 263/364, loss: 0.19645, accuracy: 0.92764\n",
            "Epoch: 26/30, step: 264/364, loss: 0.19631, accuracy: 0.92773\n",
            "Epoch: 26/30, step: 265/364, loss: 0.19666, accuracy: 0.92765\n",
            "Epoch: 26/30, step: 266/364, loss: 0.19712, accuracy: 0.92763\n",
            "Epoch: 26/30, step: 267/364, loss: 0.19692, accuracy: 0.92779\n",
            "Epoch: 26/30, step: 268/364, loss: 0.19683, accuracy: 0.92788\n",
            "Epoch: 26/30, step: 269/364, loss: 0.19666, accuracy: 0.92780\n",
            "Epoch: 26/30, step: 270/364, loss: 0.19662, accuracy: 0.92772\n",
            "Epoch: 26/30, step: 271/364, loss: 0.19619, accuracy: 0.92799\n",
            "Epoch: 26/30, step: 272/364, loss: 0.19623, accuracy: 0.92796\n",
            "Epoch: 26/30, step: 273/364, loss: 0.19617, accuracy: 0.92800\n",
            "Epoch: 26/30, step: 274/364, loss: 0.19569, accuracy: 0.92826\n",
            "Epoch: 26/30, step: 275/364, loss: 0.19563, accuracy: 0.92818\n",
            "Epoch: 26/30, step: 276/364, loss: 0.19542, accuracy: 0.92833\n",
            "Epoch: 26/30, step: 277/364, loss: 0.19558, accuracy: 0.92831\n",
            "Epoch: 26/30, step: 278/364, loss: 0.19518, accuracy: 0.92856\n",
            "Epoch: 26/30, step: 279/364, loss: 0.19507, accuracy: 0.92860\n",
            "Epoch: 26/30, step: 280/364, loss: 0.19491, accuracy: 0.92863\n",
            "Epoch: 26/30, step: 281/364, loss: 0.19470, accuracy: 0.92883\n",
            "Epoch: 26/30, step: 282/364, loss: 0.19519, accuracy: 0.92875\n",
            "Epoch: 26/30, step: 283/364, loss: 0.19505, accuracy: 0.92889\n",
            "Epoch: 26/30, step: 284/364, loss: 0.19520, accuracy: 0.92881\n",
            "Epoch: 26/30, step: 285/364, loss: 0.19474, accuracy: 0.92906\n",
            "Epoch: 26/30, step: 286/364, loss: 0.19524, accuracy: 0.92881\n",
            "Epoch: 26/30, step: 287/364, loss: 0.19509, accuracy: 0.92901\n",
            "Epoch: 26/30, step: 288/364, loss: 0.19505, accuracy: 0.92904\n",
            "Epoch: 26/30, step: 289/364, loss: 0.19517, accuracy: 0.92912\n",
            "Epoch: 26/30, step: 290/364, loss: 0.19530, accuracy: 0.92899\n",
            "Epoch: 26/30, step: 291/364, loss: 0.19535, accuracy: 0.92896\n",
            "Epoch: 26/30, train loss: 0.19535, train accuracy: 0.92896, valid loss: 0.85106, valid accuracy: 0.66230\n",
            "Epoch: 27/30, step: 1/364, loss: 0.14280, accuracy: 0.96875\n",
            "Epoch: 27/30, step: 2/364, loss: 0.14253, accuracy: 0.96875\n",
            "Epoch: 27/30, step: 3/364, loss: 0.14984, accuracy: 0.96875\n",
            "Epoch: 27/30, step: 4/364, loss: 0.15243, accuracy: 0.95703\n",
            "Epoch: 27/30, step: 5/364, loss: 0.16255, accuracy: 0.95000\n",
            "Epoch: 27/30, step: 6/364, loss: 0.17792, accuracy: 0.94010\n",
            "Epoch: 27/30, step: 7/364, loss: 0.17328, accuracy: 0.94420\n",
            "Epoch: 27/30, step: 8/364, loss: 0.17452, accuracy: 0.94727\n",
            "Epoch: 27/30, step: 9/364, loss: 0.17876, accuracy: 0.94444\n",
            "Epoch: 27/30, step: 10/364, loss: 0.17755, accuracy: 0.94375\n",
            "Epoch: 27/30, step: 11/364, loss: 0.17527, accuracy: 0.94460\n",
            "Epoch: 27/30, step: 12/364, loss: 0.17604, accuracy: 0.94141\n",
            "Epoch: 27/30, step: 13/364, loss: 0.17122, accuracy: 0.94351\n",
            "Epoch: 27/30, step: 14/364, loss: 0.16739, accuracy: 0.94531\n",
            "Epoch: 27/30, step: 15/364, loss: 0.16365, accuracy: 0.94583\n",
            "Epoch: 27/30, step: 16/364, loss: 0.16569, accuracy: 0.94434\n",
            "Epoch: 27/30, step: 17/364, loss: 0.16653, accuracy: 0.94485\n",
            "Epoch: 27/30, step: 18/364, loss: 0.16811, accuracy: 0.94444\n",
            "Epoch: 27/30, step: 19/364, loss: 0.16829, accuracy: 0.94490\n",
            "Epoch: 27/30, step: 20/364, loss: 0.16985, accuracy: 0.94375\n",
            "Epoch: 27/30, step: 21/364, loss: 0.17738, accuracy: 0.93973\n",
            "Epoch: 27/30, step: 22/364, loss: 0.17961, accuracy: 0.93892\n",
            "Epoch: 27/30, step: 23/364, loss: 0.17982, accuracy: 0.93954\n",
            "Epoch: 27/30, step: 24/364, loss: 0.17587, accuracy: 0.94141\n",
            "Epoch: 27/30, step: 25/364, loss: 0.17551, accuracy: 0.94187\n",
            "Epoch: 27/30, step: 26/364, loss: 0.17660, accuracy: 0.94231\n",
            "Epoch: 27/30, step: 27/364, loss: 0.17382, accuracy: 0.94387\n",
            "Epoch: 27/30, step: 28/364, loss: 0.17346, accuracy: 0.94252\n",
            "Epoch: 27/30, step: 29/364, loss: 0.17125, accuracy: 0.94343\n",
            "Epoch: 27/30, step: 30/364, loss: 0.17078, accuracy: 0.94375\n",
            "Epoch: 27/30, step: 31/364, loss: 0.17065, accuracy: 0.94355\n",
            "Epoch: 27/30, step: 32/364, loss: 0.17282, accuracy: 0.94141\n",
            "Epoch: 27/30, step: 33/364, loss: 0.17264, accuracy: 0.94176\n",
            "Epoch: 27/30, step: 34/364, loss: 0.17221, accuracy: 0.94164\n",
            "Epoch: 27/30, step: 35/364, loss: 0.17091, accuracy: 0.94241\n",
            "Epoch: 27/30, step: 36/364, loss: 0.17130, accuracy: 0.94271\n",
            "Epoch: 27/30, step: 37/364, loss: 0.17205, accuracy: 0.94257\n",
            "Epoch: 27/30, step: 38/364, loss: 0.17081, accuracy: 0.94326\n",
            "Epoch: 27/30, step: 39/364, loss: 0.17051, accuracy: 0.94311\n",
            "Epoch: 27/30, step: 40/364, loss: 0.17087, accuracy: 0.94258\n",
            "Epoch: 27/30, step: 41/364, loss: 0.17220, accuracy: 0.94245\n",
            "Epoch: 27/30, step: 42/364, loss: 0.17108, accuracy: 0.94345\n",
            "Epoch: 27/30, step: 43/364, loss: 0.17142, accuracy: 0.94295\n",
            "Epoch: 27/30, step: 44/364, loss: 0.17131, accuracy: 0.94283\n",
            "Epoch: 27/30, step: 45/364, loss: 0.17292, accuracy: 0.94097\n",
            "Epoch: 27/30, step: 46/364, loss: 0.17375, accuracy: 0.94090\n",
            "Epoch: 27/30, step: 47/364, loss: 0.17400, accuracy: 0.94082\n",
            "Epoch: 27/30, step: 48/364, loss: 0.17277, accuracy: 0.94108\n",
            "Epoch: 27/30, step: 49/364, loss: 0.17132, accuracy: 0.94228\n",
            "Epoch: 27/30, step: 50/364, loss: 0.17167, accuracy: 0.94187\n",
            "Epoch: 27/30, step: 51/364, loss: 0.17092, accuracy: 0.94240\n",
            "Epoch: 27/30, step: 52/364, loss: 0.17163, accuracy: 0.94141\n",
            "Epoch: 27/30, step: 53/364, loss: 0.17204, accuracy: 0.94133\n",
            "Epoch: 27/30, step: 54/364, loss: 0.17084, accuracy: 0.94213\n",
            "Epoch: 27/30, step: 55/364, loss: 0.17517, accuracy: 0.93977\n",
            "Epoch: 27/30, step: 56/364, loss: 0.17539, accuracy: 0.93973\n",
            "Epoch: 27/30, step: 57/364, loss: 0.17524, accuracy: 0.93969\n",
            "Epoch: 27/30, step: 58/364, loss: 0.17416, accuracy: 0.93992\n",
            "Epoch: 27/30, step: 59/364, loss: 0.17328, accuracy: 0.94015\n",
            "Epoch: 27/30, step: 60/364, loss: 0.17369, accuracy: 0.93984\n",
            "Epoch: 27/30, step: 61/364, loss: 0.17401, accuracy: 0.93955\n",
            "Epoch: 27/30, step: 62/364, loss: 0.17401, accuracy: 0.93977\n",
            "Epoch: 27/30, step: 63/364, loss: 0.17403, accuracy: 0.93973\n",
            "Epoch: 27/30, step: 64/364, loss: 0.17309, accuracy: 0.94019\n",
            "Epoch: 27/30, step: 65/364, loss: 0.17226, accuracy: 0.94038\n",
            "Epoch: 27/30, step: 66/364, loss: 0.17139, accuracy: 0.94081\n",
            "Epoch: 27/30, step: 67/364, loss: 0.17226, accuracy: 0.94076\n",
            "Epoch: 27/30, step: 68/364, loss: 0.17258, accuracy: 0.94072\n",
            "Epoch: 27/30, step: 69/364, loss: 0.17252, accuracy: 0.94090\n",
            "Epoch: 27/30, step: 70/364, loss: 0.17271, accuracy: 0.94085\n",
            "Epoch: 27/30, step: 71/364, loss: 0.17238, accuracy: 0.94146\n",
            "Epoch: 27/30, step: 72/364, loss: 0.17349, accuracy: 0.94054\n",
            "Epoch: 27/30, step: 73/364, loss: 0.17357, accuracy: 0.94071\n",
            "Epoch: 27/30, step: 74/364, loss: 0.17419, accuracy: 0.94046\n",
            "Epoch: 27/30, step: 75/364, loss: 0.17489, accuracy: 0.94021\n",
            "Epoch: 27/30, step: 76/364, loss: 0.17458, accuracy: 0.94038\n",
            "Epoch: 27/30, step: 77/364, loss: 0.17603, accuracy: 0.93912\n",
            "Epoch: 27/30, step: 78/364, loss: 0.17543, accuracy: 0.93930\n",
            "Epoch: 27/30, step: 79/364, loss: 0.17447, accuracy: 0.93987\n",
            "Epoch: 27/30, step: 80/364, loss: 0.17563, accuracy: 0.93906\n",
            "Epoch: 27/30, step: 81/364, loss: 0.17687, accuracy: 0.93808\n",
            "Epoch: 27/30, step: 82/364, loss: 0.17713, accuracy: 0.93788\n",
            "Epoch: 27/30, step: 83/364, loss: 0.17748, accuracy: 0.93769\n",
            "Epoch: 27/30, step: 84/364, loss: 0.17731, accuracy: 0.93787\n",
            "Epoch: 27/30, step: 85/364, loss: 0.17706, accuracy: 0.93787\n",
            "Epoch: 27/30, step: 86/364, loss: 0.17673, accuracy: 0.93805\n",
            "Epoch: 27/30, step: 87/364, loss: 0.17632, accuracy: 0.93840\n",
            "Epoch: 27/30, step: 88/364, loss: 0.17737, accuracy: 0.93803\n",
            "Epoch: 27/30, step: 89/364, loss: 0.17729, accuracy: 0.93803\n",
            "Epoch: 27/30, step: 90/364, loss: 0.17636, accuracy: 0.93854\n",
            "Epoch: 27/30, step: 91/364, loss: 0.17582, accuracy: 0.93870\n",
            "Epoch: 27/30, step: 92/364, loss: 0.17552, accuracy: 0.93903\n",
            "Epoch: 27/30, step: 93/364, loss: 0.17495, accuracy: 0.93918\n",
            "Epoch: 27/30, step: 94/364, loss: 0.17452, accuracy: 0.93933\n",
            "Epoch: 27/30, step: 95/364, loss: 0.17467, accuracy: 0.93931\n",
            "Epoch: 27/30, step: 96/364, loss: 0.17489, accuracy: 0.93929\n",
            "Epoch: 27/30, step: 97/364, loss: 0.17533, accuracy: 0.93863\n",
            "Epoch: 27/30, step: 98/364, loss: 0.17573, accuracy: 0.93846\n",
            "Epoch: 27/30, step: 99/364, loss: 0.17547, accuracy: 0.93876\n",
            "Epoch: 27/30, step: 100/364, loss: 0.17520, accuracy: 0.93906\n",
            "Epoch: 27/30, step: 101/364, loss: 0.17440, accuracy: 0.93951\n",
            "Epoch: 27/30, step: 102/364, loss: 0.17422, accuracy: 0.93964\n",
            "Epoch: 27/30, step: 103/364, loss: 0.17575, accuracy: 0.93902\n",
            "Epoch: 27/30, step: 104/364, loss: 0.17542, accuracy: 0.93930\n",
            "Epoch: 27/30, step: 105/364, loss: 0.17506, accuracy: 0.93943\n",
            "Epoch: 27/30, step: 106/364, loss: 0.17539, accuracy: 0.93956\n",
            "Epoch: 27/30, step: 107/364, loss: 0.17467, accuracy: 0.94013\n",
            "Epoch: 27/30, step: 108/364, loss: 0.17426, accuracy: 0.94039\n",
            "Epoch: 27/30, step: 109/364, loss: 0.17376, accuracy: 0.94065\n",
            "Epoch: 27/30, step: 110/364, loss: 0.17424, accuracy: 0.94020\n",
            "Epoch: 27/30, step: 111/364, loss: 0.17379, accuracy: 0.94046\n",
            "Epoch: 27/30, step: 112/364, loss: 0.17355, accuracy: 0.94057\n",
            "Epoch: 27/30, step: 113/364, loss: 0.17332, accuracy: 0.94096\n",
            "Epoch: 27/30, step: 114/364, loss: 0.17357, accuracy: 0.94093\n",
            "Epoch: 27/30, step: 115/364, loss: 0.17392, accuracy: 0.94076\n",
            "Epoch: 27/30, step: 116/364, loss: 0.17410, accuracy: 0.94033\n",
            "Epoch: 27/30, step: 117/364, loss: 0.17387, accuracy: 0.94071\n",
            "Epoch: 27/30, step: 118/364, loss: 0.17369, accuracy: 0.94068\n",
            "Epoch: 27/30, step: 119/364, loss: 0.17324, accuracy: 0.94091\n",
            "Epoch: 27/30, step: 120/364, loss: 0.17327, accuracy: 0.94102\n",
            "Epoch: 27/30, step: 121/364, loss: 0.17287, accuracy: 0.94099\n",
            "Epoch: 27/30, step: 122/364, loss: 0.17340, accuracy: 0.94083\n",
            "Epoch: 27/30, step: 123/364, loss: 0.17331, accuracy: 0.94093\n",
            "Epoch: 27/30, step: 124/364, loss: 0.17332, accuracy: 0.94103\n",
            "Epoch: 27/30, step: 125/364, loss: 0.17286, accuracy: 0.94138\n",
            "Epoch: 27/30, step: 126/364, loss: 0.17290, accuracy: 0.94134\n",
            "Epoch: 27/30, step: 127/364, loss: 0.17230, accuracy: 0.94168\n",
            "Epoch: 27/30, step: 128/364, loss: 0.17205, accuracy: 0.94202\n",
            "Epoch: 27/30, step: 129/364, loss: 0.17236, accuracy: 0.94186\n",
            "Epoch: 27/30, step: 130/364, loss: 0.17196, accuracy: 0.94207\n",
            "Epoch: 27/30, step: 131/364, loss: 0.17297, accuracy: 0.94132\n",
            "Epoch: 27/30, step: 132/364, loss: 0.17396, accuracy: 0.94093\n",
            "Epoch: 27/30, step: 133/364, loss: 0.17344, accuracy: 0.94126\n",
            "Epoch: 27/30, step: 134/364, loss: 0.17368, accuracy: 0.94135\n",
            "Epoch: 27/30, step: 135/364, loss: 0.17439, accuracy: 0.94097\n",
            "Epoch: 27/30, step: 136/364, loss: 0.17439, accuracy: 0.94083\n",
            "Epoch: 27/30, step: 137/364, loss: 0.17472, accuracy: 0.94081\n",
            "Epoch: 27/30, step: 138/364, loss: 0.17438, accuracy: 0.94101\n",
            "Epoch: 27/30, step: 139/364, loss: 0.17427, accuracy: 0.94110\n",
            "Epoch: 27/30, step: 140/364, loss: 0.17387, accuracy: 0.94129\n",
            "Epoch: 27/30, step: 141/364, loss: 0.17321, accuracy: 0.94171\n",
            "Epoch: 27/30, step: 142/364, loss: 0.17299, accuracy: 0.94179\n",
            "Epoch: 27/30, step: 143/364, loss: 0.17257, accuracy: 0.94209\n",
            "Epoch: 27/30, step: 144/364, loss: 0.17209, accuracy: 0.94238\n",
            "Epoch: 27/30, step: 145/364, loss: 0.17207, accuracy: 0.94235\n",
            "Epoch: 27/30, step: 146/364, loss: 0.17171, accuracy: 0.94253\n",
            "Epoch: 27/30, step: 147/364, loss: 0.17220, accuracy: 0.94207\n",
            "Epoch: 27/30, step: 148/364, loss: 0.17227, accuracy: 0.94204\n",
            "Epoch: 27/30, step: 149/364, loss: 0.17226, accuracy: 0.94201\n",
            "Epoch: 27/30, step: 150/364, loss: 0.17235, accuracy: 0.94187\n",
            "Epoch: 27/30, step: 151/364, loss: 0.17196, accuracy: 0.94195\n",
            "Epoch: 27/30, step: 152/364, loss: 0.17182, accuracy: 0.94213\n",
            "Epoch: 27/30, step: 153/364, loss: 0.17151, accuracy: 0.94230\n",
            "Epoch: 27/30, step: 154/364, loss: 0.17172, accuracy: 0.94196\n",
            "Epoch: 27/30, step: 155/364, loss: 0.17168, accuracy: 0.94204\n",
            "Epoch: 27/30, step: 156/364, loss: 0.17184, accuracy: 0.94171\n",
            "Epoch: 27/30, step: 157/364, loss: 0.17124, accuracy: 0.94198\n",
            "Epoch: 27/30, step: 158/364, loss: 0.17138, accuracy: 0.94195\n",
            "Epoch: 27/30, step: 159/364, loss: 0.17189, accuracy: 0.94153\n",
            "Epoch: 27/30, step: 160/364, loss: 0.17215, accuracy: 0.94141\n",
            "Epoch: 27/30, step: 161/364, loss: 0.17200, accuracy: 0.94148\n",
            "Epoch: 27/30, step: 162/364, loss: 0.17378, accuracy: 0.94030\n",
            "Epoch: 27/30, step: 163/364, loss: 0.17489, accuracy: 0.93970\n",
            "Epoch: 27/30, step: 164/364, loss: 0.17495, accuracy: 0.93941\n",
            "Epoch: 27/30, step: 165/364, loss: 0.17566, accuracy: 0.93892\n",
            "Epoch: 27/30, step: 166/364, loss: 0.17516, accuracy: 0.93919\n",
            "Epoch: 27/30, step: 167/364, loss: 0.17499, accuracy: 0.93937\n",
            "Epoch: 27/30, step: 168/364, loss: 0.17503, accuracy: 0.93936\n",
            "Epoch: 27/30, step: 169/364, loss: 0.17546, accuracy: 0.93916\n",
            "Epoch: 27/30, step: 170/364, loss: 0.17503, accuracy: 0.93943\n",
            "Epoch: 27/30, step: 171/364, loss: 0.17647, accuracy: 0.93887\n",
            "Epoch: 27/30, step: 172/364, loss: 0.17594, accuracy: 0.93923\n",
            "Epoch: 27/30, step: 173/364, loss: 0.17577, accuracy: 0.93940\n",
            "Epoch: 27/30, step: 174/364, loss: 0.17570, accuracy: 0.93948\n",
            "Epoch: 27/30, step: 175/364, loss: 0.17547, accuracy: 0.93955\n",
            "Epoch: 27/30, step: 176/364, loss: 0.17585, accuracy: 0.93919\n",
            "Epoch: 27/30, step: 177/364, loss: 0.17613, accuracy: 0.93918\n",
            "Epoch: 27/30, step: 178/364, loss: 0.17583, accuracy: 0.93917\n",
            "Epoch: 27/30, step: 179/364, loss: 0.17626, accuracy: 0.93881\n",
            "Epoch: 27/30, step: 180/364, loss: 0.17582, accuracy: 0.93915\n",
            "Epoch: 27/30, step: 181/364, loss: 0.17557, accuracy: 0.93940\n",
            "Epoch: 27/30, step: 182/364, loss: 0.17568, accuracy: 0.93930\n",
            "Epoch: 27/30, step: 183/364, loss: 0.17515, accuracy: 0.93955\n",
            "Epoch: 27/30, step: 184/364, loss: 0.17503, accuracy: 0.93962\n",
            "Epoch: 27/30, step: 185/364, loss: 0.17534, accuracy: 0.93961\n",
            "Epoch: 27/30, step: 186/364, loss: 0.17518, accuracy: 0.93968\n",
            "Epoch: 27/30, step: 187/364, loss: 0.17495, accuracy: 0.93984\n",
            "Epoch: 27/30, step: 188/364, loss: 0.17498, accuracy: 0.93991\n",
            "Epoch: 27/30, step: 189/364, loss: 0.17474, accuracy: 0.94015\n",
            "Epoch: 27/30, step: 190/364, loss: 0.17452, accuracy: 0.94038\n",
            "Epoch: 27/30, step: 191/364, loss: 0.17397, accuracy: 0.94069\n",
            "Epoch: 27/30, step: 192/364, loss: 0.17364, accuracy: 0.94084\n",
            "Epoch: 27/30, step: 193/364, loss: 0.17348, accuracy: 0.94098\n",
            "Epoch: 27/30, step: 194/364, loss: 0.17407, accuracy: 0.94064\n",
            "Epoch: 27/30, step: 195/364, loss: 0.17381, accuracy: 0.94079\n",
            "Epoch: 27/30, step: 196/364, loss: 0.17364, accuracy: 0.94093\n",
            "Epoch: 27/30, step: 197/364, loss: 0.17374, accuracy: 0.94083\n",
            "Epoch: 27/30, step: 198/364, loss: 0.17390, accuracy: 0.94066\n",
            "Epoch: 27/30, step: 199/364, loss: 0.17359, accuracy: 0.94088\n",
            "Epoch: 27/30, step: 200/364, loss: 0.17350, accuracy: 0.94094\n",
            "Epoch: 27/30, step: 201/364, loss: 0.17362, accuracy: 0.94084\n",
            "Epoch: 27/30, step: 202/364, loss: 0.17337, accuracy: 0.94098\n",
            "Epoch: 27/30, step: 203/364, loss: 0.17314, accuracy: 0.94119\n",
            "Epoch: 27/30, step: 204/364, loss: 0.17290, accuracy: 0.94148\n",
            "Epoch: 27/30, step: 205/364, loss: 0.17328, accuracy: 0.94108\n",
            "Epoch: 27/30, step: 206/364, loss: 0.17322, accuracy: 0.94122\n",
            "Epoch: 27/30, step: 207/364, loss: 0.17284, accuracy: 0.94135\n",
            "Epoch: 27/30, step: 208/364, loss: 0.17278, accuracy: 0.94141\n",
            "Epoch: 27/30, step: 209/364, loss: 0.17320, accuracy: 0.94101\n",
            "Epoch: 27/30, step: 210/364, loss: 0.17294, accuracy: 0.94107\n",
            "Epoch: 27/30, step: 211/364, loss: 0.17414, accuracy: 0.94046\n",
            "Epoch: 27/30, step: 212/364, loss: 0.17402, accuracy: 0.94052\n",
            "Epoch: 27/30, step: 213/364, loss: 0.17408, accuracy: 0.94043\n",
            "Epoch: 27/30, step: 214/364, loss: 0.17381, accuracy: 0.94057\n",
            "Epoch: 27/30, step: 215/364, loss: 0.17366, accuracy: 0.94063\n",
            "Epoch: 27/30, step: 216/364, loss: 0.17356, accuracy: 0.94068\n",
            "Epoch: 27/30, step: 217/364, loss: 0.17330, accuracy: 0.94081\n",
            "Epoch: 27/30, step: 218/364, loss: 0.17324, accuracy: 0.94087\n",
            "Epoch: 27/30, step: 219/364, loss: 0.17293, accuracy: 0.94107\n",
            "Epoch: 27/30, step: 220/364, loss: 0.17270, accuracy: 0.94119\n",
            "Epoch: 27/30, step: 221/364, loss: 0.17263, accuracy: 0.94111\n",
            "Epoch: 27/30, step: 222/364, loss: 0.17248, accuracy: 0.94116\n",
            "Epoch: 27/30, step: 223/364, loss: 0.17253, accuracy: 0.94121\n",
            "Epoch: 27/30, step: 224/364, loss: 0.17242, accuracy: 0.94113\n",
            "Epoch: 27/30, step: 225/364, loss: 0.17262, accuracy: 0.94111\n",
            "Epoch: 27/30, step: 226/364, loss: 0.17289, accuracy: 0.94089\n",
            "Epoch: 27/30, step: 227/364, loss: 0.17291, accuracy: 0.94087\n",
            "Epoch: 27/30, step: 228/364, loss: 0.17323, accuracy: 0.94072\n",
            "Epoch: 27/30, step: 229/364, loss: 0.17434, accuracy: 0.94023\n",
            "Epoch: 27/30, step: 230/364, loss: 0.17443, accuracy: 0.94015\n",
            "Epoch: 27/30, step: 231/364, loss: 0.17526, accuracy: 0.93966\n",
            "Epoch: 27/30, step: 232/364, loss: 0.17592, accuracy: 0.93932\n",
            "Epoch: 27/30, step: 233/364, loss: 0.17640, accuracy: 0.93898\n",
            "Epoch: 27/30, step: 234/364, loss: 0.17664, accuracy: 0.93884\n",
            "Epoch: 27/30, step: 235/364, loss: 0.17698, accuracy: 0.93863\n",
            "Epoch: 27/30, step: 236/364, loss: 0.17702, accuracy: 0.93869\n",
            "Epoch: 27/30, step: 237/364, loss: 0.17743, accuracy: 0.93836\n",
            "Epoch: 27/30, step: 238/364, loss: 0.17709, accuracy: 0.93855\n",
            "Epoch: 27/30, step: 239/364, loss: 0.17713, accuracy: 0.93855\n",
            "Epoch: 27/30, step: 240/364, loss: 0.17690, accuracy: 0.93867\n",
            "Epoch: 27/30, step: 241/364, loss: 0.17684, accuracy: 0.93867\n",
            "Epoch: 27/30, step: 242/364, loss: 0.17691, accuracy: 0.93866\n",
            "Epoch: 27/30, step: 243/364, loss: 0.17665, accuracy: 0.93879\n",
            "Epoch: 27/30, step: 244/364, loss: 0.17658, accuracy: 0.93884\n",
            "Epoch: 27/30, step: 245/364, loss: 0.17665, accuracy: 0.93884\n",
            "Epoch: 27/30, step: 246/364, loss: 0.17660, accuracy: 0.93896\n",
            "Epoch: 27/30, step: 247/364, loss: 0.17655, accuracy: 0.93895\n",
            "Epoch: 27/30, step: 248/364, loss: 0.17678, accuracy: 0.93876\n",
            "Epoch: 27/30, step: 249/364, loss: 0.17679, accuracy: 0.93876\n",
            "Epoch: 27/30, step: 250/364, loss: 0.17661, accuracy: 0.93888\n",
            "Epoch: 27/30, step: 251/364, loss: 0.17648, accuracy: 0.93899\n",
            "Epoch: 27/30, step: 252/364, loss: 0.17633, accuracy: 0.93905\n",
            "Epoch: 27/30, step: 253/364, loss: 0.17604, accuracy: 0.93923\n",
            "Epoch: 27/30, step: 254/364, loss: 0.17585, accuracy: 0.93947\n",
            "Epoch: 27/30, step: 255/364, loss: 0.17572, accuracy: 0.93958\n",
            "Epoch: 27/30, step: 256/364, loss: 0.17547, accuracy: 0.93970\n",
            "Epoch: 27/30, step: 257/364, loss: 0.17540, accuracy: 0.93981\n",
            "Epoch: 27/30, step: 258/364, loss: 0.17538, accuracy: 0.93992\n",
            "Epoch: 27/30, step: 259/364, loss: 0.17526, accuracy: 0.93985\n",
            "Epoch: 27/30, step: 260/364, loss: 0.17503, accuracy: 0.94002\n",
            "Epoch: 27/30, step: 261/364, loss: 0.17535, accuracy: 0.93983\n",
            "Epoch: 27/30, step: 262/364, loss: 0.17582, accuracy: 0.93977\n",
            "Epoch: 27/30, step: 263/364, loss: 0.17569, accuracy: 0.93988\n",
            "Epoch: 27/30, step: 264/364, loss: 0.17543, accuracy: 0.94004\n",
            "Epoch: 27/30, step: 265/364, loss: 0.17621, accuracy: 0.93968\n",
            "Epoch: 27/30, step: 266/364, loss: 0.17638, accuracy: 0.93961\n",
            "Epoch: 27/30, step: 267/364, loss: 0.17614, accuracy: 0.93967\n",
            "Epoch: 27/30, step: 268/364, loss: 0.17612, accuracy: 0.93954\n",
            "Epoch: 27/30, step: 269/364, loss: 0.17653, accuracy: 0.93924\n",
            "Epoch: 27/30, step: 270/364, loss: 0.17673, accuracy: 0.93918\n",
            "Epoch: 27/30, step: 271/364, loss: 0.17672, accuracy: 0.93906\n",
            "Epoch: 27/30, step: 272/364, loss: 0.17669, accuracy: 0.93911\n",
            "Epoch: 27/30, step: 273/364, loss: 0.17685, accuracy: 0.93899\n",
            "Epoch: 27/30, step: 274/364, loss: 0.17654, accuracy: 0.93915\n",
            "Epoch: 27/30, step: 275/364, loss: 0.17663, accuracy: 0.93898\n",
            "Epoch: 27/30, step: 276/364, loss: 0.17638, accuracy: 0.93914\n",
            "Epoch: 27/30, step: 277/364, loss: 0.17740, accuracy: 0.93857\n",
            "Epoch: 27/30, step: 278/364, loss: 0.17790, accuracy: 0.93846\n",
            "Epoch: 27/30, step: 279/364, loss: 0.17762, accuracy: 0.93862\n",
            "Epoch: 27/30, step: 280/364, loss: 0.17764, accuracy: 0.93856\n",
            "Epoch: 27/30, step: 281/364, loss: 0.17770, accuracy: 0.93839\n",
            "Epoch: 27/30, step: 282/364, loss: 0.17769, accuracy: 0.93833\n",
            "Epoch: 27/30, step: 283/364, loss: 0.17779, accuracy: 0.93822\n",
            "Epoch: 27/30, step: 284/364, loss: 0.17773, accuracy: 0.93816\n",
            "Epoch: 27/30, step: 285/364, loss: 0.17766, accuracy: 0.93821\n",
            "Epoch: 27/30, step: 286/364, loss: 0.17753, accuracy: 0.93832\n",
            "Epoch: 27/30, step: 287/364, loss: 0.17752, accuracy: 0.93837\n",
            "Epoch: 27/30, step: 288/364, loss: 0.17789, accuracy: 0.93821\n",
            "Epoch: 27/30, step: 289/364, loss: 0.17814, accuracy: 0.93804\n",
            "Epoch: 27/30, step: 290/364, loss: 0.17821, accuracy: 0.93788\n",
            "Epoch: 27/30, step: 291/364, loss: 0.17794, accuracy: 0.93804\n",
            "Epoch: 27/30, train loss: 0.17794, train accuracy: 0.93804, valid loss: 0.83957, valid accuracy: 0.67584\n",
            "Epoch: 28/30, step: 1/364, loss: 0.23225, accuracy: 0.90625\n",
            "Epoch: 28/30, step: 2/364, loss: 0.18096, accuracy: 0.92969\n",
            "Epoch: 28/30, step: 3/364, loss: 0.18576, accuracy: 0.92708\n",
            "Epoch: 28/30, step: 4/364, loss: 0.17828, accuracy: 0.93750\n",
            "Epoch: 28/30, step: 5/364, loss: 0.20315, accuracy: 0.91875\n",
            "Epoch: 28/30, step: 6/364, loss: 0.18643, accuracy: 0.92969\n",
            "Epoch: 28/30, step: 7/364, loss: 0.19171, accuracy: 0.92634\n",
            "Epoch: 28/30, step: 8/364, loss: 0.17552, accuracy: 0.93555\n",
            "Epoch: 28/30, step: 9/364, loss: 0.17451, accuracy: 0.93750\n",
            "Epoch: 28/30, step: 10/364, loss: 0.17382, accuracy: 0.93906\n",
            "Epoch: 28/30, step: 11/364, loss: 0.16896, accuracy: 0.93892\n",
            "Epoch: 28/30, step: 12/364, loss: 0.17020, accuracy: 0.93750\n",
            "Epoch: 28/30, step: 13/364, loss: 0.16631, accuracy: 0.94111\n",
            "Epoch: 28/30, step: 14/364, loss: 0.17198, accuracy: 0.93638\n",
            "Epoch: 28/30, step: 15/364, loss: 0.17219, accuracy: 0.93542\n",
            "Epoch: 28/30, step: 16/364, loss: 0.17043, accuracy: 0.93652\n",
            "Epoch: 28/30, step: 17/364, loss: 0.16817, accuracy: 0.93842\n",
            "Epoch: 28/30, step: 18/364, loss: 0.16612, accuracy: 0.94097\n",
            "Epoch: 28/30, step: 19/364, loss: 0.16552, accuracy: 0.94243\n",
            "Epoch: 28/30, step: 20/364, loss: 0.16420, accuracy: 0.94219\n",
            "Epoch: 28/30, step: 21/364, loss: 0.16293, accuracy: 0.94345\n",
            "Epoch: 28/30, step: 22/364, loss: 0.16847, accuracy: 0.94034\n",
            "Epoch: 28/30, step: 23/364, loss: 0.16695, accuracy: 0.94158\n",
            "Epoch: 28/30, step: 24/364, loss: 0.16658, accuracy: 0.94141\n",
            "Epoch: 28/30, step: 25/364, loss: 0.16570, accuracy: 0.94187\n",
            "Epoch: 28/30, step: 26/364, loss: 0.16507, accuracy: 0.94171\n",
            "Epoch: 28/30, step: 27/364, loss: 0.16811, accuracy: 0.93924\n",
            "Epoch: 28/30, step: 28/364, loss: 0.17462, accuracy: 0.93359\n",
            "Epoch: 28/30, step: 29/364, loss: 0.17196, accuracy: 0.93588\n",
            "Epoch: 28/30, step: 30/364, loss: 0.17184, accuracy: 0.93698\n",
            "Epoch: 28/30, step: 31/364, loss: 0.17599, accuracy: 0.93448\n",
            "Epoch: 28/30, step: 32/364, loss: 0.17529, accuracy: 0.93457\n",
            "Epoch: 28/30, step: 33/364, loss: 0.17351, accuracy: 0.93513\n",
            "Epoch: 28/30, step: 34/364, loss: 0.17904, accuracy: 0.93199\n",
            "Epoch: 28/30, step: 35/364, loss: 0.17756, accuracy: 0.93304\n",
            "Epoch: 28/30, step: 36/364, loss: 0.17995, accuracy: 0.93273\n",
            "Epoch: 28/30, step: 37/364, loss: 0.17732, accuracy: 0.93454\n",
            "Epoch: 28/30, step: 38/364, loss: 0.17769, accuracy: 0.93380\n",
            "Epoch: 28/30, step: 39/364, loss: 0.17797, accuracy: 0.93349\n",
            "Epoch: 28/30, step: 40/364, loss: 0.18108, accuracy: 0.93203\n",
            "Epoch: 28/30, step: 41/364, loss: 0.18081, accuracy: 0.93293\n",
            "Epoch: 28/30, step: 42/364, loss: 0.17950, accuracy: 0.93415\n",
            "Epoch: 28/30, step: 43/364, loss: 0.18006, accuracy: 0.93423\n",
            "Epoch: 28/30, step: 44/364, loss: 0.17933, accuracy: 0.93430\n",
            "Epoch: 28/30, step: 45/364, loss: 0.17767, accuracy: 0.93542\n",
            "Epoch: 28/30, step: 46/364, loss: 0.17673, accuracy: 0.93580\n",
            "Epoch: 28/30, step: 47/364, loss: 0.17580, accuracy: 0.93617\n",
            "Epoch: 28/30, step: 48/364, loss: 0.17589, accuracy: 0.93555\n",
            "Epoch: 28/30, step: 49/364, loss: 0.17733, accuracy: 0.93431\n",
            "Epoch: 28/30, step: 50/364, loss: 0.17802, accuracy: 0.93313\n",
            "Epoch: 28/30, step: 51/364, loss: 0.17687, accuracy: 0.93382\n",
            "Epoch: 28/30, step: 52/364, loss: 0.17563, accuracy: 0.93419\n",
            "Epoch: 28/30, step: 53/364, loss: 0.17427, accuracy: 0.93485\n",
            "Epoch: 28/30, step: 54/364, loss: 0.17271, accuracy: 0.93519\n",
            "Epoch: 28/30, step: 55/364, loss: 0.17157, accuracy: 0.93551\n",
            "Epoch: 28/30, step: 56/364, loss: 0.17390, accuracy: 0.93387\n",
            "Epoch: 28/30, step: 57/364, loss: 0.17342, accuracy: 0.93448\n",
            "Epoch: 28/30, step: 58/364, loss: 0.17374, accuracy: 0.93427\n",
            "Epoch: 28/30, step: 59/364, loss: 0.17445, accuracy: 0.93406\n",
            "Epoch: 28/30, step: 60/364, loss: 0.17458, accuracy: 0.93464\n",
            "Epoch: 28/30, step: 61/364, loss: 0.17383, accuracy: 0.93494\n",
            "Epoch: 28/30, step: 62/364, loss: 0.17317, accuracy: 0.93498\n",
            "Epoch: 28/30, step: 63/364, loss: 0.17286, accuracy: 0.93477\n",
            "Epoch: 28/30, step: 64/364, loss: 0.17558, accuracy: 0.93335\n",
            "Epoch: 28/30, step: 65/364, loss: 0.17784, accuracy: 0.93173\n",
            "Epoch: 28/30, step: 66/364, loss: 0.17773, accuracy: 0.93134\n",
            "Epoch: 28/30, step: 67/364, loss: 0.17728, accuracy: 0.93190\n",
            "Epoch: 28/30, step: 68/364, loss: 0.17858, accuracy: 0.93084\n",
            "Epoch: 28/30, step: 69/364, loss: 0.17852, accuracy: 0.93116\n",
            "Epoch: 28/30, step: 70/364, loss: 0.17763, accuracy: 0.93147\n",
            "Epoch: 28/30, step: 71/364, loss: 0.17722, accuracy: 0.93200\n",
            "Epoch: 28/30, step: 72/364, loss: 0.17713, accuracy: 0.93164\n",
            "Epoch: 28/30, step: 73/364, loss: 0.17609, accuracy: 0.93215\n",
            "Epoch: 28/30, step: 74/364, loss: 0.17715, accuracy: 0.93201\n",
            "Epoch: 28/30, step: 75/364, loss: 0.17668, accuracy: 0.93250\n",
            "Epoch: 28/30, step: 76/364, loss: 0.17735, accuracy: 0.93257\n",
            "Epoch: 28/30, step: 77/364, loss: 0.17931, accuracy: 0.93162\n",
            "Epoch: 28/30, step: 78/364, loss: 0.17898, accuracy: 0.93229\n",
            "Epoch: 28/30, step: 79/364, loss: 0.17846, accuracy: 0.93275\n",
            "Epoch: 28/30, step: 80/364, loss: 0.17872, accuracy: 0.93281\n",
            "Epoch: 28/30, step: 81/364, loss: 0.17974, accuracy: 0.93191\n",
            "Epoch: 28/30, step: 82/364, loss: 0.17986, accuracy: 0.93197\n",
            "Epoch: 28/30, step: 83/364, loss: 0.18069, accuracy: 0.93166\n",
            "Epoch: 28/30, step: 84/364, loss: 0.17962, accuracy: 0.93248\n",
            "Epoch: 28/30, step: 85/364, loss: 0.18268, accuracy: 0.93107\n",
            "Epoch: 28/30, step: 86/364, loss: 0.18286, accuracy: 0.93096\n",
            "Epoch: 28/30, step: 87/364, loss: 0.18190, accuracy: 0.93139\n",
            "Epoch: 28/30, step: 88/364, loss: 0.18251, accuracy: 0.93075\n",
            "Epoch: 28/30, step: 89/364, loss: 0.18355, accuracy: 0.93013\n",
            "Epoch: 28/30, step: 90/364, loss: 0.18301, accuracy: 0.93056\n",
            "Epoch: 28/30, step: 91/364, loss: 0.18439, accuracy: 0.92977\n",
            "Epoch: 28/30, step: 92/364, loss: 0.18411, accuracy: 0.93020\n",
            "Epoch: 28/30, step: 93/364, loss: 0.18298, accuracy: 0.93095\n",
            "Epoch: 28/30, step: 94/364, loss: 0.18272, accuracy: 0.93102\n",
            "Epoch: 28/30, step: 95/364, loss: 0.18174, accuracy: 0.93158\n",
            "Epoch: 28/30, step: 96/364, loss: 0.18085, accuracy: 0.93229\n",
            "Epoch: 28/30, step: 97/364, loss: 0.18084, accuracy: 0.93218\n",
            "Epoch: 28/30, step: 98/364, loss: 0.18039, accuracy: 0.93224\n",
            "Epoch: 28/30, step: 99/364, loss: 0.18175, accuracy: 0.93182\n",
            "Epoch: 28/30, step: 100/364, loss: 0.18154, accuracy: 0.93172\n",
            "Epoch: 28/30, step: 101/364, loss: 0.18142, accuracy: 0.93209\n",
            "Epoch: 28/30, step: 102/364, loss: 0.18093, accuracy: 0.93275\n",
            "Epoch: 28/30, step: 103/364, loss: 0.18058, accuracy: 0.93295\n",
            "Epoch: 28/30, step: 104/364, loss: 0.17983, accuracy: 0.93344\n",
            "Epoch: 28/30, step: 105/364, loss: 0.18086, accuracy: 0.93289\n",
            "Epoch: 28/30, step: 106/364, loss: 0.18014, accuracy: 0.93308\n",
            "Epoch: 28/30, step: 107/364, loss: 0.17975, accuracy: 0.93312\n",
            "Epoch: 28/30, step: 108/364, loss: 0.18178, accuracy: 0.93200\n",
            "Epoch: 28/30, step: 109/364, loss: 0.18190, accuracy: 0.93177\n",
            "Epoch: 28/30, step: 110/364, loss: 0.18178, accuracy: 0.93196\n",
            "Epoch: 28/30, step: 111/364, loss: 0.18232, accuracy: 0.93201\n",
            "Epoch: 28/30, step: 112/364, loss: 0.18221, accuracy: 0.93220\n",
            "Epoch: 28/30, step: 113/364, loss: 0.18213, accuracy: 0.93225\n",
            "Epoch: 28/30, step: 114/364, loss: 0.18136, accuracy: 0.93257\n",
            "Epoch: 28/30, step: 115/364, loss: 0.18072, accuracy: 0.93302\n",
            "Epoch: 28/30, step: 116/364, loss: 0.18062, accuracy: 0.93319\n",
            "Epoch: 28/30, step: 117/364, loss: 0.17970, accuracy: 0.93376\n",
            "Epoch: 28/30, step: 118/364, loss: 0.17924, accuracy: 0.93406\n",
            "Epoch: 28/30, step: 119/364, loss: 0.17836, accuracy: 0.93461\n",
            "Epoch: 28/30, step: 120/364, loss: 0.17814, accuracy: 0.93490\n",
            "Epoch: 28/30, step: 121/364, loss: 0.17770, accuracy: 0.93518\n",
            "Epoch: 28/30, step: 122/364, loss: 0.17761, accuracy: 0.93519\n",
            "Epoch: 28/30, step: 123/364, loss: 0.17779, accuracy: 0.93521\n",
            "Epoch: 28/30, step: 124/364, loss: 0.17777, accuracy: 0.93511\n",
            "Epoch: 28/30, step: 125/364, loss: 0.17737, accuracy: 0.93525\n",
            "Epoch: 28/30, step: 126/364, loss: 0.17731, accuracy: 0.93539\n",
            "Epoch: 28/30, step: 127/364, loss: 0.17653, accuracy: 0.93565\n",
            "Epoch: 28/30, step: 128/364, loss: 0.17620, accuracy: 0.93604\n",
            "Epoch: 28/30, step: 129/364, loss: 0.17578, accuracy: 0.93641\n",
            "Epoch: 28/30, step: 130/364, loss: 0.17561, accuracy: 0.93642\n",
            "Epoch: 28/30, step: 131/364, loss: 0.17539, accuracy: 0.93667\n",
            "Epoch: 28/30, step: 132/364, loss: 0.17499, accuracy: 0.93691\n",
            "Epoch: 28/30, step: 133/364, loss: 0.17441, accuracy: 0.93715\n",
            "Epoch: 28/30, step: 134/364, loss: 0.17539, accuracy: 0.93680\n",
            "Epoch: 28/30, step: 135/364, loss: 0.17449, accuracy: 0.93727\n",
            "Epoch: 28/30, step: 136/364, loss: 0.17419, accuracy: 0.93739\n",
            "Epoch: 28/30, step: 137/364, loss: 0.17427, accuracy: 0.93727\n",
            "Epoch: 28/30, step: 138/364, loss: 0.17417, accuracy: 0.93739\n",
            "Epoch: 28/30, step: 139/364, loss: 0.17377, accuracy: 0.93761\n",
            "Epoch: 28/30, step: 140/364, loss: 0.17396, accuracy: 0.93739\n",
            "Epoch: 28/30, step: 141/364, loss: 0.17395, accuracy: 0.93728\n",
            "Epoch: 28/30, step: 142/364, loss: 0.17352, accuracy: 0.93750\n",
            "Epoch: 28/30, step: 143/364, loss: 0.17347, accuracy: 0.93761\n",
            "Epoch: 28/30, step: 144/364, loss: 0.17368, accuracy: 0.93728\n",
            "Epoch: 28/30, step: 145/364, loss: 0.17332, accuracy: 0.93750\n",
            "Epoch: 28/30, step: 146/364, loss: 0.17339, accuracy: 0.93729\n",
            "Epoch: 28/30, step: 147/364, loss: 0.17396, accuracy: 0.93697\n",
            "Epoch: 28/30, step: 148/364, loss: 0.17381, accuracy: 0.93718\n",
            "Epoch: 28/30, step: 149/364, loss: 0.17441, accuracy: 0.93666\n",
            "Epoch: 28/30, step: 150/364, loss: 0.17438, accuracy: 0.93677\n",
            "Epoch: 28/30, step: 151/364, loss: 0.17399, accuracy: 0.93698\n",
            "Epoch: 28/30, step: 152/364, loss: 0.17352, accuracy: 0.93719\n",
            "Epoch: 28/30, step: 153/364, loss: 0.17337, accuracy: 0.93740\n",
            "Epoch: 28/30, step: 154/364, loss: 0.17328, accuracy: 0.93740\n",
            "Epoch: 28/30, step: 155/364, loss: 0.17281, accuracy: 0.93770\n",
            "Epoch: 28/30, step: 156/364, loss: 0.17254, accuracy: 0.93790\n",
            "Epoch: 28/30, step: 157/364, loss: 0.17231, accuracy: 0.93790\n",
            "Epoch: 28/30, step: 158/364, loss: 0.17205, accuracy: 0.93809\n",
            "Epoch: 28/30, step: 159/364, loss: 0.17223, accuracy: 0.93789\n",
            "Epoch: 28/30, step: 160/364, loss: 0.17153, accuracy: 0.93818\n",
            "Epoch: 28/30, step: 161/364, loss: 0.17125, accuracy: 0.93837\n",
            "Epoch: 28/30, step: 162/364, loss: 0.17115, accuracy: 0.93846\n",
            "Epoch: 28/30, step: 163/364, loss: 0.17258, accuracy: 0.93779\n",
            "Epoch: 28/30, step: 164/364, loss: 0.17250, accuracy: 0.93779\n",
            "Epoch: 28/30, step: 165/364, loss: 0.17234, accuracy: 0.93788\n",
            "Epoch: 28/30, step: 166/364, loss: 0.17233, accuracy: 0.93788\n",
            "Epoch: 28/30, step: 167/364, loss: 0.17192, accuracy: 0.93806\n",
            "Epoch: 28/30, step: 168/364, loss: 0.17211, accuracy: 0.93778\n",
            "Epoch: 28/30, step: 169/364, loss: 0.17188, accuracy: 0.93815\n",
            "Epoch: 28/30, step: 170/364, loss: 0.17211, accuracy: 0.93787\n",
            "Epoch: 28/30, step: 171/364, loss: 0.17175, accuracy: 0.93805\n",
            "Epoch: 28/30, step: 172/364, loss: 0.17173, accuracy: 0.93805\n",
            "Epoch: 28/30, step: 173/364, loss: 0.17163, accuracy: 0.93804\n",
            "Epoch: 28/30, step: 174/364, loss: 0.17156, accuracy: 0.93822\n",
            "Epoch: 28/30, step: 175/364, loss: 0.17174, accuracy: 0.93804\n",
            "Epoch: 28/30, step: 176/364, loss: 0.17181, accuracy: 0.93786\n",
            "Epoch: 28/30, step: 177/364, loss: 0.17195, accuracy: 0.93768\n",
            "Epoch: 28/30, step: 178/364, loss: 0.17151, accuracy: 0.93803\n",
            "Epoch: 28/30, step: 179/364, loss: 0.17176, accuracy: 0.93767\n",
            "Epoch: 28/30, step: 180/364, loss: 0.17253, accuracy: 0.93698\n",
            "Epoch: 28/30, step: 181/364, loss: 0.17268, accuracy: 0.93690\n",
            "Epoch: 28/30, step: 182/364, loss: 0.17252, accuracy: 0.93698\n",
            "Epoch: 28/30, step: 183/364, loss: 0.17222, accuracy: 0.93724\n",
            "Epoch: 28/30, step: 184/364, loss: 0.17216, accuracy: 0.93716\n",
            "Epoch: 28/30, step: 185/364, loss: 0.17177, accuracy: 0.93733\n",
            "Epoch: 28/30, step: 186/364, loss: 0.17178, accuracy: 0.93716\n",
            "Epoch: 28/30, step: 187/364, loss: 0.17193, accuracy: 0.93717\n",
            "Epoch: 28/30, step: 188/364, loss: 0.17189, accuracy: 0.93708\n",
            "Epoch: 28/30, step: 189/364, loss: 0.17168, accuracy: 0.93733\n",
            "Epoch: 28/30, step: 190/364, loss: 0.17117, accuracy: 0.93750\n",
            "Epoch: 28/30, step: 191/364, loss: 0.17121, accuracy: 0.93734\n",
            "Epoch: 28/30, step: 192/364, loss: 0.17124, accuracy: 0.93726\n",
            "Epoch: 28/30, step: 193/364, loss: 0.17082, accuracy: 0.93758\n",
            "Epoch: 28/30, step: 194/364, loss: 0.17051, accuracy: 0.93774\n",
            "Epoch: 28/30, step: 195/364, loss: 0.17037, accuracy: 0.93790\n",
            "Epoch: 28/30, step: 196/364, loss: 0.17008, accuracy: 0.93806\n",
            "Epoch: 28/30, step: 197/364, loss: 0.16974, accuracy: 0.93813\n",
            "Epoch: 28/30, step: 198/364, loss: 0.16965, accuracy: 0.93805\n",
            "Epoch: 28/30, step: 199/364, loss: 0.16950, accuracy: 0.93805\n",
            "Epoch: 28/30, step: 200/364, loss: 0.16960, accuracy: 0.93797\n",
            "Epoch: 28/30, step: 201/364, loss: 0.16920, accuracy: 0.93820\n",
            "Epoch: 28/30, step: 202/364, loss: 0.16926, accuracy: 0.93827\n",
            "Epoch: 28/30, step: 203/364, loss: 0.16980, accuracy: 0.93804\n",
            "Epoch: 28/30, step: 204/364, loss: 0.16969, accuracy: 0.93811\n",
            "Epoch: 28/30, step: 205/364, loss: 0.16986, accuracy: 0.93796\n",
            "Epoch: 28/30, step: 206/364, loss: 0.16948, accuracy: 0.93818\n",
            "Epoch: 28/30, step: 207/364, loss: 0.16915, accuracy: 0.93841\n",
            "Epoch: 28/30, step: 208/364, loss: 0.16899, accuracy: 0.93848\n",
            "Epoch: 28/30, step: 209/364, loss: 0.16903, accuracy: 0.93832\n",
            "Epoch: 28/30, step: 210/364, loss: 0.16912, accuracy: 0.93832\n",
            "Epoch: 28/30, step: 211/364, loss: 0.16902, accuracy: 0.93839\n",
            "Epoch: 28/30, step: 212/364, loss: 0.16929, accuracy: 0.93831\n",
            "Epoch: 28/30, step: 213/364, loss: 0.16892, accuracy: 0.93845\n",
            "Epoch: 28/30, step: 214/364, loss: 0.16902, accuracy: 0.93845\n",
            "Epoch: 28/30, step: 215/364, loss: 0.16951, accuracy: 0.93823\n",
            "Epoch: 28/30, step: 216/364, loss: 0.16963, accuracy: 0.93822\n",
            "Epoch: 28/30, step: 217/364, loss: 0.16961, accuracy: 0.93815\n",
            "Epoch: 28/30, step: 218/364, loss: 0.16937, accuracy: 0.93829\n",
            "Epoch: 28/30, step: 219/364, loss: 0.16903, accuracy: 0.93843\n",
            "Epoch: 28/30, step: 220/364, loss: 0.16907, accuracy: 0.93835\n",
            "Epoch: 28/30, step: 221/364, loss: 0.16869, accuracy: 0.93863\n",
            "Epoch: 28/30, step: 222/364, loss: 0.16913, accuracy: 0.93820\n",
            "Epoch: 28/30, step: 223/364, loss: 0.16915, accuracy: 0.93813\n",
            "Epoch: 28/30, step: 224/364, loss: 0.16919, accuracy: 0.93806\n",
            "Epoch: 28/30, step: 225/364, loss: 0.16882, accuracy: 0.93819\n",
            "Epoch: 28/30, step: 226/364, loss: 0.16898, accuracy: 0.93805\n",
            "Epoch: 28/30, step: 227/364, loss: 0.16887, accuracy: 0.93812\n",
            "Epoch: 28/30, step: 228/364, loss: 0.16940, accuracy: 0.93777\n",
            "Epoch: 28/30, step: 229/364, loss: 0.16903, accuracy: 0.93805\n",
            "Epoch: 28/30, step: 230/364, loss: 0.16908, accuracy: 0.93804\n",
            "Epoch: 28/30, step: 231/364, loss: 0.16905, accuracy: 0.93818\n",
            "Epoch: 28/30, step: 232/364, loss: 0.16886, accuracy: 0.93824\n",
            "Epoch: 28/30, step: 233/364, loss: 0.16919, accuracy: 0.93784\n",
            "Epoch: 28/30, step: 234/364, loss: 0.16900, accuracy: 0.93797\n",
            "Epoch: 28/30, step: 235/364, loss: 0.16903, accuracy: 0.93803\n",
            "Epoch: 28/30, step: 236/364, loss: 0.16856, accuracy: 0.93829\n",
            "Epoch: 28/30, step: 237/364, loss: 0.16847, accuracy: 0.93842\n",
            "Epoch: 28/30, step: 238/364, loss: 0.16876, accuracy: 0.93822\n",
            "Epoch: 28/30, step: 239/364, loss: 0.16853, accuracy: 0.93835\n",
            "Epoch: 28/30, step: 240/364, loss: 0.16858, accuracy: 0.93848\n",
            "Epoch: 28/30, step: 241/364, loss: 0.16858, accuracy: 0.93854\n",
            "Epoch: 28/30, step: 242/364, loss: 0.16842, accuracy: 0.93873\n",
            "Epoch: 28/30, step: 243/364, loss: 0.16821, accuracy: 0.93885\n",
            "Epoch: 28/30, step: 244/364, loss: 0.16794, accuracy: 0.93904\n",
            "Epoch: 28/30, step: 245/364, loss: 0.16772, accuracy: 0.93909\n",
            "Epoch: 28/30, step: 246/364, loss: 0.16760, accuracy: 0.93909\n",
            "Epoch: 28/30, step: 247/364, loss: 0.16752, accuracy: 0.93914\n",
            "Epoch: 28/30, step: 248/364, loss: 0.16726, accuracy: 0.93926\n",
            "Epoch: 28/30, step: 249/364, loss: 0.16732, accuracy: 0.93932\n",
            "Epoch: 28/30, step: 250/364, loss: 0.16718, accuracy: 0.93931\n",
            "Epoch: 28/30, step: 251/364, loss: 0.16753, accuracy: 0.93918\n",
            "Epoch: 28/30, step: 252/364, loss: 0.16777, accuracy: 0.93893\n",
            "Epoch: 28/30, step: 253/364, loss: 0.16740, accuracy: 0.93911\n",
            "Epoch: 28/30, step: 254/364, loss: 0.16769, accuracy: 0.93891\n",
            "Epoch: 28/30, step: 255/364, loss: 0.16735, accuracy: 0.93909\n",
            "Epoch: 28/30, step: 256/364, loss: 0.16721, accuracy: 0.93915\n",
            "Epoch: 28/30, step: 257/364, loss: 0.16684, accuracy: 0.93932\n",
            "Epoch: 28/30, step: 258/364, loss: 0.16675, accuracy: 0.93938\n",
            "Epoch: 28/30, step: 259/364, loss: 0.16717, accuracy: 0.93913\n",
            "Epoch: 28/30, step: 260/364, loss: 0.16736, accuracy: 0.93906\n",
            "Epoch: 28/30, step: 261/364, loss: 0.16718, accuracy: 0.93906\n",
            "Epoch: 28/30, step: 262/364, loss: 0.16723, accuracy: 0.93905\n",
            "Epoch: 28/30, step: 263/364, loss: 0.16701, accuracy: 0.93916\n",
            "Epoch: 28/30, step: 264/364, loss: 0.16688, accuracy: 0.93922\n",
            "Epoch: 28/30, step: 265/364, loss: 0.16682, accuracy: 0.93927\n",
            "Epoch: 28/30, step: 266/364, loss: 0.16697, accuracy: 0.93926\n",
            "Epoch: 28/30, step: 267/364, loss: 0.16682, accuracy: 0.93943\n",
            "Epoch: 28/30, step: 268/364, loss: 0.16653, accuracy: 0.93960\n",
            "Epoch: 28/30, step: 269/364, loss: 0.16659, accuracy: 0.93965\n",
            "Epoch: 28/30, step: 270/364, loss: 0.16640, accuracy: 0.93981\n",
            "Epoch: 28/30, step: 271/364, loss: 0.16647, accuracy: 0.93969\n",
            "Epoch: 28/30, step: 272/364, loss: 0.16630, accuracy: 0.93986\n",
            "Epoch: 28/30, step: 273/364, loss: 0.16640, accuracy: 0.93985\n",
            "Epoch: 28/30, step: 274/364, loss: 0.16614, accuracy: 0.94001\n",
            "Epoch: 28/30, step: 275/364, loss: 0.16616, accuracy: 0.93994\n",
            "Epoch: 28/30, step: 276/364, loss: 0.16596, accuracy: 0.94010\n",
            "Epoch: 28/30, step: 277/364, loss: 0.16571, accuracy: 0.94015\n",
            "Epoch: 28/30, step: 278/364, loss: 0.16559, accuracy: 0.94020\n",
            "Epoch: 28/30, step: 279/364, loss: 0.16545, accuracy: 0.94024\n",
            "Epoch: 28/30, step: 280/364, loss: 0.16553, accuracy: 0.94018\n",
            "Epoch: 28/30, step: 281/364, loss: 0.16546, accuracy: 0.94028\n",
            "Epoch: 28/30, step: 282/364, loss: 0.16534, accuracy: 0.94033\n",
            "Epoch: 28/30, step: 283/364, loss: 0.16508, accuracy: 0.94043\n",
            "Epoch: 28/30, step: 284/364, loss: 0.16571, accuracy: 0.94014\n",
            "Epoch: 28/30, step: 285/364, loss: 0.16582, accuracy: 0.93991\n",
            "Epoch: 28/30, step: 286/364, loss: 0.16600, accuracy: 0.93985\n",
            "Epoch: 28/30, step: 287/364, loss: 0.16573, accuracy: 0.94000\n",
            "Epoch: 28/30, step: 288/364, loss: 0.16543, accuracy: 0.94021\n",
            "Epoch: 28/30, step: 289/364, loss: 0.16533, accuracy: 0.94031\n",
            "Epoch: 28/30, step: 290/364, loss: 0.16511, accuracy: 0.94046\n",
            "Epoch: 28/30, step: 291/364, loss: 0.16540, accuracy: 0.94030\n",
            "Epoch: 28/30, train loss: 0.16540, train accuracy: 0.94030, valid loss: 0.93355, valid accuracy: 0.65800\n",
            "Epoch: 29/30, step: 1/364, loss: 0.14948, accuracy: 0.93750\n",
            "Epoch: 29/30, step: 2/364, loss: 0.19594, accuracy: 0.91406\n",
            "Epoch: 29/30, step: 3/364, loss: 0.22147, accuracy: 0.90625\n",
            "Epoch: 29/30, step: 4/364, loss: 0.21400, accuracy: 0.91406\n",
            "Epoch: 29/30, step: 5/364, loss: 0.24377, accuracy: 0.89688\n",
            "Epoch: 29/30, step: 6/364, loss: 0.23184, accuracy: 0.89844\n",
            "Epoch: 29/30, step: 7/364, loss: 0.21692, accuracy: 0.90848\n",
            "Epoch: 29/30, step: 8/364, loss: 0.22990, accuracy: 0.90234\n",
            "Epoch: 29/30, step: 9/364, loss: 0.22676, accuracy: 0.90799\n",
            "Epoch: 29/30, step: 10/364, loss: 0.21204, accuracy: 0.91406\n",
            "Epoch: 29/30, step: 11/364, loss: 0.20845, accuracy: 0.91761\n",
            "Epoch: 29/30, step: 12/364, loss: 0.20237, accuracy: 0.91927\n",
            "Epoch: 29/30, step: 13/364, loss: 0.19803, accuracy: 0.92308\n",
            "Epoch: 29/30, step: 14/364, loss: 0.19531, accuracy: 0.92188\n",
            "Epoch: 29/30, step: 15/364, loss: 0.19266, accuracy: 0.92500\n",
            "Epoch: 29/30, step: 16/364, loss: 0.18752, accuracy: 0.92773\n",
            "Epoch: 29/30, step: 17/364, loss: 0.18308, accuracy: 0.92923\n",
            "Epoch: 29/30, step: 18/364, loss: 0.18175, accuracy: 0.92969\n",
            "Epoch: 29/30, step: 19/364, loss: 0.18834, accuracy: 0.92516\n",
            "Epoch: 29/30, step: 20/364, loss: 0.18580, accuracy: 0.92734\n",
            "Epoch: 29/30, step: 21/364, loss: 0.18153, accuracy: 0.93006\n",
            "Epoch: 29/30, step: 22/364, loss: 0.17857, accuracy: 0.93253\n",
            "Epoch: 29/30, step: 23/364, loss: 0.17917, accuracy: 0.93207\n",
            "Epoch: 29/30, step: 24/364, loss: 0.17741, accuracy: 0.93229\n",
            "Epoch: 29/30, step: 25/364, loss: 0.17286, accuracy: 0.93500\n",
            "Epoch: 29/30, step: 26/364, loss: 0.17186, accuracy: 0.93510\n",
            "Epoch: 29/30, step: 27/364, loss: 0.17544, accuracy: 0.93345\n",
            "Epoch: 29/30, step: 28/364, loss: 0.17318, accuracy: 0.93527\n",
            "Epoch: 29/30, step: 29/364, loss: 0.17574, accuracy: 0.93373\n",
            "Epoch: 29/30, step: 30/364, loss: 0.17464, accuracy: 0.93490\n",
            "Epoch: 29/30, step: 31/364, loss: 0.17587, accuracy: 0.93498\n",
            "Epoch: 29/30, step: 32/364, loss: 0.17509, accuracy: 0.93506\n",
            "Epoch: 29/30, step: 33/364, loss: 0.17403, accuracy: 0.93513\n",
            "Epoch: 29/30, step: 34/364, loss: 0.17580, accuracy: 0.93382\n",
            "Epoch: 29/30, step: 35/364, loss: 0.17281, accuracy: 0.93527\n",
            "Epoch: 29/30, step: 36/364, loss: 0.17191, accuracy: 0.93663\n",
            "Epoch: 29/30, step: 37/364, loss: 0.17353, accuracy: 0.93497\n",
            "Epoch: 29/30, step: 38/364, loss: 0.17372, accuracy: 0.93462\n",
            "Epoch: 29/30, step: 39/364, loss: 0.17471, accuracy: 0.93429\n",
            "Epoch: 29/30, step: 40/364, loss: 0.17638, accuracy: 0.93359\n",
            "Epoch: 29/30, step: 41/364, loss: 0.17482, accuracy: 0.93445\n",
            "Epoch: 29/30, step: 42/364, loss: 0.17270, accuracy: 0.93527\n",
            "Epoch: 29/30, step: 43/364, loss: 0.17143, accuracy: 0.93568\n",
            "Epoch: 29/30, step: 44/364, loss: 0.17221, accuracy: 0.93572\n",
            "Epoch: 29/30, step: 45/364, loss: 0.17193, accuracy: 0.93611\n",
            "Epoch: 29/30, step: 46/364, loss: 0.17227, accuracy: 0.93614\n",
            "Epoch: 29/30, step: 47/364, loss: 0.17051, accuracy: 0.93750\n",
            "Epoch: 29/30, step: 48/364, loss: 0.16963, accuracy: 0.93815\n",
            "Epoch: 29/30, step: 49/364, loss: 0.16909, accuracy: 0.93846\n",
            "Epoch: 29/30, step: 50/364, loss: 0.16800, accuracy: 0.93875\n",
            "Epoch: 29/30, step: 51/364, loss: 0.16682, accuracy: 0.93964\n",
            "Epoch: 29/30, step: 52/364, loss: 0.16734, accuracy: 0.93930\n",
            "Epoch: 29/30, step: 53/364, loss: 0.16838, accuracy: 0.93927\n",
            "Epoch: 29/30, step: 54/364, loss: 0.16914, accuracy: 0.93866\n",
            "Epoch: 29/30, step: 55/364, loss: 0.16760, accuracy: 0.93977\n",
            "Epoch: 29/30, step: 56/364, loss: 0.16602, accuracy: 0.94085\n",
            "Epoch: 29/30, step: 57/364, loss: 0.16551, accuracy: 0.94134\n",
            "Epoch: 29/30, step: 58/364, loss: 0.16522, accuracy: 0.94073\n",
            "Epoch: 29/30, step: 59/364, loss: 0.16416, accuracy: 0.94121\n",
            "Epoch: 29/30, step: 60/364, loss: 0.16352, accuracy: 0.94193\n",
            "Epoch: 29/30, step: 61/364, loss: 0.16294, accuracy: 0.94237\n",
            "Epoch: 29/30, step: 62/364, loss: 0.16355, accuracy: 0.94178\n",
            "Epoch: 29/30, step: 63/364, loss: 0.16356, accuracy: 0.94221\n",
            "Epoch: 29/30, step: 64/364, loss: 0.16220, accuracy: 0.94287\n",
            "Epoch: 29/30, step: 65/364, loss: 0.16135, accuracy: 0.94351\n",
            "Epoch: 29/30, step: 66/364, loss: 0.16155, accuracy: 0.94342\n",
            "Epoch: 29/30, step: 67/364, loss: 0.16221, accuracy: 0.94356\n",
            "Epoch: 29/30, step: 68/364, loss: 0.16279, accuracy: 0.94278\n",
            "Epoch: 29/30, step: 69/364, loss: 0.16208, accuracy: 0.94293\n",
            "Epoch: 29/30, step: 70/364, loss: 0.16150, accuracy: 0.94308\n",
            "Epoch: 29/30, step: 71/364, loss: 0.16133, accuracy: 0.94322\n",
            "Epoch: 29/30, step: 72/364, loss: 0.16109, accuracy: 0.94336\n",
            "Epoch: 29/30, step: 73/364, loss: 0.16321, accuracy: 0.94285\n",
            "Epoch: 29/30, step: 74/364, loss: 0.16261, accuracy: 0.94320\n",
            "Epoch: 29/30, step: 75/364, loss: 0.16128, accuracy: 0.94396\n",
            "Epoch: 29/30, step: 76/364, loss: 0.16234, accuracy: 0.94346\n",
            "Epoch: 29/30, step: 77/364, loss: 0.16491, accuracy: 0.94217\n",
            "Epoch: 29/30, step: 78/364, loss: 0.16504, accuracy: 0.94231\n",
            "Epoch: 29/30, step: 79/364, loss: 0.16412, accuracy: 0.94264\n",
            "Epoch: 29/30, step: 80/364, loss: 0.16439, accuracy: 0.94219\n",
            "Epoch: 29/30, step: 81/364, loss: 0.16423, accuracy: 0.94232\n",
            "Epoch: 29/30, step: 82/364, loss: 0.16377, accuracy: 0.94264\n",
            "Epoch: 29/30, step: 83/364, loss: 0.16491, accuracy: 0.94164\n",
            "Epoch: 29/30, step: 84/364, loss: 0.16442, accuracy: 0.94159\n",
            "Epoch: 29/30, step: 85/364, loss: 0.16361, accuracy: 0.94191\n",
            "Epoch: 29/30, step: 86/364, loss: 0.16313, accuracy: 0.94241\n",
            "Epoch: 29/30, step: 87/364, loss: 0.16472, accuracy: 0.94181\n",
            "Epoch: 29/30, step: 88/364, loss: 0.16507, accuracy: 0.94158\n",
            "Epoch: 29/30, step: 89/364, loss: 0.16518, accuracy: 0.94154\n",
            "Epoch: 29/30, step: 90/364, loss: 0.16521, accuracy: 0.94184\n",
            "Epoch: 29/30, step: 91/364, loss: 0.16607, accuracy: 0.94128\n",
            "Epoch: 29/30, step: 92/364, loss: 0.16561, accuracy: 0.94158\n",
            "Epoch: 29/30, step: 93/364, loss: 0.16538, accuracy: 0.94170\n",
            "Epoch: 29/30, step: 94/364, loss: 0.16526, accuracy: 0.94166\n",
            "Epoch: 29/30, step: 95/364, loss: 0.16459, accuracy: 0.94211\n",
            "Epoch: 29/30, step: 96/364, loss: 0.16467, accuracy: 0.94189\n",
            "Epoch: 29/30, step: 97/364, loss: 0.16462, accuracy: 0.94201\n",
            "Epoch: 29/30, step: 98/364, loss: 0.16433, accuracy: 0.94180\n",
            "Epoch: 29/30, step: 99/364, loss: 0.16438, accuracy: 0.94160\n",
            "Epoch: 29/30, step: 100/364, loss: 0.16393, accuracy: 0.94187\n",
            "Epoch: 29/30, step: 101/364, loss: 0.16496, accuracy: 0.94183\n",
            "Epoch: 29/30, step: 102/364, loss: 0.16413, accuracy: 0.94240\n",
            "Epoch: 29/30, step: 103/364, loss: 0.16369, accuracy: 0.94281\n",
            "Epoch: 29/30, step: 104/364, loss: 0.16287, accuracy: 0.94306\n",
            "Epoch: 29/30, step: 105/364, loss: 0.16329, accuracy: 0.94241\n",
            "Epoch: 29/30, step: 106/364, loss: 0.16403, accuracy: 0.94207\n",
            "Epoch: 29/30, step: 107/364, loss: 0.16356, accuracy: 0.94232\n",
            "Epoch: 29/30, step: 108/364, loss: 0.16278, accuracy: 0.94285\n",
            "Epoch: 29/30, step: 109/364, loss: 0.16260, accuracy: 0.94280\n",
            "Epoch: 29/30, step: 110/364, loss: 0.16201, accuracy: 0.94304\n",
            "Epoch: 29/30, step: 111/364, loss: 0.16214, accuracy: 0.94299\n",
            "Epoch: 29/30, step: 112/364, loss: 0.16197, accuracy: 0.94322\n",
            "Epoch: 29/30, step: 113/364, loss: 0.16154, accuracy: 0.94331\n",
            "Epoch: 29/30, step: 114/364, loss: 0.16144, accuracy: 0.94353\n",
            "Epoch: 29/30, step: 115/364, loss: 0.16061, accuracy: 0.94402\n",
            "Epoch: 29/30, step: 116/364, loss: 0.16045, accuracy: 0.94397\n",
            "Epoch: 29/30, step: 117/364, loss: 0.16001, accuracy: 0.94404\n",
            "Epoch: 29/30, step: 118/364, loss: 0.16002, accuracy: 0.94412\n",
            "Epoch: 29/30, step: 119/364, loss: 0.16006, accuracy: 0.94420\n",
            "Epoch: 29/30, step: 120/364, loss: 0.15947, accuracy: 0.94440\n",
            "Epoch: 29/30, step: 121/364, loss: 0.15911, accuracy: 0.94447\n",
            "Epoch: 29/30, step: 122/364, loss: 0.15882, accuracy: 0.94454\n",
            "Epoch: 29/30, step: 123/364, loss: 0.15847, accuracy: 0.94461\n",
            "Epoch: 29/30, step: 124/364, loss: 0.15839, accuracy: 0.94468\n",
            "Epoch: 29/30, step: 125/364, loss: 0.15907, accuracy: 0.94437\n",
            "Epoch: 29/30, step: 126/364, loss: 0.15852, accuracy: 0.94469\n",
            "Epoch: 29/30, step: 127/364, loss: 0.15783, accuracy: 0.94488\n",
            "Epoch: 29/30, step: 128/364, loss: 0.15747, accuracy: 0.94519\n",
            "Epoch: 29/30, step: 129/364, loss: 0.15792, accuracy: 0.94501\n",
            "Epoch: 29/30, step: 130/364, loss: 0.15735, accuracy: 0.94519\n",
            "Epoch: 29/30, step: 131/364, loss: 0.15701, accuracy: 0.94537\n",
            "Epoch: 29/30, step: 132/364, loss: 0.15745, accuracy: 0.94519\n",
            "Epoch: 29/30, step: 133/364, loss: 0.15710, accuracy: 0.94549\n",
            "Epoch: 29/30, step: 134/364, loss: 0.15705, accuracy: 0.94543\n",
            "Epoch: 29/30, step: 135/364, loss: 0.15696, accuracy: 0.94549\n",
            "Epoch: 29/30, step: 136/364, loss: 0.15741, accuracy: 0.94531\n",
            "Epoch: 29/30, step: 137/364, loss: 0.15727, accuracy: 0.94537\n",
            "Epoch: 29/30, step: 138/364, loss: 0.15703, accuracy: 0.94554\n",
            "Epoch: 29/30, step: 139/364, loss: 0.15756, accuracy: 0.94526\n",
            "Epoch: 29/30, step: 140/364, loss: 0.15767, accuracy: 0.94509\n",
            "Epoch: 29/30, step: 141/364, loss: 0.15821, accuracy: 0.94492\n",
            "Epoch: 29/30, step: 142/364, loss: 0.15793, accuracy: 0.94509\n",
            "Epoch: 29/30, step: 143/364, loss: 0.15759, accuracy: 0.94537\n",
            "Epoch: 29/30, step: 144/364, loss: 0.15744, accuracy: 0.94553\n",
            "Epoch: 29/30, step: 145/364, loss: 0.15761, accuracy: 0.94526\n",
            "Epoch: 29/30, step: 146/364, loss: 0.15872, accuracy: 0.94467\n",
            "Epoch: 29/30, step: 147/364, loss: 0.15817, accuracy: 0.94483\n",
            "Epoch: 29/30, step: 148/364, loss: 0.15827, accuracy: 0.94478\n",
            "Epoch: 29/30, step: 149/364, loss: 0.15772, accuracy: 0.94484\n",
            "Epoch: 29/30, step: 150/364, loss: 0.15720, accuracy: 0.94521\n",
            "Epoch: 29/30, step: 151/364, loss: 0.15719, accuracy: 0.94505\n",
            "Epoch: 29/30, step: 152/364, loss: 0.15707, accuracy: 0.94500\n",
            "Epoch: 29/30, step: 153/364, loss: 0.15740, accuracy: 0.94485\n",
            "Epoch: 29/30, step: 154/364, loss: 0.15690, accuracy: 0.94521\n",
            "Epoch: 29/30, step: 155/364, loss: 0.15694, accuracy: 0.94546\n",
            "Epoch: 29/30, step: 156/364, loss: 0.15675, accuracy: 0.94561\n",
            "Epoch: 29/30, step: 157/364, loss: 0.15705, accuracy: 0.94536\n",
            "Epoch: 29/30, step: 158/364, loss: 0.15802, accuracy: 0.94482\n",
            "Epoch: 29/30, step: 159/364, loss: 0.15819, accuracy: 0.94467\n",
            "Epoch: 29/30, step: 160/364, loss: 0.15811, accuracy: 0.94463\n",
            "Epoch: 29/30, step: 161/364, loss: 0.15785, accuracy: 0.94478\n",
            "Epoch: 29/30, step: 162/364, loss: 0.15743, accuracy: 0.94502\n",
            "Epoch: 29/30, step: 163/364, loss: 0.15724, accuracy: 0.94507\n",
            "Epoch: 29/30, step: 164/364, loss: 0.15722, accuracy: 0.94512\n",
            "Epoch: 29/30, step: 165/364, loss: 0.15709, accuracy: 0.94527\n",
            "Epoch: 29/30, step: 166/364, loss: 0.15697, accuracy: 0.94550\n",
            "Epoch: 29/30, step: 167/364, loss: 0.15714, accuracy: 0.94545\n",
            "Epoch: 29/30, step: 168/364, loss: 0.15691, accuracy: 0.94568\n",
            "Epoch: 29/30, step: 169/364, loss: 0.15707, accuracy: 0.94573\n",
            "Epoch: 29/30, step: 170/364, loss: 0.15672, accuracy: 0.94586\n",
            "Epoch: 29/30, step: 171/364, loss: 0.15669, accuracy: 0.94591\n",
            "Epoch: 29/30, step: 172/364, loss: 0.15706, accuracy: 0.94568\n",
            "Epoch: 29/30, step: 173/364, loss: 0.15684, accuracy: 0.94581\n",
            "Epoch: 29/30, step: 174/364, loss: 0.15692, accuracy: 0.94567\n",
            "Epoch: 29/30, step: 175/364, loss: 0.15700, accuracy: 0.94563\n",
            "Epoch: 29/30, step: 176/364, loss: 0.15749, accuracy: 0.94522\n",
            "Epoch: 29/30, step: 177/364, loss: 0.15710, accuracy: 0.94544\n",
            "Epoch: 29/30, step: 178/364, loss: 0.15684, accuracy: 0.94566\n",
            "Epoch: 29/30, step: 179/364, loss: 0.15661, accuracy: 0.94579\n",
            "Epoch: 29/30, step: 180/364, loss: 0.15637, accuracy: 0.94592\n",
            "Epoch: 29/30, step: 181/364, loss: 0.15681, accuracy: 0.94587\n",
            "Epoch: 29/30, step: 182/364, loss: 0.15677, accuracy: 0.94583\n",
            "Epoch: 29/30, step: 183/364, loss: 0.15660, accuracy: 0.94595\n",
            "Epoch: 29/30, step: 184/364, loss: 0.15618, accuracy: 0.94616\n",
            "Epoch: 29/30, step: 185/364, loss: 0.15604, accuracy: 0.94628\n",
            "Epoch: 29/30, step: 186/364, loss: 0.15655, accuracy: 0.94590\n",
            "Epoch: 29/30, step: 187/364, loss: 0.15633, accuracy: 0.94594\n",
            "Epoch: 29/30, step: 188/364, loss: 0.15611, accuracy: 0.94606\n",
            "Epoch: 29/30, step: 189/364, loss: 0.15644, accuracy: 0.94585\n",
            "Epoch: 29/30, step: 190/364, loss: 0.15648, accuracy: 0.94564\n",
            "Epoch: 29/30, step: 191/364, loss: 0.15664, accuracy: 0.94552\n",
            "Epoch: 29/30, step: 192/364, loss: 0.15621, accuracy: 0.94580\n",
            "Epoch: 29/30, step: 193/364, loss: 0.15620, accuracy: 0.94584\n",
            "Epoch: 29/30, step: 194/364, loss: 0.15597, accuracy: 0.94580\n",
            "Epoch: 29/30, step: 195/364, loss: 0.15562, accuracy: 0.94591\n",
            "Epoch: 29/30, step: 196/364, loss: 0.15518, accuracy: 0.94611\n",
            "Epoch: 29/30, step: 197/364, loss: 0.15507, accuracy: 0.94615\n",
            "Epoch: 29/30, step: 198/364, loss: 0.15468, accuracy: 0.94626\n",
            "Epoch: 29/30, step: 199/364, loss: 0.15442, accuracy: 0.94637\n",
            "Epoch: 29/30, step: 200/364, loss: 0.15392, accuracy: 0.94664\n",
            "Epoch: 29/30, step: 201/364, loss: 0.15434, accuracy: 0.94652\n",
            "Epoch: 29/30, step: 202/364, loss: 0.15422, accuracy: 0.94647\n",
            "Epoch: 29/30, step: 203/364, loss: 0.15384, accuracy: 0.94666\n",
            "Epoch: 29/30, step: 204/364, loss: 0.15354, accuracy: 0.94684\n",
            "Epoch: 29/30, step: 205/364, loss: 0.15422, accuracy: 0.94657\n",
            "Epoch: 29/30, step: 206/364, loss: 0.15438, accuracy: 0.94660\n",
            "Epoch: 29/30, step: 207/364, loss: 0.15417, accuracy: 0.94678\n",
            "Epoch: 29/30, step: 208/364, loss: 0.15391, accuracy: 0.94697\n",
            "Epoch: 29/30, step: 209/364, loss: 0.15366, accuracy: 0.94714\n",
            "Epoch: 29/30, step: 210/364, loss: 0.15353, accuracy: 0.94725\n",
            "Epoch: 29/30, step: 211/364, loss: 0.15350, accuracy: 0.94735\n",
            "Epoch: 29/30, step: 212/364, loss: 0.15374, accuracy: 0.94716\n",
            "Epoch: 29/30, step: 213/364, loss: 0.15425, accuracy: 0.94696\n",
            "Epoch: 29/30, step: 214/364, loss: 0.15415, accuracy: 0.94706\n",
            "Epoch: 29/30, step: 215/364, loss: 0.15458, accuracy: 0.94687\n",
            "Epoch: 29/30, step: 216/364, loss: 0.15467, accuracy: 0.94698\n",
            "Epoch: 29/30, step: 217/364, loss: 0.15446, accuracy: 0.94700\n",
            "Epoch: 29/30, step: 218/364, loss: 0.15425, accuracy: 0.94710\n",
            "Epoch: 29/30, step: 219/364, loss: 0.15406, accuracy: 0.94727\n",
            "Epoch: 29/30, step: 220/364, loss: 0.15429, accuracy: 0.94709\n",
            "Epoch: 29/30, step: 221/364, loss: 0.15461, accuracy: 0.94676\n",
            "Epoch: 29/30, step: 222/364, loss: 0.15478, accuracy: 0.94658\n",
            "Epoch: 29/30, step: 223/364, loss: 0.15464, accuracy: 0.94661\n",
            "Epoch: 29/30, step: 224/364, loss: 0.15453, accuracy: 0.94664\n",
            "Epoch: 29/30, step: 225/364, loss: 0.15491, accuracy: 0.94639\n",
            "Epoch: 29/30, step: 226/364, loss: 0.15494, accuracy: 0.94656\n",
            "Epoch: 29/30, step: 227/364, loss: 0.15466, accuracy: 0.94679\n",
            "Epoch: 29/30, step: 228/364, loss: 0.15478, accuracy: 0.94668\n",
            "Epoch: 29/30, step: 229/364, loss: 0.15464, accuracy: 0.94685\n",
            "Epoch: 29/30, step: 230/364, loss: 0.15462, accuracy: 0.94674\n",
            "Epoch: 29/30, step: 231/364, loss: 0.15435, accuracy: 0.94690\n",
            "Epoch: 29/30, step: 232/364, loss: 0.15434, accuracy: 0.94693\n",
            "Epoch: 29/30, step: 233/364, loss: 0.15437, accuracy: 0.94682\n",
            "Epoch: 29/30, step: 234/364, loss: 0.15430, accuracy: 0.94692\n",
            "Epoch: 29/30, step: 235/364, loss: 0.15411, accuracy: 0.94694\n",
            "Epoch: 29/30, step: 236/364, loss: 0.15406, accuracy: 0.94697\n",
            "Epoch: 29/30, step: 237/364, loss: 0.15415, accuracy: 0.94680\n",
            "Epoch: 29/30, step: 238/364, loss: 0.15444, accuracy: 0.94669\n",
            "Epoch: 29/30, step: 239/364, loss: 0.15442, accuracy: 0.94678\n",
            "Epoch: 29/30, step: 240/364, loss: 0.15440, accuracy: 0.94687\n",
            "Epoch: 29/30, step: 241/364, loss: 0.15435, accuracy: 0.94697\n",
            "Epoch: 29/30, step: 242/364, loss: 0.15430, accuracy: 0.94699\n",
            "Epoch: 29/30, step: 243/364, loss: 0.15417, accuracy: 0.94702\n",
            "Epoch: 29/30, step: 244/364, loss: 0.15465, accuracy: 0.94653\n",
            "Epoch: 29/30, step: 245/364, loss: 0.15508, accuracy: 0.94624\n",
            "Epoch: 29/30, step: 246/364, loss: 0.15488, accuracy: 0.94639\n",
            "Epoch: 29/30, step: 247/364, loss: 0.15470, accuracy: 0.94655\n",
            "Epoch: 29/30, step: 248/364, loss: 0.15467, accuracy: 0.94664\n",
            "Epoch: 29/30, step: 249/364, loss: 0.15452, accuracy: 0.94672\n",
            "Epoch: 29/30, step: 250/364, loss: 0.15434, accuracy: 0.94681\n",
            "Epoch: 29/30, step: 251/364, loss: 0.15489, accuracy: 0.94653\n",
            "Epoch: 29/30, step: 252/364, loss: 0.15471, accuracy: 0.94655\n",
            "Epoch: 29/30, step: 253/364, loss: 0.15432, accuracy: 0.94676\n",
            "Epoch: 29/30, step: 254/364, loss: 0.15508, accuracy: 0.94630\n",
            "Epoch: 29/30, step: 255/364, loss: 0.15513, accuracy: 0.94608\n",
            "Epoch: 29/30, step: 256/364, loss: 0.15487, accuracy: 0.94623\n",
            "Epoch: 29/30, step: 257/364, loss: 0.15475, accuracy: 0.94632\n",
            "Epoch: 29/30, step: 258/364, loss: 0.15477, accuracy: 0.94628\n",
            "Epoch: 29/30, step: 259/364, loss: 0.15465, accuracy: 0.94637\n",
            "Epoch: 29/30, step: 260/364, loss: 0.15486, accuracy: 0.94633\n",
            "Epoch: 29/30, step: 261/364, loss: 0.15472, accuracy: 0.94636\n",
            "Epoch: 29/30, step: 262/364, loss: 0.15482, accuracy: 0.94633\n",
            "Epoch: 29/30, step: 263/364, loss: 0.15449, accuracy: 0.94647\n",
            "Epoch: 29/30, step: 264/364, loss: 0.15431, accuracy: 0.94656\n",
            "Epoch: 29/30, step: 265/364, loss: 0.15420, accuracy: 0.94658\n",
            "Epoch: 29/30, step: 266/364, loss: 0.15423, accuracy: 0.94660\n",
            "Epoch: 29/30, step: 267/364, loss: 0.15431, accuracy: 0.94663\n",
            "Epoch: 29/30, step: 268/364, loss: 0.15426, accuracy: 0.94665\n",
            "Epoch: 29/30, step: 269/364, loss: 0.15426, accuracy: 0.94662\n",
            "Epoch: 29/30, step: 270/364, loss: 0.15412, accuracy: 0.94676\n",
            "Epoch: 29/30, step: 271/364, loss: 0.15437, accuracy: 0.94661\n",
            "Epoch: 29/30, step: 272/364, loss: 0.15507, accuracy: 0.94629\n",
            "Epoch: 29/30, step: 273/364, loss: 0.15479, accuracy: 0.94643\n",
            "Epoch: 29/30, step: 274/364, loss: 0.15485, accuracy: 0.94628\n",
            "Epoch: 29/30, step: 275/364, loss: 0.15458, accuracy: 0.94642\n",
            "Epoch: 29/30, step: 276/364, loss: 0.15430, accuracy: 0.94661\n",
            "Epoch: 29/30, step: 277/364, loss: 0.15396, accuracy: 0.94681\n",
            "Epoch: 29/30, step: 278/364, loss: 0.15390, accuracy: 0.94689\n",
            "Epoch: 29/30, step: 279/364, loss: 0.15367, accuracy: 0.94702\n",
            "Epoch: 29/30, step: 280/364, loss: 0.15350, accuracy: 0.94710\n",
            "Epoch: 29/30, step: 281/364, loss: 0.15339, accuracy: 0.94718\n",
            "Epoch: 29/30, step: 282/364, loss: 0.15317, accuracy: 0.94736\n",
            "Epoch: 29/30, step: 283/364, loss: 0.15355, accuracy: 0.94727\n",
            "Epoch: 29/30, step: 284/364, loss: 0.15352, accuracy: 0.94729\n",
            "Epoch: 29/30, step: 285/364, loss: 0.15389, accuracy: 0.94698\n",
            "Epoch: 29/30, step: 286/364, loss: 0.15368, accuracy: 0.94717\n",
            "Epoch: 29/30, step: 287/364, loss: 0.15359, accuracy: 0.94719\n",
            "Epoch: 29/30, step: 288/364, loss: 0.15332, accuracy: 0.94737\n",
            "Epoch: 29/30, step: 289/364, loss: 0.15342, accuracy: 0.94729\n",
            "Epoch: 29/30, step: 290/364, loss: 0.15326, accuracy: 0.94736\n",
            "Epoch: 29/30, step: 291/364, loss: 0.15339, accuracy: 0.94723\n",
            "Epoch: 29/30, train loss: 0.15339, train accuracy: 0.94723, valid loss: 0.88870, valid accuracy: 0.67713\n",
            "Epoch: 30/30, step: 1/364, loss: 0.10867, accuracy: 0.95312\n",
            "Epoch: 30/30, step: 2/364, loss: 0.10376, accuracy: 0.96094\n",
            "Epoch: 30/30, step: 3/364, loss: 0.09167, accuracy: 0.97396\n",
            "Epoch: 30/30, step: 4/364, loss: 0.09655, accuracy: 0.96875\n",
            "Epoch: 30/30, step: 5/364, loss: 0.09215, accuracy: 0.96875\n",
            "Epoch: 30/30, step: 6/364, loss: 0.11415, accuracy: 0.95833\n",
            "Epoch: 30/30, step: 7/364, loss: 0.13934, accuracy: 0.94866\n",
            "Epoch: 30/30, step: 8/364, loss: 0.15378, accuracy: 0.94336\n",
            "Epoch: 30/30, step: 9/364, loss: 0.14974, accuracy: 0.94444\n",
            "Epoch: 30/30, step: 10/364, loss: 0.14514, accuracy: 0.94844\n",
            "Epoch: 30/30, step: 11/364, loss: 0.14517, accuracy: 0.94886\n",
            "Epoch: 30/30, step: 12/364, loss: 0.14430, accuracy: 0.94922\n",
            "Epoch: 30/30, step: 13/364, loss: 0.14255, accuracy: 0.95192\n",
            "Epoch: 30/30, step: 14/364, loss: 0.13993, accuracy: 0.95424\n",
            "Epoch: 30/30, step: 15/364, loss: 0.13578, accuracy: 0.95521\n",
            "Epoch: 30/30, step: 16/364, loss: 0.13886, accuracy: 0.95410\n",
            "Epoch: 30/30, step: 17/364, loss: 0.13962, accuracy: 0.95037\n",
            "Epoch: 30/30, step: 18/364, loss: 0.14020, accuracy: 0.94965\n",
            "Epoch: 30/30, step: 19/364, loss: 0.13622, accuracy: 0.95230\n",
            "Epoch: 30/30, step: 20/364, loss: 0.13332, accuracy: 0.95312\n",
            "Epoch: 30/30, step: 21/364, loss: 0.13242, accuracy: 0.95387\n",
            "Epoch: 30/30, step: 22/364, loss: 0.13327, accuracy: 0.95241\n",
            "Epoch: 30/30, step: 23/364, loss: 0.13519, accuracy: 0.95245\n",
            "Epoch: 30/30, step: 24/364, loss: 0.13307, accuracy: 0.95378\n",
            "Epoch: 30/30, step: 25/364, loss: 0.13053, accuracy: 0.95562\n",
            "Epoch: 30/30, step: 26/364, loss: 0.13157, accuracy: 0.95613\n",
            "Epoch: 30/30, step: 27/364, loss: 0.12997, accuracy: 0.95718\n",
            "Epoch: 30/30, step: 28/364, loss: 0.13936, accuracy: 0.95257\n",
            "Epoch: 30/30, step: 29/364, loss: 0.13915, accuracy: 0.95151\n",
            "Epoch: 30/30, step: 30/364, loss: 0.13864, accuracy: 0.95208\n",
            "Epoch: 30/30, step: 31/364, loss: 0.13904, accuracy: 0.95212\n",
            "Epoch: 30/30, step: 32/364, loss: 0.14220, accuracy: 0.95020\n",
            "Epoch: 30/30, step: 33/364, loss: 0.14264, accuracy: 0.94981\n",
            "Epoch: 30/30, step: 34/364, loss: 0.14128, accuracy: 0.95083\n",
            "Epoch: 30/30, step: 35/364, loss: 0.14703, accuracy: 0.94821\n",
            "Epoch: 30/30, step: 36/364, loss: 0.14626, accuracy: 0.94878\n",
            "Epoch: 30/30, step: 37/364, loss: 0.14359, accuracy: 0.95017\n",
            "Epoch: 30/30, step: 38/364, loss: 0.14210, accuracy: 0.95066\n",
            "Epoch: 30/30, step: 39/364, loss: 0.14147, accuracy: 0.94992\n",
            "Epoch: 30/30, step: 40/364, loss: 0.14101, accuracy: 0.94961\n",
            "Epoch: 30/30, step: 41/364, loss: 0.13995, accuracy: 0.95008\n",
            "Epoch: 30/30, step: 42/364, loss: 0.14007, accuracy: 0.94978\n",
            "Epoch: 30/30, step: 43/364, loss: 0.13876, accuracy: 0.95058\n",
            "Epoch: 30/30, step: 44/364, loss: 0.13956, accuracy: 0.95064\n",
            "Epoch: 30/30, step: 45/364, loss: 0.14013, accuracy: 0.95035\n",
            "Epoch: 30/30, step: 46/364, loss: 0.13911, accuracy: 0.95075\n",
            "Epoch: 30/30, step: 47/364, loss: 0.13963, accuracy: 0.95080\n",
            "Epoch: 30/30, step: 48/364, loss: 0.13973, accuracy: 0.95020\n",
            "Epoch: 30/30, step: 49/364, loss: 0.13845, accuracy: 0.95057\n",
            "Epoch: 30/30, step: 50/364, loss: 0.13992, accuracy: 0.95031\n",
            "Epoch: 30/30, step: 51/364, loss: 0.13889, accuracy: 0.95098\n",
            "Epoch: 30/30, step: 52/364, loss: 0.13901, accuracy: 0.95102\n",
            "Epoch: 30/30, step: 53/364, loss: 0.13914, accuracy: 0.95077\n",
            "Epoch: 30/30, step: 54/364, loss: 0.13739, accuracy: 0.95168\n",
            "Epoch: 30/30, step: 55/364, loss: 0.13747, accuracy: 0.95170\n",
            "Epoch: 30/30, step: 56/364, loss: 0.13814, accuracy: 0.95089\n",
            "Epoch: 30/30, step: 57/364, loss: 0.13794, accuracy: 0.95066\n",
            "Epoch: 30/30, step: 58/364, loss: 0.13793, accuracy: 0.95124\n",
            "Epoch: 30/30, step: 59/364, loss: 0.13883, accuracy: 0.95101\n",
            "Epoch: 30/30, step: 60/364, loss: 0.13825, accuracy: 0.95156\n",
            "Epoch: 30/30, step: 61/364, loss: 0.13785, accuracy: 0.95159\n",
            "Epoch: 30/30, step: 62/364, loss: 0.13780, accuracy: 0.95212\n",
            "Epoch: 30/30, step: 63/364, loss: 0.13691, accuracy: 0.95288\n",
            "Epoch: 30/30, step: 64/364, loss: 0.13621, accuracy: 0.95312\n",
            "Epoch: 30/30, step: 65/364, loss: 0.13650, accuracy: 0.95312\n",
            "Epoch: 30/30, step: 66/364, loss: 0.13599, accuracy: 0.95360\n",
            "Epoch: 30/30, step: 67/364, loss: 0.13506, accuracy: 0.95406\n",
            "Epoch: 30/30, step: 68/364, loss: 0.13596, accuracy: 0.95381\n",
            "Epoch: 30/30, step: 69/364, loss: 0.13520, accuracy: 0.95403\n",
            "Epoch: 30/30, step: 70/364, loss: 0.13554, accuracy: 0.95402\n",
            "Epoch: 30/30, step: 71/364, loss: 0.13506, accuracy: 0.95423\n",
            "Epoch: 30/30, step: 72/364, loss: 0.13458, accuracy: 0.95443\n",
            "Epoch: 30/30, step: 73/364, loss: 0.13472, accuracy: 0.95420\n",
            "Epoch: 30/30, step: 74/364, loss: 0.13640, accuracy: 0.95334\n",
            "Epoch: 30/30, step: 75/364, loss: 0.13642, accuracy: 0.95333\n",
            "Epoch: 30/30, step: 76/364, loss: 0.14066, accuracy: 0.95148\n",
            "Epoch: 30/30, step: 77/364, loss: 0.14096, accuracy: 0.95150\n",
            "Epoch: 30/30, step: 78/364, loss: 0.14008, accuracy: 0.95212\n",
            "Epoch: 30/30, step: 79/364, loss: 0.13972, accuracy: 0.95214\n",
            "Epoch: 30/30, step: 80/364, loss: 0.13875, accuracy: 0.95273\n",
            "Epoch: 30/30, step: 81/364, loss: 0.13854, accuracy: 0.95274\n",
            "Epoch: 30/30, step: 82/364, loss: 0.13799, accuracy: 0.95293\n",
            "Epoch: 30/30, step: 83/364, loss: 0.13735, accuracy: 0.95331\n",
            "Epoch: 30/30, step: 84/364, loss: 0.13753, accuracy: 0.95294\n",
            "Epoch: 30/30, step: 85/364, loss: 0.13885, accuracy: 0.95202\n",
            "Epoch: 30/30, step: 86/364, loss: 0.13959, accuracy: 0.95094\n",
            "Epoch: 30/30, step: 87/364, loss: 0.13936, accuracy: 0.95115\n",
            "Epoch: 30/30, step: 88/364, loss: 0.13921, accuracy: 0.95117\n",
            "Epoch: 30/30, step: 89/364, loss: 0.13946, accuracy: 0.95102\n",
            "Epoch: 30/30, step: 90/364, loss: 0.13908, accuracy: 0.95139\n",
            "Epoch: 30/30, step: 91/364, loss: 0.13996, accuracy: 0.95072\n",
            "Epoch: 30/30, step: 92/364, loss: 0.13935, accuracy: 0.95109\n",
            "Epoch: 30/30, step: 93/364, loss: 0.13954, accuracy: 0.95077\n",
            "Epoch: 30/30, step: 94/364, loss: 0.13922, accuracy: 0.95096\n",
            "Epoch: 30/30, step: 95/364, loss: 0.14022, accuracy: 0.95049\n",
            "Epoch: 30/30, step: 96/364, loss: 0.14043, accuracy: 0.95036\n",
            "Epoch: 30/30, step: 97/364, loss: 0.14014, accuracy: 0.95055\n",
            "Epoch: 30/30, step: 98/364, loss: 0.13956, accuracy: 0.95073\n",
            "Epoch: 30/30, step: 99/364, loss: 0.13905, accuracy: 0.95107\n",
            "Epoch: 30/30, step: 100/364, loss: 0.13929, accuracy: 0.95094\n",
            "Epoch: 30/30, step: 101/364, loss: 0.13871, accuracy: 0.95127\n",
            "Epoch: 30/30, step: 102/364, loss: 0.13829, accuracy: 0.95159\n",
            "Epoch: 30/30, step: 103/364, loss: 0.13803, accuracy: 0.95176\n",
            "Epoch: 30/30, step: 104/364, loss: 0.13767, accuracy: 0.95192\n",
            "Epoch: 30/30, step: 105/364, loss: 0.13802, accuracy: 0.95164\n",
            "Epoch: 30/30, step: 106/364, loss: 0.13936, accuracy: 0.95077\n",
            "Epoch: 30/30, step: 107/364, loss: 0.13962, accuracy: 0.95050\n",
            "Epoch: 30/30, step: 108/364, loss: 0.13921, accuracy: 0.95081\n",
            "Epoch: 30/30, step: 109/364, loss: 0.13897, accuracy: 0.95083\n",
            "Epoch: 30/30, step: 110/364, loss: 0.13853, accuracy: 0.95114\n",
            "Epoch: 30/30, step: 111/364, loss: 0.14036, accuracy: 0.95017\n",
            "Epoch: 30/30, step: 112/364, loss: 0.14006, accuracy: 0.95033\n",
            "Epoch: 30/30, step: 113/364, loss: 0.14145, accuracy: 0.94953\n",
            "Epoch: 30/30, step: 114/364, loss: 0.14137, accuracy: 0.94942\n",
            "Epoch: 30/30, step: 115/364, loss: 0.14070, accuracy: 0.94973\n",
            "Epoch: 30/30, step: 116/364, loss: 0.14110, accuracy: 0.94949\n",
            "Epoch: 30/30, step: 117/364, loss: 0.14191, accuracy: 0.94952\n",
            "Epoch: 30/30, step: 118/364, loss: 0.14253, accuracy: 0.94902\n",
            "Epoch: 30/30, step: 119/364, loss: 0.14198, accuracy: 0.94919\n",
            "Epoch: 30/30, step: 120/364, loss: 0.14186, accuracy: 0.94922\n",
            "Epoch: 30/30, step: 121/364, loss: 0.14179, accuracy: 0.94925\n",
            "Epoch: 30/30, step: 122/364, loss: 0.14147, accuracy: 0.94954\n",
            "Epoch: 30/30, step: 123/364, loss: 0.14120, accuracy: 0.94982\n",
            "Epoch: 30/30, step: 124/364, loss: 0.14062, accuracy: 0.95010\n",
            "Epoch: 30/30, step: 125/364, loss: 0.14021, accuracy: 0.95050\n",
            "Epoch: 30/30, step: 126/364, loss: 0.14223, accuracy: 0.94978\n",
            "Epoch: 30/30, step: 127/364, loss: 0.14166, accuracy: 0.95005\n",
            "Epoch: 30/30, step: 128/364, loss: 0.14186, accuracy: 0.95007\n",
            "Epoch: 30/30, step: 129/364, loss: 0.14156, accuracy: 0.95010\n",
            "Epoch: 30/30, step: 130/364, loss: 0.14201, accuracy: 0.94988\n",
            "Epoch: 30/30, step: 131/364, loss: 0.14235, accuracy: 0.94955\n",
            "Epoch: 30/30, step: 132/364, loss: 0.14227, accuracy: 0.94946\n",
            "Epoch: 30/30, step: 133/364, loss: 0.14226, accuracy: 0.94960\n",
            "Epoch: 30/30, step: 134/364, loss: 0.14227, accuracy: 0.94951\n",
            "Epoch: 30/30, step: 135/364, loss: 0.14240, accuracy: 0.94942\n",
            "Epoch: 30/30, step: 136/364, loss: 0.14261, accuracy: 0.94922\n",
            "Epoch: 30/30, step: 137/364, loss: 0.14201, accuracy: 0.94948\n",
            "Epoch: 30/30, step: 138/364, loss: 0.14143, accuracy: 0.94984\n",
            "Epoch: 30/30, step: 139/364, loss: 0.14136, accuracy: 0.94987\n",
            "Epoch: 30/30, step: 140/364, loss: 0.14172, accuracy: 0.94978\n",
            "Epoch: 30/30, step: 141/364, loss: 0.14158, accuracy: 0.94980\n",
            "Epoch: 30/30, step: 142/364, loss: 0.14117, accuracy: 0.95015\n",
            "Epoch: 30/30, step: 143/364, loss: 0.14078, accuracy: 0.95039\n",
            "Epoch: 30/30, step: 144/364, loss: 0.14112, accuracy: 0.95009\n",
            "Epoch: 30/30, step: 145/364, loss: 0.14053, accuracy: 0.95043\n",
            "Epoch: 30/30, step: 146/364, loss: 0.14026, accuracy: 0.95056\n",
            "Epoch: 30/30, step: 147/364, loss: 0.14010, accuracy: 0.95057\n",
            "Epoch: 30/30, step: 148/364, loss: 0.13964, accuracy: 0.95080\n",
            "Epoch: 30/30, step: 149/364, loss: 0.13980, accuracy: 0.95071\n",
            "Epoch: 30/30, step: 150/364, loss: 0.13959, accuracy: 0.95063\n",
            "Epoch: 30/30, step: 151/364, loss: 0.14071, accuracy: 0.94981\n",
            "Epoch: 30/30, step: 152/364, loss: 0.14170, accuracy: 0.94942\n",
            "Epoch: 30/30, step: 153/364, loss: 0.14140, accuracy: 0.94965\n",
            "Epoch: 30/30, step: 154/364, loss: 0.14113, accuracy: 0.94978\n",
            "Epoch: 30/30, step: 155/364, loss: 0.14160, accuracy: 0.94960\n",
            "Epoch: 30/30, step: 156/364, loss: 0.14160, accuracy: 0.94952\n",
            "Epoch: 30/30, step: 157/364, loss: 0.14133, accuracy: 0.94964\n",
            "Epoch: 30/30, step: 158/364, loss: 0.14101, accuracy: 0.94986\n",
            "Epoch: 30/30, step: 159/364, loss: 0.14124, accuracy: 0.94978\n",
            "Epoch: 30/30, step: 160/364, loss: 0.14100, accuracy: 0.95000\n",
            "Epoch: 30/30, step: 161/364, loss: 0.14134, accuracy: 0.94973\n",
            "Epoch: 30/30, step: 162/364, loss: 0.14141, accuracy: 0.94946\n",
            "Epoch: 30/30, step: 163/364, loss: 0.14158, accuracy: 0.94948\n",
            "Epoch: 30/30, step: 164/364, loss: 0.14111, accuracy: 0.94970\n",
            "Epoch: 30/30, step: 165/364, loss: 0.14078, accuracy: 0.94991\n",
            "Epoch: 30/30, step: 166/364, loss: 0.14199, accuracy: 0.94927\n",
            "Epoch: 30/30, step: 167/364, loss: 0.14163, accuracy: 0.94948\n",
            "Epoch: 30/30, step: 168/364, loss: 0.14150, accuracy: 0.94950\n",
            "Epoch: 30/30, step: 169/364, loss: 0.14223, accuracy: 0.94915\n",
            "Epoch: 30/30, step: 170/364, loss: 0.14211, accuracy: 0.94926\n",
            "Epoch: 30/30, step: 171/364, loss: 0.14189, accuracy: 0.94947\n",
            "Epoch: 30/30, step: 172/364, loss: 0.14167, accuracy: 0.94958\n",
            "Epoch: 30/30, step: 173/364, loss: 0.14224, accuracy: 0.94933\n",
            "Epoch: 30/30, step: 174/364, loss: 0.14199, accuracy: 0.94944\n",
            "Epoch: 30/30, step: 175/364, loss: 0.14196, accuracy: 0.94946\n",
            "Epoch: 30/30, step: 176/364, loss: 0.14191, accuracy: 0.94949\n",
            "Epoch: 30/30, step: 177/364, loss: 0.14163, accuracy: 0.94959\n",
            "Epoch: 30/30, step: 178/364, loss: 0.14143, accuracy: 0.94979\n",
            "Epoch: 30/30, step: 179/364, loss: 0.14156, accuracy: 0.94981\n",
            "Epoch: 30/30, step: 180/364, loss: 0.14137, accuracy: 0.94991\n",
            "Epoch: 30/30, step: 181/364, loss: 0.14121, accuracy: 0.95010\n",
            "Epoch: 30/30, step: 182/364, loss: 0.14116, accuracy: 0.95012\n",
            "Epoch: 30/30, step: 183/364, loss: 0.14152, accuracy: 0.95014\n",
            "Epoch: 30/30, step: 184/364, loss: 0.14143, accuracy: 0.95024\n",
            "Epoch: 30/30, step: 185/364, loss: 0.14111, accuracy: 0.95051\n",
            "Epoch: 30/30, step: 186/364, loss: 0.14104, accuracy: 0.95052\n",
            "Epoch: 30/30, step: 187/364, loss: 0.14167, accuracy: 0.95028\n",
            "Epoch: 30/30, step: 188/364, loss: 0.14144, accuracy: 0.95038\n",
            "Epoch: 30/30, step: 189/364, loss: 0.14154, accuracy: 0.95023\n",
            "Epoch: 30/30, step: 190/364, loss: 0.14157, accuracy: 0.95016\n",
            "Epoch: 30/30, step: 191/364, loss: 0.14169, accuracy: 0.95002\n",
            "Epoch: 30/30, step: 192/364, loss: 0.14190, accuracy: 0.95003\n",
            "Epoch: 30/30, step: 193/364, loss: 0.14166, accuracy: 0.95021\n",
            "Epoch: 30/30, step: 194/364, loss: 0.14183, accuracy: 0.95023\n",
            "Epoch: 30/30, step: 195/364, loss: 0.14176, accuracy: 0.95024\n",
            "Epoch: 30/30, step: 196/364, loss: 0.14142, accuracy: 0.95049\n",
            "Epoch: 30/30, step: 197/364, loss: 0.14137, accuracy: 0.95059\n",
            "Epoch: 30/30, step: 198/364, loss: 0.14168, accuracy: 0.95052\n",
            "Epoch: 30/30, step: 199/364, loss: 0.14169, accuracy: 0.95038\n",
            "Epoch: 30/30, step: 200/364, loss: 0.14197, accuracy: 0.95008\n",
            "Epoch: 30/30, step: 201/364, loss: 0.14246, accuracy: 0.94986\n",
            "Epoch: 30/30, step: 202/364, loss: 0.14220, accuracy: 0.94995\n",
            "Epoch: 30/30, step: 203/364, loss: 0.14221, accuracy: 0.94982\n",
            "Epoch: 30/30, step: 204/364, loss: 0.14185, accuracy: 0.95006\n",
            "Epoch: 30/30, step: 205/364, loss: 0.14189, accuracy: 0.95023\n",
            "Epoch: 30/30, step: 206/364, loss: 0.14214, accuracy: 0.95032\n",
            "Epoch: 30/30, step: 207/364, loss: 0.14227, accuracy: 0.95026\n",
            "Epoch: 30/30, step: 208/364, loss: 0.14202, accuracy: 0.95042\n",
            "Epoch: 30/30, step: 209/364, loss: 0.14300, accuracy: 0.95006\n",
            "Epoch: 30/30, step: 210/364, loss: 0.14282, accuracy: 0.95015\n",
            "Epoch: 30/30, step: 211/364, loss: 0.14283, accuracy: 0.95009\n",
            "Epoch: 30/30, step: 212/364, loss: 0.14282, accuracy: 0.95010\n",
            "Epoch: 30/30, step: 213/364, loss: 0.14244, accuracy: 0.95034\n",
            "Epoch: 30/30, step: 214/364, loss: 0.14267, accuracy: 0.95028\n",
            "Epoch: 30/30, step: 215/364, loss: 0.14281, accuracy: 0.95022\n",
            "Epoch: 30/30, step: 216/364, loss: 0.14247, accuracy: 0.95038\n",
            "Epoch: 30/30, step: 217/364, loss: 0.14249, accuracy: 0.95046\n",
            "Epoch: 30/30, step: 218/364, loss: 0.14213, accuracy: 0.95062\n",
            "Epoch: 30/30, step: 219/364, loss: 0.14191, accuracy: 0.95063\n",
            "Epoch: 30/30, step: 220/364, loss: 0.14165, accuracy: 0.95071\n",
            "Epoch: 30/30, step: 221/364, loss: 0.14156, accuracy: 0.95072\n",
            "Epoch: 30/30, step: 222/364, loss: 0.14118, accuracy: 0.95094\n",
            "Epoch: 30/30, step: 223/364, loss: 0.14144, accuracy: 0.95067\n",
            "Epoch: 30/30, step: 224/364, loss: 0.14118, accuracy: 0.95082\n",
            "Epoch: 30/30, step: 225/364, loss: 0.14098, accuracy: 0.95090\n",
            "Epoch: 30/30, step: 226/364, loss: 0.14068, accuracy: 0.95105\n",
            "Epoch: 30/30, step: 227/364, loss: 0.14053, accuracy: 0.95120\n",
            "Epoch: 30/30, step: 228/364, loss: 0.14056, accuracy: 0.95121\n",
            "Epoch: 30/30, step: 229/364, loss: 0.14040, accuracy: 0.95128\n",
            "Epoch: 30/30, step: 230/364, loss: 0.14038, accuracy: 0.95129\n",
            "Epoch: 30/30, step: 231/364, loss: 0.14065, accuracy: 0.95110\n",
            "Epoch: 30/30, step: 232/364, loss: 0.14070, accuracy: 0.95110\n",
            "Epoch: 30/30, step: 233/364, loss: 0.14101, accuracy: 0.95091\n",
            "Epoch: 30/30, step: 234/364, loss: 0.14106, accuracy: 0.95092\n",
            "Epoch: 30/30, step: 235/364, loss: 0.14143, accuracy: 0.95066\n",
            "Epoch: 30/30, step: 236/364, loss: 0.14130, accuracy: 0.95074\n",
            "Epoch: 30/30, step: 237/364, loss: 0.14146, accuracy: 0.95062\n",
            "Epoch: 30/30, step: 238/364, loss: 0.14161, accuracy: 0.95050\n",
            "Epoch: 30/30, step: 239/364, loss: 0.14144, accuracy: 0.95058\n",
            "Epoch: 30/30, step: 240/364, loss: 0.14118, accuracy: 0.95065\n",
            "Epoch: 30/30, step: 241/364, loss: 0.14098, accuracy: 0.95073\n",
            "Epoch: 30/30, step: 242/364, loss: 0.14080, accuracy: 0.95087\n",
            "Epoch: 30/30, step: 243/364, loss: 0.14076, accuracy: 0.95087\n",
            "Epoch: 30/30, step: 244/364, loss: 0.14055, accuracy: 0.95095\n",
            "Epoch: 30/30, step: 245/364, loss: 0.14045, accuracy: 0.95096\n",
            "Epoch: 30/30, step: 246/364, loss: 0.14037, accuracy: 0.95097\n",
            "Epoch: 30/30, step: 247/364, loss: 0.14030, accuracy: 0.95091\n",
            "Epoch: 30/30, step: 248/364, loss: 0.14039, accuracy: 0.95086\n",
            "Epoch: 30/30, step: 249/364, loss: 0.14026, accuracy: 0.95093\n",
            "Epoch: 30/30, step: 250/364, loss: 0.14013, accuracy: 0.95106\n",
            "Epoch: 30/30, step: 251/364, loss: 0.14019, accuracy: 0.95101\n",
            "Epoch: 30/30, step: 252/364, loss: 0.14016, accuracy: 0.95108\n",
            "Epoch: 30/30, step: 253/364, loss: 0.14026, accuracy: 0.95096\n",
            "Epoch: 30/30, step: 254/364, loss: 0.14003, accuracy: 0.95103\n",
            "Epoch: 30/30, step: 255/364, loss: 0.13997, accuracy: 0.95110\n",
            "Epoch: 30/30, step: 256/364, loss: 0.13982, accuracy: 0.95123\n",
            "Epoch: 30/30, step: 257/364, loss: 0.13988, accuracy: 0.95118\n",
            "Epoch: 30/30, step: 258/364, loss: 0.13998, accuracy: 0.95119\n",
            "Epoch: 30/30, step: 259/364, loss: 0.13976, accuracy: 0.95125\n",
            "Epoch: 30/30, step: 260/364, loss: 0.13993, accuracy: 0.95102\n",
            "Epoch: 30/30, step: 261/364, loss: 0.13969, accuracy: 0.95115\n",
            "Epoch: 30/30, step: 262/364, loss: 0.13978, accuracy: 0.95110\n",
            "Epoch: 30/30, step: 263/364, loss: 0.13967, accuracy: 0.95122\n",
            "Epoch: 30/30, step: 264/364, loss: 0.13959, accuracy: 0.95117\n",
            "Epoch: 30/30, step: 265/364, loss: 0.13981, accuracy: 0.95112\n",
            "Epoch: 30/30, step: 266/364, loss: 0.13984, accuracy: 0.95119\n",
            "Epoch: 30/30, step: 267/364, loss: 0.13949, accuracy: 0.95137\n",
            "Epoch: 30/30, step: 268/364, loss: 0.13928, accuracy: 0.95155\n",
            "Epoch: 30/30, step: 269/364, loss: 0.13898, accuracy: 0.95173\n",
            "Epoch: 30/30, step: 270/364, loss: 0.13890, accuracy: 0.95185\n",
            "Epoch: 30/30, step: 271/364, loss: 0.13895, accuracy: 0.95180\n",
            "Epoch: 30/30, step: 272/364, loss: 0.13879, accuracy: 0.95186\n",
            "Epoch: 30/30, step: 273/364, loss: 0.13857, accuracy: 0.95198\n",
            "Epoch: 30/30, step: 274/364, loss: 0.13847, accuracy: 0.95210\n",
            "Epoch: 30/30, step: 275/364, loss: 0.13845, accuracy: 0.95205\n",
            "Epoch: 30/30, step: 276/364, loss: 0.13816, accuracy: 0.95216\n",
            "Epoch: 30/30, step: 277/364, loss: 0.13819, accuracy: 0.95217\n",
            "Epoch: 30/30, step: 278/364, loss: 0.13830, accuracy: 0.95211\n",
            "Epoch: 30/30, step: 279/364, loss: 0.13814, accuracy: 0.95223\n",
            "Epoch: 30/30, step: 280/364, loss: 0.13802, accuracy: 0.95229\n",
            "Epoch: 30/30, step: 281/364, loss: 0.13789, accuracy: 0.95235\n",
            "Epoch: 30/30, step: 282/364, loss: 0.13827, accuracy: 0.95213\n",
            "Epoch: 30/30, step: 283/364, loss: 0.13816, accuracy: 0.95219\n",
            "Epoch: 30/30, step: 284/364, loss: 0.13823, accuracy: 0.95219\n",
            "Epoch: 30/30, step: 285/364, loss: 0.13797, accuracy: 0.95230\n",
            "Epoch: 30/30, step: 286/364, loss: 0.13764, accuracy: 0.95247\n",
            "Epoch: 30/30, step: 287/364, loss: 0.13759, accuracy: 0.95253\n",
            "Epoch: 30/30, step: 288/364, loss: 0.13741, accuracy: 0.95264\n",
            "Epoch: 30/30, step: 289/364, loss: 0.13722, accuracy: 0.95280\n",
            "Epoch: 30/30, step: 290/364, loss: 0.13749, accuracy: 0.95275\n",
            "Epoch: 30/30, step: 291/364, loss: 0.13733, accuracy: 0.95277\n",
            "Epoch: 30/30, train loss: 0.13733, train accuracy: 0.95277, valid loss: 0.91658, valid accuracy: 0.66810\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Plain Network\n",
        "    - Plain_34\n",
        "        - train: 0.94, valid: 0.68\n",
        "    - Plain_50\n",
        "        - train: 0.95, valid: 0.66\n",
        "*   ResNet\n",
        "    - ResNet_34\n",
        "        - train: 0.93, valid: 0.69\n",
        "    - ResNet_50\n",
        "        - train: 0.95, valid: 0.68\n"
      ],
      "metadata": {
        "id": "L-hTrEpO_jBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1vYz-K2mfoqp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}