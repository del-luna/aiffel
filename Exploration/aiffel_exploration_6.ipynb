{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65a1fcc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "train_data = pd.read_table('./data/ratings_train.txt')\n",
    "test_data = pd.read_table('./data/ratings_test.txt')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4cab327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def load_data(train_data, test_data, num_words=10000):\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    train_data = train_data.dropna(how = 'any') \n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    test_data = test_data.dropna(how = 'any') \n",
    "\n",
    "    X_train = []\n",
    "    for sentence in train_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_train.append(temp_X)\n",
    "\n",
    "    X_test = []\n",
    "    for sentence in test_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_test.append(temp_X)\n",
    "\n",
    "    words = np.concatenate(X_train).tolist()\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(10000-4)\n",
    "    vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "\n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
    "\n",
    "    X_train = list(map(wordlist_to_indexlist, X_train))\n",
    "    X_test = list(map(wordlist_to_indexlist, X_test))\n",
    "\n",
    "    return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index\n",
    "    \n",
    "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03be5a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37414a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다. \n",
    "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "# 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "# 여러 개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35c3d1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  15.96940191154864\n",
      "문장길이 최대 :  116\n",
      "문장길이 표준편차 :  12.843571191092\n",
      "pad_sequences maxlen :  41\n",
      "전체 문장의 0.9342988343341575%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "total_data_text = list(X_train) + list(X_test)\n",
    "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc22334b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146182, 41)\n"
     ]
    }
   ],
   "source": [
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train,\n",
    "                                                        value=word_to_index[\"<PAD>\"],\n",
    "                                                        padding='post', # 혹은 'pre'\n",
    "                                                        maxlen=maxlen)\n",
    "\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='post', # 혹은 'pre'\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52563768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126182, 41)\n",
      "(126182,)\n",
      "(20000, 41)\n",
      "(20000,)\n"
     ]
    }
   ],
   "source": [
    "# validation set 20000건 분리\n",
    "x_val = X_train[:20000]   \n",
    "y_val = y_train[:20000]\n",
    "\n",
    "# validation set을 제외한 나머지\n",
    "partial_X_train = X_train[20000:]  \n",
    "partial_y_train = y_train[20000:]\n",
    "\n",
    "print(partial_X_train.shape)\n",
    "print(partial_y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ef74ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(index_to_word)\n",
    "word_vector_dim = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1936e381",
   "metadata": {},
   "source": [
    "## 1-D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3761e2be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 300)         2999100   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 256)         537856    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, None, 64)          114752    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 520       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 3,652,237\n",
      "Trainable params: 3,652,237\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "CNN_model = keras.Sequential(name=\"CNN\")\n",
    "CNN_model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "CNN_model.add(keras.layers.Conv1D(256, 7, activation='relu'))\n",
    "CNN_model.add(keras.layers.MaxPooling1D(5))\n",
    "CNN_model.add(keras.layers.Conv1D(64, 7, activation='relu'))\n",
    "CNN_model.add(keras.layers.GlobalMaxPooling1D())\n",
    "CNN_model.add(keras.layers.Dense(8, activation='relu'))\n",
    "CNN_model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "CNN_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de27ec9",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87dd79b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 300)         2999100   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 8)                 9888      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 3,009,069\n",
      "Trainable params: 3,009,069\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "LSTM_model = keras.Sequential(name=\"LSTM\")\n",
    "LSTM_model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "LSTM_model.add(keras.layers.LSTM(8, dropout=0.7))\n",
    "LSTM_model.add(keras.layers.Dense(8, activation='relu'))\n",
    "LSTM_model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "LSTM_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247a0e99",
   "metadata": {},
   "source": [
    "## GMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c2471f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"GMP\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 300)         2999100   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8)                 2408      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 3,001,517\n",
      "Trainable params: 3,001,517\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "GMP_model = keras.Sequential(name=\"GMP\")\n",
    "GMP_model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "GMP_model.add(keras.layers.GlobalMaxPooling1D())\n",
    "GMP_model.add(keras.layers.Dense(8, activation='relu'))\n",
    "GMP_model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "GMP_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b900383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CNN', 'LSTM', 'GMP']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lst = [CNN_model.name, LSTM_model.name, GMP_model.name]\n",
    "model_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de08cf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Start fitting CNN ...\n",
      "Epoch 1/20\n",
      "247/247 [==============================] - 10s 24ms/step - loss: 0.4041 - accuracy: 0.8086 - val_loss: 0.3275 - val_accuracy: 0.8582\n",
      "Epoch 2/20\n",
      "247/247 [==============================] - 6s 22ms/step - loss: 0.2800 - accuracy: 0.8832 - val_loss: 0.3156 - val_accuracy: 0.8647\n",
      "Epoch 3/20\n",
      "247/247 [==============================] - 6s 22ms/step - loss: 0.1930 - accuracy: 0.9252 - val_loss: 0.3569 - val_accuracy: 0.8582\n",
      "Epoch 4/20\n",
      "247/247 [==============================] - 6s 23ms/step - loss: 0.1089 - accuracy: 0.9614 - val_loss: 0.4599 - val_accuracy: 0.8460\n",
      "Epoch 5/20\n",
      "247/247 [==============================] - 6s 23ms/step - loss: 0.0653 - accuracy: 0.9781 - val_loss: 0.5803 - val_accuracy: 0.8478\n",
      "Epoch 6/20\n",
      "247/247 [==============================] - 6s 23ms/step - loss: 0.0440 - accuracy: 0.9856 - val_loss: 0.7043 - val_accuracy: 0.8488\n",
      "Epoch 7/20\n",
      "247/247 [==============================] - 6s 22ms/step - loss: 0.0342 - accuracy: 0.9888 - val_loss: 0.7625 - val_accuracy: 0.8469\n",
      "Epoch 8/20\n",
      "247/247 [==============================] - 6s 22ms/step - loss: 0.0346 - accuracy: 0.9880 - val_loss: 0.8572 - val_accuracy: 0.8488\n",
      "Epoch 9/20\n",
      "247/247 [==============================] - 5s 22ms/step - loss: 0.0354 - accuracy: 0.9873 - val_loss: 0.8513 - val_accuracy: 0.8445\n",
      "Epoch 10/20\n",
      "247/247 [==============================] - 5s 22ms/step - loss: 0.0279 - accuracy: 0.9899 - val_loss: 0.8703 - val_accuracy: 0.8414\n",
      "Epoch 11/20\n",
      "247/247 [==============================] - 5s 22ms/step - loss: 0.0236 - accuracy: 0.9915 - val_loss: 0.9853 - val_accuracy: 0.8438\n",
      "Epoch 12/20\n",
      "247/247 [==============================] - 5s 22ms/step - loss: 0.0180 - accuracy: 0.9934 - val_loss: 1.0959 - val_accuracy: 0.8490\n",
      "Epoch 13/20\n",
      "247/247 [==============================] - 5s 22ms/step - loss: 0.0183 - accuracy: 0.9932 - val_loss: 1.0739 - val_accuracy: 0.8426\n",
      "Epoch 14/20\n",
      "247/247 [==============================] - 5s 22ms/step - loss: 0.0180 - accuracy: 0.9931 - val_loss: 1.1419 - val_accuracy: 0.8450\n",
      "Epoch 15/20\n",
      "247/247 [==============================] - 5s 22ms/step - loss: 0.0187 - accuracy: 0.9931 - val_loss: 1.1065 - val_accuracy: 0.8436\n",
      "Epoch 16/20\n",
      "247/247 [==============================] - 5s 22ms/step - loss: 0.0152 - accuracy: 0.9941 - val_loss: 1.1277 - val_accuracy: 0.8463\n",
      "Epoch 17/20\n",
      "247/247 [==============================] - 5s 22ms/step - loss: 0.0105 - accuracy: 0.9958 - val_loss: 1.2250 - val_accuracy: 0.8418\n",
      "Epoch 18/20\n",
      "247/247 [==============================] - 5s 22ms/step - loss: 0.0101 - accuracy: 0.9962 - val_loss: 1.2664 - val_accuracy: 0.8421\n",
      "Epoch 19/20\n",
      "247/247 [==============================] - 5s 22ms/step - loss: 0.0142 - accuracy: 0.9946 - val_loss: 1.2512 - val_accuracy: 0.8429\n",
      "Epoch 20/20\n",
      "247/247 [==============================] - 5s 22ms/step - loss: 0.0135 - accuracy: 0.9946 - val_loss: 1.1809 - val_accuracy: 0.8449\n",
      "Start evaluating CNN ...\n",
      "1537/1537 - 3s - loss: 1.2163 - accuracy: 0.8406\n",
      "----------------------------------------\n",
      "Start fitting LSTM ...\n",
      "Epoch 1/20\n",
      "247/247 [==============================] - 4s 10ms/step - loss: 0.5775 - accuracy: 0.6614 - val_loss: 0.3859 - val_accuracy: 0.8299\n",
      "Epoch 2/20\n",
      "247/247 [==============================] - 2s 8ms/step - loss: 0.3576 - accuracy: 0.8473 - val_loss: 0.3406 - val_accuracy: 0.8521\n",
      "Epoch 3/20\n",
      "247/247 [==============================] - 2s 8ms/step - loss: 0.3190 - accuracy: 0.8649 - val_loss: 0.3354 - val_accuracy: 0.8548\n",
      "Epoch 4/20\n",
      "247/247 [==============================] - 2s 8ms/step - loss: 0.2978 - accuracy: 0.8741 - val_loss: 0.3471 - val_accuracy: 0.8537\n",
      "Epoch 5/20\n",
      "247/247 [==============================] - 2s 8ms/step - loss: 0.2816 - accuracy: 0.8809 - val_loss: 0.3416 - val_accuracy: 0.8576\n",
      "Epoch 6/20\n",
      "247/247 [==============================] - 2s 9ms/step - loss: 0.2664 - accuracy: 0.8876 - val_loss: 0.3380 - val_accuracy: 0.8598\n",
      "Epoch 7/20\n",
      "247/247 [==============================] - 2s 9ms/step - loss: 0.2540 - accuracy: 0.8942 - val_loss: 0.3415 - val_accuracy: 0.8614\n",
      "Epoch 8/20\n",
      "247/247 [==============================] - 2s 9ms/step - loss: 0.2435 - accuracy: 0.8983 - val_loss: 0.3577 - val_accuracy: 0.8589\n",
      "Epoch 9/20\n",
      "247/247 [==============================] - 2s 9ms/step - loss: 0.2359 - accuracy: 0.9018 - val_loss: 0.3653 - val_accuracy: 0.8584\n",
      "Epoch 10/20\n",
      "247/247 [==============================] - 2s 9ms/step - loss: 0.2268 - accuracy: 0.9061 - val_loss: 0.3697 - val_accuracy: 0.8582\n",
      "Epoch 11/20\n",
      "247/247 [==============================] - 2s 9ms/step - loss: 0.2183 - accuracy: 0.9092 - val_loss: 0.3785 - val_accuracy: 0.8568\n",
      "Epoch 12/20\n",
      "247/247 [==============================] - 2s 9ms/step - loss: 0.2132 - accuracy: 0.9123 - val_loss: 0.3881 - val_accuracy: 0.8564\n",
      "Epoch 13/20\n",
      "247/247 [==============================] - 2s 8ms/step - loss: 0.2056 - accuracy: 0.9159 - val_loss: 0.3803 - val_accuracy: 0.8568\n",
      "Epoch 14/20\n",
      "247/247 [==============================] - 2s 8ms/step - loss: 0.2006 - accuracy: 0.9182 - val_loss: 0.3882 - val_accuracy: 0.8555\n",
      "Epoch 15/20\n",
      "247/247 [==============================] - 2s 9ms/step - loss: 0.1963 - accuracy: 0.9195 - val_loss: 0.3972 - val_accuracy: 0.8564\n",
      "Epoch 16/20\n",
      "247/247 [==============================] - 2s 8ms/step - loss: 0.1911 - accuracy: 0.9227 - val_loss: 0.4213 - val_accuracy: 0.8545\n",
      "Epoch 17/20\n",
      "247/247 [==============================] - 2s 8ms/step - loss: 0.1867 - accuracy: 0.9246 - val_loss: 0.3972 - val_accuracy: 0.8569\n",
      "Epoch 18/20\n",
      "247/247 [==============================] - 2s 8ms/step - loss: 0.1818 - accuracy: 0.9272 - val_loss: 0.4238 - val_accuracy: 0.8544\n",
      "Epoch 19/20\n",
      "247/247 [==============================] - 2s 9ms/step - loss: 0.1774 - accuracy: 0.9283 - val_loss: 0.4316 - val_accuracy: 0.8545\n",
      "Epoch 20/20\n",
      "247/247 [==============================] - 2s 9ms/step - loss: 0.1741 - accuracy: 0.9303 - val_loss: 0.4255 - val_accuracy: 0.8526\n",
      "Start evaluating LSTM ...\n",
      "1537/1537 - 3s - loss: 0.4307 - accuracy: 0.8487\n",
      "----------------------------------------\n",
      "Start fitting GMP ...\n",
      "Epoch 1/20\n",
      "247/247 [==============================] - 2s 6ms/step - loss: 0.4801 - accuracy: 0.7728 - val_loss: 0.3471 - val_accuracy: 0.8482\n",
      "Epoch 2/20\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.3121 - accuracy: 0.8694 - val_loss: 0.3317 - val_accuracy: 0.8551\n",
      "Epoch 3/20\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.2639 - accuracy: 0.8937 - val_loss: 0.3375 - val_accuracy: 0.8566\n",
      "Epoch 4/20\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.2211 - accuracy: 0.9145 - val_loss: 0.3524 - val_accuracy: 0.8554\n",
      "Epoch 5/20\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.1769 - accuracy: 0.9356 - val_loss: 0.3764 - val_accuracy: 0.8546\n",
      "Epoch 6/20\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.1329 - accuracy: 0.9565 - val_loss: 0.4074 - val_accuracy: 0.8511\n",
      "Epoch 7/20\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.0927 - accuracy: 0.9729 - val_loss: 0.4502 - val_accuracy: 0.8456\n",
      "Epoch 8/20\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.0624 - accuracy: 0.9841 - val_loss: 0.4914 - val_accuracy: 0.8450\n",
      "Epoch 9/20\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.0418 - accuracy: 0.9900 - val_loss: 0.5310 - val_accuracy: 0.8440\n",
      "Epoch 10/20\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.0292 - accuracy: 0.9929 - val_loss: 0.5711 - val_accuracy: 0.8443\n",
      "Epoch 11/20\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.0216 - accuracy: 0.9946 - val_loss: 0.6059 - val_accuracy: 0.8407\n",
      "Epoch 12/20\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.0165 - accuracy: 0.9955 - val_loss: 0.6352 - val_accuracy: 0.8419\n",
      "Epoch 13/20\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.0137 - accuracy: 0.9961 - val_loss: 0.6638 - val_accuracy: 0.8421\n",
      "Epoch 14/20\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.0117 - accuracy: 0.9961 - val_loss: 0.6843 - val_accuracy: 0.8433\n",
      "Epoch 15/20\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.0102 - accuracy: 0.9965 - val_loss: 0.7123 - val_accuracy: 0.8403\n",
      "Epoch 16/20\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.0093 - accuracy: 0.9965 - val_loss: 0.7314 - val_accuracy: 0.8428\n",
      "Epoch 17/20\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.0083 - accuracy: 0.9968 - val_loss: 0.7513 - val_accuracy: 0.8412\n",
      "Epoch 18/20\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.0081 - accuracy: 0.9967 - val_loss: 0.7708 - val_accuracy: 0.8408\n",
      "Epoch 19/20\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.0073 - accuracy: 0.9970 - val_loss: 0.7878 - val_accuracy: 0.8430\n",
      "Epoch 20/20\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.0073 - accuracy: 0.9970 - val_loss: 0.8012 - val_accuracy: 0.8417\n",
      "Start evaluating GMP ...\n",
      "1537/1537 - 2s - loss: 0.8121 - accuracy: 0.8414\n"
     ]
    }
   ],
   "source": [
    "model_result = {}\n",
    "\n",
    "for model_name in model_lst:\n",
    "    \n",
    "    if model_name == \"CNN\":\n",
    "        model = CNN_model\n",
    "    elif model_name == \"LSTM\":\n",
    "        model = LSTM_model\n",
    "    else :\n",
    "        model = GMP_model\n",
    "    \n",
    "    print('-'*40)\n",
    "    print(\"Start fitting {} ...\".format(model_name))\n",
    "    model.compile(optimizer='Adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    epochs=20\n",
    "\n",
    "    history = model.fit(partial_X_train,\n",
    "                        partial_y_train,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=512,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        verbose=1)\n",
    "    \n",
    "    \n",
    "    print(\"Start evaluating {} ...\".format(model_name))\n",
    "    results = model.evaluate(X_test,  y_test, verbose=2)\n",
    "    model_result[model_name] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e47fe64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CNN': [1.2162566184997559, 0.8406127095222473],\n",
       " 'LSTM': [0.43067866563796997, 0.8487499356269836],\n",
       " 'GMP': [0.8120701909065247, 0.841426432132721]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ba9b2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_file_path = './data/ko.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4eb60a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec.load(word2vec_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59a1a0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_517/503750501.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  v1 = word2vec['사랑']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.3740246 , -1.7353463 ,  3.3915305 , -2.569253  , -1.4016607 ,\n",
       "        1.4556127 ,  0.9414557 ,  1.9207907 ,  0.16471806,  0.4838317 ,\n",
       "       -0.8547181 ,  2.0879807 ,  0.86741775,  0.87539405, -0.09962013,\n",
       "        0.22928311, -1.1858722 ,  0.00858838,  1.4999928 , -0.16196461,\n",
       "       -0.35184434, -0.92390764,  1.0849575 ,  0.3025011 ,  2.7021565 ,\n",
       "       -1.0263684 ,  0.32864776, -0.76589465, -2.510981  , -0.66225356,\n",
       "        2.8434615 ,  0.50130975, -1.021874  , -1.4366034 ,  1.1110784 ,\n",
       "        0.5812605 , -0.5830406 , -0.5785423 ,  1.3634988 ,  2.3074338 ,\n",
       "       -1.4314893 ,  0.45745876,  1.1073523 , -3.2135262 , -0.2898375 ,\n",
       "       -1.1622221 ,  1.2369208 , -0.7622987 , -0.37757635,  1.1376442 ,\n",
       "        0.01065568, -0.69105595,  1.5159112 ,  1.1534518 , -1.0119992 ,\n",
       "       -0.5757404 ,  1.1349088 , -1.1289831 ,  0.13004152,  2.0451715 ,\n",
       "       -0.23940353,  1.3604902 ,  0.72700524,  0.32545742,  1.0612459 ,\n",
       "        0.42252553,  1.1442151 ,  2.8774905 ,  2.4377263 , -1.340305  ,\n",
       "        0.12629706, -0.07772489, -0.59053177, -0.19007324,  0.1396541 ,\n",
       "       -1.8655105 ,  0.9401054 ,  0.5150856 ,  0.7795373 , -0.86505556,\n",
       "        0.11842118, -1.8303713 ,  1.337177  , -1.0102932 , -0.37180334,\n",
       "        0.00893255, -0.49141577, -1.05802   , -2.5987291 ,  0.9731856 ,\n",
       "        0.34080654, -2.5973568 ,  1.0046519 , -1.3914212 , -0.6504351 ,\n",
       "       -0.9010805 , -1.1341541 ,  0.75565654,  1.2941337 ,  0.0880572 ,\n",
       "       -1.0341461 , -0.1750075 , -0.01880708, -1.0835075 , -2.0333962 ,\n",
       "        1.1372623 ,  1.0626172 , -1.8369784 , -2.2662086 , -3.382057  ,\n",
       "        1.6751666 , -0.2988223 , -0.25563756, -1.5594274 ,  0.6313433 ,\n",
       "       -1.2667153 , -1.6857744 , -1.0949599 ,  0.7742313 , -0.6095523 ,\n",
       "        3.19503   ,  0.13200459,  1.7937473 , -2.8782516 ,  1.3821276 ,\n",
       "        2.2895143 ,  0.0741943 , -0.41046414,  1.438796  ,  0.19373988,\n",
       "        1.4294034 ,  1.5025262 ,  1.4849502 ,  1.5754777 ,  2.7793512 ,\n",
       "       -0.6885003 , -0.30154693, -1.708323  ,  1.1030879 , -2.2597387 ,\n",
       "        1.1909146 ,  2.4399316 ,  0.3990314 ,  0.904154  ,  0.5454401 ,\n",
       "       -1.3235748 , -0.64812386,  0.22390233,  0.9657619 , -0.47360668,\n",
       "       -0.10278235, -1.0679734 , -0.91414386,  0.92069   ,  0.3549338 ,\n",
       "        0.32858834,  0.84870636,  3.596926  , -1.6651102 ,  0.23658653,\n",
       "        1.0515738 ,  0.40531915, -0.773514  , -0.93460965, -0.3946274 ,\n",
       "       -1.5657727 ,  1.183652  ,  2.5277    ,  0.57700926,  1.7051374 ,\n",
       "       -1.8249958 , -2.0328498 ,  0.6617798 ,  0.85747904,  0.31782728,\n",
       "       -1.1660796 ,  0.32923874,  2.2055087 , -0.12782003,  2.0455444 ,\n",
       "       -0.1724252 ,  0.46001154,  1.559042  , -1.6152996 , -0.84242785,\n",
       "        0.7553168 ,  0.39734274,  0.07714175,  0.05610155,  0.32837135,\n",
       "        1.0220716 ,  1.3816743 ,  0.8049544 ,  0.28728685, -0.97610044,\n",
       "        0.8861181 , -0.01250968, -1.4845604 , -1.5236791 , -1.5451258 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = word2vec['사랑']\n",
    "v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59b64c14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_517/1073688978.py:8: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if index_to_word[i] in word2vec:\n",
      "/tmp/ipykernel_517/1073688978.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  embedding_matrix[i] = word2vec[index_to_word[i]]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000\n",
    "word_vector_dim = 200\n",
    "\n",
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "\n",
    "# embedding_matrix에 Word2Vec 워드벡터를 단어 하나씩마다 차례차례 카피한다.\n",
    "for i in range(4,vocab_size):\n",
    "    if index_to_word[i] in word2vec:\n",
    "        embedding_matrix[i] = word2vec[index_to_word[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06189008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 41, 200)           2000000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               467968    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 2056      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,470,033\n",
      "Trainable params: 2,470,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.initializers import Constant\n",
    "\n",
    "vocab_size = 10000\n",
    "word_vector_dim = 200\n",
    "\n",
    "# 모델 구성\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, \n",
    "                                 word_vector_dim, \n",
    "                                 embeddings_initializer=Constant(embedding_matrix), \n",
    "                                 input_length=maxlen, \n",
    "                                 trainable=True))   # trainable을 True로 주면 Fine-tuning\n",
    "\n",
    "model.add(keras.layers.LSTM(256))\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7170c6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "986/986 [==============================] - 12s 11ms/step - loss: 0.5153 - accuracy: 0.7297 - val_loss: 0.3659 - val_accuracy: 0.8374\n",
      "Epoch 2/20\n",
      "986/986 [==============================] - 11s 11ms/step - loss: 0.3356 - accuracy: 0.8544 - val_loss: 0.3223 - val_accuracy: 0.8605\n",
      "Epoch 3/20\n",
      "986/986 [==============================] - 11s 11ms/step - loss: 0.2881 - accuracy: 0.8782 - val_loss: 0.3186 - val_accuracy: 0.8612\n",
      "Epoch 4/20\n",
      "986/986 [==============================] - 11s 11ms/step - loss: 0.2559 - accuracy: 0.8941 - val_loss: 0.3111 - val_accuracy: 0.8665\n",
      "Epoch 5/20\n",
      "986/986 [==============================] - 10s 11ms/step - loss: 0.2267 - accuracy: 0.9074 - val_loss: 0.3136 - val_accuracy: 0.8680\n",
      "Epoch 6/20\n",
      "986/986 [==============================] - 10s 11ms/step - loss: 0.1976 - accuracy: 0.9219 - val_loss: 0.3497 - val_accuracy: 0.8679\n",
      "Epoch 7/20\n",
      "986/986 [==============================] - 10s 11ms/step - loss: 0.1676 - accuracy: 0.9354 - val_loss: 0.3635 - val_accuracy: 0.8646\n",
      "Epoch 8/20\n",
      "986/986 [==============================] - 11s 11ms/step - loss: 0.1412 - accuracy: 0.9469 - val_loss: 0.4184 - val_accuracy: 0.8638\n",
      "Epoch 9/20\n",
      "986/986 [==============================] - 10s 11ms/step - loss: 0.1176 - accuracy: 0.9571 - val_loss: 0.4452 - val_accuracy: 0.8637\n",
      "Epoch 10/20\n",
      "986/986 [==============================] - 10s 11ms/step - loss: 0.0975 - accuracy: 0.9658 - val_loss: 0.5042 - val_accuracy: 0.8482\n",
      "Epoch 11/20\n",
      "986/986 [==============================] - 11s 11ms/step - loss: 0.0830 - accuracy: 0.9704 - val_loss: 0.4930 - val_accuracy: 0.8551\n",
      "Epoch 12/20\n",
      "986/986 [==============================] - 11s 11ms/step - loss: 0.0712 - accuracy: 0.9756 - val_loss: 0.5026 - val_accuracy: 0.8577\n",
      "Epoch 13/20\n",
      "986/986 [==============================] - 10s 11ms/step - loss: 0.0637 - accuracy: 0.9783 - val_loss: 0.5388 - val_accuracy: 0.8579\n",
      "Epoch 14/20\n",
      "986/986 [==============================] - 11s 11ms/step - loss: 0.0554 - accuracy: 0.9810 - val_loss: 0.5823 - val_accuracy: 0.8576\n",
      "Epoch 15/20\n",
      "986/986 [==============================] - 10s 11ms/step - loss: 0.0522 - accuracy: 0.9820 - val_loss: 0.6078 - val_accuracy: 0.8554\n",
      "Epoch 16/20\n",
      "986/986 [==============================] - 11s 11ms/step - loss: 0.0478 - accuracy: 0.9836 - val_loss: 0.6309 - val_accuracy: 0.8602\n",
      "Epoch 17/20\n",
      "986/986 [==============================] - 11s 11ms/step - loss: 0.0404 - accuracy: 0.9861 - val_loss: 0.6713 - val_accuracy: 0.8568\n",
      "Epoch 18/20\n",
      "986/986 [==============================] - 11s 11ms/step - loss: 0.0378 - accuracy: 0.9867 - val_loss: 0.6624 - val_accuracy: 0.8570\n",
      "Epoch 19/20\n",
      "986/986 [==============================] - 11s 11ms/step - loss: 0.0351 - accuracy: 0.9874 - val_loss: 0.6921 - val_accuracy: 0.8561\n",
      "Epoch 20/20\n",
      "986/986 [==============================] - 11s 11ms/step - loss: 0.0322 - accuracy: 0.9888 - val_loss: 0.6876 - val_accuracy: 0.8548\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20\n",
    "\n",
    "history = model.fit(partial_X_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=128,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f647c9b",
   "metadata": {},
   "source": [
    "- 0.848 -> 0.854로 성능 향상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7416ed8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
